{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using neural classification\n",
    "As has been proven by [Wang (2017)](https://arxiv.org/abs/1705.00648), neural classifiers carry better results than non-neural classifiers when detecting fake news. However, it is unknown how well neural networks classify fake news when using previously mentioned text embeddings. \n",
    "In this notebook, the second research question will be answered: *how well do neural network architecture classify fake news compared to non-neural classification algorithms?*\n",
    "\n",
    "<hr>\n",
    "\n",
    "## On the usage of neural networks\n",
    "Literature on CNNs and Bi-LSTMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Reshape, Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "# Set offline mode for plotly\n",
    "init_notebook_mode(connected = True)\n",
    "\n",
    "# The DataLoader class gives access to pretrained vectors from the Liar dataset\n",
    "from data_loader import DataLoader\n",
    "data = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "general = data.get_dfs()\n",
    "\n",
    "# Recode labels from 6 to 3\n",
    "def recode(label):\n",
    "    if label == 'false' or label == 'pants-fire' or label == 'barely-true':\n",
    "        return 0\n",
    "    elif label == 'true' or label == 'mostly-true':\n",
    "        return 2\n",
    "    elif label == 'half-true':\n",
    "        return 1\n",
    "\n",
    "for dataset in general.keys():\n",
    "    general[dataset]['label'] = general[dataset]['label'].apply(lambda label: recode(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Bidirectional LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = data.get_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max-pooled BERT embeddings from RQ1\n",
    "def max_pool(statement):\n",
    "    if len(statement) > 1:\n",
    "        return [row.max() for row in np.transpose([[token_row.max() for token_row in np.transpose(np.array(sentence))] for sentence in statement])]\n",
    "    else:\n",
    "        return [token_row.max() for token_row in np.transpose(statement[0])]\n",
    "\n",
    "max_pooled_bert = {\n",
    "    dataset: pd.DataFrame(list(bert[dataset].statement.apply(lambda statement: max_pool(statement)).values))\n",
    "    for dataset in bert.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_score(X_train, X_test, X_validation, y_train = general['train']['label'], y_test = general['test']['label'], y_validation = general['validation']['label'], reshape = True):\n",
    "    # Rearrange data types    \n",
    "    params = locals().copy()    \n",
    "    inputs = {\n",
    "        dataset: np.array(params[dataset])\n",
    "        for dataset in params.keys()\n",
    "    }\n",
    "    \n",
    "    for dataset in inputs.keys():\n",
    "        if dataset[0:1] == 'X' and reshape:\n",
    "            # Reshape datasets from 2D to 3D\n",
    "            inputs[dataset] = np.reshape(inputs[dataset], (inputs[dataset].shape[0], inputs[dataset].shape[1], 1))\n",
    "        elif dataset[0:1] == 'y':\n",
    "            inputs[dataset] = np_utils.to_categorical(np.array(inputs[dataset]), 3)\n",
    "    \n",
    "    # Set model parameters\n",
    "    epochs = 5\n",
    "    batch_size = 32\n",
    "    input_shape = X_train.shape\n",
    "\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, input_shape = input_shape)))\n",
    "    model.add(Dropout(0.8))\n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "    model.compile('sgd', 'categorical_crossentropy', metrics = ['accuracy']) \n",
    "    \n",
    "    # Fit the training set over the model and correct on the validation set\n",
    "    model.fit(inputs['X_train'], inputs['y_train'],\n",
    "            batch_size = batch_size,\n",
    "            epochs = epochs,\n",
    "            validation_data = (inputs['X_validation'], inputs['y_validation']))\n",
    "    \n",
    "    # Get score over the test set\n",
    "    score, acc = model.evaluate(inputs['X_test'], inputs['y_test'])\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "2019-05-21 12:40:37,047 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "2019-05-21 12:40:37,448 From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-05-21 12:40:37,537 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 829s 81ms/step - loss: 1.0743 - acc: 0.4087 - val_loss: 1.0423 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 844s 82ms/step - loss: 1.0632 - acc: 0.4300 - val_loss: 1.0392 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 838s 82ms/step - loss: 1.0590 - acc: 0.4354 - val_loss: 1.0408 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 840s 82ms/step - loss: 1.0592 - acc: 0.4354 - val_loss: 1.0432 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 809s 79ms/step - loss: 1.0591 - acc: 0.4337 - val_loss: 1.0395 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 16s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.43715415052745654"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bilstm_score(max_pooled_bert['train'], max_pooled_bert['test'], max_pooled_bert['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, the condensed datasets from RQ1 do not perform well when using a neural classifier. The next step is trying out a padding approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.1184 - acc: 0.4145 - val_loss: 1.0170 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0597 - acc: 0.4532 - val_loss: 1.0196 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0432 - acc: 0.4707 - val_loss: 1.0158 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0362 - acc: 0.4775 - val_loss: 1.0134 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0279 - acc: 0.4821 - val_loss: 1.0089 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.1094 - acc: 0.4181 - val_loss: 1.0184 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0558 - acc: 0.4556 - val_loss: 1.0125 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0428 - acc: 0.4693 - val_loss: 1.0125 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0332 - acc: 0.4863 - val_loss: 1.0140 - val_acc: 0.5008\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0303 - acc: 0.4824 - val_loss: 1.0051 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 1s 594us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.1187 - acc: 0.4151 - val_loss: 1.0248 - val_acc: 0.5109\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0541 - acc: 0.4529 - val_loss: 1.0166 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0422 - acc: 0.4718 - val_loss: 1.0122 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0347 - acc: 0.4808 - val_loss: 1.0118 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0292 - acc: 0.4889 - val_loss: 1.0056 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 1s 969us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.1133 - acc: 0.4260 - val_loss: 1.0178 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0581 - acc: 0.4538 - val_loss: 1.0119 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0404 - acc: 0.4719 - val_loss: 1.0130 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0340 - acc: 0.4828 - val_loss: 1.0065 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0276 - acc: 0.4863 - val_loss: 1.0027 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 1s 799us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.1089 - acc: 0.4181 - val_loss: 1.0162 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0551 - acc: 0.4594 - val_loss: 1.0202 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0381 - acc: 0.4771 - val_loss: 1.0091 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0320 - acc: 0.4829 - val_loss: 1.0067 - val_acc: 0.5000\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0247 - acc: 0.4908 - val_loss: 1.0029 - val_acc: 0.5016\n",
      "1265/1265 [==============================] - 1s 846us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.1186 - acc: 0.4209 - val_loss: 1.0138 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0519 - acc: 0.4614 - val_loss: 1.0098 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0398 - acc: 0.4782 - val_loss: 1.0044 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0280 - acc: 0.4896 - val_loss: 1.0080 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0241 - acc: 0.4943 - val_loss: 1.0050 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 1s 929us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.1218 - acc: 0.4208 - val_loss: 1.0134 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0544 - acc: 0.4600 - val_loss: 1.0069 - val_acc: 0.5202\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0398 - acc: 0.4773 - val_loss: 1.0043 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0296 - acc: 0.4862 - val_loss: 1.0027 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0265 - acc: 0.4872 - val_loss: 0.9985 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 1.1138 - acc: 0.4279 - val_loss: 1.0142 - val_acc: 0.5109\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0477 - acc: 0.4682 - val_loss: 1.0073 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0368 - acc: 0.4805 - val_loss: 1.0111 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0281 - acc: 0.4867 - val_loss: 1.0011 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0280 - acc: 0.4947 - val_loss: 1.0041 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 80s 8ms/step - loss: 1.1205 - acc: 0.4236 - val_loss: 1.0263 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0541 - acc: 0.4649 - val_loss: 1.0081 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0391 - acc: 0.4769 - val_loss: 1.0083 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0296 - acc: 0.4863 - val_loss: 1.0060 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0239 - acc: 0.4927 - val_loss: 1.0026 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 86s 8ms/step - loss: 1.1171 - acc: 0.4152 - val_loss: 1.0107 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0496 - acc: 0.4637 - val_loss: 1.0054 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0413 - acc: 0.4780 - val_loss: 1.0082 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0297 - acc: 0.4904 - val_loss: 1.0004 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0234 - acc: 0.4949 - val_loss: 0.9993 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 92s 9ms/step - loss: 1.1165 - acc: 0.4206 - val_loss: 1.0214 - val_acc: 0.5148\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0553 - acc: 0.4656 - val_loss: 1.0178 - val_acc: 0.5109\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0350 - acc: 0.4852 - val_loss: 1.0104 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0312 - acc: 0.4890 - val_loss: 1.0051 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0229 - acc: 0.4910 - val_loss: 1.0055 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 91s 9ms/step - loss: 1.1168 - acc: 0.4210 - val_loss: 1.0127 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0508 - acc: 0.4668 - val_loss: 1.0166 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0364 - acc: 0.4793 - val_loss: 1.0032 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0281 - acc: 0.4893 - val_loss: 1.0000 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0228 - acc: 0.4973 - val_loss: 1.0013 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 98s 10ms/step - loss: 1.1218 - acc: 0.4131 - val_loss: 1.0302 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0546 - acc: 0.4616 - val_loss: 1.0226 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 36s 4ms/step - loss: 1.0387 - acc: 0.4726 - val_loss: 1.0115 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0314 - acc: 0.4861 - val_loss: 1.0049 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0245 - acc: 0.4940 - val_loss: 1.0055 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 103s 10ms/step - loss: 1.1113 - acc: 0.4235 - val_loss: 1.0184 - val_acc: 0.5234\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0534 - acc: 0.4643 - val_loss: 1.0104 - val_acc: 0.5226\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 36s 3ms/step - loss: 1.0409 - acc: 0.4802 - val_loss: 1.0079 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 36s 4ms/step - loss: 1.0321 - acc: 0.4882 - val_loss: 1.0040 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0239 - acc: 0.5000 - val_loss: 1.0034 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 101s 10ms/step - loss: 1.1158 - acc: 0.4191 - val_loss: 1.0152 - val_acc: 0.5226\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0550 - acc: 0.4584 - val_loss: 1.0119 - val_acc: 0.5249\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0378 - acc: 0.4787 - val_loss: 1.0057 - val_acc: 0.5296\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0310 - acc: 0.4812 - val_loss: 1.0030 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0227 - acc: 0.4948 - val_loss: 0.9970 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 109s 11ms/step - loss: 1.1228 - acc: 0.4128 - val_loss: 1.0199 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0554 - acc: 0.4577 - val_loss: 1.0189 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0387 - acc: 0.4784 - val_loss: 1.0061 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0341 - acc: 0.4836 - val_loss: 1.0074 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0242 - acc: 0.4958 - val_loss: 1.0025 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 111s 11ms/step - loss: 1.1078 - acc: 0.4219 - val_loss: 1.0287 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0534 - acc: 0.4610 - val_loss: 1.0140 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0422 - acc: 0.4721 - val_loss: 1.0134 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0265 - acc: 0.4914 - val_loss: 1.0109 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0268 - acc: 0.4949 - val_loss: 1.0093 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.1146 - acc: 0.4152 - val_loss: 1.0207 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0577 - acc: 0.4632 - val_loss: 1.0113 - val_acc: 0.5249\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0401 - acc: 0.4759 - val_loss: 1.0133 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0335 - acc: 0.4819 - val_loss: 1.0071 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0266 - acc: 0.4901 - val_loss: 1.0044 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 6s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 110s 11ms/step - loss: 1.1089 - acc: 0.4237 - val_loss: 1.0265 - val_acc: 0.5140\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0583 - acc: 0.4500 - val_loss: 1.0147 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0430 - acc: 0.4778 - val_loss: 1.0108 - val_acc: 0.5241\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0318 - acc: 0.4808 - val_loss: 1.0067 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0303 - acc: 0.4843 - val_loss: 1.0078 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.1008 - acc: 0.4262 - val_loss: 1.0131 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0570 - acc: 0.4583 - val_loss: 1.0094 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0434 - acc: 0.4738 - val_loss: 1.0143 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0325 - acc: 0.4833 - val_loss: 1.0063 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0260 - acc: 0.4900 - val_loss: 1.0022 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.1036 - acc: 0.4241 - val_loss: 1.0248 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0551 - acc: 0.4624 - val_loss: 1.0112 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0423 - acc: 0.4752 - val_loss: 1.0161 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0296 - acc: 0.4879 - val_loss: 1.0038 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0270 - acc: 0.4869 - val_loss: 1.0063 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.1012 - acc: 0.4247 - val_loss: 1.0202 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0566 - acc: 0.4576 - val_loss: 1.0107 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0400 - acc: 0.4775 - val_loss: 1.0086 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0354 - acc: 0.4865 - val_loss: 1.0027 - val_acc: 0.5265\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0266 - acc: 0.4919 - val_loss: 1.0065 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0999 - acc: 0.4246 - val_loss: 1.0188 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0588 - acc: 0.4596 - val_loss: 1.0145 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0415 - acc: 0.4750 - val_loss: 1.0089 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0319 - acc: 0.4848 - val_loss: 1.0036 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0260 - acc: 0.4872 - val_loss: 1.0034 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 133s 13ms/step - loss: 1.1086 - acc: 0.4064 - val_loss: 1.0186 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0563 - acc: 0.4585 - val_loss: 1.0152 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0411 - acc: 0.4723 - val_loss: 1.0108 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0383 - acc: 0.4797 - val_loss: 1.0096 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0304 - acc: 0.4826 - val_loss: 1.0036 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0960 - acc: 0.4262 - val_loss: 1.0261 - val_acc: 0.4774\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0572 - acc: 0.4566 - val_loss: 1.0126 - val_acc: 0.5202\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0404 - acc: 0.4806 - val_loss: 1.0154 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0341 - acc: 0.4777 - val_loss: 1.0074 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0242 - acc: 0.4919 - val_loss: 1.0010 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 1.0877 - acc: 0.4269 - val_loss: 1.0218 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0545 - acc: 0.4550 - val_loss: 1.0150 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0455 - acc: 0.4722 - val_loss: 1.0101 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0369 - acc: 0.4793 - val_loss: 1.0025 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0328 - acc: 0.4806 - val_loss: 1.0035 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 1.0966 - acc: 0.4192 - val_loss: 1.0159 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0535 - acc: 0.4648 - val_loss: 1.0140 - val_acc: 0.5179\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0407 - acc: 0.4737 - val_loss: 1.0057 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0344 - acc: 0.4890 - val_loss: 1.0030 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0281 - acc: 0.4929 - val_loss: 1.0092 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0952 - acc: 0.4262 - val_loss: 1.0169 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.0548 - acc: 0.4621 - val_loss: 1.0139 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0452 - acc: 0.4678 - val_loss: 1.0068 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0353 - acc: 0.4783 - val_loss: 1.0042 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0292 - acc: 0.4875 - val_loss: 0.9996 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 138s 14ms/step - loss: 1.0938 - acc: 0.4192 - val_loss: 1.0201 - val_acc: 0.5156\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 1.0523 - acc: 0.4641 - val_loss: 1.0134 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0419 - acc: 0.4745 - val_loss: 1.0075 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0364 - acc: 0.4815 - val_loss: 1.0043 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0280 - acc: 0.4910 - val_loss: 1.0054 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 1.1040 - acc: 0.4251 - val_loss: 1.0204 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 77s 8ms/step - loss: 1.0522 - acc: 0.4651 - val_loss: 1.0147 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0452 - acc: 0.4717 - val_loss: 1.0112 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0349 - acc: 0.4837 - val_loss: 1.0142 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0292 - acc: 0.4872 - val_loss: 0.9990 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 147s 14ms/step - loss: 1.0870 - acc: 0.4226 - val_loss: 1.0187 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 79s 8ms/step - loss: 1.0529 - acc: 0.4603 - val_loss: 1.0116 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.0456 - acc: 0.4740 - val_loss: 1.0122 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0368 - acc: 0.4828 - val_loss: 1.0076 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.0324 - acc: 0.4843 - val_loss: 1.0060 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "CPU times: user 8h 57s, sys: 2h 14min 29s, total: 10h 15min 26s\n",
      "Wall time: 2h 44min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Store accuracies\n",
    "accuracies = {\n",
    "    padding_len: 0.0 for padding_len in list(range(5,36))\n",
    "}\n",
    "\n",
    "concatenated_bert = {\n",
    "    dataset: [np.concatenate(np.array(statement)) for statement in bert[dataset].statement]\n",
    "    for dataset in bert.keys()\n",
    "}\n",
    "\n",
    "for max_len in accuracies.keys():\n",
    "    padded_bert = {\n",
    "        dataset: sequence.pad_sequences(concatenated_bert[dataset], maxlen = max_len, dtype = float)\n",
    "        for dataset in concatenated_bert.keys()\n",
    "    }\n",
    "    \n",
    "    accuracies[max_len] = get_bilstm_score(padded_bert['train'], padded_bert['test'], padded_bert['validation'], reshape = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{5: 0.513043478637816,\n",
       "  6: 0.5122529648038239,\n",
       "  7: 0.5075098817998712,\n",
       "  8: 0.5083003953747127,\n",
       "  9: 0.49407114631573673,\n",
       "  10: 0.5098814233018476,\n",
       "  11: 0.507509881752753,\n",
       "  12: 0.5035573126299108,\n",
       "  13: 0.4996047434599503,\n",
       "  14: 0.5027667987488004,\n",
       "  15: 0.507509881752753,\n",
       "  16: 0.5090909091144682,\n",
       "  17: 0.5067193679187609,\n",
       "  18: 0.5114624509227135,\n",
       "  19: 0.5098814233018476,\n",
       "  20: 0.5162055336203971,\n",
       "  21: 0.5193675893097527,\n",
       "  22: 0.5193675893097527,\n",
       "  23: 0.5233201584797131,\n",
       "  24: 0.5217391304583417,\n",
       "  25: 0.5106719371358397,\n",
       "  26: 0.5106719367824524,\n",
       "  27: 0.5177865616417685,\n",
       "  28: 0.5075098817998712,\n",
       "  29: 0.513043478637816,\n",
       "  30: 0.5043478264639029,\n",
       "  31: 0.5162055336203971,\n",
       "  32: 0.5075098817998712,\n",
       "  33: 0.5114624509698318,\n",
       "  34: 0.513833992471808,\n",
       "  35: 0.5075098817998712},\n",
       " {5: 0.5098814233018476,\n",
       "  6: 0.5098814229484603,\n",
       "  7: 0.49723320160458684,\n",
       "  8: 0.5090909091144682,\n",
       "  9: 0.5051383399445077,\n",
       "  10: 0.505928854131887,\n",
       "  11: 0.498814229272571,\n",
       "  12: 0.5114624509698318,\n",
       "  13: 0.4940711466220057,\n",
       "  14: 0.49169960512002936,\n",
       "  15: 0.4996047431065631,\n",
       "  16: 0.5027667984425315,\n",
       "  17: 0.4956521739366026,\n",
       "  18: 0.5067193679658791,\n",
       "  19: 0.5067193676124919,\n",
       "  20: 0.5043478264167846,\n",
       "  21: 0.5146245063058001,\n",
       "  22: 0.5256916999816894,\n",
       "  23: 0.5201581031437448,\n",
       "  24: 0.515415019786405,\n",
       "  25: 0.5169960478077764,\n",
       "  26: 0.521739130811729,\n",
       "  27: 0.5185770751223734,\n",
       "  28: 0.5154150201397922,\n",
       "  29: 0.5209486169777369,\n",
       "  30: 0.5027667987959187,\n",
       "  31: 0.5011857711279345,\n",
       "  32: 0.5154150201397922,\n",
       "  33: 0.5083003956338633,\n",
       "  34: 0.5075098817998712,\n",
       "  35: 0.5185770754286423},\n",
       " {5: 0.5035573122765236,\n",
       "  6: 0.5027667984425315,\n",
       "  7: 0.5090909094207372,\n",
       "  8: 0.49090909128603727,\n",
       "  9: 0.5098814233018476,\n",
       "  10: 0.505928854131887,\n",
       "  11: 0.5075098817998712,\n",
       "  12: 0.5035573123236419,\n",
       "  13: 0.5003952572468241,\n",
       "  14: 0.488537549784061,\n",
       "  15: 0.5075098817998712,\n",
       "  16: 0.5035573122765236,\n",
       "  17: 0.5003952572468241,\n",
       "  18: 0.5122529648038239,\n",
       "  19: 0.5051383402507766,\n",
       "  20: 0.5177865616417685,\n",
       "  21: 0.5106719371358397,\n",
       "  22: 0.5019762849619266,\n",
       "  23: 0.5106719368295707,\n",
       "  24: 0.5083003956338633,\n",
       "  25: 0.5146245063058001,\n",
       "  26: 0.5146245063058001,\n",
       "  27: 0.5177865616417685,\n",
       "  28: 0.5067193676596102,\n",
       "  29: 0.5098814233018476,\n",
       "  30: 0.5146245063058001,\n",
       "  31: 0.5138339921184208,\n",
       "  32: 0.5043478264639029,\n",
       "  33: 0.507509881446484,\n",
       "  34: 0.5154150201397922,\n",
       "  35: 0.5067193679658791},\n",
       " {5: 0.49249011895402145,\n",
       "  6: 0.5098814229484603,\n",
       "  7: 0.5019762849148083,\n",
       "  8: 0.505928854131887,\n",
       "  9: 0.5059288537784998,\n",
       "  10: 0.4996047434599503,\n",
       "  11: 0.5035573126299108,\n",
       "  12: 0.4996047434599503,\n",
       "  13: 0.5003952572468241,\n",
       "  14: 0.4893280636180531,\n",
       "  15: 0.5035573126299108,\n",
       "  16: 0.49802371543857893,\n",
       "  17: 0.513043478637816,\n",
       "  18: 0.5019762846085394,\n",
       "  19: 0.5106719371358397,\n",
       "  20: 0.5114624509227135,\n",
       "  21: 0.5122529648038239,\n",
       "  22: 0.515415019786405,\n",
       "  23: 0.5154150201397922,\n",
       "  24: 0.522529644645721,\n",
       "  25: 0.505138340297895,\n",
       "  26: 0.5169960478077764,\n",
       "  27: 0.5201581031437448,\n",
       "  28: 0.5106719371358397,\n",
       "  29: 0.4996047434599503,\n",
       "  30: 0.5169960478077764,\n",
       "  31: 0.5067193676124919,\n",
       "  32: 0.5146245063058001,\n",
       "  33: 0.5043478264639029,\n",
       "  34: 0.5043478261105157,\n",
       "  35: 0.5106719367824524},\n",
       " {5: 0.505138340297895,\n",
       "  6: 0.505928854131887,\n",
       "  7: 0.4948616604559978,\n",
       "  8: 0.5051383399445077,\n",
       "  9: 0.498814229272571,\n",
       "  10: 0.5122529647567056,\n",
       "  11: 0.49723320195797405,\n",
       "  12: 0.5146245063058001,\n",
       "  13: 0.505928854131887,\n",
       "  14: 0.4940711466220057,\n",
       "  15: 0.5106719368295707,\n",
       "  16: 0.5106719367824524,\n",
       "  17: 0.5027667987959187,\n",
       "  18: 0.5059288537784998,\n",
       "  19: 0.5083003956338633,\n",
       "  20: 0.5098814233018476,\n",
       "  21: 0.5138339921184208,\n",
       "  22: 0.5027667987959187,\n",
       "  23: 0.5146245059995312,\n",
       "  24: 0.5059288540847687,\n",
       "  25: 0.5130434783315471,\n",
       "  26: 0.5169960478077764,\n",
       "  27: 0.5114624509698318,\n",
       "  28: 0.505928854131887,\n",
       "  29: 0.505928854131887,\n",
       "  30: 0.5098814233018476,\n",
       "  31: 0.5114624509698318,\n",
       "  32: 0.5059288537784998,\n",
       "  33: 0.5130434783315471,\n",
       "  34: 0.49723320195797405,\n",
       "  35: 0.5114624509698318}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Round 0",
         "type": "scatter",
         "uid": "36b39775-a328-4c54-a3ca-4455c6dc01d6",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.513043478637816,
          0.5122529648038239,
          0.5075098817998712,
          0.5083003953747127,
          0.49407114631573673,
          0.5098814233018476,
          0.507509881752753,
          0.5035573126299108,
          0.4996047434599503,
          0.5027667987488004,
          0.507509881752753,
          0.5090909091144682,
          0.5067193679187609,
          0.5114624509227135,
          0.5098814233018476,
          0.5162055336203971,
          0.5193675893097527,
          0.5193675893097527,
          0.5233201584797131,
          0.5217391304583417,
          0.5106719371358397,
          0.5106719367824524,
          0.5177865616417685,
          0.5075098817998712,
          0.513043478637816,
          0.5043478264639029,
          0.5162055336203971,
          0.5075098817998712,
          0.5114624509698318,
          0.513833992471808,
          0.5075098817998712
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 1",
         "type": "scatter",
         "uid": "05ff3bde-811d-43d5-8421-2f3acd734853",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5098814233018476,
          0.5098814229484603,
          0.49723320160458684,
          0.5090909091144682,
          0.5051383399445077,
          0.505928854131887,
          0.498814229272571,
          0.5114624509698318,
          0.4940711466220057,
          0.49169960512002936,
          0.4996047431065631,
          0.5027667984425315,
          0.4956521739366026,
          0.5067193679658791,
          0.5067193676124919,
          0.5043478264167846,
          0.5146245063058001,
          0.5256916999816894,
          0.5201581031437448,
          0.515415019786405,
          0.5169960478077764,
          0.521739130811729,
          0.5185770751223734,
          0.5154150201397922,
          0.5209486169777369,
          0.5027667987959187,
          0.5011857711279345,
          0.5154150201397922,
          0.5083003956338633,
          0.5075098817998712,
          0.5185770754286423
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 2",
         "type": "scatter",
         "uid": "332e414a-c977-4eeb-894a-69644b06f03d",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5035573122765236,
          0.5027667984425315,
          0.5090909094207372,
          0.49090909128603727,
          0.5098814233018476,
          0.505928854131887,
          0.5075098817998712,
          0.5035573123236419,
          0.5003952572468241,
          0.488537549784061,
          0.5075098817998712,
          0.5035573122765236,
          0.5003952572468241,
          0.5122529648038239,
          0.5051383402507766,
          0.5177865616417685,
          0.5106719371358397,
          0.5019762849619266,
          0.5106719368295707,
          0.5083003956338633,
          0.5146245063058001,
          0.5146245063058001,
          0.5177865616417685,
          0.5067193676596102,
          0.5098814233018476,
          0.5146245063058001,
          0.5138339921184208,
          0.5043478264639029,
          0.507509881446484,
          0.5154150201397922,
          0.5067193679658791
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 3",
         "type": "scatter",
         "uid": "0b216ed9-0130-4d9d-837c-77b49dbb95d4",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.49249011895402145,
          0.5098814229484603,
          0.5019762849148083,
          0.505928854131887,
          0.5059288537784998,
          0.4996047434599503,
          0.5035573126299108,
          0.4996047434599503,
          0.5003952572468241,
          0.4893280636180531,
          0.5035573126299108,
          0.49802371543857893,
          0.513043478637816,
          0.5019762846085394,
          0.5106719371358397,
          0.5114624509227135,
          0.5122529648038239,
          0.515415019786405,
          0.5154150201397922,
          0.522529644645721,
          0.505138340297895,
          0.5169960478077764,
          0.5201581031437448,
          0.5106719371358397,
          0.4996047434599503,
          0.5169960478077764,
          0.5067193676124919,
          0.5146245063058001,
          0.5043478264639029,
          0.5043478261105157,
          0.5106719367824524
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 4",
         "type": "scatter",
         "uid": "2f9230fa-3bf5-47ea-b643-5e2650adb502",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.505138340297895,
          0.505928854131887,
          0.4948616604559978,
          0.5051383399445077,
          0.498814229272571,
          0.5122529647567056,
          0.49723320195797405,
          0.5146245063058001,
          0.505928854131887,
          0.4940711466220057,
          0.5106719368295707,
          0.5106719367824524,
          0.5027667987959187,
          0.5059288537784998,
          0.5083003956338633,
          0.5098814233018476,
          0.5138339921184208,
          0.5027667987959187,
          0.5146245059995312,
          0.5059288540847687,
          0.5130434783315471,
          0.5169960478077764,
          0.5114624509698318,
          0.505928854131887,
          0.505928854131887,
          0.5098814233018476,
          0.5114624509698318,
          0.5059288537784998,
          0.5130434783315471,
          0.49723320195797405,
          0.5114624509698318
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "Test set accuracy of padded BERT dataset with variable maximum lengths"
        }
       }
      },
      "text/html": [
       "<div id=\"6142f664-ca53-4b83-9386-37ca9a91d9a5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\")) {\n",
       "    Plotly.newPlot(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.513043478637816, 0.5122529648038239, 0.5075098817998712, 0.5083003953747127, 0.49407114631573673, 0.5098814233018476, 0.507509881752753, 0.5035573126299108, 0.4996047434599503, 0.5027667987488004, 0.507509881752753, 0.5090909091144682, 0.5067193679187609, 0.5114624509227135, 0.5098814233018476, 0.5162055336203971, 0.5193675893097527, 0.5193675893097527, 0.5233201584797131, 0.5217391304583417, 0.5106719371358397, 0.5106719367824524, 0.5177865616417685, 0.5075098817998712, 0.513043478637816, 0.5043478264639029, 0.5162055336203971, 0.5075098817998712, 0.5114624509698318, 0.513833992471808, 0.5075098817998712], \"type\": \"scatter\", \"uid\": \"36b39775-a328-4c54-a3ca-4455c6dc01d6\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5098814233018476, 0.5098814229484603, 0.49723320160458684, 0.5090909091144682, 0.5051383399445077, 0.505928854131887, 0.498814229272571, 0.5114624509698318, 0.4940711466220057, 0.49169960512002936, 0.4996047431065631, 0.5027667984425315, 0.4956521739366026, 0.5067193679658791, 0.5067193676124919, 0.5043478264167846, 0.5146245063058001, 0.5256916999816894, 0.5201581031437448, 0.515415019786405, 0.5169960478077764, 0.521739130811729, 0.5185770751223734, 0.5154150201397922, 0.5209486169777369, 0.5027667987959187, 0.5011857711279345, 0.5154150201397922, 0.5083003956338633, 0.5075098817998712, 0.5185770754286423], \"type\": \"scatter\", \"uid\": \"05ff3bde-811d-43d5-8421-2f3acd734853\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5035573122765236, 0.5027667984425315, 0.5090909094207372, 0.49090909128603727, 0.5098814233018476, 0.505928854131887, 0.5075098817998712, 0.5035573123236419, 0.5003952572468241, 0.488537549784061, 0.5075098817998712, 0.5035573122765236, 0.5003952572468241, 0.5122529648038239, 0.5051383402507766, 0.5177865616417685, 0.5106719371358397, 0.5019762849619266, 0.5106719368295707, 0.5083003956338633, 0.5146245063058001, 0.5146245063058001, 0.5177865616417685, 0.5067193676596102, 0.5098814233018476, 0.5146245063058001, 0.5138339921184208, 0.5043478264639029, 0.507509881446484, 0.5154150201397922, 0.5067193679658791], \"type\": \"scatter\", \"uid\": \"332e414a-c977-4eeb-894a-69644b06f03d\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49249011895402145, 0.5098814229484603, 0.5019762849148083, 0.505928854131887, 0.5059288537784998, 0.4996047434599503, 0.5035573126299108, 0.4996047434599503, 0.5003952572468241, 0.4893280636180531, 0.5035573126299108, 0.49802371543857893, 0.513043478637816, 0.5019762846085394, 0.5106719371358397, 0.5114624509227135, 0.5122529648038239, 0.515415019786405, 0.5154150201397922, 0.522529644645721, 0.505138340297895, 0.5169960478077764, 0.5201581031437448, 0.5106719371358397, 0.4996047434599503, 0.5169960478077764, 0.5067193676124919, 0.5146245063058001, 0.5043478264639029, 0.5043478261105157, 0.5106719367824524], \"type\": \"scatter\", \"uid\": \"0b216ed9-0130-4d9d-837c-77b49dbb95d4\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.505138340297895, 0.505928854131887, 0.4948616604559978, 0.5051383399445077, 0.498814229272571, 0.5122529647567056, 0.49723320195797405, 0.5146245063058001, 0.505928854131887, 0.4940711466220057, 0.5106719368295707, 0.5106719367824524, 0.5027667987959187, 0.5059288537784998, 0.5083003956338633, 0.5098814233018476, 0.5138339921184208, 0.5027667987959187, 0.5146245059995312, 0.5059288540847687, 0.5130434783315471, 0.5169960478077764, 0.5114624509698318, 0.505928854131887, 0.505928854131887, 0.5098814233018476, 0.5114624509698318, 0.5059288537784998, 0.5130434783315471, 0.49723320195797405, 0.5114624509698318], \"type\": \"scatter\", \"uid\": \"2f9230fa-3bf5-47ea-b643-5e2650adb502\"}], {\"title\": {\"text\": \"Test set accuracy of padded BERT dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\")) {window._Plotly.Plots.resize(document.getElementById(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"6142f664-ca53-4b83-9386-37ca9a91d9a5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\")) {\n",
       "    Plotly.newPlot(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.513043478637816, 0.5122529648038239, 0.5075098817998712, 0.5083003953747127, 0.49407114631573673, 0.5098814233018476, 0.507509881752753, 0.5035573126299108, 0.4996047434599503, 0.5027667987488004, 0.507509881752753, 0.5090909091144682, 0.5067193679187609, 0.5114624509227135, 0.5098814233018476, 0.5162055336203971, 0.5193675893097527, 0.5193675893097527, 0.5233201584797131, 0.5217391304583417, 0.5106719371358397, 0.5106719367824524, 0.5177865616417685, 0.5075098817998712, 0.513043478637816, 0.5043478264639029, 0.5162055336203971, 0.5075098817998712, 0.5114624509698318, 0.513833992471808, 0.5075098817998712], \"type\": \"scatter\", \"uid\": \"36b39775-a328-4c54-a3ca-4455c6dc01d6\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5098814233018476, 0.5098814229484603, 0.49723320160458684, 0.5090909091144682, 0.5051383399445077, 0.505928854131887, 0.498814229272571, 0.5114624509698318, 0.4940711466220057, 0.49169960512002936, 0.4996047431065631, 0.5027667984425315, 0.4956521739366026, 0.5067193679658791, 0.5067193676124919, 0.5043478264167846, 0.5146245063058001, 0.5256916999816894, 0.5201581031437448, 0.515415019786405, 0.5169960478077764, 0.521739130811729, 0.5185770751223734, 0.5154150201397922, 0.5209486169777369, 0.5027667987959187, 0.5011857711279345, 0.5154150201397922, 0.5083003956338633, 0.5075098817998712, 0.5185770754286423], \"type\": \"scatter\", \"uid\": \"05ff3bde-811d-43d5-8421-2f3acd734853\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5035573122765236, 0.5027667984425315, 0.5090909094207372, 0.49090909128603727, 0.5098814233018476, 0.505928854131887, 0.5075098817998712, 0.5035573123236419, 0.5003952572468241, 0.488537549784061, 0.5075098817998712, 0.5035573122765236, 0.5003952572468241, 0.5122529648038239, 0.5051383402507766, 0.5177865616417685, 0.5106719371358397, 0.5019762849619266, 0.5106719368295707, 0.5083003956338633, 0.5146245063058001, 0.5146245063058001, 0.5177865616417685, 0.5067193676596102, 0.5098814233018476, 0.5146245063058001, 0.5138339921184208, 0.5043478264639029, 0.507509881446484, 0.5154150201397922, 0.5067193679658791], \"type\": \"scatter\", \"uid\": \"332e414a-c977-4eeb-894a-69644b06f03d\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49249011895402145, 0.5098814229484603, 0.5019762849148083, 0.505928854131887, 0.5059288537784998, 0.4996047434599503, 0.5035573126299108, 0.4996047434599503, 0.5003952572468241, 0.4893280636180531, 0.5035573126299108, 0.49802371543857893, 0.513043478637816, 0.5019762846085394, 0.5106719371358397, 0.5114624509227135, 0.5122529648038239, 0.515415019786405, 0.5154150201397922, 0.522529644645721, 0.505138340297895, 0.5169960478077764, 0.5201581031437448, 0.5106719371358397, 0.4996047434599503, 0.5169960478077764, 0.5067193676124919, 0.5146245063058001, 0.5043478264639029, 0.5043478261105157, 0.5106719367824524], \"type\": \"scatter\", \"uid\": \"0b216ed9-0130-4d9d-837c-77b49dbb95d4\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.505138340297895, 0.505928854131887, 0.4948616604559978, 0.5051383399445077, 0.498814229272571, 0.5122529647567056, 0.49723320195797405, 0.5146245063058001, 0.505928854131887, 0.4940711466220057, 0.5106719368295707, 0.5106719367824524, 0.5027667987959187, 0.5059288537784998, 0.5083003956338633, 0.5098814233018476, 0.5138339921184208, 0.5027667987959187, 0.5146245059995312, 0.5059288540847687, 0.5130434783315471, 0.5169960478077764, 0.5114624509698318, 0.505928854131887, 0.505928854131887, 0.5098814233018476, 0.5114624509698318, 0.5059288537784998, 0.5130434783315471, 0.49723320195797405, 0.5114624509698318], \"type\": \"scatter\", \"uid\": \"2f9230fa-3bf5-47ea-b643-5e2650adb502\"}], {\"title\": {\"text\": \"Test set accuracy of padded BERT dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\")) {window._Plotly.Plots.resize(document.getElementById(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traces = [round1, round2, round3, round4, round5]\n",
    "\n",
    "# Create traces\n",
    "bert_trace = go.Scatter(\n",
    "    x = list(round1.keys()),\n",
    "    y = list(round1.values()),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'BERT'\n",
    ")\n",
    "\n",
    "def create_scatter(counter):\n",
    "    acc_dict = traces[counter]\n",
    "    \n",
    "    return go.Scatter(\n",
    "        x = list(acc_dict.keys()),\n",
    "        y = list(acc_dict.values()),\n",
    "        mode = 'lines+markers',\n",
    "        name = 'Round ' + str(counter)\n",
    "    )\n",
    "\n",
    "trace_data = [create_scatter(trace) for trace in range(len(traces))]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'Test set accuracy of padded BERT dataset with variable maximum lengths',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = trace_data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = data.get_elmo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_round(dataset):\n",
    "    # Store accuracies\n",
    "    accuracies = {\n",
    "        padding_len: 0.0 for padding_len in list(range(5,36))\n",
    "    }\n",
    "\n",
    "    for max_len in accuracies.keys():\n",
    "        padded_dataset = {\n",
    "            fold: sequence.pad_sequences(dataset[fold], maxlen = max_len, dtype = float)\n",
    "            for fold in dataset.keys()\n",
    "        }\n",
    "\n",
    "        accuracies[max_len] = get_bilstm_score(padded_dataset['train'], padded_dataset['test'], padded_dataset['validation'], reshape = False)\n",
    "\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.1143 - acc: 0.4189 - val_loss: 1.0181 - val_acc: 0.5140\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0561 - acc: 0.4553 - val_loss: 1.0130 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 15s 2ms/step - loss: 1.0397 - acc: 0.4775 - val_loss: 1.0096 - val_acc: 0.5273\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0286 - acc: 0.4833 - val_loss: 1.0050 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0222 - acc: 0.4871 - val_loss: 1.0077 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 1s 469us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.1385 - acc: 0.4140 - val_loss: 1.0252 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0572 - acc: 0.4552 - val_loss: 1.0123 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0357 - acc: 0.4815 - val_loss: 1.0104 - val_acc: 0.5226\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0261 - acc: 0.4882 - val_loss: 1.0082 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0202 - acc: 0.4915 - val_loss: 1.0068 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 1s 532us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.1288 - acc: 0.4163 - val_loss: 1.0151 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0550 - acc: 0.4638 - val_loss: 1.0161 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0348 - acc: 0.4786 - val_loss: 1.0075 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0253 - acc: 0.4869 - val_loss: 1.0040 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0195 - acc: 0.4944 - val_loss: 1.0050 - val_acc: 0.5070\n",
      "1265/1265 [==============================] - 1s 619us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.1208 - acc: 0.4120 - val_loss: 1.0168 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0541 - acc: 0.4576 - val_loss: 1.0119 - val_acc: 0.5179\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0404 - acc: 0.4760 - val_loss: 1.0104 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0290 - acc: 0.4927 - val_loss: 1.0090 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0231 - acc: 0.4921 - val_loss: 1.0066 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 1s 660us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.1246 - acc: 0.4189 - val_loss: 1.0144 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0557 - acc: 0.4589 - val_loss: 1.0089 - val_acc: 0.5179\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0402 - acc: 0.4798 - val_loss: 1.0068 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0236 - acc: 0.4930 - val_loss: 1.0042 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0184 - acc: 0.4948 - val_loss: 1.0022 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 1s 718us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.1193 - acc: 0.4262 - val_loss: 1.0098 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0514 - acc: 0.4635 - val_loss: 1.0087 - val_acc: 0.5241\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0334 - acc: 0.4847 - val_loss: 1.0010 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0247 - acc: 0.4897 - val_loss: 1.0002 - val_acc: 0.5241\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0189 - acc: 0.4952 - val_loss: 0.9990 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 1s 780us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.1063 - acc: 0.4269 - val_loss: 1.0162 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0496 - acc: 0.4694 - val_loss: 1.0080 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0371 - acc: 0.4783 - val_loss: 1.0054 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0270 - acc: 0.4925 - val_loss: 1.0087 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0180 - acc: 0.5027 - val_loss: 0.9995 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 1s 858us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.1060 - acc: 0.4328 - val_loss: 1.0163 - val_acc: 0.5171\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0532 - acc: 0.4650 - val_loss: 1.0089 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0366 - acc: 0.4820 - val_loss: 1.0089 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0237 - acc: 0.4980 - val_loss: 1.0005 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0177 - acc: 0.4965 - val_loss: 0.9977 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 1s 925us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.1172 - acc: 0.4206 - val_loss: 1.0109 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0542 - acc: 0.4690 - val_loss: 1.0032 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0308 - acc: 0.4889 - val_loss: 1.0000 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0241 - acc: 0.4930 - val_loss: 0.9991 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0104 - acc: 0.5052 - val_loss: 0.9935 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.1222 - acc: 0.4185 - val_loss: 1.0197 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0536 - acc: 0.4619 - val_loss: 1.0077 - val_acc: 0.5226\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0324 - acc: 0.4814 - val_loss: 1.0117 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0205 - acc: 0.4944 - val_loss: 1.0003 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0145 - acc: 0.5042 - val_loss: 0.9962 - val_acc: 0.5265\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.1169 - acc: 0.4188 - val_loss: 1.0123 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0532 - acc: 0.4688 - val_loss: 1.0112 - val_acc: 0.5078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0325 - acc: 0.4848 - val_loss: 1.0044 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0227 - acc: 0.4971 - val_loss: 1.0029 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0171 - acc: 0.5015 - val_loss: 0.9987 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.1160 - acc: 0.4218 - val_loss: 1.0094 - val_acc: 0.5171\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 36s 4ms/step - loss: 1.0522 - acc: 0.4704 - val_loss: 1.0106 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0328 - acc: 0.4847 - val_loss: 1.0103 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 36s 3ms/step - loss: 1.0181 - acc: 0.5016 - val_loss: 1.0030 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 36s 4ms/step - loss: 1.0132 - acc: 0.5075 - val_loss: 0.9996 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.1163 - acc: 0.4253 - val_loss: 1.0220 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0535 - acc: 0.4621 - val_loss: 1.0108 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0388 - acc: 0.4813 - val_loss: 1.0148 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0229 - acc: 0.4973 - val_loss: 1.0019 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0181 - acc: 0.5009 - val_loss: 1.0072 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 56s 6ms/step - loss: 1.1199 - acc: 0.4170 - val_loss: 1.0227 - val_acc: 0.5210\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0508 - acc: 0.4633 - val_loss: 1.0196 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0351 - acc: 0.4787 - val_loss: 1.0081 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0228 - acc: 0.4937 - val_loss: 1.0014 - val_acc: 0.5257\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0147 - acc: 0.5051 - val_loss: 1.0014 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.1173 - acc: 0.4246 - val_loss: 1.0163 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0532 - acc: 0.4660 - val_loss: 1.0132 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0374 - acc: 0.4824 - val_loss: 1.0038 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0256 - acc: 0.4957 - val_loss: 1.0011 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0175 - acc: 0.4971 - val_loss: 0.9951 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.1132 - acc: 0.4309 - val_loss: 1.0120 - val_acc: 0.5296\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0525 - acc: 0.4735 - val_loss: 1.0038 - val_acc: 0.5257\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0334 - acc: 0.4853 - val_loss: 0.9998 - val_acc: 0.5265\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0207 - acc: 0.4972 - val_loss: 0.9960 - val_acc: 0.5312\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0121 - acc: 0.5047 - val_loss: 0.9928 - val_acc: 0.5312\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.1098 - acc: 0.4220 - val_loss: 1.0175 - val_acc: 0.5140\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0534 - acc: 0.4638 - val_loss: 1.0178 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0375 - acc: 0.4790 - val_loss: 1.0093 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0294 - acc: 0.4900 - val_loss: 1.0012 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0206 - acc: 0.4987 - val_loss: 0.9990 - val_acc: 0.5280\n",
      "1265/1265 [==============================] - 4s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.1087 - acc: 0.4266 - val_loss: 1.0205 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0537 - acc: 0.4690 - val_loss: 1.0121 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0341 - acc: 0.4874 - val_loss: 1.0069 - val_acc: 0.5249\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0237 - acc: 0.4853 - val_loss: 1.0080 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0161 - acc: 0.5037 - val_loss: 0.9948 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.0994 - acc: 0.4234 - val_loss: 1.0213 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0522 - acc: 0.4637 - val_loss: 1.0116 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0355 - acc: 0.4858 - val_loss: 1.0043 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0299 - acc: 0.4901 - val_loss: 1.0016 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0199 - acc: 0.5002 - val_loss: 0.9964 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 80s 8ms/step - loss: 1.0999 - acc: 0.4297 - val_loss: 1.0204 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0554 - acc: 0.4602 - val_loss: 1.0093 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0366 - acc: 0.4775 - val_loss: 1.0056 - val_acc: 0.5265\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0271 - acc: 0.4925 - val_loss: 1.0009 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0211 - acc: 0.4937 - val_loss: 0.9973 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 83s 8ms/step - loss: 1.1007 - acc: 0.4341 - val_loss: 1.0166 - val_acc: 0.5148\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0557 - acc: 0.4621 - val_loss: 1.0096 - val_acc: 0.5249\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0361 - acc: 0.4833 - val_loss: 1.0023 - val_acc: 0.5273\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0258 - acc: 0.4966 - val_loss: 0.9961 - val_acc: 0.5265\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0203 - acc: 0.5007 - val_loss: 0.9926 - val_acc: 0.5319\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 85s 8ms/step - loss: 1.0951 - acc: 0.4267 - val_loss: 1.0208 - val_acc: 0.5202\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0519 - acc: 0.4621 - val_loss: 1.0075 - val_acc: 0.5257\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0413 - acc: 0.4771 - val_loss: 1.0054 - val_acc: 0.5241\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0287 - acc: 0.4955 - val_loss: 1.0061 - val_acc: 0.5265\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0204 - acc: 0.4984 - val_loss: 1.0026 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 91s 9ms/step - loss: 1.0903 - acc: 0.4276 - val_loss: 1.0179 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0543 - acc: 0.4637 - val_loss: 1.0136 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0400 - acc: 0.4815 - val_loss: 1.0025 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0232 - acc: 0.4976 - val_loss: 1.0009 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0209 - acc: 0.4972 - val_loss: 0.9985 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 89s 9ms/step - loss: 1.0922 - acc: 0.4259 - val_loss: 1.0268 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0565 - acc: 0.4587 - val_loss: 1.0136 - val_acc: 0.5241\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0403 - acc: 0.4802 - val_loss: 1.0107 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0319 - acc: 0.4878 - val_loss: 1.0099 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0253 - acc: 0.4933 - val_loss: 1.0011 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 93s 9ms/step - loss: 1.0916 - acc: 0.4319 - val_loss: 1.0245 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 1.0555 - acc: 0.4632 - val_loss: 1.0157 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0390 - acc: 0.4801 - val_loss: 1.0081 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0338 - acc: 0.4826 - val_loss: 1.0077 - val_acc: 0.5265\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0228 - acc: 0.4953 - val_loss: 1.0053 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 97s 9ms/step - loss: 1.0891 - acc: 0.4333 - val_loss: 1.0186 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.0485 - acc: 0.4684 - val_loss: 1.0075 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0337 - acc: 0.4852 - val_loss: 1.0123 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0262 - acc: 0.4984 - val_loss: 1.0046 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0186 - acc: 0.4987 - val_loss: 0.9982 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 99s 10ms/step - loss: 1.0905 - acc: 0.4275 - val_loss: 1.0171 - val_acc: 0.5132\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 77s 8ms/step - loss: 1.0532 - acc: 0.4698 - val_loss: 1.0117 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0405 - acc: 0.4772 - val_loss: 1.0037 - val_acc: 0.5226\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0354 - acc: 0.4829 - val_loss: 0.9989 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0264 - acc: 0.4905 - val_loss: 0.9976 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 101s 10ms/step - loss: 1.0883 - acc: 0.4266 - val_loss: 1.0239 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 83s 8ms/step - loss: 1.0521 - acc: 0.4630 - val_loss: 1.0194 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0377 - acc: 0.4755 - val_loss: 1.0100 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0308 - acc: 0.4878 - val_loss: 1.0111 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0241 - acc: 0.4957 - val_loss: 1.0005 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 104s 10ms/step - loss: 1.0824 - acc: 0.4362 - val_loss: 1.0135 - val_acc: 0.5171\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.0496 - acc: 0.4651 - val_loss: 1.0093 - val_acc: 0.5312\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0391 - acc: 0.4768 - val_loss: 1.0053 - val_acc: 0.5312\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0327 - acc: 0.4899 - val_loss: 0.9999 - val_acc: 0.5288\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0238 - acc: 0.4921 - val_loss: 1.0030 - val_acc: 0.5288\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 106s 10ms/step - loss: 1.0859 - acc: 0.4332 - val_loss: 1.0285 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 83s 8ms/step - loss: 1.0538 - acc: 0.4639 - val_loss: 1.0182 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0392 - acc: 0.4769 - val_loss: 1.0155 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0323 - acc: 0.4832 - val_loss: 1.0055 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0256 - acc: 0.4939 - val_loss: 0.9984 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 108s 11ms/step - loss: 1.0838 - acc: 0.4342 - val_loss: 1.0198 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 84s 8ms/step - loss: 1.0540 - acc: 0.4552 - val_loss: 1.0165 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.0460 - acc: 0.4758 - val_loss: 1.0124 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.0309 - acc: 0.4817 - val_loss: 1.0072 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.0261 - acc: 0.4940 - val_loss: 1.0014 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.1180 - acc: 0.4182 - val_loss: 1.0215 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0558 - acc: 0.4538 - val_loss: 1.0083 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0431 - acc: 0.4713 - val_loss: 1.0113 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0295 - acc: 0.4847 - val_loss: 1.0097 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0258 - acc: 0.4852 - val_loss: 1.0037 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 1s 486us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.1106 - acc: 0.4268 - val_loss: 1.0166 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0540 - acc: 0.4617 - val_loss: 1.0149 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0394 - acc: 0.4754 - val_loss: 1.0081 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0282 - acc: 0.4851 - val_loss: 1.0090 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0177 - acc: 0.4983 - val_loss: 1.0079 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 1s 583us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.1229 - acc: 0.4224 - val_loss: 1.0217 - val_acc: 0.5156\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0535 - acc: 0.4589 - val_loss: 1.0107 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0376 - acc: 0.4794 - val_loss: 1.0091 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0285 - acc: 0.4880 - val_loss: 1.0060 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0192 - acc: 0.4940 - val_loss: 1.0062 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 1s 631us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.1348 - acc: 0.4101 - val_loss: 1.0199 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0580 - acc: 0.4587 - val_loss: 1.0151 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0349 - acc: 0.4788 - val_loss: 1.0095 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0278 - acc: 0.4900 - val_loss: 1.0120 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0182 - acc: 0.5005 - val_loss: 1.0101 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 1s 711us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.1210 - acc: 0.4249 - val_loss: 1.0106 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0513 - acc: 0.4616 - val_loss: 1.0074 - val_acc: 0.5218\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0364 - acc: 0.4789 - val_loss: 1.0000 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0230 - acc: 0.4984 - val_loss: 0.9998 - val_acc: 0.5296\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0166 - acc: 0.5002 - val_loss: 0.9972 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 1s 838us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.1260 - acc: 0.4171 - val_loss: 1.0245 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0526 - acc: 0.4620 - val_loss: 1.0128 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0366 - acc: 0.4826 - val_loss: 1.0112 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0272 - acc: 0.4926 - val_loss: 1.0055 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0202 - acc: 0.4992 - val_loss: 1.0053 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.1263 - acc: 0.4262 - val_loss: 1.0159 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0508 - acc: 0.4650 - val_loss: 1.0111 - val_acc: 0.5226\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0306 - acc: 0.4873 - val_loss: 1.0037 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0243 - acc: 0.4930 - val_loss: 1.0043 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0175 - acc: 0.5028 - val_loss: 1.0021 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.1063 - acc: 0.4280 - val_loss: 1.0185 - val_acc: 0.5132\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0493 - acc: 0.4696 - val_loss: 1.0110 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0336 - acc: 0.4887 - val_loss: 1.0082 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0247 - acc: 0.4975 - val_loss: 1.0055 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0155 - acc: 0.5003 - val_loss: 1.0034 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.1193 - acc: 0.4238 - val_loss: 1.0083 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0553 - acc: 0.4615 - val_loss: 1.0066 - val_acc: 0.5241\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0307 - acc: 0.4854 - val_loss: 0.9993 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0236 - acc: 0.4922 - val_loss: 0.9955 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0134 - acc: 0.5038 - val_loss: 0.9990 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.1236 - acc: 0.4225 - val_loss: 1.0227 - val_acc: 0.5171\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0545 - acc: 0.4664 - val_loss: 1.0143 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0342 - acc: 0.4843 - val_loss: 1.0083 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0261 - acc: 0.4912 - val_loss: 1.0050 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0193 - acc: 0.4981 - val_loss: 1.0021 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.1237 - acc: 0.4212 - val_loss: 1.0160 - val_acc: 0.5140\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0486 - acc: 0.4745 - val_loss: 1.0142 - val_acc: 0.5257\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0336 - acc: 0.4889 - val_loss: 1.0027 - val_acc: 0.5288\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0260 - acc: 0.4915 - val_loss: 1.0037 - val_acc: 0.5312\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 36s 3ms/step - loss: 1.0176 - acc: 0.4985 - val_loss: 1.0002 - val_acc: 0.5312\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.1220 - acc: 0.4189 - val_loss: 1.0171 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0504 - acc: 0.4652 - val_loss: 1.0035 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 36s 3ms/step - loss: 1.0354 - acc: 0.4853 - val_loss: 1.0044 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0257 - acc: 0.4884 - val_loss: 1.0025 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0200 - acc: 0.5033 - val_loss: 0.9969 - val_acc: 0.5312\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.1157 - acc: 0.4278 - val_loss: 1.0219 - val_acc: 0.5195\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0516 - acc: 0.4717 - val_loss: 1.0102 - val_acc: 0.5241\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0308 - acc: 0.4845 - val_loss: 1.0063 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 36s 3ms/step - loss: 1.0218 - acc: 0.4971 - val_loss: 0.9993 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 36s 4ms/step - loss: 1.0164 - acc: 0.5035 - val_loss: 0.9975 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 1.1068 - acc: 0.4277 - val_loss: 1.0189 - val_acc: 0.5171\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0482 - acc: 0.4692 - val_loss: 1.0097 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0352 - acc: 0.4850 - val_loss: 1.0049 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0260 - acc: 0.4884 - val_loss: 0.9996 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0191 - acc: 0.5036 - val_loss: 0.9990 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.1046 - acc: 0.4276 - val_loss: 1.0186 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0520 - acc: 0.4679 - val_loss: 1.0106 - val_acc: 0.5210\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0362 - acc: 0.4852 - val_loss: 1.0029 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0273 - acc: 0.4907 - val_loss: 1.0070 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0118 - acc: 0.5067 - val_loss: 0.9957 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 80s 8ms/step - loss: 1.1069 - acc: 0.4263 - val_loss: 1.0137 - val_acc: 0.5210\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0520 - acc: 0.4705 - val_loss: 1.0062 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0362 - acc: 0.4836 - val_loss: 1.0016 - val_acc: 0.5241\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0248 - acc: 0.4952 - val_loss: 0.9970 - val_acc: 0.5296\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0168 - acc: 0.5020 - val_loss: 0.9961 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 84s 8ms/step - loss: 1.1092 - acc: 0.4242 - val_loss: 1.0167 - val_acc: 0.5179\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0592 - acc: 0.4637 - val_loss: 1.0071 - val_acc: 0.5234\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0393 - acc: 0.4758 - val_loss: 1.0061 - val_acc: 0.5296\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0274 - acc: 0.4894 - val_loss: 0.9984 - val_acc: 0.5257\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0163 - acc: 0.5019 - val_loss: 0.9959 - val_acc: 0.5280\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 90s 9ms/step - loss: 1.1075 - acc: 0.4219 - val_loss: 1.0206 - val_acc: 0.5109\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0581 - acc: 0.4589 - val_loss: 1.0080 - val_acc: 0.5218\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0404 - acc: 0.4814 - val_loss: 1.0050 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0303 - acc: 0.4911 - val_loss: 1.0022 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0223 - acc: 0.4907 - val_loss: 1.0018 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.1040 - acc: 0.4249 - val_loss: 1.0247 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0543 - acc: 0.4670 - val_loss: 1.0139 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0367 - acc: 0.4836 - val_loss: 1.0075 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0286 - acc: 0.4885 - val_loss: 1.0105 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0196 - acc: 0.5038 - val_loss: 1.0031 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 97s 9ms/step - loss: 1.1048 - acc: 0.4239 - val_loss: 1.0184 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0561 - acc: 0.4656 - val_loss: 1.0070 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0372 - acc: 0.4827 - val_loss: 1.0052 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0291 - acc: 0.4899 - val_loss: 1.0012 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0187 - acc: 0.5059 - val_loss: 0.9967 - val_acc: 0.5265\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 93s 9ms/step - loss: 1.0919 - acc: 0.4280 - val_loss: 1.0183 - val_acc: 0.5187\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0539 - acc: 0.4639 - val_loss: 1.0118 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0408 - acc: 0.4715 - val_loss: 1.0054 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0249 - acc: 0.4886 - val_loss: 1.0089 - val_acc: 0.5047\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0189 - acc: 0.5016 - val_loss: 0.9977 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 99s 10ms/step - loss: 1.1019 - acc: 0.4234 - val_loss: 1.0147 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0522 - acc: 0.4701 - val_loss: 1.0108 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0391 - acc: 0.4792 - val_loss: 1.0001 - val_acc: 0.5296\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0312 - acc: 0.4903 - val_loss: 0.9985 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0216 - acc: 0.4970 - val_loss: 0.9974 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0986 - acc: 0.4331 - val_loss: 1.0188 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0511 - acc: 0.4643 - val_loss: 1.0121 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0358 - acc: 0.4878 - val_loss: 1.0062 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0301 - acc: 0.4911 - val_loss: 1.0061 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0208 - acc: 0.4964 - val_loss: 1.0047 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 95s 9ms/step - loss: 1.0997 - acc: 0.4191 - val_loss: 1.0282 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0576 - acc: 0.4590 - val_loss: 1.0149 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0397 - acc: 0.4723 - val_loss: 1.0050 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0331 - acc: 0.4798 - val_loss: 1.0007 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0247 - acc: 0.4884 - val_loss: 0.9964 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 92s 9ms/step - loss: 1.0933 - acc: 0.4223 - val_loss: 1.0171 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0522 - acc: 0.4635 - val_loss: 1.0091 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0417 - acc: 0.4744 - val_loss: 1.0048 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 4763s 465ms/step - loss: 1.0288 - acc: 0.4919 - val_loss: 1.0018 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 2143s 209ms/step - loss: 1.0240 - acc: 0.5013 - val_loss: 1.0018 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 3208s 3s/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 9049s 884ms/step - loss: 1.0846 - acc: 0.4331 - val_loss: 1.0292 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 4827s 472ms/step - loss: 1.0527 - acc: 0.4629 - val_loss: 1.0198 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0401 - acc: 0.4776 - val_loss: 1.0062 - val_acc: 0.5234\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0286 - acc: 0.4912 - val_loss: 1.0039 - val_acc: 0.5280\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 2196s 215ms/step - loss: 1.0277 - acc: 0.4906 - val_loss: 1.0012 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 3208s 313ms/step - loss: 1.0891 - acc: 0.4258 - val_loss: 1.0237 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 94s 9ms/step - loss: 1.0520 - acc: 0.4544 - val_loss: 1.0202 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 95s 9ms/step - loss: 1.0416 - acc: 0.4716 - val_loss: 1.0081 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 95s 9ms/step - loss: 1.0345 - acc: 0.4822 - val_loss: 1.0062 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 95s 9ms/step - loss: 1.0276 - acc: 0.4883 - val_loss: 1.0014 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0895 - acc: 0.4276 - val_loss: 1.0166 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 99s 10ms/step - loss: 1.0535 - acc: 0.4580 - val_loss: 1.0069 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0391 - acc: 0.4782 - val_loss: 1.0034 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 105s 10ms/step - loss: 1.0302 - acc: 0.4927 - val_loss: 0.9982 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 100s 10ms/step - loss: 1.0255 - acc: 0.4913 - val_loss: 0.9959 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0863 - acc: 0.4222 - val_loss: 1.0267 - val_acc: 0.5109\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 103s 10ms/step - loss: 1.0530 - acc: 0.4569 - val_loss: 1.0238 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 104s 10ms/step - loss: 1.0447 - acc: 0.4675 - val_loss: 1.0136 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 104s 10ms/step - loss: 1.0362 - acc: 0.4828 - val_loss: 1.0077 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 107s 10ms/step - loss: 1.0303 - acc: 0.4867 - val_loss: 1.0028 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0837 - acc: 0.4280 - val_loss: 1.0212 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 107s 10ms/step - loss: 1.0516 - acc: 0.4621 - val_loss: 1.0137 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 106s 10ms/step - loss: 1.0459 - acc: 0.4732 - val_loss: 1.0065 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 106s 10ms/step - loss: 1.0363 - acc: 0.4806 - val_loss: 1.0044 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 107s 10ms/step - loss: 1.0290 - acc: 0.4901 - val_loss: 1.0003 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 12s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0885 - acc: 0.4234 - val_loss: 1.0220 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0535 - acc: 0.4692 - val_loss: 1.0131 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 111s 11ms/step - loss: 1.0391 - acc: 0.4779 - val_loss: 1.0110 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 110s 11ms/step - loss: 1.0318 - acc: 0.4881 - val_loss: 1.0067 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 110s 11ms/step - loss: 1.0247 - acc: 0.4952 - val_loss: 1.0008 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 12s 9ms/step\n",
      "CPU times: user 19h 25min 37s, sys: 4h 43min 20s, total: 1d 8min 57s\n",
      "Wall time: 13h 17min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "concatenated_elmo = {\n",
    "    fold: [np.concatenate(np.array(statement)) for statement in elmo[fold]['statement']]\n",
    "    for fold in elmo.keys()\n",
    "}\n",
    "\n",
    "elmo_rounds = [calculate_round(concatenated_elmo) for round in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{5: 0.5177865616417685,\n",
       "  6: 0.5241106722665869,\n",
       "  7: 0.5114624509698318,\n",
       "  8: 0.5067193676124919,\n",
       "  9: 0.5067193679187609,\n",
       "  10: 0.5059288540847687,\n",
       "  11: 0.49802371574484783,\n",
       "  12: 0.5011857710808162,\n",
       "  13: 0.49169960507291105,\n",
       "  14: 0.49723320191085574,\n",
       "  15: 0.4932806327408953,\n",
       "  16: 0.5169960478077764,\n",
       "  17: 0.5154150200455556,\n",
       "  18: 0.513043478637816,\n",
       "  19: 0.5083003955396268,\n",
       "  20: 0.5185770754286423,\n",
       "  21: 0.5067193679187609,\n",
       "  22: 0.5146245063058001,\n",
       "  23: 0.5011857710808162,\n",
       "  24: 0.5106719370887214,\n",
       "  25: 0.5169960477606581,\n",
       "  26: 0.5193675893097527,\n",
       "  27: 0.5114624509227135,\n",
       "  28: 0.5130434785906977,\n",
       "  29: 0.5241106723137052,\n",
       "  30: 0.5169960477606581,\n",
       "  31: 0.5169960477606581,\n",
       "  32: 0.5146245063058001,\n",
       "  33: 0.5185770754286423,\n",
       "  34: 0.5051383402507766,\n",
       "  35: 0.5035573125827925},\n",
       " {5: 0.5122529647567056,\n",
       "  6: 0.5169960478077764,\n",
       "  7: 0.5146245062586818,\n",
       "  8: 0.5233201584325948,\n",
       "  9: 0.5138339924246897,\n",
       "  10: 0.4996047434599503,\n",
       "  11: 0.49090909123891896,\n",
       "  12: 0.49090909123891896,\n",
       "  13: 0.5011857710808162,\n",
       "  14: 0.5051383402036584,\n",
       "  15: 0.5067193679187609,\n",
       "  16: 0.5122529648038239,\n",
       "  17: 0.5146245062586818,\n",
       "  18: 0.5146245062586818,\n",
       "  19: 0.516205533926666,\n",
       "  20: 0.5083003955867451,\n",
       "  21: 0.5304347829385238,\n",
       "  22: 0.5217391307646106,\n",
       "  23: 0.5106719370887214,\n",
       "  24: 0.5138339924246897,\n",
       "  25: 0.5209486169306186,\n",
       "  26: 0.5233201584325948,\n",
       "  27: 0.505138340297895,\n",
       "  28: 0.5201581030966265,\n",
       "  29: 0.5225296445986027,\n",
       "  30: 0.5098814233018476,\n",
       "  31: 0.5090909094207372,\n",
       "  32: 0.5122529647567056,\n",
       "  33: 0.5130434785906977,\n",
       "  34: 0.499604743412832,\n",
       "  35: 0.5154150200926739},\n",
       " {5: 0.516205533926666,\n",
       "  6: 0.5264822137685633,\n",
       "  7: 0.5146245062586818,\n",
       "  8: 0.5067193679187609,\n",
       "  9: 0.505928854131887,\n",
       "  10: 0.5035573125827925,\n",
       "  11: 0.4988142295788399,\n",
       "  12: 0.5146245063058001,\n",
       "  13: 0.4940711465748874,\n",
       "  14: 0.5035573125827925,\n",
       "  15: 0.5083003955867451,\n",
       "  16: 0.5067193679187609,\n",
       "  17: 0.5098814232547293,\n",
       "  18: 0.5177865616417685,\n",
       "  19: 0.5059288540847687,\n",
       "  20: 0.513043478637816,\n",
       "  21: 0.5067193679187609,\n",
       "  22: 0.5312252967725158,\n",
       "  23: 0.5209486169306186,\n",
       "  24: 0.5122529647567056,\n",
       "  25: 0.5138339924246897,\n",
       "  26: 0.5185770754757606,\n",
       "  27: 0.5169960478077764,\n",
       "  28: 0.5177865616417685,\n",
       "  29: 0.5193675892626344,\n",
       "  30: 0.5185770754286423,\n",
       "  31: 0.5043478264167846,\n",
       "  32: 0.5090909094678555,\n",
       "  33: 0.5177865616417685,\n",
       "  34: 0.5138339924246897,\n",
       "  35: 0.5098814232547293}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.1243 - acc: 0.4187 - val_loss: 1.0217 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0583 - acc: 0.4553 - val_loss: 1.0125 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0449 - acc: 0.4633 - val_loss: 1.0117 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 15s 2ms/step - loss: 1.0315 - acc: 0.4838 - val_loss: 1.0111 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0219 - acc: 0.4914 - val_loss: 1.0065 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.1169 - acc: 0.4278 - val_loss: 1.0176 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0533 - acc: 0.4648 - val_loss: 1.0107 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0368 - acc: 0.4801 - val_loss: 1.0056 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 15s 2ms/step - loss: 1.0289 - acc: 0.4872 - val_loss: 1.0039 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 2ms/step - loss: 1.0236 - acc: 0.4975 - val_loss: 1.0024 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 1s 513us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.1063 - acc: 0.4257 - val_loss: 1.0174 - val_acc: 0.5132\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0572 - acc: 0.4564 - val_loss: 1.0187 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0406 - acc: 0.4793 - val_loss: 1.0093 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.0311 - acc: 0.4820 - val_loss: 1.0090 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0233 - acc: 0.4930 - val_loss: 1.0075 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 1s 637us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 996s 97ms/step - loss: 1.1288 - acc: 0.4247 - val_loss: 1.0203 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 2217s 217ms/step - loss: 1.0593 - acc: 0.4586 - val_loss: 1.0086 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0346 - acc: 0.4841 - val_loss: 1.0059 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0262 - acc: 0.4935 - val_loss: 1.0050 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0207 - acc: 0.5001 - val_loss: 1.0054 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 3136s 306ms/step - loss: 1.1164 - acc: 0.4217 - val_loss: 1.0180 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 1847s 180ms/step - loss: 1.0530 - acc: 0.4607 - val_loss: 1.0090 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 3252s 318ms/step - loss: 1.0359 - acc: 0.4763 - val_loss: 1.0140 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0250 - acc: 0.4927 - val_loss: 1.0069 - val_acc: 0.5241\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 3243s 317ms/step - loss: 1.0169 - acc: 0.5016 - val_loss: 1.0009 - val_acc: 0.5070\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 6544s 639ms/step - loss: 1.1246 - acc: 0.4205 - val_loss: 1.0115 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 1909s 187ms/step - loss: 1.0560 - acc: 0.4655 - val_loss: 1.0107 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0357 - acc: 0.4855 - val_loss: 1.0021 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 2216s 217ms/step - loss: 1.0232 - acc: 0.4940 - val_loss: 1.0089 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 1231s 120ms/step - loss: 1.0172 - acc: 0.5010 - val_loss: 1.0004 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 109s 11ms/step - loss: 1.1134 - acc: 0.4231 - val_loss: 1.0104 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0491 - acc: 0.4680 - val_loss: 1.0062 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.0352 - acc: 0.4846 - val_loss: 1.0073 - val_acc: 0.5234\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.0275 - acc: 0.4869 - val_loss: 1.0015 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0177 - acc: 0.4993 - val_loss: 0.9977 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 195s 19ms/step - loss: 1.1147 - acc: 0.4212 - val_loss: 1.0108 - val_acc: 0.5195\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0480 - acc: 0.4735 - val_loss: 1.0068 - val_acc: 0.5257\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0352 - acc: 0.4824 - val_loss: 1.0066 - val_acc: 0.5241\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0243 - acc: 0.4936 - val_loss: 1.0051 - val_acc: 0.5265\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0157 - acc: 0.5065 - val_loss: 0.9979 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.1194 - acc: 0.4188 - val_loss: 1.0094 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0517 - acc: 0.4737 - val_loss: 1.0073 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0329 - acc: 0.4866 - val_loss: 0.9991 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0193 - acc: 0.4948 - val_loss: 0.9952 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0127 - acc: 0.5032 - val_loss: 0.9950 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 77s 8ms/step - loss: 1.1172 - acc: 0.4227 - val_loss: 1.0185 - val_acc: 0.5156\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0488 - acc: 0.4712 - val_loss: 1.0090 - val_acc: 0.5241\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0338 - acc: 0.4872 - val_loss: 1.0067 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0258 - acc: 0.4950 - val_loss: 0.9992 - val_acc: 0.5241\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0166 - acc: 0.5015 - val_loss: 0.9963 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.1111 - acc: 0.4220 - val_loss: 1.0172 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0492 - acc: 0.4663 - val_loss: 1.0124 - val_acc: 0.5187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0372 - acc: 0.4811 - val_loss: 1.0016 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0252 - acc: 0.4962 - val_loss: 1.0026 - val_acc: 0.5257\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0180 - acc: 0.5010 - val_loss: 1.0044 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 85s 8ms/step - loss: 1.1063 - acc: 0.4277 - val_loss: 1.0178 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0543 - acc: 0.4613 - val_loss: 1.0091 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0371 - acc: 0.4764 - val_loss: 1.0025 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0261 - acc: 0.4920 - val_loss: 1.0017 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0158 - acc: 0.5007 - val_loss: 0.9982 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 91s 9ms/step - loss: 1.1088 - acc: 0.4286 - val_loss: 1.0243 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0499 - acc: 0.4713 - val_loss: 1.0160 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0382 - acc: 0.4763 - val_loss: 1.0102 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0248 - acc: 0.4914 - val_loss: 1.0115 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0192 - acc: 0.4957 - val_loss: 1.0020 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 92s 9ms/step - loss: 1.1201 - acc: 0.4143 - val_loss: 1.0211 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0592 - acc: 0.4576 - val_loss: 1.0141 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0340 - acc: 0.4834 - val_loss: 1.0063 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0251 - acc: 0.4946 - val_loss: 1.0020 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 36s 4ms/step - loss: 1.0145 - acc: 0.4981 - val_loss: 0.9993 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 97s 9ms/step - loss: 1.1183 - acc: 0.4182 - val_loss: 1.0241 - val_acc: 0.5164\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0524 - acc: 0.4614 - val_loss: 1.0152 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0368 - acc: 0.4838 - val_loss: 1.0065 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0236 - acc: 0.4938 - val_loss: 1.0030 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0193 - acc: 0.4990 - val_loss: 1.0067 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 104s 10ms/step - loss: 1.1071 - acc: 0.4227 - val_loss: 1.0164 - val_acc: 0.5218\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0518 - acc: 0.4669 - val_loss: 1.0061 - val_acc: 0.5280\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0368 - acc: 0.4825 - val_loss: 1.0028 - val_acc: 0.5273\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0197 - acc: 0.5028 - val_loss: 1.0032 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0145 - acc: 0.4998 - val_loss: 0.9971 - val_acc: 0.5312\n",
      "1265/1265 [==============================] - 5s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "concatenated_elmo = {\n",
    "    fold: [np.concatenate(np.array(statement)) for statement in elmo[fold]['statement']]\n",
    "    for fold in elmo.keys()\n",
    "}\n",
    "\n",
    "elmo_round2 = [calculate_round(concatenated_elmo) for round in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Round 0",
         "type": "scatter",
         "uid": "bba852b0-ed03-4ae4-872d-4cd6af458177",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5177865616417685,
          0.5241106722665869,
          0.5114624509698318,
          0.5067193676124919,
          0.5067193679187609,
          0.5059288540847687,
          0.49802371574484783,
          0.5011857710808162,
          0.49169960507291105,
          0.49723320191085574,
          0.4932806327408953,
          0.5169960478077764,
          0.5154150200455556,
          0.513043478637816,
          0.5083003955396268,
          0.5185770754286423,
          0.5067193679187609,
          0.5146245063058001,
          0.5011857710808162,
          0.5106719370887214,
          0.5169960477606581,
          0.5193675893097527,
          0.5114624509227135,
          0.5130434785906977,
          0.5241106723137052,
          0.5169960477606581,
          0.5169960477606581,
          0.5146245063058001,
          0.5185770754286423,
          0.5051383402507766,
          0.5035573125827925
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 1",
         "type": "scatter",
         "uid": "d905941a-161c-44b2-87b3-9f208592be38",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5122529647567056,
          0.5169960478077764,
          0.5146245062586818,
          0.5233201584325948,
          0.5138339924246897,
          0.4996047434599503,
          0.49090909123891896,
          0.49090909123891896,
          0.5011857710808162,
          0.5051383402036584,
          0.5067193679187609,
          0.5122529648038239,
          0.5146245062586818,
          0.5146245062586818,
          0.516205533926666,
          0.5083003955867451,
          0.5304347829385238,
          0.5217391307646106,
          0.5106719370887214,
          0.5138339924246897,
          0.5209486169306186,
          0.5233201584325948,
          0.505138340297895,
          0.5201581030966265,
          0.5225296445986027,
          0.5098814233018476,
          0.5090909094207372,
          0.5122529647567056,
          0.5130434785906977,
          0.499604743412832,
          0.5154150200926739
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 2",
         "type": "scatter",
         "uid": "d7f1909a-4238-4f9b-8375-05998547eb98",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.516205533926666,
          0.5264822137685633,
          0.5146245062586818,
          0.5067193679187609,
          0.505928854131887,
          0.5035573125827925,
          0.4988142295788399,
          0.5146245063058001,
          0.4940711465748874,
          0.5035573125827925,
          0.5083003955867451,
          0.5067193679187609,
          0.5098814232547293,
          0.5177865616417685,
          0.5059288540847687,
          0.513043478637816,
          0.5067193679187609,
          0.5312252967725158,
          0.5209486169306186,
          0.5122529647567056,
          0.5138339924246897,
          0.5185770754757606,
          0.5169960478077764,
          0.5177865616417685,
          0.5193675892626344,
          0.5185770754286423,
          0.5043478264167846,
          0.5090909094678555,
          0.5177865616417685,
          0.5138339924246897,
          0.5098814232547293
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "Test set accuracy of padded ELMo dataset with variable maximum lengths"
        }
       }
      },
      "text/html": [
       "<div id=\"9f7ee60d-6ab6-4e26-bdb1-79b2a514303c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"9f7ee60d-6ab6-4e26-bdb1-79b2a514303c\")) {\n",
       "    Plotly.newPlot(\"9f7ee60d-6ab6-4e26-bdb1-79b2a514303c\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5177865616417685, 0.5241106722665869, 0.5114624509698318, 0.5067193676124919, 0.5067193679187609, 0.5059288540847687, 0.49802371574484783, 0.5011857710808162, 0.49169960507291105, 0.49723320191085574, 0.4932806327408953, 0.5169960478077764, 0.5154150200455556, 0.513043478637816, 0.5083003955396268, 0.5185770754286423, 0.5067193679187609, 0.5146245063058001, 0.5011857710808162, 0.5106719370887214, 0.5169960477606581, 0.5193675893097527, 0.5114624509227135, 0.5130434785906977, 0.5241106723137052, 0.5169960477606581, 0.5169960477606581, 0.5146245063058001, 0.5185770754286423, 0.5051383402507766, 0.5035573125827925], \"type\": \"scatter\", \"uid\": \"bba852b0-ed03-4ae4-872d-4cd6af458177\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5122529647567056, 0.5169960478077764, 0.5146245062586818, 0.5233201584325948, 0.5138339924246897, 0.4996047434599503, 0.49090909123891896, 0.49090909123891896, 0.5011857710808162, 0.5051383402036584, 0.5067193679187609, 0.5122529648038239, 0.5146245062586818, 0.5146245062586818, 0.516205533926666, 0.5083003955867451, 0.5304347829385238, 0.5217391307646106, 0.5106719370887214, 0.5138339924246897, 0.5209486169306186, 0.5233201584325948, 0.505138340297895, 0.5201581030966265, 0.5225296445986027, 0.5098814233018476, 0.5090909094207372, 0.5122529647567056, 0.5130434785906977, 0.499604743412832, 0.5154150200926739], \"type\": \"scatter\", \"uid\": \"d905941a-161c-44b2-87b3-9f208592be38\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.516205533926666, 0.5264822137685633, 0.5146245062586818, 0.5067193679187609, 0.505928854131887, 0.5035573125827925, 0.4988142295788399, 0.5146245063058001, 0.4940711465748874, 0.5035573125827925, 0.5083003955867451, 0.5067193679187609, 0.5098814232547293, 0.5177865616417685, 0.5059288540847687, 0.513043478637816, 0.5067193679187609, 0.5312252967725158, 0.5209486169306186, 0.5122529647567056, 0.5138339924246897, 0.5185770754757606, 0.5169960478077764, 0.5177865616417685, 0.5193675892626344, 0.5185770754286423, 0.5043478264167846, 0.5090909094678555, 0.5177865616417685, 0.5138339924246897, 0.5098814232547293], \"type\": \"scatter\", \"uid\": \"d7f1909a-4238-4f9b-8375-05998547eb98\"}], {\"title\": {\"text\": \"Test set accuracy of padded ELMo dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"9f7ee60d-6ab6-4e26-bdb1-79b2a514303c\")) {window._Plotly.Plots.resize(document.getElementById(\"9f7ee60d-6ab6-4e26-bdb1-79b2a514303c\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"9f7ee60d-6ab6-4e26-bdb1-79b2a514303c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"9f7ee60d-6ab6-4e26-bdb1-79b2a514303c\")) {\n",
       "    Plotly.newPlot(\"9f7ee60d-6ab6-4e26-bdb1-79b2a514303c\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5177865616417685, 0.5241106722665869, 0.5114624509698318, 0.5067193676124919, 0.5067193679187609, 0.5059288540847687, 0.49802371574484783, 0.5011857710808162, 0.49169960507291105, 0.49723320191085574, 0.4932806327408953, 0.5169960478077764, 0.5154150200455556, 0.513043478637816, 0.5083003955396268, 0.5185770754286423, 0.5067193679187609, 0.5146245063058001, 0.5011857710808162, 0.5106719370887214, 0.5169960477606581, 0.5193675893097527, 0.5114624509227135, 0.5130434785906977, 0.5241106723137052, 0.5169960477606581, 0.5169960477606581, 0.5146245063058001, 0.5185770754286423, 0.5051383402507766, 0.5035573125827925], \"type\": \"scatter\", \"uid\": \"bba852b0-ed03-4ae4-872d-4cd6af458177\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5122529647567056, 0.5169960478077764, 0.5146245062586818, 0.5233201584325948, 0.5138339924246897, 0.4996047434599503, 0.49090909123891896, 0.49090909123891896, 0.5011857710808162, 0.5051383402036584, 0.5067193679187609, 0.5122529648038239, 0.5146245062586818, 0.5146245062586818, 0.516205533926666, 0.5083003955867451, 0.5304347829385238, 0.5217391307646106, 0.5106719370887214, 0.5138339924246897, 0.5209486169306186, 0.5233201584325948, 0.505138340297895, 0.5201581030966265, 0.5225296445986027, 0.5098814233018476, 0.5090909094207372, 0.5122529647567056, 0.5130434785906977, 0.499604743412832, 0.5154150200926739], \"type\": \"scatter\", \"uid\": \"d905941a-161c-44b2-87b3-9f208592be38\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.516205533926666, 0.5264822137685633, 0.5146245062586818, 0.5067193679187609, 0.505928854131887, 0.5035573125827925, 0.4988142295788399, 0.5146245063058001, 0.4940711465748874, 0.5035573125827925, 0.5083003955867451, 0.5067193679187609, 0.5098814232547293, 0.5177865616417685, 0.5059288540847687, 0.513043478637816, 0.5067193679187609, 0.5312252967725158, 0.5209486169306186, 0.5122529647567056, 0.5138339924246897, 0.5185770754757606, 0.5169960478077764, 0.5177865616417685, 0.5193675892626344, 0.5185770754286423, 0.5043478264167846, 0.5090909094678555, 0.5177865616417685, 0.5138339924246897, 0.5098814232547293], \"type\": \"scatter\", \"uid\": \"d7f1909a-4238-4f9b-8375-05998547eb98\"}], {\"title\": {\"text\": \"Test set accuracy of padded ELMo dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"9f7ee60d-6ab6-4e26-bdb1-79b2a514303c\")) {window._Plotly.Plots.resize(document.getElementById(\"9f7ee60d-6ab6-4e26-bdb1-79b2a514303c\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traces = elmo_rounds\n",
    "\n",
    "# Create traces\n",
    "def create_scatter(counter):\n",
    "    acc_dict = traces[counter]\n",
    "    \n",
    "    return go.Scatter(\n",
    "        x = list(acc_dict.keys()),\n",
    "        y = list(acc_dict.values()),\n",
    "        mode = 'lines+markers',\n",
    "        name = 'Round ' + str(counter)\n",
    "    )\n",
    "\n",
    "trace_data = [create_scatter(trace) for trace in range(len(traces))]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'Test set accuracy of padded ELMo dataset with variable maximum lengths',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = trace_data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "fill": "tozerox",
         "fillcolor": "rgba(0,100,80,0.2)",
         "line": {
          "color": "rgba(255,255,255,0)"
         },
         "name": "BERT",
         "showlegend": false,
         "type": "scatter",
         "uid": "dcf74d6a-dc4b-47af-aa65-7290dd96e8a2",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          35,
          34,
          33,
          32,
          31,
          30,
          29,
          28,
          27,
          26,
          25,
          24,
          23,
          22,
          21,
          20,
          19,
          18,
          17,
          16,
          15,
          14,
          13,
          12,
          11,
          10,
          9,
          8,
          7,
          6,
          5
         ],
         "y": [
          0.513043478637816,
          0.5122529648038239,
          0.5090909094207372,
          0.5090909091144682,
          0.5098814233018476,
          0.5122529647567056,
          0.5075098817998712,
          0.5146245063058001,
          0.505928854131887,
          0.5027667987488004,
          0.5106719368295707,
          0.5106719367824524,
          0.513043478637816,
          0.5122529648038239,
          0.5106719371358397,
          0.5177865616417685,
          0.5193675893097527,
          0.5256916999816894,
          0.5233201584797131,
          0.522529644645721,
          0.5169960478077764,
          0.521739130811729,
          0.5201581031437448,
          0.5154150201397922,
          0.5209486169777369,
          0.5169960478077764,
          0.5162055336203971,
          0.5154150201397922,
          0.5130434783315471,
          0.5154150201397922,
          0.5185770754286423,
          0.5067193679658791,
          0.49723320195797405,
          0.5043478264639029,
          0.5043478264639029,
          0.5011857711279345,
          0.5027667987959187,
          0.4996047434599503,
          0.505928854131887,
          0.5114624509698318,
          0.5106719367824524,
          0.505138340297895,
          0.5059288540847687,
          0.5106719368295707,
          0.5019762849619266,
          0.5106719371358397,
          0.5043478264167846,
          0.5051383402507766,
          0.5019762846085394,
          0.4956521739366026,
          0.49802371543857893,
          0.4996047431065631,
          0.488537549784061,
          0.4940711466220057,
          0.4996047434599503,
          0.49723320195797405,
          0.4996047434599503,
          0.49407114631573673,
          0.49090909128603727,
          0.4948616604559978,
          0.5027667984425315,
          0.49249011895402145
         ]
        },
        {
         "line": {
          "color": "rgb(0,100,80)"
         },
         "mode": "lines+markers",
         "name": "BERT",
         "type": "scatter",
         "uid": "6af4330d-88c0-46e9-8af4-397e4249f269",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5048221346936207,
          0.5081422926550326,
          0.5021343876392003,
          0.5038735179703225,
          0.5027667985226326,
          0.5067193679564556,
          0.502924901482616,
          0.5065612651378271,
          0.5000790517414982,
          0.4932806327785899,
          0.5057707512237337,
          0.504822134410911,
          0.5037154153071844,
          0.5076679844158911,
          0.5081422927869638,
          0.5119367591807024,
          0.5141501979347274,
          0.5130434785671384,
          0.5168379449184705,
          0.51478260892182,
          0.5120948619757717,
          0.5162055339031069,
          0.5171541505038973,
          0.5092490121734,
          0.5098814233018476,
          0.5097233205350491,
          0.5098814230898153,
          0.5095652176975732,
          0.5089328065691258,
          0.5076679844959922,
          0.5109881425893354
         ]
        },
        {
         "fill": "tozerox",
         "fillcolor": "rgba(0,176,246,0.2)",
         "line": {
          "color": "rgba(255,255,255,0)"
         },
         "name": "ELMo",
         "showlegend": false,
         "type": "scatter",
         "uid": "0f3f9e7f-5480-4b72-9c9c-b142638b91b0",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          35,
          34,
          33,
          32,
          31,
          30,
          29,
          28,
          27,
          26,
          25,
          24,
          23,
          22,
          21,
          20,
          19,
          18,
          17,
          16,
          15,
          14,
          13,
          12,
          11,
          10,
          9,
          8,
          7,
          6,
          5
         ],
         "y": [
          0.5177865616417685,
          0.5264822137685633,
          0.5146245062586818,
          0.5233201584325948,
          0.5138339924246897,
          0.5059288540847687,
          0.4988142295788399,
          0.5146245063058001,
          0.5011857710808162,
          0.5051383402036584,
          0.5083003955867451,
          0.5169960478077764,
          0.5154150200455556,
          0.5177865616417685,
          0.516205533926666,
          0.5185770754286423,
          0.5304347829385238,
          0.5312252967725158,
          0.5209486169306186,
          0.5138339924246897,
          0.5209486169306186,
          0.5233201584325948,
          0.5169960478077764,
          0.5201581030966265,
          0.5241106723137052,
          0.5185770754286423,
          0.5169960477606581,
          0.5146245063058001,
          0.5185770754286423,
          0.5138339924246897,
          0.5154150200926739,
          0.5035573125827925,
          0.499604743412832,
          0.5130434785906977,
          0.5090909094678555,
          0.5043478264167846,
          0.5098814233018476,
          0.5193675892626344,
          0.5130434785906977,
          0.505138340297895,
          0.5185770754757606,
          0.5138339924246897,
          0.5106719370887214,
          0.5011857710808162,
          0.5146245063058001,
          0.5067193679187609,
          0.5083003955867451,
          0.5059288540847687,
          0.513043478637816,
          0.5098814232547293,
          0.5067193679187609,
          0.4932806327408953,
          0.49723320191085574,
          0.49169960507291105,
          0.49090909123891896,
          0.49090909123891896,
          0.4996047434599503,
          0.505928854131887,
          0.5067193676124919,
          0.5114624509698318,
          0.5169960478077764,
          0.5122529647567056
         ]
        },
        {
         "line": {
          "color": "rgb(0,176,246)"
         },
         "mode": "lines+markers",
         "name": "ELMo",
         "type": "scatter",
         "uid": "2705188a-2538-4f76-bb6f-bd14d18d072c",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.51541502010838,
          0.522529644614309,
          0.5135704878290651,
          0.5122529646546159,
          0.5088274048251126,
          0.5030303033758372,
          0.4959156788542023,
          0.5022397895418451,
          0.4956521742428716,
          0.5019762848991022,
          0.5027667987488005,
          0.511989460176787,
          0.5133069831863223,
          0.5151515155127554,
          0.5101449278503538,
          0.5133069832177345,
          0.5146245062586818,
          0.522529644614309,
          0.5109354417000521,
          0.5122529647567056,
          0.5172595523719888,
          0.5204216077393694,
          0.511198946342795,
          0.5169960477763642,
          0.5220026353916475,
          0.5151515154970493,
          0.5101449278660599,
          0.511989460176787,
          0.5164690385537029,
          0.5061923586960995,
          0.5096179186433986
         ]
        }
       ],
       "layout": {
        "paper_bgcolor": "rgb(255,255,255)",
        "plot_bgcolor": "rgb(229,229,229)",
        "title": {
         "text": "Test set accuracy of padded datasets with variable maximum lengths"
        },
        "xaxis": {
         "gridcolor": "rgb(255,255,255)",
         "range": [
          5,
          35
         ],
         "showgrid": true,
         "showline": false,
         "showticklabels": true,
         "tickcolor": "rgb(127,127,127)",
         "ticks": "outside",
         "zeroline": false
        },
        "yaxis": {
         "gridcolor": "rgb(255,255,255)",
         "showgrid": true,
         "showline": false,
         "showticklabels": true,
         "tickcolor": "rgb(127,127,127)",
         "ticks": "outside",
         "zeroline": false
        }
       }
      },
      "text/html": [
       "<div id=\"d168741c-8320-4c3c-85d3-c8a7104bdba5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"d168741c-8320-4c3c-85d3-c8a7104bdba5\")) {\n",
       "    Plotly.newPlot(\"d168741c-8320-4c3c-85d3-c8a7104bdba5\", [{\"fill\": \"tozerox\", \"fillcolor\": \"rgba(0,100,80,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"BERT\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.513043478637816, 0.5122529648038239, 0.5090909094207372, 0.5090909091144682, 0.5098814233018476, 0.5122529647567056, 0.5075098817998712, 0.5146245063058001, 0.505928854131887, 0.5027667987488004, 0.5106719368295707, 0.5106719367824524, 0.513043478637816, 0.5122529648038239, 0.5106719371358397, 0.5177865616417685, 0.5193675893097527, 0.5256916999816894, 0.5233201584797131, 0.522529644645721, 0.5169960478077764, 0.521739130811729, 0.5201581031437448, 0.5154150201397922, 0.5209486169777369, 0.5169960478077764, 0.5162055336203971, 0.5154150201397922, 0.5130434783315471, 0.5154150201397922, 0.5185770754286423, 0.5067193679658791, 0.49723320195797405, 0.5043478264639029, 0.5043478264639029, 0.5011857711279345, 0.5027667987959187, 0.4996047434599503, 0.505928854131887, 0.5114624509698318, 0.5106719367824524, 0.505138340297895, 0.5059288540847687, 0.5106719368295707, 0.5019762849619266, 0.5106719371358397, 0.5043478264167846, 0.5051383402507766, 0.5019762846085394, 0.4956521739366026, 0.49802371543857893, 0.4996047431065631, 0.488537549784061, 0.4940711466220057, 0.4996047434599503, 0.49723320195797405, 0.4996047434599503, 0.49407114631573673, 0.49090909128603727, 0.4948616604559978, 0.5027667984425315, 0.49249011895402145], \"type\": \"scatter\", \"uid\": \"dcf74d6a-dc4b-47af-aa65-7290dd96e8a2\"}, {\"line\": {\"color\": \"rgb(0,100,80)\"}, \"mode\": \"lines+markers\", \"name\": \"BERT\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5048221346936207, 0.5081422926550326, 0.5021343876392003, 0.5038735179703225, 0.5027667985226326, 0.5067193679564556, 0.502924901482616, 0.5065612651378271, 0.5000790517414982, 0.4932806327785899, 0.5057707512237337, 0.504822134410911, 0.5037154153071844, 0.5076679844158911, 0.5081422927869638, 0.5119367591807024, 0.5141501979347274, 0.5130434785671384, 0.5168379449184705, 0.51478260892182, 0.5120948619757717, 0.5162055339031069, 0.5171541505038973, 0.5092490121734, 0.5098814233018476, 0.5097233205350491, 0.5098814230898153, 0.5095652176975732, 0.5089328065691258, 0.5076679844959922, 0.5109881425893354], \"type\": \"scatter\", \"uid\": \"6af4330d-88c0-46e9-8af4-397e4249f269\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(0,176,246,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"ELMo\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.5177865616417685, 0.5264822137685633, 0.5146245062586818, 0.5233201584325948, 0.5138339924246897, 0.5059288540847687, 0.4988142295788399, 0.5146245063058001, 0.5011857710808162, 0.5051383402036584, 0.5083003955867451, 0.5169960478077764, 0.5154150200455556, 0.5177865616417685, 0.516205533926666, 0.5185770754286423, 0.5304347829385238, 0.5312252967725158, 0.5209486169306186, 0.5138339924246897, 0.5209486169306186, 0.5233201584325948, 0.5169960478077764, 0.5201581030966265, 0.5241106723137052, 0.5185770754286423, 0.5169960477606581, 0.5146245063058001, 0.5185770754286423, 0.5138339924246897, 0.5154150200926739, 0.5035573125827925, 0.499604743412832, 0.5130434785906977, 0.5090909094678555, 0.5043478264167846, 0.5098814233018476, 0.5193675892626344, 0.5130434785906977, 0.505138340297895, 0.5185770754757606, 0.5138339924246897, 0.5106719370887214, 0.5011857710808162, 0.5146245063058001, 0.5067193679187609, 0.5083003955867451, 0.5059288540847687, 0.513043478637816, 0.5098814232547293, 0.5067193679187609, 0.4932806327408953, 0.49723320191085574, 0.49169960507291105, 0.49090909123891896, 0.49090909123891896, 0.4996047434599503, 0.505928854131887, 0.5067193676124919, 0.5114624509698318, 0.5169960478077764, 0.5122529647567056], \"type\": \"scatter\", \"uid\": \"0f3f9e7f-5480-4b72-9c9c-b142638b91b0\"}, {\"line\": {\"color\": \"rgb(0,176,246)\"}, \"mode\": \"lines+markers\", \"name\": \"ELMo\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.51541502010838, 0.522529644614309, 0.5135704878290651, 0.5122529646546159, 0.5088274048251126, 0.5030303033758372, 0.4959156788542023, 0.5022397895418451, 0.4956521742428716, 0.5019762848991022, 0.5027667987488005, 0.511989460176787, 0.5133069831863223, 0.5151515155127554, 0.5101449278503538, 0.5133069832177345, 0.5146245062586818, 0.522529644614309, 0.5109354417000521, 0.5122529647567056, 0.5172595523719888, 0.5204216077393694, 0.511198946342795, 0.5169960477763642, 0.5220026353916475, 0.5151515154970493, 0.5101449278660599, 0.511989460176787, 0.5164690385537029, 0.5061923586960995, 0.5096179186433986], \"type\": \"scatter\", \"uid\": \"2705188a-2538-4f76-bb6f-bd14d18d072c\"}], {\"paper_bgcolor\": \"rgb(255,255,255)\", \"plot_bgcolor\": \"rgb(229,229,229)\", \"title\": {\"text\": \"Test set accuracy of padded datasets with variable maximum lengths\"}, \"xaxis\": {\"gridcolor\": \"rgb(255,255,255)\", \"range\": [5, 35], \"showgrid\": true, \"showline\": false, \"showticklabels\": true, \"tickcolor\": \"rgb(127,127,127)\", \"ticks\": \"outside\", \"zeroline\": false}, \"yaxis\": {\"gridcolor\": \"rgb(255,255,255)\", \"showgrid\": true, \"showline\": false, \"showticklabels\": true, \"tickcolor\": \"rgb(127,127,127)\", \"ticks\": \"outside\", \"zeroline\": false}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"d168741c-8320-4c3c-85d3-c8a7104bdba5\")) {window._Plotly.Plots.resize(document.getElementById(\"d168741c-8320-4c3c-85d3-c8a7104bdba5\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"d168741c-8320-4c3c-85d3-c8a7104bdba5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"d168741c-8320-4c3c-85d3-c8a7104bdba5\")) {\n",
       "    Plotly.newPlot(\"d168741c-8320-4c3c-85d3-c8a7104bdba5\", [{\"fill\": \"tozerox\", \"fillcolor\": \"rgba(0,100,80,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"BERT\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.513043478637816, 0.5122529648038239, 0.5090909094207372, 0.5090909091144682, 0.5098814233018476, 0.5122529647567056, 0.5075098817998712, 0.5146245063058001, 0.505928854131887, 0.5027667987488004, 0.5106719368295707, 0.5106719367824524, 0.513043478637816, 0.5122529648038239, 0.5106719371358397, 0.5177865616417685, 0.5193675893097527, 0.5256916999816894, 0.5233201584797131, 0.522529644645721, 0.5169960478077764, 0.521739130811729, 0.5201581031437448, 0.5154150201397922, 0.5209486169777369, 0.5169960478077764, 0.5162055336203971, 0.5154150201397922, 0.5130434783315471, 0.5154150201397922, 0.5185770754286423, 0.5067193679658791, 0.49723320195797405, 0.5043478264639029, 0.5043478264639029, 0.5011857711279345, 0.5027667987959187, 0.4996047434599503, 0.505928854131887, 0.5114624509698318, 0.5106719367824524, 0.505138340297895, 0.5059288540847687, 0.5106719368295707, 0.5019762849619266, 0.5106719371358397, 0.5043478264167846, 0.5051383402507766, 0.5019762846085394, 0.4956521739366026, 0.49802371543857893, 0.4996047431065631, 0.488537549784061, 0.4940711466220057, 0.4996047434599503, 0.49723320195797405, 0.4996047434599503, 0.49407114631573673, 0.49090909128603727, 0.4948616604559978, 0.5027667984425315, 0.49249011895402145], \"type\": \"scatter\", \"uid\": \"dcf74d6a-dc4b-47af-aa65-7290dd96e8a2\"}, {\"line\": {\"color\": \"rgb(0,100,80)\"}, \"mode\": \"lines+markers\", \"name\": \"BERT\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5048221346936207, 0.5081422926550326, 0.5021343876392003, 0.5038735179703225, 0.5027667985226326, 0.5067193679564556, 0.502924901482616, 0.5065612651378271, 0.5000790517414982, 0.4932806327785899, 0.5057707512237337, 0.504822134410911, 0.5037154153071844, 0.5076679844158911, 0.5081422927869638, 0.5119367591807024, 0.5141501979347274, 0.5130434785671384, 0.5168379449184705, 0.51478260892182, 0.5120948619757717, 0.5162055339031069, 0.5171541505038973, 0.5092490121734, 0.5098814233018476, 0.5097233205350491, 0.5098814230898153, 0.5095652176975732, 0.5089328065691258, 0.5076679844959922, 0.5109881425893354], \"type\": \"scatter\", \"uid\": \"6af4330d-88c0-46e9-8af4-397e4249f269\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(0,176,246,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"ELMo\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.5177865616417685, 0.5264822137685633, 0.5146245062586818, 0.5233201584325948, 0.5138339924246897, 0.5059288540847687, 0.4988142295788399, 0.5146245063058001, 0.5011857710808162, 0.5051383402036584, 0.5083003955867451, 0.5169960478077764, 0.5154150200455556, 0.5177865616417685, 0.516205533926666, 0.5185770754286423, 0.5304347829385238, 0.5312252967725158, 0.5209486169306186, 0.5138339924246897, 0.5209486169306186, 0.5233201584325948, 0.5169960478077764, 0.5201581030966265, 0.5241106723137052, 0.5185770754286423, 0.5169960477606581, 0.5146245063058001, 0.5185770754286423, 0.5138339924246897, 0.5154150200926739, 0.5035573125827925, 0.499604743412832, 0.5130434785906977, 0.5090909094678555, 0.5043478264167846, 0.5098814233018476, 0.5193675892626344, 0.5130434785906977, 0.505138340297895, 0.5185770754757606, 0.5138339924246897, 0.5106719370887214, 0.5011857710808162, 0.5146245063058001, 0.5067193679187609, 0.5083003955867451, 0.5059288540847687, 0.513043478637816, 0.5098814232547293, 0.5067193679187609, 0.4932806327408953, 0.49723320191085574, 0.49169960507291105, 0.49090909123891896, 0.49090909123891896, 0.4996047434599503, 0.505928854131887, 0.5067193676124919, 0.5114624509698318, 0.5169960478077764, 0.5122529647567056], \"type\": \"scatter\", \"uid\": \"0f3f9e7f-5480-4b72-9c9c-b142638b91b0\"}, {\"line\": {\"color\": \"rgb(0,176,246)\"}, \"mode\": \"lines+markers\", \"name\": \"ELMo\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.51541502010838, 0.522529644614309, 0.5135704878290651, 0.5122529646546159, 0.5088274048251126, 0.5030303033758372, 0.4959156788542023, 0.5022397895418451, 0.4956521742428716, 0.5019762848991022, 0.5027667987488005, 0.511989460176787, 0.5133069831863223, 0.5151515155127554, 0.5101449278503538, 0.5133069832177345, 0.5146245062586818, 0.522529644614309, 0.5109354417000521, 0.5122529647567056, 0.5172595523719888, 0.5204216077393694, 0.511198946342795, 0.5169960477763642, 0.5220026353916475, 0.5151515154970493, 0.5101449278660599, 0.511989460176787, 0.5164690385537029, 0.5061923586960995, 0.5096179186433986], \"type\": \"scatter\", \"uid\": \"2705188a-2538-4f76-bb6f-bd14d18d072c\"}], {\"paper_bgcolor\": \"rgb(255,255,255)\", \"plot_bgcolor\": \"rgb(229,229,229)\", \"title\": {\"text\": \"Test set accuracy of padded datasets with variable maximum lengths\"}, \"xaxis\": {\"gridcolor\": \"rgb(255,255,255)\", \"range\": [5, 35], \"showgrid\": true, \"showline\": false, \"showticklabels\": true, \"tickcolor\": \"rgb(127,127,127)\", \"ticks\": \"outside\", \"zeroline\": false}, \"yaxis\": {\"gridcolor\": \"rgb(255,255,255)\", \"showgrid\": true, \"showline\": false, \"showticklabels\": true, \"tickcolor\": \"rgb(127,127,127)\", \"ticks\": \"outside\", \"zeroline\": false}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"d168741c-8320-4c3c-85d3-c8a7104bdba5\")) {window._Plotly.Plots.resize(document.getElementById(\"d168741c-8320-4c3c-85d3-c8a7104bdba5\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(5, 36))\n",
    "x_rev = x[::-1]\n",
    "\n",
    "# BERT\n",
    "bert_matrix = np.transpose(np.array([np.array(list(acc_round.values())) for acc_round in bert_rounds]))\n",
    "bert_y = [np.average(row) for row in bert_matrix]\n",
    "bert_y_upper = [row.max() for row in bert_matrix]\n",
    "bert_y_lower = [row.min() for row in bert_matrix]\n",
    "bert_y_lower = bert_y_lower[::-1]\n",
    "\n",
    "bert1 = go.Scatter(\n",
    "    x = x + x_rev,\n",
    "    y = bert_y_upper + bert_y_lower,\n",
    "    fill = 'tozerox',\n",
    "    fillcolor = 'rgba(0,100,80,0.2)',\n",
    "    line = dict(color = 'rgba(255,255,255,0)'),\n",
    "    showlegend = False,\n",
    "    name = 'BERT',\n",
    ")\n",
    "bert2 = go.Scatter(\n",
    "    x = x,\n",
    "    y = bert_y,\n",
    "    line = dict(color='rgb(0,100,80)'),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'BERT',\n",
    ")\n",
    "\n",
    "# ELMo\n",
    "elmo_matrix = np.transpose(np.array([np.array(list(acc_round.values())) for acc_round in elmo_rounds]))\n",
    "elmo_y = [np.average(row) for row in elmo_matrix]\n",
    "elmo_y_upper = [row.max() for row in elmo_matrix]\n",
    "elmo_y_lower = [row.min() for row in elmo_matrix]\n",
    "elmo_y_lower = elmo_y_lower[::-1]\n",
    "\n",
    "elmo1 = go.Scatter(\n",
    "    x = x + x_rev,\n",
    "    y = elmo_y_upper + elmo_y_lower,\n",
    "    fill = 'tozerox',\n",
    "    fillcolor = 'rgba(0,176,246,0.2)',\n",
    "    line = dict(color = 'rgba(255,255,255,0)'),\n",
    "    showlegend = False,\n",
    "    name = 'ELMo',\n",
    ")\n",
    "elmo2 = go.Scatter(\n",
    "    x = x,\n",
    "    y = elmo_y,\n",
    "    line = dict(color='rgb(0,176,246)'),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'ELMo',\n",
    ")\n",
    "\n",
    "\n",
    "data = [bert1, bert2, elmo1, elmo2]\n",
    "layout = go.Layout(\n",
    "    title = 'Test set accuracy of padded datasets with variable maximum lengths',\n",
    "    paper_bgcolor = 'rgb(255,255,255)',\n",
    "    plot_bgcolor = 'rgb(229,229,229)',\n",
    "    xaxis = dict(\n",
    "        gridcolor = 'rgb(255,255,255)',\n",
    "        range = [5,35],\n",
    "        showgrid = True,\n",
    "        showline = False,\n",
    "        showticklabels = True,\n",
    "        tickcolor = 'rgb(127,127,127)',\n",
    "        ticks = 'outside',\n",
    "        zeroline = False\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        gridcolor='rgb(255,255,255)',\n",
    "        showgrid = True,\n",
    "        showline = False,\n",
    "        showticklabels = True,\n",
    "        tickcolor = 'rgb(127,127,127)',\n",
    "        ticks = 'outside',\n",
    "        zeroline = False\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Convolutional neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_score(X_train, X_test, X_validation, y_train = general['train']['label'], y_test = general['test']['label'], y_validation = general['validation']['label']):\n",
    "    # Rearrange data types    \n",
    "    params = locals().copy()    \n",
    "    inputs = {\n",
    "        dataset: np.array(params[dataset])\n",
    "        for dataset in params.keys()\n",
    "    }\n",
    "    \n",
    "    # Reshape datasets\n",
    "    for dataset in inputs.keys():\n",
    "        if dataset[0:1] == 'X':\n",
    "            inputs[dataset] = np.reshape(inputs[dataset], (inputs[dataset].shape[0], inputs[dataset].shape[1], 1))\n",
    "        elif dataset[0:1] == 'y':\n",
    "            inputs[dataset] = np_utils.to_categorical(np.array(inputs[dataset]), 3)\n",
    "    \n",
    "    # Set model parameters\n",
    "    epochs = 5\n",
    "    batch_size = 32\n",
    "    input_shape =  inputs['X_train'].shape\n",
    "    print(inputs['X_train'].shape)\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape = input_shape))\n",
    "    #model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.25))\n",
    "    #model.add(Flatten())\n",
    "    #model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "    model.compile('sgd', 'categorical_crossentropy', metrics = ['accuracy']) \n",
    "    \n",
    "    # Fit the training set over the model and correct on the validation set\n",
    "    model.fit(inputs['X_train'], inputs['y_train'],\n",
    "            batch_size = batch_size,\n",
    "            epochs = epochs,\n",
    "            validation_data = (inputs['X_validation'], inputs['y_validation']))\n",
    "    \n",
    "    # Get score over the test set\n",
    "    score, acc = model.evaluate(inputs['X_test'], inputs['y_test'])\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cnn_score(max_pooled_bert['train'], max_pooled_bert['test'], max_pooled_bert['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### References\n",
    "\n",
    "```\n",
    "@article{DBLP:journals/corr/Wang17j,\n",
    "  author    = {William Yang Wang},\n",
    "  title     = {\"Liar, Liar Pants on Fire\": {A} New Benchmark Dataset for Fake News\n",
    "               Detection},\n",
    "  journal   = {CoRR},\n",
    "  volume    = {abs/1705.00648},\n",
    "  year      = {2017},\n",
    "  url       = {http://arxiv.org/abs/1705.00648},\n",
    "  archivePrefix = {arXiv},\n",
    "  eprint    = {1705.00648},\n",
    "  timestamp = {Mon, 13 Aug 2018 16:48:58 +0200},\n",
    "  biburl    = {https://dblp.org/rec/bib/journals/corr/Wang17j},\n",
    "  bibsource = {dblp computer science bibliography, https://dblp.org}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
