{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using neural classification\n",
    "As has been proven by [Wang (2017)](https://arxiv.org/abs/1705.00648), neural classifiers carry better results than non-neural classifiers when detecting fake news. However, it is unknown how well neural networks classify fake news when using previously mentioned text embeddings. \n",
    "In this notebook, the second research question will be answered: *how well do neural network architecture classify fake news compared to non-neural classification algorithms?*\n",
    "\n",
    "<hr>\n",
    "\n",
    "## On the usage of neural networks\n",
    "Literature on CNNs and Bi-LSTMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Reshape, Conv1D, Flatten\n",
    "\n",
    "# Set offline mode for plotly\n",
    "init_notebook_mode(connected = True)\n",
    "\n",
    "# The DataLoader class gives access to pretrained vectors from the Liar dataset\n",
    "from data_loader import DataLoader\n",
    "data = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "general = data.get_dfs()\n",
    "\n",
    "# Recode labels from 6 to 3\n",
    "def recode(label):\n",
    "    if label == 'false' or label == 'pants-fire' or label == 'barely-true':\n",
    "        return 0\n",
    "    elif label == 'true' or label == 'mostly-true':\n",
    "        return 2\n",
    "    elif label == 'half-true':\n",
    "        return 1\n",
    "\n",
    "for dataset in general.keys():\n",
    "    general[dataset]['label'] = general[dataset]['label'].apply(lambda label: recode(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Bidirectional LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = data.get_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max-pooled BERT embeddings from RQ1\n",
    "def max_pool(statement):\n",
    "    if len(statement) > 1:\n",
    "        return [row.max() for row in np.transpose([[token_row.max() for token_row in np.transpose(np.array(sentence))] for sentence in statement])]\n",
    "    else:\n",
    "        return [token_row.max() for token_row in np.transpose(statement[0])]\n",
    "\n",
    "max_pooled_bert = {\n",
    "    dataset: pd.DataFrame(list(bert[dataset].statement.apply(lambda statement: max_pool(statement)).values))\n",
    "    for dataset in bert.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_score(X_train, X_test, X_validation, y_train = general['train']['label'], y_test = general['test']['label'], y_validation = general['validation']['label'], reshape = True):\n",
    "    # Rearrange data types    \n",
    "    params = locals().copy()    \n",
    "    inputs = {\n",
    "        dataset: np.array(params[dataset])\n",
    "        for dataset in params.keys()\n",
    "    }\n",
    "    \n",
    "    for dataset in inputs.keys():\n",
    "        if dataset[0:1] == 'X' and reshape:\n",
    "            # Reshape datasets from 2D to 3D\n",
    "            inputs[dataset] = np.reshape(inputs[dataset], (inputs[dataset].shape[0], inputs[dataset].shape[1], 1))\n",
    "        elif dataset[0:1] == 'y':\n",
    "            inputs[dataset] = np_utils.to_categorical(np.array(inputs[dataset]), 3)\n",
    "    \n",
    "    # Set model parameters\n",
    "    epochs = 5\n",
    "    batch_size = 32\n",
    "    input_shape = X_train.shape\n",
    "\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, input_shape = input_shape)))\n",
    "    model.add(Dropout(0.8))\n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "    model.compile('sgd', 'categorical_crossentropy', metrics = ['accuracy']) \n",
    "    \n",
    "    # Fit the training set over the model and correct on the validation set\n",
    "    model.fit(inputs['X_train'], inputs['y_train'],\n",
    "            batch_size = batch_size,\n",
    "            epochs = epochs,\n",
    "            validation_data = (inputs['X_validation'], inputs['y_validation']))\n",
    "    \n",
    "    # Get score over the test set\n",
    "    score, acc = model.evaluate(inputs['X_test'], inputs['y_test'])\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "2019-05-21 12:40:37,047 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "2019-05-21 12:40:37,448 From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-05-21 12:40:37,537 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 829s 81ms/step - loss: 1.0743 - acc: 0.4087 - val_loss: 1.0423 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 844s 82ms/step - loss: 1.0632 - acc: 0.4300 - val_loss: 1.0392 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 838s 82ms/step - loss: 1.0590 - acc: 0.4354 - val_loss: 1.0408 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 840s 82ms/step - loss: 1.0592 - acc: 0.4354 - val_loss: 1.0432 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 809s 79ms/step - loss: 1.0591 - acc: 0.4337 - val_loss: 1.0395 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 16s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.43715415052745654"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bilstm_score(max_pooled_bert['train'], max_pooled_bert['test'], max_pooled_bert['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, the condensed datasets from RQ1 do not perform well when using a neural classifier. The next step is trying out a padding approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.1184 - acc: 0.4145 - val_loss: 1.0170 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0597 - acc: 0.4532 - val_loss: 1.0196 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0432 - acc: 0.4707 - val_loss: 1.0158 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0362 - acc: 0.4775 - val_loss: 1.0134 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0279 - acc: 0.4821 - val_loss: 1.0089 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.1094 - acc: 0.4181 - val_loss: 1.0184 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0558 - acc: 0.4556 - val_loss: 1.0125 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0428 - acc: 0.4693 - val_loss: 1.0125 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0332 - acc: 0.4863 - val_loss: 1.0140 - val_acc: 0.5008\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0303 - acc: 0.4824 - val_loss: 1.0051 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 1s 594us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.1187 - acc: 0.4151 - val_loss: 1.0248 - val_acc: 0.5109\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0541 - acc: 0.4529 - val_loss: 1.0166 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0422 - acc: 0.4718 - val_loss: 1.0122 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0347 - acc: 0.4808 - val_loss: 1.0118 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0292 - acc: 0.4889 - val_loss: 1.0056 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 1s 969us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.1133 - acc: 0.4260 - val_loss: 1.0178 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0581 - acc: 0.4538 - val_loss: 1.0119 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0404 - acc: 0.4719 - val_loss: 1.0130 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0340 - acc: 0.4828 - val_loss: 1.0065 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0276 - acc: 0.4863 - val_loss: 1.0027 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 1s 799us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.1089 - acc: 0.4181 - val_loss: 1.0162 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0551 - acc: 0.4594 - val_loss: 1.0202 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0381 - acc: 0.4771 - val_loss: 1.0091 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0320 - acc: 0.4829 - val_loss: 1.0067 - val_acc: 0.5000\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0247 - acc: 0.4908 - val_loss: 1.0029 - val_acc: 0.5016\n",
      "1265/1265 [==============================] - 1s 846us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.1186 - acc: 0.4209 - val_loss: 1.0138 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0519 - acc: 0.4614 - val_loss: 1.0098 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0398 - acc: 0.4782 - val_loss: 1.0044 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0280 - acc: 0.4896 - val_loss: 1.0080 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0241 - acc: 0.4943 - val_loss: 1.0050 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 1s 929us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.1218 - acc: 0.4208 - val_loss: 1.0134 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0544 - acc: 0.4600 - val_loss: 1.0069 - val_acc: 0.5202\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0398 - acc: 0.4773 - val_loss: 1.0043 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0296 - acc: 0.4862 - val_loss: 1.0027 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0265 - acc: 0.4872 - val_loss: 0.9985 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 1.1138 - acc: 0.4279 - val_loss: 1.0142 - val_acc: 0.5109\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0477 - acc: 0.4682 - val_loss: 1.0073 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0368 - acc: 0.4805 - val_loss: 1.0111 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0281 - acc: 0.4867 - val_loss: 1.0011 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0280 - acc: 0.4947 - val_loss: 1.0041 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 80s 8ms/step - loss: 1.1205 - acc: 0.4236 - val_loss: 1.0263 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0541 - acc: 0.4649 - val_loss: 1.0081 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0391 - acc: 0.4769 - val_loss: 1.0083 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0296 - acc: 0.4863 - val_loss: 1.0060 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0239 - acc: 0.4927 - val_loss: 1.0026 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 86s 8ms/step - loss: 1.1171 - acc: 0.4152 - val_loss: 1.0107 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0496 - acc: 0.4637 - val_loss: 1.0054 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0413 - acc: 0.4780 - val_loss: 1.0082 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0297 - acc: 0.4904 - val_loss: 1.0004 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0234 - acc: 0.4949 - val_loss: 0.9993 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 92s 9ms/step - loss: 1.1165 - acc: 0.4206 - val_loss: 1.0214 - val_acc: 0.5148\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0553 - acc: 0.4656 - val_loss: 1.0178 - val_acc: 0.5109\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0350 - acc: 0.4852 - val_loss: 1.0104 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0312 - acc: 0.4890 - val_loss: 1.0051 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0229 - acc: 0.4910 - val_loss: 1.0055 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 91s 9ms/step - loss: 1.1168 - acc: 0.4210 - val_loss: 1.0127 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0508 - acc: 0.4668 - val_loss: 1.0166 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0364 - acc: 0.4793 - val_loss: 1.0032 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0281 - acc: 0.4893 - val_loss: 1.0000 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0228 - acc: 0.4973 - val_loss: 1.0013 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 98s 10ms/step - loss: 1.1218 - acc: 0.4131 - val_loss: 1.0302 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0546 - acc: 0.4616 - val_loss: 1.0226 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 36s 4ms/step - loss: 1.0387 - acc: 0.4726 - val_loss: 1.0115 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0314 - acc: 0.4861 - val_loss: 1.0049 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0245 - acc: 0.4940 - val_loss: 1.0055 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 103s 10ms/step - loss: 1.1113 - acc: 0.4235 - val_loss: 1.0184 - val_acc: 0.5234\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0534 - acc: 0.4643 - val_loss: 1.0104 - val_acc: 0.5226\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 36s 3ms/step - loss: 1.0409 - acc: 0.4802 - val_loss: 1.0079 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 36s 4ms/step - loss: 1.0321 - acc: 0.4882 - val_loss: 1.0040 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0239 - acc: 0.5000 - val_loss: 1.0034 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 101s 10ms/step - loss: 1.1158 - acc: 0.4191 - val_loss: 1.0152 - val_acc: 0.5226\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0550 - acc: 0.4584 - val_loss: 1.0119 - val_acc: 0.5249\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0378 - acc: 0.4787 - val_loss: 1.0057 - val_acc: 0.5296\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0310 - acc: 0.4812 - val_loss: 1.0030 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0227 - acc: 0.4948 - val_loss: 0.9970 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 109s 11ms/step - loss: 1.1228 - acc: 0.4128 - val_loss: 1.0199 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0554 - acc: 0.4577 - val_loss: 1.0189 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0387 - acc: 0.4784 - val_loss: 1.0061 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0341 - acc: 0.4836 - val_loss: 1.0074 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0242 - acc: 0.4958 - val_loss: 1.0025 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 111s 11ms/step - loss: 1.1078 - acc: 0.4219 - val_loss: 1.0287 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0534 - acc: 0.4610 - val_loss: 1.0140 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0422 - acc: 0.4721 - val_loss: 1.0134 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0265 - acc: 0.4914 - val_loss: 1.0109 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0268 - acc: 0.4949 - val_loss: 1.0093 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.1146 - acc: 0.4152 - val_loss: 1.0207 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0577 - acc: 0.4632 - val_loss: 1.0113 - val_acc: 0.5249\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0401 - acc: 0.4759 - val_loss: 1.0133 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0335 - acc: 0.4819 - val_loss: 1.0071 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0266 - acc: 0.4901 - val_loss: 1.0044 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 6s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 110s 11ms/step - loss: 1.1089 - acc: 0.4237 - val_loss: 1.0265 - val_acc: 0.5140\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0583 - acc: 0.4500 - val_loss: 1.0147 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0430 - acc: 0.4778 - val_loss: 1.0108 - val_acc: 0.5241\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0318 - acc: 0.4808 - val_loss: 1.0067 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0303 - acc: 0.4843 - val_loss: 1.0078 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.1008 - acc: 0.4262 - val_loss: 1.0131 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0570 - acc: 0.4583 - val_loss: 1.0094 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0434 - acc: 0.4738 - val_loss: 1.0143 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0325 - acc: 0.4833 - val_loss: 1.0063 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0260 - acc: 0.4900 - val_loss: 1.0022 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.1036 - acc: 0.4241 - val_loss: 1.0248 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0551 - acc: 0.4624 - val_loss: 1.0112 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0423 - acc: 0.4752 - val_loss: 1.0161 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0296 - acc: 0.4879 - val_loss: 1.0038 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0270 - acc: 0.4869 - val_loss: 1.0063 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.1012 - acc: 0.4247 - val_loss: 1.0202 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0566 - acc: 0.4576 - val_loss: 1.0107 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0400 - acc: 0.4775 - val_loss: 1.0086 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0354 - acc: 0.4865 - val_loss: 1.0027 - val_acc: 0.5265\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0266 - acc: 0.4919 - val_loss: 1.0065 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0999 - acc: 0.4246 - val_loss: 1.0188 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0588 - acc: 0.4596 - val_loss: 1.0145 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0415 - acc: 0.4750 - val_loss: 1.0089 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0319 - acc: 0.4848 - val_loss: 1.0036 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0260 - acc: 0.4872 - val_loss: 1.0034 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 133s 13ms/step - loss: 1.1086 - acc: 0.4064 - val_loss: 1.0186 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0563 - acc: 0.4585 - val_loss: 1.0152 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0411 - acc: 0.4723 - val_loss: 1.0108 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0383 - acc: 0.4797 - val_loss: 1.0096 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0304 - acc: 0.4826 - val_loss: 1.0036 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0960 - acc: 0.4262 - val_loss: 1.0261 - val_acc: 0.4774\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0572 - acc: 0.4566 - val_loss: 1.0126 - val_acc: 0.5202\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0404 - acc: 0.4806 - val_loss: 1.0154 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0341 - acc: 0.4777 - val_loss: 1.0074 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0242 - acc: 0.4919 - val_loss: 1.0010 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 1.0877 - acc: 0.4269 - val_loss: 1.0218 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0545 - acc: 0.4550 - val_loss: 1.0150 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0455 - acc: 0.4722 - val_loss: 1.0101 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0369 - acc: 0.4793 - val_loss: 1.0025 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0328 - acc: 0.4806 - val_loss: 1.0035 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 1.0966 - acc: 0.4192 - val_loss: 1.0159 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0535 - acc: 0.4648 - val_loss: 1.0140 - val_acc: 0.5179\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0407 - acc: 0.4737 - val_loss: 1.0057 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0344 - acc: 0.4890 - val_loss: 1.0030 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0281 - acc: 0.4929 - val_loss: 1.0092 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0952 - acc: 0.4262 - val_loss: 1.0169 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.0548 - acc: 0.4621 - val_loss: 1.0139 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0452 - acc: 0.4678 - val_loss: 1.0068 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0353 - acc: 0.4783 - val_loss: 1.0042 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0292 - acc: 0.4875 - val_loss: 0.9996 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 138s 14ms/step - loss: 1.0938 - acc: 0.4192 - val_loss: 1.0201 - val_acc: 0.5156\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 1.0523 - acc: 0.4641 - val_loss: 1.0134 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0419 - acc: 0.4745 - val_loss: 1.0075 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0364 - acc: 0.4815 - val_loss: 1.0043 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0280 - acc: 0.4910 - val_loss: 1.0054 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 1.1040 - acc: 0.4251 - val_loss: 1.0204 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 77s 8ms/step - loss: 1.0522 - acc: 0.4651 - val_loss: 1.0147 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0452 - acc: 0.4717 - val_loss: 1.0112 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0349 - acc: 0.4837 - val_loss: 1.0142 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0292 - acc: 0.4872 - val_loss: 0.9990 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 147s 14ms/step - loss: 1.0870 - acc: 0.4226 - val_loss: 1.0187 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 79s 8ms/step - loss: 1.0529 - acc: 0.4603 - val_loss: 1.0116 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.0456 - acc: 0.4740 - val_loss: 1.0122 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0368 - acc: 0.4828 - val_loss: 1.0076 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.0324 - acc: 0.4843 - val_loss: 1.0060 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "CPU times: user 8h 57s, sys: 2h 14min 29s, total: 10h 15min 26s\n",
      "Wall time: 2h 44min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Store accuracies\n",
    "accuracies = {\n",
    "    padding_len: 0.0 for padding_len in list(range(5,36))\n",
    "}\n",
    "\n",
    "concatenated_bert = {\n",
    "    dataset: [np.concatenate(np.array(statement)) for statement in bert[dataset].statement]\n",
    "    for dataset in bert.keys()\n",
    "}\n",
    "\n",
    "for max_len in accuracies.keys():\n",
    "    padded_bert = {\n",
    "        dataset: sequence.pad_sequences(concatenated_bert[dataset], maxlen = max_len, dtype = float)\n",
    "        for dataset in concatenated_bert.keys()\n",
    "    }\n",
    "    \n",
    "    accuracies[max_len] = get_bilstm_score(padded_bert['train'], padded_bert['test'], padded_bert['validation'], reshape = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{5: 0.513043478637816,\n",
       "  6: 0.5122529648038239,\n",
       "  7: 0.5075098817998712,\n",
       "  8: 0.5083003953747127,\n",
       "  9: 0.49407114631573673,\n",
       "  10: 0.5098814233018476,\n",
       "  11: 0.507509881752753,\n",
       "  12: 0.5035573126299108,\n",
       "  13: 0.4996047434599503,\n",
       "  14: 0.5027667987488004,\n",
       "  15: 0.507509881752753,\n",
       "  16: 0.5090909091144682,\n",
       "  17: 0.5067193679187609,\n",
       "  18: 0.5114624509227135,\n",
       "  19: 0.5098814233018476,\n",
       "  20: 0.5162055336203971,\n",
       "  21: 0.5193675893097527,\n",
       "  22: 0.5193675893097527,\n",
       "  23: 0.5233201584797131,\n",
       "  24: 0.5217391304583417,\n",
       "  25: 0.5106719371358397,\n",
       "  26: 0.5106719367824524,\n",
       "  27: 0.5177865616417685,\n",
       "  28: 0.5075098817998712,\n",
       "  29: 0.513043478637816,\n",
       "  30: 0.5043478264639029,\n",
       "  31: 0.5162055336203971,\n",
       "  32: 0.5075098817998712,\n",
       "  33: 0.5114624509698318,\n",
       "  34: 0.513833992471808,\n",
       "  35: 0.5075098817998712},\n",
       " {5: 0.5098814233018476,\n",
       "  6: 0.5098814229484603,\n",
       "  7: 0.49723320160458684,\n",
       "  8: 0.5090909091144682,\n",
       "  9: 0.5051383399445077,\n",
       "  10: 0.505928854131887,\n",
       "  11: 0.498814229272571,\n",
       "  12: 0.5114624509698318,\n",
       "  13: 0.4940711466220057,\n",
       "  14: 0.49169960512002936,\n",
       "  15: 0.4996047431065631,\n",
       "  16: 0.5027667984425315,\n",
       "  17: 0.4956521739366026,\n",
       "  18: 0.5067193679658791,\n",
       "  19: 0.5067193676124919,\n",
       "  20: 0.5043478264167846,\n",
       "  21: 0.5146245063058001,\n",
       "  22: 0.5256916999816894,\n",
       "  23: 0.5201581031437448,\n",
       "  24: 0.515415019786405,\n",
       "  25: 0.5169960478077764,\n",
       "  26: 0.521739130811729,\n",
       "  27: 0.5185770751223734,\n",
       "  28: 0.5154150201397922,\n",
       "  29: 0.5209486169777369,\n",
       "  30: 0.5027667987959187,\n",
       "  31: 0.5011857711279345,\n",
       "  32: 0.5154150201397922,\n",
       "  33: 0.5083003956338633,\n",
       "  34: 0.5075098817998712,\n",
       "  35: 0.5185770754286423},\n",
       " {5: 0.5035573122765236,\n",
       "  6: 0.5027667984425315,\n",
       "  7: 0.5090909094207372,\n",
       "  8: 0.49090909128603727,\n",
       "  9: 0.5098814233018476,\n",
       "  10: 0.505928854131887,\n",
       "  11: 0.5075098817998712,\n",
       "  12: 0.5035573123236419,\n",
       "  13: 0.5003952572468241,\n",
       "  14: 0.488537549784061,\n",
       "  15: 0.5075098817998712,\n",
       "  16: 0.5035573122765236,\n",
       "  17: 0.5003952572468241,\n",
       "  18: 0.5122529648038239,\n",
       "  19: 0.5051383402507766,\n",
       "  20: 0.5177865616417685,\n",
       "  21: 0.5106719371358397,\n",
       "  22: 0.5019762849619266,\n",
       "  23: 0.5106719368295707,\n",
       "  24: 0.5083003956338633,\n",
       "  25: 0.5146245063058001,\n",
       "  26: 0.5146245063058001,\n",
       "  27: 0.5177865616417685,\n",
       "  28: 0.5067193676596102,\n",
       "  29: 0.5098814233018476,\n",
       "  30: 0.5146245063058001,\n",
       "  31: 0.5138339921184208,\n",
       "  32: 0.5043478264639029,\n",
       "  33: 0.507509881446484,\n",
       "  34: 0.5154150201397922,\n",
       "  35: 0.5067193679658791},\n",
       " {5: 0.49249011895402145,\n",
       "  6: 0.5098814229484603,\n",
       "  7: 0.5019762849148083,\n",
       "  8: 0.505928854131887,\n",
       "  9: 0.5059288537784998,\n",
       "  10: 0.4996047434599503,\n",
       "  11: 0.5035573126299108,\n",
       "  12: 0.4996047434599503,\n",
       "  13: 0.5003952572468241,\n",
       "  14: 0.4893280636180531,\n",
       "  15: 0.5035573126299108,\n",
       "  16: 0.49802371543857893,\n",
       "  17: 0.513043478637816,\n",
       "  18: 0.5019762846085394,\n",
       "  19: 0.5106719371358397,\n",
       "  20: 0.5114624509227135,\n",
       "  21: 0.5122529648038239,\n",
       "  22: 0.515415019786405,\n",
       "  23: 0.5154150201397922,\n",
       "  24: 0.522529644645721,\n",
       "  25: 0.505138340297895,\n",
       "  26: 0.5169960478077764,\n",
       "  27: 0.5201581031437448,\n",
       "  28: 0.5106719371358397,\n",
       "  29: 0.4996047434599503,\n",
       "  30: 0.5169960478077764,\n",
       "  31: 0.5067193676124919,\n",
       "  32: 0.5146245063058001,\n",
       "  33: 0.5043478264639029,\n",
       "  34: 0.5043478261105157,\n",
       "  35: 0.5106719367824524},\n",
       " {5: 0.505138340297895,\n",
       "  6: 0.505928854131887,\n",
       "  7: 0.4948616604559978,\n",
       "  8: 0.5051383399445077,\n",
       "  9: 0.498814229272571,\n",
       "  10: 0.5122529647567056,\n",
       "  11: 0.49723320195797405,\n",
       "  12: 0.5146245063058001,\n",
       "  13: 0.505928854131887,\n",
       "  14: 0.4940711466220057,\n",
       "  15: 0.5106719368295707,\n",
       "  16: 0.5106719367824524,\n",
       "  17: 0.5027667987959187,\n",
       "  18: 0.5059288537784998,\n",
       "  19: 0.5083003956338633,\n",
       "  20: 0.5098814233018476,\n",
       "  21: 0.5138339921184208,\n",
       "  22: 0.5027667987959187,\n",
       "  23: 0.5146245059995312,\n",
       "  24: 0.5059288540847687,\n",
       "  25: 0.5130434783315471,\n",
       "  26: 0.5169960478077764,\n",
       "  27: 0.5114624509698318,\n",
       "  28: 0.505928854131887,\n",
       "  29: 0.505928854131887,\n",
       "  30: 0.5098814233018476,\n",
       "  31: 0.5114624509698318,\n",
       "  32: 0.5059288537784998,\n",
       "  33: 0.5130434783315471,\n",
       "  34: 0.49723320195797405,\n",
       "  35: 0.5114624509698318}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Round 0",
         "type": "scatter",
         "uid": "36b39775-a328-4c54-a3ca-4455c6dc01d6",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.513043478637816,
          0.5122529648038239,
          0.5075098817998712,
          0.5083003953747127,
          0.49407114631573673,
          0.5098814233018476,
          0.507509881752753,
          0.5035573126299108,
          0.4996047434599503,
          0.5027667987488004,
          0.507509881752753,
          0.5090909091144682,
          0.5067193679187609,
          0.5114624509227135,
          0.5098814233018476,
          0.5162055336203971,
          0.5193675893097527,
          0.5193675893097527,
          0.5233201584797131,
          0.5217391304583417,
          0.5106719371358397,
          0.5106719367824524,
          0.5177865616417685,
          0.5075098817998712,
          0.513043478637816,
          0.5043478264639029,
          0.5162055336203971,
          0.5075098817998712,
          0.5114624509698318,
          0.513833992471808,
          0.5075098817998712
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 1",
         "type": "scatter",
         "uid": "05ff3bde-811d-43d5-8421-2f3acd734853",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5098814233018476,
          0.5098814229484603,
          0.49723320160458684,
          0.5090909091144682,
          0.5051383399445077,
          0.505928854131887,
          0.498814229272571,
          0.5114624509698318,
          0.4940711466220057,
          0.49169960512002936,
          0.4996047431065631,
          0.5027667984425315,
          0.4956521739366026,
          0.5067193679658791,
          0.5067193676124919,
          0.5043478264167846,
          0.5146245063058001,
          0.5256916999816894,
          0.5201581031437448,
          0.515415019786405,
          0.5169960478077764,
          0.521739130811729,
          0.5185770751223734,
          0.5154150201397922,
          0.5209486169777369,
          0.5027667987959187,
          0.5011857711279345,
          0.5154150201397922,
          0.5083003956338633,
          0.5075098817998712,
          0.5185770754286423
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 2",
         "type": "scatter",
         "uid": "332e414a-c977-4eeb-894a-69644b06f03d",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5035573122765236,
          0.5027667984425315,
          0.5090909094207372,
          0.49090909128603727,
          0.5098814233018476,
          0.505928854131887,
          0.5075098817998712,
          0.5035573123236419,
          0.5003952572468241,
          0.488537549784061,
          0.5075098817998712,
          0.5035573122765236,
          0.5003952572468241,
          0.5122529648038239,
          0.5051383402507766,
          0.5177865616417685,
          0.5106719371358397,
          0.5019762849619266,
          0.5106719368295707,
          0.5083003956338633,
          0.5146245063058001,
          0.5146245063058001,
          0.5177865616417685,
          0.5067193676596102,
          0.5098814233018476,
          0.5146245063058001,
          0.5138339921184208,
          0.5043478264639029,
          0.507509881446484,
          0.5154150201397922,
          0.5067193679658791
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 3",
         "type": "scatter",
         "uid": "0b216ed9-0130-4d9d-837c-77b49dbb95d4",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.49249011895402145,
          0.5098814229484603,
          0.5019762849148083,
          0.505928854131887,
          0.5059288537784998,
          0.4996047434599503,
          0.5035573126299108,
          0.4996047434599503,
          0.5003952572468241,
          0.4893280636180531,
          0.5035573126299108,
          0.49802371543857893,
          0.513043478637816,
          0.5019762846085394,
          0.5106719371358397,
          0.5114624509227135,
          0.5122529648038239,
          0.515415019786405,
          0.5154150201397922,
          0.522529644645721,
          0.505138340297895,
          0.5169960478077764,
          0.5201581031437448,
          0.5106719371358397,
          0.4996047434599503,
          0.5169960478077764,
          0.5067193676124919,
          0.5146245063058001,
          0.5043478264639029,
          0.5043478261105157,
          0.5106719367824524
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 4",
         "type": "scatter",
         "uid": "2f9230fa-3bf5-47ea-b643-5e2650adb502",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.505138340297895,
          0.505928854131887,
          0.4948616604559978,
          0.5051383399445077,
          0.498814229272571,
          0.5122529647567056,
          0.49723320195797405,
          0.5146245063058001,
          0.505928854131887,
          0.4940711466220057,
          0.5106719368295707,
          0.5106719367824524,
          0.5027667987959187,
          0.5059288537784998,
          0.5083003956338633,
          0.5098814233018476,
          0.5138339921184208,
          0.5027667987959187,
          0.5146245059995312,
          0.5059288540847687,
          0.5130434783315471,
          0.5169960478077764,
          0.5114624509698318,
          0.505928854131887,
          0.505928854131887,
          0.5098814233018476,
          0.5114624509698318,
          0.5059288537784998,
          0.5130434783315471,
          0.49723320195797405,
          0.5114624509698318
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "Test set accuracy of padded BERT dataset with variable maximum lengths"
        }
       }
      },
      "text/html": [
       "<div id=\"6142f664-ca53-4b83-9386-37ca9a91d9a5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\")) {\n",
       "    Plotly.newPlot(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.513043478637816, 0.5122529648038239, 0.5075098817998712, 0.5083003953747127, 0.49407114631573673, 0.5098814233018476, 0.507509881752753, 0.5035573126299108, 0.4996047434599503, 0.5027667987488004, 0.507509881752753, 0.5090909091144682, 0.5067193679187609, 0.5114624509227135, 0.5098814233018476, 0.5162055336203971, 0.5193675893097527, 0.5193675893097527, 0.5233201584797131, 0.5217391304583417, 0.5106719371358397, 0.5106719367824524, 0.5177865616417685, 0.5075098817998712, 0.513043478637816, 0.5043478264639029, 0.5162055336203971, 0.5075098817998712, 0.5114624509698318, 0.513833992471808, 0.5075098817998712], \"type\": \"scatter\", \"uid\": \"36b39775-a328-4c54-a3ca-4455c6dc01d6\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5098814233018476, 0.5098814229484603, 0.49723320160458684, 0.5090909091144682, 0.5051383399445077, 0.505928854131887, 0.498814229272571, 0.5114624509698318, 0.4940711466220057, 0.49169960512002936, 0.4996047431065631, 0.5027667984425315, 0.4956521739366026, 0.5067193679658791, 0.5067193676124919, 0.5043478264167846, 0.5146245063058001, 0.5256916999816894, 0.5201581031437448, 0.515415019786405, 0.5169960478077764, 0.521739130811729, 0.5185770751223734, 0.5154150201397922, 0.5209486169777369, 0.5027667987959187, 0.5011857711279345, 0.5154150201397922, 0.5083003956338633, 0.5075098817998712, 0.5185770754286423], \"type\": \"scatter\", \"uid\": \"05ff3bde-811d-43d5-8421-2f3acd734853\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5035573122765236, 0.5027667984425315, 0.5090909094207372, 0.49090909128603727, 0.5098814233018476, 0.505928854131887, 0.5075098817998712, 0.5035573123236419, 0.5003952572468241, 0.488537549784061, 0.5075098817998712, 0.5035573122765236, 0.5003952572468241, 0.5122529648038239, 0.5051383402507766, 0.5177865616417685, 0.5106719371358397, 0.5019762849619266, 0.5106719368295707, 0.5083003956338633, 0.5146245063058001, 0.5146245063058001, 0.5177865616417685, 0.5067193676596102, 0.5098814233018476, 0.5146245063058001, 0.5138339921184208, 0.5043478264639029, 0.507509881446484, 0.5154150201397922, 0.5067193679658791], \"type\": \"scatter\", \"uid\": \"332e414a-c977-4eeb-894a-69644b06f03d\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49249011895402145, 0.5098814229484603, 0.5019762849148083, 0.505928854131887, 0.5059288537784998, 0.4996047434599503, 0.5035573126299108, 0.4996047434599503, 0.5003952572468241, 0.4893280636180531, 0.5035573126299108, 0.49802371543857893, 0.513043478637816, 0.5019762846085394, 0.5106719371358397, 0.5114624509227135, 0.5122529648038239, 0.515415019786405, 0.5154150201397922, 0.522529644645721, 0.505138340297895, 0.5169960478077764, 0.5201581031437448, 0.5106719371358397, 0.4996047434599503, 0.5169960478077764, 0.5067193676124919, 0.5146245063058001, 0.5043478264639029, 0.5043478261105157, 0.5106719367824524], \"type\": \"scatter\", \"uid\": \"0b216ed9-0130-4d9d-837c-77b49dbb95d4\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.505138340297895, 0.505928854131887, 0.4948616604559978, 0.5051383399445077, 0.498814229272571, 0.5122529647567056, 0.49723320195797405, 0.5146245063058001, 0.505928854131887, 0.4940711466220057, 0.5106719368295707, 0.5106719367824524, 0.5027667987959187, 0.5059288537784998, 0.5083003956338633, 0.5098814233018476, 0.5138339921184208, 0.5027667987959187, 0.5146245059995312, 0.5059288540847687, 0.5130434783315471, 0.5169960478077764, 0.5114624509698318, 0.505928854131887, 0.505928854131887, 0.5098814233018476, 0.5114624509698318, 0.5059288537784998, 0.5130434783315471, 0.49723320195797405, 0.5114624509698318], \"type\": \"scatter\", \"uid\": \"2f9230fa-3bf5-47ea-b643-5e2650adb502\"}], {\"title\": {\"text\": \"Test set accuracy of padded BERT dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\")) {window._Plotly.Plots.resize(document.getElementById(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"6142f664-ca53-4b83-9386-37ca9a91d9a5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\")) {\n",
       "    Plotly.newPlot(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.513043478637816, 0.5122529648038239, 0.5075098817998712, 0.5083003953747127, 0.49407114631573673, 0.5098814233018476, 0.507509881752753, 0.5035573126299108, 0.4996047434599503, 0.5027667987488004, 0.507509881752753, 0.5090909091144682, 0.5067193679187609, 0.5114624509227135, 0.5098814233018476, 0.5162055336203971, 0.5193675893097527, 0.5193675893097527, 0.5233201584797131, 0.5217391304583417, 0.5106719371358397, 0.5106719367824524, 0.5177865616417685, 0.5075098817998712, 0.513043478637816, 0.5043478264639029, 0.5162055336203971, 0.5075098817998712, 0.5114624509698318, 0.513833992471808, 0.5075098817998712], \"type\": \"scatter\", \"uid\": \"36b39775-a328-4c54-a3ca-4455c6dc01d6\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5098814233018476, 0.5098814229484603, 0.49723320160458684, 0.5090909091144682, 0.5051383399445077, 0.505928854131887, 0.498814229272571, 0.5114624509698318, 0.4940711466220057, 0.49169960512002936, 0.4996047431065631, 0.5027667984425315, 0.4956521739366026, 0.5067193679658791, 0.5067193676124919, 0.5043478264167846, 0.5146245063058001, 0.5256916999816894, 0.5201581031437448, 0.515415019786405, 0.5169960478077764, 0.521739130811729, 0.5185770751223734, 0.5154150201397922, 0.5209486169777369, 0.5027667987959187, 0.5011857711279345, 0.5154150201397922, 0.5083003956338633, 0.5075098817998712, 0.5185770754286423], \"type\": \"scatter\", \"uid\": \"05ff3bde-811d-43d5-8421-2f3acd734853\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5035573122765236, 0.5027667984425315, 0.5090909094207372, 0.49090909128603727, 0.5098814233018476, 0.505928854131887, 0.5075098817998712, 0.5035573123236419, 0.5003952572468241, 0.488537549784061, 0.5075098817998712, 0.5035573122765236, 0.5003952572468241, 0.5122529648038239, 0.5051383402507766, 0.5177865616417685, 0.5106719371358397, 0.5019762849619266, 0.5106719368295707, 0.5083003956338633, 0.5146245063058001, 0.5146245063058001, 0.5177865616417685, 0.5067193676596102, 0.5098814233018476, 0.5146245063058001, 0.5138339921184208, 0.5043478264639029, 0.507509881446484, 0.5154150201397922, 0.5067193679658791], \"type\": \"scatter\", \"uid\": \"332e414a-c977-4eeb-894a-69644b06f03d\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49249011895402145, 0.5098814229484603, 0.5019762849148083, 0.505928854131887, 0.5059288537784998, 0.4996047434599503, 0.5035573126299108, 0.4996047434599503, 0.5003952572468241, 0.4893280636180531, 0.5035573126299108, 0.49802371543857893, 0.513043478637816, 0.5019762846085394, 0.5106719371358397, 0.5114624509227135, 0.5122529648038239, 0.515415019786405, 0.5154150201397922, 0.522529644645721, 0.505138340297895, 0.5169960478077764, 0.5201581031437448, 0.5106719371358397, 0.4996047434599503, 0.5169960478077764, 0.5067193676124919, 0.5146245063058001, 0.5043478264639029, 0.5043478261105157, 0.5106719367824524], \"type\": \"scatter\", \"uid\": \"0b216ed9-0130-4d9d-837c-77b49dbb95d4\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.505138340297895, 0.505928854131887, 0.4948616604559978, 0.5051383399445077, 0.498814229272571, 0.5122529647567056, 0.49723320195797405, 0.5146245063058001, 0.505928854131887, 0.4940711466220057, 0.5106719368295707, 0.5106719367824524, 0.5027667987959187, 0.5059288537784998, 0.5083003956338633, 0.5098814233018476, 0.5138339921184208, 0.5027667987959187, 0.5146245059995312, 0.5059288540847687, 0.5130434783315471, 0.5169960478077764, 0.5114624509698318, 0.505928854131887, 0.505928854131887, 0.5098814233018476, 0.5114624509698318, 0.5059288537784998, 0.5130434783315471, 0.49723320195797405, 0.5114624509698318], \"type\": \"scatter\", \"uid\": \"2f9230fa-3bf5-47ea-b643-5e2650adb502\"}], {\"title\": {\"text\": \"Test set accuracy of padded BERT dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\")) {window._Plotly.Plots.resize(document.getElementById(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traces = [round1, round2, round3, round4, round5]\n",
    "\n",
    "# Create traces\n",
    "bert_trace = go.Scatter(\n",
    "    x = list(round1.keys()),\n",
    "    y = list(round1.values()),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'BERT'\n",
    ")\n",
    "\n",
    "def create_scatter(counter):\n",
    "    acc_dict = traces[counter]\n",
    "    \n",
    "    return go.Scatter(\n",
    "        x = list(acc_dict.keys()),\n",
    "        y = list(acc_dict.values()),\n",
    "        mode = 'lines+markers',\n",
    "        name = 'Round ' + str(counter)\n",
    "    )\n",
    "\n",
    "trace_data = [create_scatter(trace) for trace in range(len(traces))]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'Test set accuracy of padded BERT dataset with variable maximum lengths',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = trace_data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = data.get_elmo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_round(dataset):\n",
    "    # Store accuracies\n",
    "    accuracies = {\n",
    "        padding_len: 0.0 for padding_len in list(range(5,36))\n",
    "    }\n",
    "\n",
    "    for max_len in accuracies.keys():\n",
    "        padded_dataset = {\n",
    "            fold: sequence.pad_sequences(dataset[fold], maxlen = max_len, dtype = float)\n",
    "            for fold in dataset.keys()\n",
    "        }\n",
    "\n",
    "        accuracies[max_len] = get_bilstm_score(padded_dataset['train'], padded_dataset['test'], padded_dataset['validation'], reshape = False)\n",
    "\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "2019-06-04 08:49:28,085 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "2019-06-04 08:49:29,218 From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-06-04 08:49:29,336 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.1231 - acc: 0.4109 - val_loss: 1.0209 - val_acc: 0.5132\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0595 - acc: 0.4525 - val_loss: 1.0150 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0383 - acc: 0.4803 - val_loss: 1.0079 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0308 - acc: 0.4804 - val_loss: 1.0084 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0214 - acc: 0.4905 - val_loss: 1.0035 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 1s 976us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.1167 - acc: 0.4189 - val_loss: 1.0217 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0523 - acc: 0.4639 - val_loss: 1.0156 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0370 - acc: 0.4768 - val_loss: 1.0143 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0299 - acc: 0.4821 - val_loss: 1.0149 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0195 - acc: 0.4913 - val_loss: 1.0048 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.1217 - acc: 0.4253 - val_loss: 1.0176 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0519 - acc: 0.4686 - val_loss: 1.0084 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0349 - acc: 0.4787 - val_loss: 1.0108 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0223 - acc: 0.4999 - val_loss: 1.0038 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0190 - acc: 0.5024 - val_loss: 1.0035 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 1s 603us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.1229 - acc: 0.4177 - val_loss: 1.0027 - val_acc: 0.5234\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0514 - acc: 0.4736 - val_loss: 1.0046 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0353 - acc: 0.4785 - val_loss: 1.0000 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0236 - acc: 0.4977 - val_loss: 1.0011 - val_acc: 0.5265\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0156 - acc: 0.5013 - val_loss: 0.9955 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 1s 693us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.1301 - acc: 0.4194 - val_loss: 1.0173 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0536 - acc: 0.4634 - val_loss: 1.0070 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0346 - acc: 0.4853 - val_loss: 1.0047 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0252 - acc: 0.4909 - val_loss: 1.0040 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0197 - acc: 0.4994 - val_loss: 1.0040 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 1s 736us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.1315 - acc: 0.4200 - val_loss: 1.0127 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0574 - acc: 0.4640 - val_loss: 1.0057 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0345 - acc: 0.4799 - val_loss: 1.0039 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0257 - acc: 0.4910 - val_loss: 1.0002 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0188 - acc: 0.4982 - val_loss: 1.0004 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 1s 889us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.1205 - acc: 0.4319 - val_loss: 1.0081 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0486 - acc: 0.4710 - val_loss: 1.0146 - val_acc: 0.5218\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0332 - acc: 0.4878 - val_loss: 1.0067 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0219 - acc: 0.4874 - val_loss: 1.0054 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0164 - acc: 0.5048 - val_loss: 0.9998 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 1s 861us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.1176 - acc: 0.4283 - val_loss: 1.0073 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0519 - acc: 0.4675 - val_loss: 1.0026 - val_acc: 0.5202\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0326 - acc: 0.4869 - val_loss: 1.0003 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0246 - acc: 0.4937 - val_loss: 1.0000 - val_acc: 0.5249\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0157 - acc: 0.5025 - val_loss: 0.9957 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 1s 961us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.1207 - acc: 0.4320 - val_loss: 1.0211 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0517 - acc: 0.4702 - val_loss: 1.0160 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0308 - acc: 0.4884 - val_loss: 1.0031 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0219 - acc: 0.4962 - val_loss: 1.0021 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0132 - acc: 0.5085 - val_loss: 0.9967 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.1081 - acc: 0.4299 - val_loss: 1.0038 - val_acc: 0.5148\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0480 - acc: 0.4731 - val_loss: 1.0029 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0338 - acc: 0.4838 - val_loss: 0.9986 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0252 - acc: 0.4940 - val_loss: 0.9948 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0173 - acc: 0.5001 - val_loss: 0.9952 - val_acc: 0.5304\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.1159 - acc: 0.4307 - val_loss: 1.0145 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0539 - acc: 0.4660 - val_loss: 1.0119 - val_acc: 0.5241\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0376 - acc: 0.4810 - val_loss: 1.0023 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0254 - acc: 0.4942 - val_loss: 1.0029 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0151 - acc: 0.5005 - val_loss: 0.9986 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.1116 - acc: 0.4280 - val_loss: 1.0146 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0522 - acc: 0.4614 - val_loss: 1.0047 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0323 - acc: 0.4829 - val_loss: 1.0021 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0226 - acc: 0.4975 - val_loss: 0.9980 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0145 - acc: 0.5009 - val_loss: 0.9958 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.1100 - acc: 0.4297 - val_loss: 1.0106 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0491 - acc: 0.4758 - val_loss: 1.0045 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0337 - acc: 0.4851 - val_loss: 1.0016 - val_acc: 0.5257\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0218 - acc: 0.4977 - val_loss: 1.0015 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0155 - acc: 0.5094 - val_loss: 0.9978 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.1046 - acc: 0.4262 - val_loss: 1.0209 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0444 - acc: 0.4710 - val_loss: 1.0108 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0373 - acc: 0.4827 - val_loss: 1.0073 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0255 - acc: 0.4903 - val_loss: 0.9997 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0189 - acc: 0.5031 - val_loss: 1.0030 - val_acc: 0.5273\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.1119 - acc: 0.4246 - val_loss: 1.0191 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0542 - acc: 0.4693 - val_loss: 1.0102 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0323 - acc: 0.4845 - val_loss: 1.0067 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0226 - acc: 0.4977 - val_loss: 1.0000 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0150 - acc: 0.5075 - val_loss: 0.9978 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.1130 - acc: 0.4248 - val_loss: 1.0219 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0533 - acc: 0.4693 - val_loss: 1.0114 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0386 - acc: 0.4756 - val_loss: 1.0107 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0249 - acc: 0.4981 - val_loss: 1.0015 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0178 - acc: 0.5035 - val_loss: 0.9976 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.1114 - acc: 0.4328 - val_loss: 1.0196 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0550 - acc: 0.4624 - val_loss: 1.0083 - val_acc: 0.5210\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0401 - acc: 0.4788 - val_loss: 1.0085 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0260 - acc: 0.4947 - val_loss: 1.0104 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0154 - acc: 0.5027 - val_loss: 1.0011 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.1098 - acc: 0.4213 - val_loss: 1.0179 - val_acc: 0.5187\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0559 - acc: 0.4659 - val_loss: 1.0179 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0370 - acc: 0.4813 - val_loss: 1.0063 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0263 - acc: 0.4923 - val_loss: 1.0022 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0179 - acc: 0.5044 - val_loss: 1.0010 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 77s 8ms/step - loss: 1.1053 - acc: 0.4298 - val_loss: 1.0166 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0537 - acc: 0.4683 - val_loss: 1.0147 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0386 - acc: 0.4815 - val_loss: 1.0042 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0252 - acc: 0.4949 - val_loss: 1.0019 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0171 - acc: 0.5017 - val_loss: 1.0018 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 6s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.1049 - acc: 0.4234 - val_loss: 1.0253 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0562 - acc: 0.4676 - val_loss: 1.0136 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0383 - acc: 0.4799 - val_loss: 1.0103 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0291 - acc: 0.4906 - val_loss: 1.0088 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0189 - acc: 0.4974 - val_loss: 0.9991 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 82s 8ms/step - loss: 1.0997 - acc: 0.4202 - val_loss: 1.0207 - val_acc: 0.5171\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0539 - acc: 0.4663 - val_loss: 1.0153 - val_acc: 0.5226\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0407 - acc: 0.4841 - val_loss: 1.0067 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0305 - acc: 0.4888 - val_loss: 1.0041 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0242 - acc: 0.4967 - val_loss: 1.0020 - val_acc: 0.5273\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 92s 9ms/step - loss: 1.0929 - acc: 0.4319 - val_loss: 1.0242 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.0557 - acc: 0.4656 - val_loss: 1.0122 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0428 - acc: 0.4768 - val_loss: 1.0064 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0296 - acc: 0.4952 - val_loss: 0.9998 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0219 - acc: 0.4967 - val_loss: 0.9976 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 89s 9ms/step - loss: 1.0916 - acc: 0.4263 - val_loss: 1.0179 - val_acc: 0.5234\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.0553 - acc: 0.4643 - val_loss: 1.0136 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0419 - acc: 0.4820 - val_loss: 1.0032 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0267 - acc: 0.4969 - val_loss: 1.0052 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0238 - acc: 0.4944 - val_loss: 1.0009 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 96s 9ms/step - loss: 1.1015 - acc: 0.4337 - val_loss: 1.0205 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.0565 - acc: 0.4613 - val_loss: 1.0087 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0414 - acc: 0.4827 - val_loss: 1.0037 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0279 - acc: 0.4897 - val_loss: 0.9966 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0232 - acc: 0.4973 - val_loss: 0.9984 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 98s 10ms/step - loss: 1.0877 - acc: 0.4276 - val_loss: 1.0195 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.0520 - acc: 0.4664 - val_loss: 1.0108 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.0377 - acc: 0.4810 - val_loss: 1.0109 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0299 - acc: 0.4877 - val_loss: 1.0029 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0245 - acc: 0.4924 - val_loss: 1.0048 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 99s 10ms/step - loss: 1.0832 - acc: 0.4339 - val_loss: 1.0227 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 82s 8ms/step - loss: 1.0560 - acc: 0.4565 - val_loss: 1.0177 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0434 - acc: 0.4694 - val_loss: 1.0077 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0374 - acc: 0.4791 - val_loss: 1.0058 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0250 - acc: 0.4925 - val_loss: 1.0009 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0894 - acc: 0.4293 - val_loss: 1.0238 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 82s 8ms/step - loss: 1.0533 - acc: 0.4647 - val_loss: 1.0107 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0378 - acc: 0.4809 - val_loss: 1.0060 - val_acc: 0.5241\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0297 - acc: 0.4920 - val_loss: 1.0018 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0235 - acc: 0.4967 - val_loss: 0.9994 - val_acc: 0.5312\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 105s 10ms/step - loss: 1.0882 - acc: 0.4199 - val_loss: 1.0226 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 83s 8ms/step - loss: 1.0509 - acc: 0.4627 - val_loss: 1.0152 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0414 - acc: 0.4792 - val_loss: 1.0036 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0336 - acc: 0.4829 - val_loss: 1.0052 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0206 - acc: 0.4984 - val_loss: 0.9965 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 109s 11ms/step - loss: 1.0867 - acc: 0.4291 - val_loss: 1.0143 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 88s 9ms/step - loss: 1.0541 - acc: 0.4648 - val_loss: 1.0082 - val_acc: 0.5226\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.0381 - acc: 0.4800 - val_loss: 1.0098 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.0359 - acc: 0.4850 - val_loss: 1.0044 - val_acc: 0.5195\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.0263 - acc: 0.4929 - val_loss: 0.9962 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0812 - acc: 0.4300 - val_loss: 1.0226 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 89s 9ms/step - loss: 1.0550 - acc: 0.4587 - val_loss: 1.0114 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.0377 - acc: 0.4820 - val_loss: 1.0068 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.0326 - acc: 0.4815 - val_loss: 1.0002 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.0230 - acc: 0.5020 - val_loss: 1.0016 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0779 - acc: 0.4358 - val_loss: 1.0203 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 94s 9ms/step - loss: 1.0488 - acc: 0.4633 - val_loss: 1.0100 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.0379 - acc: 0.4818 - val_loss: 1.0051 - val_acc: 0.5249\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.0333 - acc: 0.4880 - val_loss: 1.0019 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.0242 - acc: 0.4993 - val_loss: 0.9981 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "CPU times: user 9h 15min 19s, sys: 2h 16min 35s, total: 11h 31min 54s\n",
      "Wall time: 2h 32min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "concatenated_elmo = {\n",
    "    fold: [np.concatenate(np.array(statement)) for statement in elmo[fold]['statement']]\n",
    "    for fold in elmo.keys()\n",
    "}\n",
    "\n",
    "elmo_rounds = [calculate_round(concatenated_elmo) for round in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{5: 0.5177865616417685,\n",
       "  6: 0.5241106722665869,\n",
       "  7: 0.5114624509698318,\n",
       "  8: 0.5067193676124919,\n",
       "  9: 0.5067193679187609,\n",
       "  10: 0.5059288540847687,\n",
       "  11: 0.49802371574484783,\n",
       "  12: 0.5011857710808162,\n",
       "  13: 0.49169960507291105,\n",
       "  14: 0.49723320191085574,\n",
       "  15: 0.4932806327408953,\n",
       "  16: 0.5169960478077764,\n",
       "  17: 0.5154150200455556,\n",
       "  18: 0.513043478637816,\n",
       "  19: 0.5083003955396268,\n",
       "  20: 0.5185770754286423,\n",
       "  21: 0.5067193679187609,\n",
       "  22: 0.5146245063058001,\n",
       "  23: 0.5011857710808162,\n",
       "  24: 0.5106719370887214,\n",
       "  25: 0.5169960477606581,\n",
       "  26: 0.5193675893097527,\n",
       "  27: 0.5114624509227135,\n",
       "  28: 0.5130434785906977,\n",
       "  29: 0.5241106723137052,\n",
       "  30: 0.5169960477606581,\n",
       "  31: 0.5169960477606581,\n",
       "  32: 0.5146245063058001,\n",
       "  33: 0.5185770754286423,\n",
       "  34: 0.5051383402507766,\n",
       "  35: 0.5035573125827925},\n",
       " {5: 0.5122529647567056,\n",
       "  6: 0.5169960478077764,\n",
       "  7: 0.5146245062586818,\n",
       "  8: 0.5233201584325948,\n",
       "  9: 0.5138339924246897,\n",
       "  10: 0.4996047434599503,\n",
       "  11: 0.49090909123891896,\n",
       "  12: 0.49090909123891896,\n",
       "  13: 0.5011857710808162,\n",
       "  14: 0.5051383402036584,\n",
       "  15: 0.5067193679187609,\n",
       "  16: 0.5122529648038239,\n",
       "  17: 0.5146245062586818,\n",
       "  18: 0.5146245062586818,\n",
       "  19: 0.516205533926666,\n",
       "  20: 0.5083003955867451,\n",
       "  21: 0.5304347829385238,\n",
       "  22: 0.5217391307646106,\n",
       "  23: 0.5106719370887214,\n",
       "  24: 0.5138339924246897,\n",
       "  25: 0.5209486169306186,\n",
       "  26: 0.5233201584325948,\n",
       "  27: 0.505138340297895,\n",
       "  28: 0.5201581030966265,\n",
       "  29: 0.5225296445986027,\n",
       "  30: 0.5098814233018476,\n",
       "  31: 0.5090909094207372,\n",
       "  32: 0.5122529647567056,\n",
       "  33: 0.5130434785906977,\n",
       "  34: 0.499604743412832,\n",
       "  35: 0.5154150200926739},\n",
       " {5: 0.516205533926666,\n",
       "  6: 0.5264822137685633,\n",
       "  7: 0.5146245062586818,\n",
       "  8: 0.5067193679187609,\n",
       "  9: 0.505928854131887,\n",
       "  10: 0.5035573125827925,\n",
       "  11: 0.4988142295788399,\n",
       "  12: 0.5146245063058001,\n",
       "  13: 0.4940711465748874,\n",
       "  14: 0.5035573125827925,\n",
       "  15: 0.5083003955867451,\n",
       "  16: 0.5067193679187609,\n",
       "  17: 0.5098814232547293,\n",
       "  18: 0.5177865616417685,\n",
       "  19: 0.5059288540847687,\n",
       "  20: 0.513043478637816,\n",
       "  21: 0.5067193679187609,\n",
       "  22: 0.5312252967725158,\n",
       "  23: 0.5209486169306186,\n",
       "  24: 0.5122529647567056,\n",
       "  25: 0.5138339924246897,\n",
       "  26: 0.5185770754757606,\n",
       "  27: 0.5169960478077764,\n",
       "  28: 0.5177865616417685,\n",
       "  29: 0.5193675892626344,\n",
       "  30: 0.5185770754286423,\n",
       "  31: 0.5043478264167846,\n",
       "  32: 0.5090909094678555,\n",
       "  33: 0.5177865616417685,\n",
       "  34: 0.5138339924246897,\n",
       "  35: 0.5098814232547293},\n",
       " {5: 0.505928854131887,\n",
       "  6: 0.5241106723137052,\n",
       "  7: 0.5003952572468241,\n",
       "  8: 0.5154150201397922,\n",
       "  9: 0.4996047431065631,\n",
       "  10: 0.5019762849148083,\n",
       "  11: 0.49802371543857893,\n",
       "  12: 0.5011857710808162,\n",
       "  13: 0.5003952572468241,\n",
       "  14: 0.5122529648038239,\n",
       "  15: 0.5083003955867451,\n",
       "  16: 0.5114624509227135,\n",
       "  17: 0.5169960477606581,\n",
       "  18: 0.5090909094207372,\n",
       "  19: 0.516205533926666,\n",
       "  20: 0.49802371574484783,\n",
       "  21: 0.5177865615946502,\n",
       "  22: 0.5106719370887214,\n",
       "  23: 0.5177865612883813,\n",
       "  24: 0.5193675892626344,\n",
       "  25: 0.5177865616417685,\n",
       "  26: 0.5122529648038239,\n",
       "  27: 0.5154150200926739,\n",
       "  28: 0.5122529648038239,\n",
       "  29: 0.4988142295788399,\n",
       "  30: 0.516205533926666,\n",
       "  31: 0.5035573126299108,\n",
       "  32: 0.5106719370887214,\n",
       "  33: 0.5098814232547293,\n",
       "  34: 0.513833992471808,\n",
       "  35: 0.5225296445986027},\n",
       " {5: 0.5114624509698318,\n",
       "  6: 0.5130434785906977,\n",
       "  7: 0.5019762849148083,\n",
       "  8: 0.5154150200926739,\n",
       "  9: 0.5154150201397922,\n",
       "  10: 0.5075098817998712,\n",
       "  11: 0.5027667987488004,\n",
       "  12: 0.49090909128603727,\n",
       "  13: 0.5027667987488004,\n",
       "  14: 0.5059288540376505,\n",
       "  15: 0.5090909094207372,\n",
       "  16: 0.524901186100579,\n",
       "  17: 0.507509881752753,\n",
       "  18: 0.5177865615475319,\n",
       "  19: 0.5067193679187609,\n",
       "  20: 0.507509881752753,\n",
       "  21: 0.5272727276025554,\n",
       "  22: 0.5264822138156815,\n",
       "  23: 0.5217391307646106,\n",
       "  24: 0.5106719371358397,\n",
       "  25: 0.5146245062586818,\n",
       "  26: 0.5146245062586818,\n",
       "  27: 0.507509881752753,\n",
       "  28: 0.5162055339737843,\n",
       "  29: 0.5138339924246897,\n",
       "  30: 0.5209486169306186,\n",
       "  31: 0.5233201584325948,\n",
       "  32: 0.5177865615946502,\n",
       "  33: 0.5090909094207372,\n",
       "  34: 0.5130434785906977,\n",
       "  35: 0.516205533926666}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Round 0",
         "type": "scatter",
         "uid": "ee5219cc-6129-477f-abcd-fa309019fae0",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5177865616417685,
          0.5241106722665869,
          0.5114624509698318,
          0.5067193676124919,
          0.5067193679187609,
          0.5059288540847687,
          0.49802371574484783,
          0.5011857710808162,
          0.49169960507291105,
          0.49723320191085574,
          0.4932806327408953,
          0.5169960478077764,
          0.5154150200455556,
          0.513043478637816,
          0.5083003955396268,
          0.5185770754286423,
          0.5067193679187609,
          0.5146245063058001,
          0.5011857710808162,
          0.5106719370887214,
          0.5169960477606581,
          0.5193675893097527,
          0.5114624509227135,
          0.5130434785906977,
          0.5241106723137052,
          0.5169960477606581,
          0.5169960477606581,
          0.5146245063058001,
          0.5185770754286423,
          0.5051383402507766,
          0.5035573125827925
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 1",
         "type": "scatter",
         "uid": "2d68eaf6-378a-449b-ae52-b4c0c62e34f0",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5122529647567056,
          0.5169960478077764,
          0.5146245062586818,
          0.5233201584325948,
          0.5138339924246897,
          0.4996047434599503,
          0.49090909123891896,
          0.49090909123891896,
          0.5011857710808162,
          0.5051383402036584,
          0.5067193679187609,
          0.5122529648038239,
          0.5146245062586818,
          0.5146245062586818,
          0.516205533926666,
          0.5083003955867451,
          0.5304347829385238,
          0.5217391307646106,
          0.5106719370887214,
          0.5138339924246897,
          0.5209486169306186,
          0.5233201584325948,
          0.505138340297895,
          0.5201581030966265,
          0.5225296445986027,
          0.5098814233018476,
          0.5090909094207372,
          0.5122529647567056,
          0.5130434785906977,
          0.499604743412832,
          0.5154150200926739
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 2",
         "type": "scatter",
         "uid": "93305cef-80fb-4afa-8a42-a15e5ddb2480",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.516205533926666,
          0.5264822137685633,
          0.5146245062586818,
          0.5067193679187609,
          0.505928854131887,
          0.5035573125827925,
          0.4988142295788399,
          0.5146245063058001,
          0.4940711465748874,
          0.5035573125827925,
          0.5083003955867451,
          0.5067193679187609,
          0.5098814232547293,
          0.5177865616417685,
          0.5059288540847687,
          0.513043478637816,
          0.5067193679187609,
          0.5312252967725158,
          0.5209486169306186,
          0.5122529647567056,
          0.5138339924246897,
          0.5185770754757606,
          0.5169960478077764,
          0.5177865616417685,
          0.5193675892626344,
          0.5185770754286423,
          0.5043478264167846,
          0.5090909094678555,
          0.5177865616417685,
          0.5138339924246897,
          0.5098814232547293
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 3",
         "type": "scatter",
         "uid": "89320afe-f430-4aed-90a3-59edc22c4188",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.505928854131887,
          0.5241106723137052,
          0.5003952572468241,
          0.5154150201397922,
          0.4996047431065631,
          0.5019762849148083,
          0.49802371543857893,
          0.5011857710808162,
          0.5003952572468241,
          0.5122529648038239,
          0.5083003955867451,
          0.5114624509227135,
          0.5169960477606581,
          0.5090909094207372,
          0.516205533926666,
          0.49802371574484783,
          0.5177865615946502,
          0.5106719370887214,
          0.5177865612883813,
          0.5193675892626344,
          0.5177865616417685,
          0.5122529648038239,
          0.5154150200926739,
          0.5122529648038239,
          0.4988142295788399,
          0.516205533926666,
          0.5035573126299108,
          0.5106719370887214,
          0.5098814232547293,
          0.513833992471808,
          0.5225296445986027
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 4",
         "type": "scatter",
         "uid": "3d6c90f8-65ab-433c-b117-b1e8d64c6712",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5114624509698318,
          0.5130434785906977,
          0.5019762849148083,
          0.5154150200926739,
          0.5154150201397922,
          0.5075098817998712,
          0.5027667987488004,
          0.49090909128603727,
          0.5027667987488004,
          0.5059288540376505,
          0.5090909094207372,
          0.524901186100579,
          0.507509881752753,
          0.5177865615475319,
          0.5067193679187609,
          0.507509881752753,
          0.5272727276025554,
          0.5264822138156815,
          0.5217391307646106,
          0.5106719371358397,
          0.5146245062586818,
          0.5146245062586818,
          0.507509881752753,
          0.5162055339737843,
          0.5138339924246897,
          0.5209486169306186,
          0.5233201584325948,
          0.5177865615946502,
          0.5090909094207372,
          0.5130434785906977,
          0.516205533926666
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "Test set accuracy of padded ELMo dataset with variable maximum lengths"
        }
       }
      },
      "text/html": [
       "<div id=\"99ad5de9-afa7-4181-8346-a581665b0d58\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"99ad5de9-afa7-4181-8346-a581665b0d58\")) {\n",
       "    Plotly.newPlot(\"99ad5de9-afa7-4181-8346-a581665b0d58\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5177865616417685, 0.5241106722665869, 0.5114624509698318, 0.5067193676124919, 0.5067193679187609, 0.5059288540847687, 0.49802371574484783, 0.5011857710808162, 0.49169960507291105, 0.49723320191085574, 0.4932806327408953, 0.5169960478077764, 0.5154150200455556, 0.513043478637816, 0.5083003955396268, 0.5185770754286423, 0.5067193679187609, 0.5146245063058001, 0.5011857710808162, 0.5106719370887214, 0.5169960477606581, 0.5193675893097527, 0.5114624509227135, 0.5130434785906977, 0.5241106723137052, 0.5169960477606581, 0.5169960477606581, 0.5146245063058001, 0.5185770754286423, 0.5051383402507766, 0.5035573125827925], \"type\": \"scatter\", \"uid\": \"ee5219cc-6129-477f-abcd-fa309019fae0\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5122529647567056, 0.5169960478077764, 0.5146245062586818, 0.5233201584325948, 0.5138339924246897, 0.4996047434599503, 0.49090909123891896, 0.49090909123891896, 0.5011857710808162, 0.5051383402036584, 0.5067193679187609, 0.5122529648038239, 0.5146245062586818, 0.5146245062586818, 0.516205533926666, 0.5083003955867451, 0.5304347829385238, 0.5217391307646106, 0.5106719370887214, 0.5138339924246897, 0.5209486169306186, 0.5233201584325948, 0.505138340297895, 0.5201581030966265, 0.5225296445986027, 0.5098814233018476, 0.5090909094207372, 0.5122529647567056, 0.5130434785906977, 0.499604743412832, 0.5154150200926739], \"type\": \"scatter\", \"uid\": \"2d68eaf6-378a-449b-ae52-b4c0c62e34f0\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.516205533926666, 0.5264822137685633, 0.5146245062586818, 0.5067193679187609, 0.505928854131887, 0.5035573125827925, 0.4988142295788399, 0.5146245063058001, 0.4940711465748874, 0.5035573125827925, 0.5083003955867451, 0.5067193679187609, 0.5098814232547293, 0.5177865616417685, 0.5059288540847687, 0.513043478637816, 0.5067193679187609, 0.5312252967725158, 0.5209486169306186, 0.5122529647567056, 0.5138339924246897, 0.5185770754757606, 0.5169960478077764, 0.5177865616417685, 0.5193675892626344, 0.5185770754286423, 0.5043478264167846, 0.5090909094678555, 0.5177865616417685, 0.5138339924246897, 0.5098814232547293], \"type\": \"scatter\", \"uid\": \"93305cef-80fb-4afa-8a42-a15e5ddb2480\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.505928854131887, 0.5241106723137052, 0.5003952572468241, 0.5154150201397922, 0.4996047431065631, 0.5019762849148083, 0.49802371543857893, 0.5011857710808162, 0.5003952572468241, 0.5122529648038239, 0.5083003955867451, 0.5114624509227135, 0.5169960477606581, 0.5090909094207372, 0.516205533926666, 0.49802371574484783, 0.5177865615946502, 0.5106719370887214, 0.5177865612883813, 0.5193675892626344, 0.5177865616417685, 0.5122529648038239, 0.5154150200926739, 0.5122529648038239, 0.4988142295788399, 0.516205533926666, 0.5035573126299108, 0.5106719370887214, 0.5098814232547293, 0.513833992471808, 0.5225296445986027], \"type\": \"scatter\", \"uid\": \"89320afe-f430-4aed-90a3-59edc22c4188\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5114624509698318, 0.5130434785906977, 0.5019762849148083, 0.5154150200926739, 0.5154150201397922, 0.5075098817998712, 0.5027667987488004, 0.49090909128603727, 0.5027667987488004, 0.5059288540376505, 0.5090909094207372, 0.524901186100579, 0.507509881752753, 0.5177865615475319, 0.5067193679187609, 0.507509881752753, 0.5272727276025554, 0.5264822138156815, 0.5217391307646106, 0.5106719371358397, 0.5146245062586818, 0.5146245062586818, 0.507509881752753, 0.5162055339737843, 0.5138339924246897, 0.5209486169306186, 0.5233201584325948, 0.5177865615946502, 0.5090909094207372, 0.5130434785906977, 0.516205533926666], \"type\": \"scatter\", \"uid\": \"3d6c90f8-65ab-433c-b117-b1e8d64c6712\"}], {\"title\": {\"text\": \"Test set accuracy of padded ELMo dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"99ad5de9-afa7-4181-8346-a581665b0d58\")) {window._Plotly.Plots.resize(document.getElementById(\"99ad5de9-afa7-4181-8346-a581665b0d58\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"99ad5de9-afa7-4181-8346-a581665b0d58\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"99ad5de9-afa7-4181-8346-a581665b0d58\")) {\n",
       "    Plotly.newPlot(\"99ad5de9-afa7-4181-8346-a581665b0d58\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5177865616417685, 0.5241106722665869, 0.5114624509698318, 0.5067193676124919, 0.5067193679187609, 0.5059288540847687, 0.49802371574484783, 0.5011857710808162, 0.49169960507291105, 0.49723320191085574, 0.4932806327408953, 0.5169960478077764, 0.5154150200455556, 0.513043478637816, 0.5083003955396268, 0.5185770754286423, 0.5067193679187609, 0.5146245063058001, 0.5011857710808162, 0.5106719370887214, 0.5169960477606581, 0.5193675893097527, 0.5114624509227135, 0.5130434785906977, 0.5241106723137052, 0.5169960477606581, 0.5169960477606581, 0.5146245063058001, 0.5185770754286423, 0.5051383402507766, 0.5035573125827925], \"type\": \"scatter\", \"uid\": \"ee5219cc-6129-477f-abcd-fa309019fae0\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5122529647567056, 0.5169960478077764, 0.5146245062586818, 0.5233201584325948, 0.5138339924246897, 0.4996047434599503, 0.49090909123891896, 0.49090909123891896, 0.5011857710808162, 0.5051383402036584, 0.5067193679187609, 0.5122529648038239, 0.5146245062586818, 0.5146245062586818, 0.516205533926666, 0.5083003955867451, 0.5304347829385238, 0.5217391307646106, 0.5106719370887214, 0.5138339924246897, 0.5209486169306186, 0.5233201584325948, 0.505138340297895, 0.5201581030966265, 0.5225296445986027, 0.5098814233018476, 0.5090909094207372, 0.5122529647567056, 0.5130434785906977, 0.499604743412832, 0.5154150200926739], \"type\": \"scatter\", \"uid\": \"2d68eaf6-378a-449b-ae52-b4c0c62e34f0\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.516205533926666, 0.5264822137685633, 0.5146245062586818, 0.5067193679187609, 0.505928854131887, 0.5035573125827925, 0.4988142295788399, 0.5146245063058001, 0.4940711465748874, 0.5035573125827925, 0.5083003955867451, 0.5067193679187609, 0.5098814232547293, 0.5177865616417685, 0.5059288540847687, 0.513043478637816, 0.5067193679187609, 0.5312252967725158, 0.5209486169306186, 0.5122529647567056, 0.5138339924246897, 0.5185770754757606, 0.5169960478077764, 0.5177865616417685, 0.5193675892626344, 0.5185770754286423, 0.5043478264167846, 0.5090909094678555, 0.5177865616417685, 0.5138339924246897, 0.5098814232547293], \"type\": \"scatter\", \"uid\": \"93305cef-80fb-4afa-8a42-a15e5ddb2480\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.505928854131887, 0.5241106723137052, 0.5003952572468241, 0.5154150201397922, 0.4996047431065631, 0.5019762849148083, 0.49802371543857893, 0.5011857710808162, 0.5003952572468241, 0.5122529648038239, 0.5083003955867451, 0.5114624509227135, 0.5169960477606581, 0.5090909094207372, 0.516205533926666, 0.49802371574484783, 0.5177865615946502, 0.5106719370887214, 0.5177865612883813, 0.5193675892626344, 0.5177865616417685, 0.5122529648038239, 0.5154150200926739, 0.5122529648038239, 0.4988142295788399, 0.516205533926666, 0.5035573126299108, 0.5106719370887214, 0.5098814232547293, 0.513833992471808, 0.5225296445986027], \"type\": \"scatter\", \"uid\": \"89320afe-f430-4aed-90a3-59edc22c4188\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5114624509698318, 0.5130434785906977, 0.5019762849148083, 0.5154150200926739, 0.5154150201397922, 0.5075098817998712, 0.5027667987488004, 0.49090909128603727, 0.5027667987488004, 0.5059288540376505, 0.5090909094207372, 0.524901186100579, 0.507509881752753, 0.5177865615475319, 0.5067193679187609, 0.507509881752753, 0.5272727276025554, 0.5264822138156815, 0.5217391307646106, 0.5106719371358397, 0.5146245062586818, 0.5146245062586818, 0.507509881752753, 0.5162055339737843, 0.5138339924246897, 0.5209486169306186, 0.5233201584325948, 0.5177865615946502, 0.5090909094207372, 0.5130434785906977, 0.516205533926666], \"type\": \"scatter\", \"uid\": \"3d6c90f8-65ab-433c-b117-b1e8d64c6712\"}], {\"title\": {\"text\": \"Test set accuracy of padded ELMo dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"99ad5de9-afa7-4181-8346-a581665b0d58\")) {window._Plotly.Plots.resize(document.getElementById(\"99ad5de9-afa7-4181-8346-a581665b0d58\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traces = elmo_rounds\n",
    "\n",
    "# Create traces\n",
    "def create_scatter(counter):\n",
    "    acc_dict = traces[counter]\n",
    "    \n",
    "    return go.Scatter(\n",
    "        x = list(acc_dict.keys()),\n",
    "        y = list(acc_dict.values()),\n",
    "        mode = 'lines+markers',\n",
    "        name = 'Round ' + str(counter)\n",
    "    )\n",
    "\n",
    "trace_data = [create_scatter(trace) for trace in range(len(traces))]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'Test set accuracy of padded ELMo dataset with variable maximum lengths',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = trace_data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "fill": "tozerox",
         "fillcolor": "rgba(0,100,80,0.2)",
         "line": {
          "color": "rgba(255,255,255,0)"
         },
         "name": "BERT",
         "showlegend": false,
         "type": "scatter",
         "uid": "69aebbcd-0631-49b6-990f-39d7354ce9b3",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          35,
          34,
          33,
          32,
          31,
          30,
          29,
          28,
          27,
          26,
          25,
          24,
          23,
          22,
          21,
          20,
          19,
          18,
          17,
          16,
          15,
          14,
          13,
          12,
          11,
          10,
          9,
          8,
          7,
          6,
          5
         ],
         "y": [
          0.513043478637816,
          0.5122529648038239,
          0.5090909094207372,
          0.5090909091144682,
          0.5098814233018476,
          0.5122529647567056,
          0.5075098817998712,
          0.5146245063058001,
          0.505928854131887,
          0.5027667987488004,
          0.5106719368295707,
          0.5106719367824524,
          0.513043478637816,
          0.5122529648038239,
          0.5106719371358397,
          0.5177865616417685,
          0.5193675893097527,
          0.5256916999816894,
          0.5233201584797131,
          0.522529644645721,
          0.5169960478077764,
          0.521739130811729,
          0.5201581031437448,
          0.5154150201397922,
          0.5209486169777369,
          0.5169960478077764,
          0.5162055336203971,
          0.5154150201397922,
          0.5130434783315471,
          0.5154150201397922,
          0.5185770754286423,
          0.5067193679658791,
          0.49723320195797405,
          0.5043478264639029,
          0.5043478264639029,
          0.5011857711279345,
          0.5027667987959187,
          0.4996047434599503,
          0.505928854131887,
          0.5114624509698318,
          0.5106719367824524,
          0.505138340297895,
          0.5059288540847687,
          0.5106719368295707,
          0.5019762849619266,
          0.5106719371358397,
          0.5043478264167846,
          0.5051383402507766,
          0.5019762846085394,
          0.4956521739366026,
          0.49802371543857893,
          0.4996047431065631,
          0.488537549784061,
          0.4940711466220057,
          0.4996047434599503,
          0.49723320195797405,
          0.4996047434599503,
          0.49407114631573673,
          0.49090909128603727,
          0.4948616604559978,
          0.5027667984425315,
          0.49249011895402145
         ]
        },
        {
         "line": {
          "color": "rgb(0,100,80)"
         },
         "mode": "lines+markers",
         "name": "BERT",
         "type": "scatter",
         "uid": "05c28c98-f40f-40d1-ba4f-45aead1e8734",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5048221346936207,
          0.5081422926550326,
          0.5021343876392003,
          0.5038735179703225,
          0.5027667985226326,
          0.5067193679564556,
          0.502924901482616,
          0.5065612651378271,
          0.5000790517414982,
          0.4932806327785899,
          0.5057707512237337,
          0.504822134410911,
          0.5037154153071844,
          0.5076679844158911,
          0.5081422927869638,
          0.5119367591807024,
          0.5141501979347274,
          0.5130434785671384,
          0.5168379449184705,
          0.51478260892182,
          0.5120948619757717,
          0.5162055339031069,
          0.5171541505038973,
          0.5092490121734,
          0.5098814233018476,
          0.5097233205350491,
          0.5098814230898153,
          0.5095652176975732,
          0.5089328065691258,
          0.5076679844959922,
          0.5109881425893354
         ]
        },
        {
         "fill": "tozerox",
         "fillcolor": "rgba(0,176,246,0.2)",
         "line": {
          "color": "rgba(255,255,255,0)"
         },
         "name": "ELMo",
         "showlegend": false,
         "type": "scatter",
         "uid": "d1b50b79-bd93-4989-b14f-8b5250969300",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          35,
          34,
          33,
          32,
          31,
          30,
          29,
          28,
          27,
          26,
          25,
          24,
          23,
          22,
          21,
          20,
          19,
          18,
          17,
          16,
          15,
          14,
          13,
          12,
          11,
          10,
          9,
          8,
          7,
          6,
          5
         ],
         "y": [
          0.5177865616417685,
          0.5264822137685633,
          0.5146245062586818,
          0.5233201584325948,
          0.5154150201397922,
          0.5075098817998712,
          0.5027667987488004,
          0.5146245063058001,
          0.5027667987488004,
          0.5122529648038239,
          0.5090909094207372,
          0.524901186100579,
          0.5169960477606581,
          0.5177865616417685,
          0.516205533926666,
          0.5185770754286423,
          0.5304347829385238,
          0.5312252967725158,
          0.5217391307646106,
          0.5193675892626344,
          0.5209486169306186,
          0.5233201584325948,
          0.5169960478077764,
          0.5201581030966265,
          0.5241106723137052,
          0.5209486169306186,
          0.5233201584325948,
          0.5177865615946502,
          0.5185770754286423,
          0.513833992471808,
          0.5225296445986027,
          0.5035573125827925,
          0.499604743412832,
          0.5090909094207372,
          0.5090909094678555,
          0.5035573126299108,
          0.5098814233018476,
          0.4988142295788399,
          0.5122529648038239,
          0.505138340297895,
          0.5122529648038239,
          0.5138339924246897,
          0.5106719370887214,
          0.5011857710808162,
          0.5106719370887214,
          0.5067193679187609,
          0.49802371574484783,
          0.5059288540847687,
          0.5090909094207372,
          0.507509881752753,
          0.5067193679187609,
          0.4932806327408953,
          0.49723320191085574,
          0.49169960507291105,
          0.49090909123891896,
          0.49090909123891896,
          0.4996047434599503,
          0.4996047431065631,
          0.5067193676124919,
          0.5003952572468241,
          0.5130434785906977,
          0.505928854131887
         ]
        },
        {
         "line": {
          "color": "rgb(0,176,246)"
         },
         "mode": "lines+markers",
         "name": "ELMo",
         "type": "scatter",
         "uid": "44325a73-1789-4110-86b0-8d4e0fa3f354",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5127272730853718,
          0.5209486169494658,
          0.5086166011297656,
          0.5135177868392627,
          0.5083003955443386,
          0.5037154153684382,
          0.4977075101499972,
          0.4997628461984777,
          0.49802371574484783,
          0.5048221347077562,
          0.5051383402507768,
          0.5144664035107308,
          0.5128853758144756,
          0.514466403501307,
          0.5106719370792977,
          0.5090909094301608,
          0.5177865615946502,
          0.5209486169494658,
          0.5144664034306297,
          0.5133596841337182,
          0.5168379450032834,
          0.5176284588561227,
          0.5113043481747624,
          0.5158893284213402,
          0.5157312256356944,
          0.5165217394696866,
          0.511462450932137,
          0.5128853758427466,
          0.513675889667315,
          0.5090909094301608,
          0.513517786891093
         ]
        }
       ],
       "layout": {
        "paper_bgcolor": "rgb(255,255,255)",
        "plot_bgcolor": "rgb(229,229,229)",
        "title": {
         "text": "Bi-LSTM test set accuracy of padded datasets with variable maximum lengths"
        },
        "xaxis": {
         "gridcolor": "rgb(255,255,255)",
         "range": [
          5,
          35
         ],
         "showgrid": true,
         "showline": false,
         "showticklabels": true,
         "tickcolor": "rgb(127,127,127)",
         "ticks": "outside",
         "zeroline": false
        },
        "yaxis": {
         "gridcolor": "rgb(255,255,255)",
         "showgrid": true,
         "showline": false,
         "showticklabels": true,
         "tickcolor": "rgb(127,127,127)",
         "ticks": "outside",
         "zeroline": false
        }
       }
      },
      "text/html": [
       "<div id=\"39195259-ffa1-4fca-a359-1882b9c2d56a\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"39195259-ffa1-4fca-a359-1882b9c2d56a\")) {\n",
       "    Plotly.newPlot(\"39195259-ffa1-4fca-a359-1882b9c2d56a\", [{\"fill\": \"tozerox\", \"fillcolor\": \"rgba(0,100,80,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"BERT\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.513043478637816, 0.5122529648038239, 0.5090909094207372, 0.5090909091144682, 0.5098814233018476, 0.5122529647567056, 0.5075098817998712, 0.5146245063058001, 0.505928854131887, 0.5027667987488004, 0.5106719368295707, 0.5106719367824524, 0.513043478637816, 0.5122529648038239, 0.5106719371358397, 0.5177865616417685, 0.5193675893097527, 0.5256916999816894, 0.5233201584797131, 0.522529644645721, 0.5169960478077764, 0.521739130811729, 0.5201581031437448, 0.5154150201397922, 0.5209486169777369, 0.5169960478077764, 0.5162055336203971, 0.5154150201397922, 0.5130434783315471, 0.5154150201397922, 0.5185770754286423, 0.5067193679658791, 0.49723320195797405, 0.5043478264639029, 0.5043478264639029, 0.5011857711279345, 0.5027667987959187, 0.4996047434599503, 0.505928854131887, 0.5114624509698318, 0.5106719367824524, 0.505138340297895, 0.5059288540847687, 0.5106719368295707, 0.5019762849619266, 0.5106719371358397, 0.5043478264167846, 0.5051383402507766, 0.5019762846085394, 0.4956521739366026, 0.49802371543857893, 0.4996047431065631, 0.488537549784061, 0.4940711466220057, 0.4996047434599503, 0.49723320195797405, 0.4996047434599503, 0.49407114631573673, 0.49090909128603727, 0.4948616604559978, 0.5027667984425315, 0.49249011895402145], \"type\": \"scatter\", \"uid\": \"69aebbcd-0631-49b6-990f-39d7354ce9b3\"}, {\"line\": {\"color\": \"rgb(0,100,80)\"}, \"mode\": \"lines+markers\", \"name\": \"BERT\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5048221346936207, 0.5081422926550326, 0.5021343876392003, 0.5038735179703225, 0.5027667985226326, 0.5067193679564556, 0.502924901482616, 0.5065612651378271, 0.5000790517414982, 0.4932806327785899, 0.5057707512237337, 0.504822134410911, 0.5037154153071844, 0.5076679844158911, 0.5081422927869638, 0.5119367591807024, 0.5141501979347274, 0.5130434785671384, 0.5168379449184705, 0.51478260892182, 0.5120948619757717, 0.5162055339031069, 0.5171541505038973, 0.5092490121734, 0.5098814233018476, 0.5097233205350491, 0.5098814230898153, 0.5095652176975732, 0.5089328065691258, 0.5076679844959922, 0.5109881425893354], \"type\": \"scatter\", \"uid\": \"05c28c98-f40f-40d1-ba4f-45aead1e8734\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(0,176,246,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"ELMo\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.5177865616417685, 0.5264822137685633, 0.5146245062586818, 0.5233201584325948, 0.5154150201397922, 0.5075098817998712, 0.5027667987488004, 0.5146245063058001, 0.5027667987488004, 0.5122529648038239, 0.5090909094207372, 0.524901186100579, 0.5169960477606581, 0.5177865616417685, 0.516205533926666, 0.5185770754286423, 0.5304347829385238, 0.5312252967725158, 0.5217391307646106, 0.5193675892626344, 0.5209486169306186, 0.5233201584325948, 0.5169960478077764, 0.5201581030966265, 0.5241106723137052, 0.5209486169306186, 0.5233201584325948, 0.5177865615946502, 0.5185770754286423, 0.513833992471808, 0.5225296445986027, 0.5035573125827925, 0.499604743412832, 0.5090909094207372, 0.5090909094678555, 0.5035573126299108, 0.5098814233018476, 0.4988142295788399, 0.5122529648038239, 0.505138340297895, 0.5122529648038239, 0.5138339924246897, 0.5106719370887214, 0.5011857710808162, 0.5106719370887214, 0.5067193679187609, 0.49802371574484783, 0.5059288540847687, 0.5090909094207372, 0.507509881752753, 0.5067193679187609, 0.4932806327408953, 0.49723320191085574, 0.49169960507291105, 0.49090909123891896, 0.49090909123891896, 0.4996047434599503, 0.4996047431065631, 0.5067193676124919, 0.5003952572468241, 0.5130434785906977, 0.505928854131887], \"type\": \"scatter\", \"uid\": \"d1b50b79-bd93-4989-b14f-8b5250969300\"}, {\"line\": {\"color\": \"rgb(0,176,246)\"}, \"mode\": \"lines+markers\", \"name\": \"ELMo\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5127272730853718, 0.5209486169494658, 0.5086166011297656, 0.5135177868392627, 0.5083003955443386, 0.5037154153684382, 0.4977075101499972, 0.4997628461984777, 0.49802371574484783, 0.5048221347077562, 0.5051383402507768, 0.5144664035107308, 0.5128853758144756, 0.514466403501307, 0.5106719370792977, 0.5090909094301608, 0.5177865615946502, 0.5209486169494658, 0.5144664034306297, 0.5133596841337182, 0.5168379450032834, 0.5176284588561227, 0.5113043481747624, 0.5158893284213402, 0.5157312256356944, 0.5165217394696866, 0.511462450932137, 0.5128853758427466, 0.513675889667315, 0.5090909094301608, 0.513517786891093], \"type\": \"scatter\", \"uid\": \"44325a73-1789-4110-86b0-8d4e0fa3f354\"}], {\"paper_bgcolor\": \"rgb(255,255,255)\", \"plot_bgcolor\": \"rgb(229,229,229)\", \"title\": {\"text\": \"Bi-LSTM test set accuracy of padded datasets with variable maximum lengths\"}, \"xaxis\": {\"gridcolor\": \"rgb(255,255,255)\", \"range\": [5, 35], \"showgrid\": true, \"showline\": false, \"showticklabels\": true, \"tickcolor\": \"rgb(127,127,127)\", \"ticks\": \"outside\", \"zeroline\": false}, \"yaxis\": {\"gridcolor\": \"rgb(255,255,255)\", \"showgrid\": true, \"showline\": false, \"showticklabels\": true, \"tickcolor\": \"rgb(127,127,127)\", \"ticks\": \"outside\", \"zeroline\": false}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"39195259-ffa1-4fca-a359-1882b9c2d56a\")) {window._Plotly.Plots.resize(document.getElementById(\"39195259-ffa1-4fca-a359-1882b9c2d56a\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"39195259-ffa1-4fca-a359-1882b9c2d56a\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"39195259-ffa1-4fca-a359-1882b9c2d56a\")) {\n",
       "    Plotly.newPlot(\"39195259-ffa1-4fca-a359-1882b9c2d56a\", [{\"fill\": \"tozerox\", \"fillcolor\": \"rgba(0,100,80,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"BERT\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.513043478637816, 0.5122529648038239, 0.5090909094207372, 0.5090909091144682, 0.5098814233018476, 0.5122529647567056, 0.5075098817998712, 0.5146245063058001, 0.505928854131887, 0.5027667987488004, 0.5106719368295707, 0.5106719367824524, 0.513043478637816, 0.5122529648038239, 0.5106719371358397, 0.5177865616417685, 0.5193675893097527, 0.5256916999816894, 0.5233201584797131, 0.522529644645721, 0.5169960478077764, 0.521739130811729, 0.5201581031437448, 0.5154150201397922, 0.5209486169777369, 0.5169960478077764, 0.5162055336203971, 0.5154150201397922, 0.5130434783315471, 0.5154150201397922, 0.5185770754286423, 0.5067193679658791, 0.49723320195797405, 0.5043478264639029, 0.5043478264639029, 0.5011857711279345, 0.5027667987959187, 0.4996047434599503, 0.505928854131887, 0.5114624509698318, 0.5106719367824524, 0.505138340297895, 0.5059288540847687, 0.5106719368295707, 0.5019762849619266, 0.5106719371358397, 0.5043478264167846, 0.5051383402507766, 0.5019762846085394, 0.4956521739366026, 0.49802371543857893, 0.4996047431065631, 0.488537549784061, 0.4940711466220057, 0.4996047434599503, 0.49723320195797405, 0.4996047434599503, 0.49407114631573673, 0.49090909128603727, 0.4948616604559978, 0.5027667984425315, 0.49249011895402145], \"type\": \"scatter\", \"uid\": \"69aebbcd-0631-49b6-990f-39d7354ce9b3\"}, {\"line\": {\"color\": \"rgb(0,100,80)\"}, \"mode\": \"lines+markers\", \"name\": \"BERT\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5048221346936207, 0.5081422926550326, 0.5021343876392003, 0.5038735179703225, 0.5027667985226326, 0.5067193679564556, 0.502924901482616, 0.5065612651378271, 0.5000790517414982, 0.4932806327785899, 0.5057707512237337, 0.504822134410911, 0.5037154153071844, 0.5076679844158911, 0.5081422927869638, 0.5119367591807024, 0.5141501979347274, 0.5130434785671384, 0.5168379449184705, 0.51478260892182, 0.5120948619757717, 0.5162055339031069, 0.5171541505038973, 0.5092490121734, 0.5098814233018476, 0.5097233205350491, 0.5098814230898153, 0.5095652176975732, 0.5089328065691258, 0.5076679844959922, 0.5109881425893354], \"type\": \"scatter\", \"uid\": \"05c28c98-f40f-40d1-ba4f-45aead1e8734\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(0,176,246,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"ELMo\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.5177865616417685, 0.5264822137685633, 0.5146245062586818, 0.5233201584325948, 0.5154150201397922, 0.5075098817998712, 0.5027667987488004, 0.5146245063058001, 0.5027667987488004, 0.5122529648038239, 0.5090909094207372, 0.524901186100579, 0.5169960477606581, 0.5177865616417685, 0.516205533926666, 0.5185770754286423, 0.5304347829385238, 0.5312252967725158, 0.5217391307646106, 0.5193675892626344, 0.5209486169306186, 0.5233201584325948, 0.5169960478077764, 0.5201581030966265, 0.5241106723137052, 0.5209486169306186, 0.5233201584325948, 0.5177865615946502, 0.5185770754286423, 0.513833992471808, 0.5225296445986027, 0.5035573125827925, 0.499604743412832, 0.5090909094207372, 0.5090909094678555, 0.5035573126299108, 0.5098814233018476, 0.4988142295788399, 0.5122529648038239, 0.505138340297895, 0.5122529648038239, 0.5138339924246897, 0.5106719370887214, 0.5011857710808162, 0.5106719370887214, 0.5067193679187609, 0.49802371574484783, 0.5059288540847687, 0.5090909094207372, 0.507509881752753, 0.5067193679187609, 0.4932806327408953, 0.49723320191085574, 0.49169960507291105, 0.49090909123891896, 0.49090909123891896, 0.4996047434599503, 0.4996047431065631, 0.5067193676124919, 0.5003952572468241, 0.5130434785906977, 0.505928854131887], \"type\": \"scatter\", \"uid\": \"d1b50b79-bd93-4989-b14f-8b5250969300\"}, {\"line\": {\"color\": \"rgb(0,176,246)\"}, \"mode\": \"lines+markers\", \"name\": \"ELMo\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5127272730853718, 0.5209486169494658, 0.5086166011297656, 0.5135177868392627, 0.5083003955443386, 0.5037154153684382, 0.4977075101499972, 0.4997628461984777, 0.49802371574484783, 0.5048221347077562, 0.5051383402507768, 0.5144664035107308, 0.5128853758144756, 0.514466403501307, 0.5106719370792977, 0.5090909094301608, 0.5177865615946502, 0.5209486169494658, 0.5144664034306297, 0.5133596841337182, 0.5168379450032834, 0.5176284588561227, 0.5113043481747624, 0.5158893284213402, 0.5157312256356944, 0.5165217394696866, 0.511462450932137, 0.5128853758427466, 0.513675889667315, 0.5090909094301608, 0.513517786891093], \"type\": \"scatter\", \"uid\": \"44325a73-1789-4110-86b0-8d4e0fa3f354\"}], {\"paper_bgcolor\": \"rgb(255,255,255)\", \"plot_bgcolor\": \"rgb(229,229,229)\", \"title\": {\"text\": \"Bi-LSTM test set accuracy of padded datasets with variable maximum lengths\"}, \"xaxis\": {\"gridcolor\": \"rgb(255,255,255)\", \"range\": [5, 35], \"showgrid\": true, \"showline\": false, \"showticklabels\": true, \"tickcolor\": \"rgb(127,127,127)\", \"ticks\": \"outside\", \"zeroline\": false}, \"yaxis\": {\"gridcolor\": \"rgb(255,255,255)\", \"showgrid\": true, \"showline\": false, \"showticklabels\": true, \"tickcolor\": \"rgb(127,127,127)\", \"ticks\": \"outside\", \"zeroline\": false}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"39195259-ffa1-4fca-a359-1882b9c2d56a\")) {window._Plotly.Plots.resize(document.getElementById(\"39195259-ffa1-4fca-a359-1882b9c2d56a\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(5, 36))\n",
    "x_rev = x[::-1]\n",
    "\n",
    "# BERT\n",
    "bert_matrix = np.transpose(np.array([np.array(list(acc_round.values())) for acc_round in bert_rounds]))\n",
    "bert_y = [np.average(row) for row in bert_matrix]\n",
    "bert_y_upper = [row.max() for row in bert_matrix]\n",
    "bert_y_lower = [row.min() for row in bert_matrix]\n",
    "bert_y_lower = bert_y_lower[::-1]\n",
    "\n",
    "bert1 = go.Scatter(\n",
    "    x = x + x_rev,\n",
    "    y = bert_y_upper + bert_y_lower,\n",
    "    fill = 'tozerox',\n",
    "    fillcolor = 'rgba(0,100,80,0.2)',\n",
    "    line = dict(color = 'rgba(255,255,255,0)'),\n",
    "    showlegend = False,\n",
    "    name = 'BERT',\n",
    ")\n",
    "bert2 = go.Scatter(\n",
    "    x = x,\n",
    "    y = bert_y,\n",
    "    line = dict(color='rgb(0,100,80)'),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'BERT',\n",
    ")\n",
    "\n",
    "# ELMo\n",
    "elmo_matrix = np.transpose(np.array([np.array(list(acc_round.values())) for acc_round in elmo_rounds]))\n",
    "elmo_y = [np.average(row) for row in elmo_matrix]\n",
    "elmo_y_upper = [row.max() for row in elmo_matrix]\n",
    "elmo_y_lower = [row.min() for row in elmo_matrix]\n",
    "elmo_y_lower = elmo_y_lower[::-1]\n",
    "\n",
    "elmo1 = go.Scatter(\n",
    "    x = x + x_rev,\n",
    "    y = elmo_y_upper + elmo_y_lower,\n",
    "    fill = 'tozerox',\n",
    "    fillcolor = 'rgba(0,176,246,0.2)',\n",
    "    line = dict(color = 'rgba(255,255,255,0)'),\n",
    "    showlegend = False,\n",
    "    name = 'ELMo',\n",
    ")\n",
    "elmo2 = go.Scatter(\n",
    "    x = x,\n",
    "    y = elmo_y,\n",
    "    line = dict(color='rgb(0,176,246)'),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'ELMo',\n",
    ")\n",
    "\n",
    "\n",
    "data = [bert1, bert2, elmo1, elmo2]\n",
    "layout = go.Layout(\n",
    "    title = 'Bi-LSTM test set accuracy of padded datasets with variable maximum lengths',\n",
    "    paper_bgcolor = 'rgb(255,255,255)',\n",
    "    plot_bgcolor = 'rgb(229,229,229)',\n",
    "    xaxis = dict(\n",
    "        gridcolor = 'rgb(255,255,255)',\n",
    "        range = [5,35],\n",
    "        showgrid = True,\n",
    "        showline = False,\n",
    "        showticklabels = True,\n",
    "        tickcolor = 'rgb(127,127,127)',\n",
    "        ticks = 'outside',\n",
    "        zeroline = False\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        gridcolor='rgb(255,255,255)',\n",
    "        showgrid = True,\n",
    "        showline = False,\n",
    "        showticklabels = True,\n",
    "        tickcolor = 'rgb(127,127,127)',\n",
    "        ticks = 'outside',\n",
    "        zeroline = False\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Convolutional neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_score(X_train, X_test, X_validation, y_train = general['train']['label'], y_test = general['test']['label'], y_validation = general['validation']['label'], reshape = True):\n",
    "    # Rearrange data types    \n",
    "    params = locals().copy()    \n",
    "    inputs = {\n",
    "        dataset: np.array(params[dataset])\n",
    "        for dataset in params.keys()\n",
    "    }\n",
    "    \n",
    "    # Reshape datasets\n",
    "    for dataset in inputs.keys():\n",
    "        if dataset[0:1] == 'X':\n",
    "            if reshape:\n",
    "                inputs[dataset] = np.reshape(inputs[dataset], (inputs[dataset].shape[0], inputs[dataset].shape[1], 1))\n",
    "            \n",
    "        elif dataset[0:1] == 'y':\n",
    "            inputs[dataset] = np_utils.to_categorical(np.array(inputs[dataset]), 3)\n",
    "            \n",
    "    # Set model parameters\n",
    "    epochs = 5\n",
    "    batch_size = 32\n",
    "    input_shape =  inputs['X_train'].shape\n",
    "    \n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(128, kernel_size = 2, activation='relu', input_shape = (input_shape[1], input_shape[2]), data_format = 'channels_first'))\n",
    "    model.add(Conv1D(128, kernel_size = 3, activation='relu'))\n",
    "    model.add(Conv1D(128, kernel_size = 4, activation='relu'))\n",
    "    model.add(Dropout(0.8))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "    model.compile('sgd', 'categorical_crossentropy', metrics = ['accuracy']) \n",
    "    \n",
    "    # Fit the training set over the model and correct on the validation set\n",
    "    model.fit(inputs['X_train'], inputs['y_train'],\n",
    "            batch_size = batch_size,\n",
    "            epochs = epochs,\n",
    "            validation_data = (inputs['X_validation'], inputs['y_validation']))\n",
    "    \n",
    "    # Get score over the test set\n",
    "    score, acc = model.evaluate(inputs['X_test'], inputs['y_test'])\n",
    "    print(acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_bert = {\n",
    "    dataset: [np.concatenate(np.array(statement)) for statement in bert[dataset].statement]\n",
    "    for dataset in bert.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_round(dataset):\n",
    "    # Store accuracies\n",
    "    accuracies = {\n",
    "        padding_len: 0.0 for padding_len in list(range(5,36))\n",
    "    }\n",
    "\n",
    "    for max_len in accuracies.keys():\n",
    "        padded_dataset = {\n",
    "            fold: sequence.pad_sequences(dataset[fold], maxlen = max_len, dtype = float)\n",
    "            for fold in dataset.keys()\n",
    "        }\n",
    "\n",
    "        accuracies[max_len] = get_cnn_score(padded_dataset['train'], padded_dataset['test'], padded_dataset['validation'], reshape = False)\n",
    "        \n",
    "    print(accuracies)\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0628 - acc: 0.4374 - val_loss: 1.0260 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0405 - acc: 0.4690 - val_loss: 1.0127 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0328 - acc: 0.4813 - val_loss: 1.0137 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0260 - acc: 0.4858 - val_loss: 1.0025 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0140 - acc: 0.4943 - val_loss: 1.0029 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5067193679658791\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0658 - acc: 0.4321 - val_loss: 1.0336 - val_acc: 0.4766\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0416 - acc: 0.4671 - val_loss: 1.0154 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0289 - acc: 0.4830 - val_loss: 1.0081 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0194 - acc: 0.4926 - val_loss: 1.0208 - val_acc: 0.4930\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0048 - acc: 0.5065 - val_loss: 1.0021 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5162055339737843\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0632 - acc: 0.4349 - val_loss: 1.0215 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0413 - acc: 0.4681 - val_loss: 1.0099 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0275 - acc: 0.4859 - val_loss: 1.0068 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0160 - acc: 0.4997 - val_loss: 1.0038 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0061 - acc: 0.5033 - val_loss: 0.9960 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5130434782844288\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0670 - acc: 0.4259 - val_loss: 1.0216 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0397 - acc: 0.4730 - val_loss: 1.0382 - val_acc: 0.4650\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0284 - acc: 0.4890 - val_loss: 1.0091 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0198 - acc: 0.4973 - val_loss: 1.0041 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0090 - acc: 0.5069 - val_loss: 1.0099 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5225296443865705\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0653 - acc: 0.4367 - val_loss: 1.0289 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0410 - acc: 0.4748 - val_loss: 1.0105 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0271 - acc: 0.4912 - val_loss: 1.0199 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0189 - acc: 0.4967 - val_loss: 1.0011 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0075 - acc: 0.5097 - val_loss: 1.0015 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5098814232547293\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0637 - acc: 0.4327 - val_loss: 1.0179 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0389 - acc: 0.4709 - val_loss: 1.0111 - val_acc: 0.4953\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0269 - acc: 0.4914 - val_loss: 1.0065 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0162 - acc: 0.4955 - val_loss: 0.9997 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0043 - acc: 0.5111 - val_loss: 0.9950 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.505928854131887\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0671 - acc: 0.4272 - val_loss: 1.0208 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0398 - acc: 0.4758 - val_loss: 1.0183 - val_acc: 0.4961\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0254 - acc: 0.4912 - val_loss: 1.0059 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0133 - acc: 0.4989 - val_loss: 0.9972 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0073 - acc: 0.4975 - val_loss: 0.9935 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5027667987016821\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0684 - acc: 0.4432 - val_loss: 1.0118 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0410 - acc: 0.4807 - val_loss: 1.0126 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0252 - acc: 0.4902 - val_loss: 1.0027 - val_acc: 0.4953\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0116 - acc: 0.5079 - val_loss: 1.0014 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0031 - acc: 0.5113 - val_loss: 0.9927 - val_acc: 0.5031\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.507509881752753\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0714 - acc: 0.4350 - val_loss: 1.0247 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0422 - acc: 0.4730 - val_loss: 1.0116 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0277 - acc: 0.4885 - val_loss: 1.0053 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0151 - acc: 0.5030 - val_loss: 1.0255 - val_acc: 0.4953\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0024 - acc: 0.5114 - val_loss: 0.9995 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.49644268807686365\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0685 - acc: 0.4322 - val_loss: 1.0167 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0411 - acc: 0.4752 - val_loss: 1.0061 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0291 - acc: 0.4887 - val_loss: 0.9969 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0176 - acc: 0.5065 - val_loss: 0.9997 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0048 - acc: 0.5093 - val_loss: 1.0077 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.48458498056698224\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 133s 13ms/step - loss: 1.0735 - acc: 0.4336 - val_loss: 1.0215 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0370 - acc: 0.4797 - val_loss: 1.0095 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0269 - acc: 0.4952 - val_loss: 1.0007 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0114 - acc: 0.5042 - val_loss: 1.0095 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0017 - acc: 0.5116 - val_loss: 0.9926 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.507509881752753\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 138s 13ms/step - loss: 1.0652 - acc: 0.4345 - val_loss: 1.0294 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0373 - acc: 0.4734 - val_loss: 1.0120 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0221 - acc: 0.4930 - val_loss: 1.0020 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0122 - acc: 0.5059 - val_loss: 1.0004 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9943 - acc: 0.5177 - val_loss: 1.0105 - val_acc: 0.5031\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.4996047431536814\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0692 - acc: 0.4346 - val_loss: 1.0283 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0395 - acc: 0.4787 - val_loss: 1.0061 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0227 - acc: 0.4938 - val_loss: 1.0029 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0099 - acc: 0.5093 - val_loss: 1.0006 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0026 - acc: 0.5135 - val_loss: 0.9952 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5114624508755952\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0712 - acc: 0.4348 - val_loss: 1.0157 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0393 - acc: 0.4726 - val_loss: 1.0095 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0231 - acc: 0.4949 - val_loss: 1.0019 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0126 - acc: 0.4999 - val_loss: 0.9997 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0013 - acc: 0.5187 - val_loss: 0.9994 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.49802371574484783\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 1.0700 - acc: 0.4366 - val_loss: 1.0161 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0386 - acc: 0.4795 - val_loss: 1.0018 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0221 - acc: 0.4935 - val_loss: 0.9988 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0129 - acc: 0.5052 - val_loss: 1.0102 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9962 - acc: 0.5131 - val_loss: 0.9936 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5011857710808162\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 146s 14ms/step - loss: 1.0722 - acc: 0.4313 - val_loss: 1.0233 - val_acc: 0.4930\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0389 - acc: 0.4715 - val_loss: 1.0050 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0235 - acc: 0.4961 - val_loss: 1.0160 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0128 - acc: 0.5032 - val_loss: 1.0066 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9954 - acc: 0.5190 - val_loss: 1.0112 - val_acc: 0.5047\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5043478261105157\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 147s 14ms/step - loss: 1.0746 - acc: 0.4254 - val_loss: 1.0233 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0411 - acc: 0.4716 - val_loss: 1.0105 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0318 - acc: 0.4872 - val_loss: 1.0100 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0117 - acc: 0.5030 - val_loss: 1.0010 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0012 - acc: 0.5142 - val_loss: 1.0128 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5209486169777369\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 148s 14ms/step - loss: 1.0755 - acc: 0.4265 - val_loss: 1.0262 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0415 - acc: 0.4736 - val_loss: 1.0382 - val_acc: 0.4657\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0235 - acc: 0.4939 - val_loss: 1.0078 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0154 - acc: 0.5008 - val_loss: 1.0035 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0020 - acc: 0.5193 - val_loss: 0.9975 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.516205533926666\n",
      "{5: 0.5067193679658791, 6: 0.5162055339737843, 7: 0.5130434782844288, 8: 0.5225296443865705, 9: 0.5098814232547293, 10: 0.505928854131887, 11: 0.5027667987016821, 12: 0.507509881752753, 13: 0.49644268807686365, 14: 0.48458498056698224, 15: 0.507509881752753, 16: 0.4996047431536814, 17: 0.5114624508755952, 18: 0.49802371574484783, 19: 0.5011857710808162, 20: 0.5043478261105157, 21: 0.5209486169777369, 22: 0.516205533926666}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0607 - acc: 0.4305 - val_loss: 1.0217 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0419 - acc: 0.4672 - val_loss: 1.0184 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0309 - acc: 0.4818 - val_loss: 1.0142 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0197 - acc: 0.4963 - val_loss: 1.0098 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0080 - acc: 0.5020 - val_loss: 1.0047 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5122529644975549\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0594 - acc: 0.4399 - val_loss: 1.0197 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0418 - acc: 0.4730 - val_loss: 1.0229 - val_acc: 0.4961\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0281 - acc: 0.4889 - val_loss: 1.0044 - val_acc: 0.5055\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0179 - acc: 0.5020 - val_loss: 1.0074 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 111s 11ms/step - loss: 1.0096 - acc: 0.5063 - val_loss: 1.0190 - val_acc: 0.4945\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5027667984896498\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0642 - acc: 0.4396 - val_loss: 1.0196 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0419 - acc: 0.4727 - val_loss: 1.0234 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0288 - acc: 0.4856 - val_loss: 1.0077 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 11ms/step - loss: 1.0198 - acc: 0.4969 - val_loss: 1.0109 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0057 - acc: 0.5076 - val_loss: 1.0220 - val_acc: 0.4992\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.485375494188942\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0620 - acc: 0.4427 - val_loss: 1.0201 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0410 - acc: 0.4729 - val_loss: 1.0105 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0286 - acc: 0.4837 - val_loss: 1.0057 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0182 - acc: 0.5002 - val_loss: 1.0250 - val_acc: 0.4907\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0060 - acc: 0.5116 - val_loss: 0.9968 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5177865616417685\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0677 - acc: 0.4322 - val_loss: 1.0190 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0376 - acc: 0.4775 - val_loss: 1.0228 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0284 - acc: 0.4944 - val_loss: 1.0146 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0191 - acc: 0.5005 - val_loss: 0.9988 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0082 - acc: 0.5097 - val_loss: 0.9959 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5106719371358397\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0655 - acc: 0.4343 - val_loss: 1.0224 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0368 - acc: 0.4737 - val_loss: 1.0108 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0271 - acc: 0.4874 - val_loss: 1.0100 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0174 - acc: 0.4959 - val_loss: 1.0074 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0036 - acc: 0.5134 - val_loss: 0.9965 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5138339924246897\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0627 - acc: 0.4400 - val_loss: 1.0273 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0410 - acc: 0.4689 - val_loss: 1.0164 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0241 - acc: 0.4938 - val_loss: 1.0050 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0150 - acc: 0.4992 - val_loss: 1.0070 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0023 - acc: 0.5083 - val_loss: 1.0331 - val_acc: 0.4860\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.48695652180980786\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0676 - acc: 0.4341 - val_loss: 1.0165 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0399 - acc: 0.4739 - val_loss: 1.0085 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0276 - acc: 0.4907 - val_loss: 1.0050 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0145 - acc: 0.5005 - val_loss: 1.0021 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0030 - acc: 0.5172 - val_loss: 1.0270 - val_acc: 0.4945\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.4861660080229341\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0717 - acc: 0.4341 - val_loss: 1.0182 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0401 - acc: 0.4740 - val_loss: 1.0169 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0251 - acc: 0.4924 - val_loss: 1.0031 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0126 - acc: 0.5067 - val_loss: 1.0416 - val_acc: 0.4727\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0018 - acc: 0.5151 - val_loss: 1.0194 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.507509881752753\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0757 - acc: 0.4299 - val_loss: 1.0125 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0387 - acc: 0.4811 - val_loss: 1.0262 - val_acc: 0.4891\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0272 - acc: 0.4945 - val_loss: 1.0151 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0103 - acc: 0.5022 - val_loss: 1.0029 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0007 - acc: 0.5130 - val_loss: 0.9971 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5027667987488004\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 1.0699 - acc: 0.4389 - val_loss: 1.0229 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0392 - acc: 0.4792 - val_loss: 1.0086 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0248 - acc: 0.5009 - val_loss: 1.0237 - val_acc: 0.4938\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0140 - acc: 0.5027 - val_loss: 1.0068 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9988 - acc: 0.5130 - val_loss: 1.0078 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.5146245063058001\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0679 - acc: 0.4389 - val_loss: 1.0203 - val_acc: 0.5132\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0359 - acc: 0.4745 - val_loss: 1.0065 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0172 - acc: 0.5013 - val_loss: 0.9999 - val_acc: 0.5187\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0096 - acc: 0.5057 - val_loss: 0.9964 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 0.9983 - acc: 0.5156 - val_loss: 0.9963 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 6s 4ms/step\n",
      "0.4980237156977295\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0707 - acc: 0.4367 - val_loss: 1.0412 - val_acc: 0.4548\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0401 - acc: 0.4781 - val_loss: 1.0096 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0221 - acc: 0.4986 - val_loss: 1.0191 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0128 - acc: 0.5006 - val_loss: 0.9964 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0043 - acc: 0.5158 - val_loss: 1.0084 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.4814229252310138\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0716 - acc: 0.4404 - val_loss: 1.0156 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0400 - acc: 0.4751 - val_loss: 1.0044 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0226 - acc: 0.4982 - val_loss: 1.0068 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0149 - acc: 0.4994 - val_loss: 0.9970 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9997 - acc: 0.5149 - val_loss: 0.9958 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5098814233018476\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 1.0701 - acc: 0.4302 - val_loss: 1.0159 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0382 - acc: 0.4808 - val_loss: 1.0080 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0260 - acc: 0.4866 - val_loss: 1.0163 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0128 - acc: 0.5007 - val_loss: 1.0121 - val_acc: 0.4945\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9965 - acc: 0.5173 - val_loss: 0.9959 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.513833992471808\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 145s 14ms/step - loss: 1.0785 - acc: 0.4338 - val_loss: 1.0226 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0396 - acc: 0.4752 - val_loss: 1.0080 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0246 - acc: 0.4922 - val_loss: 0.9982 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0089 - acc: 0.5043 - val_loss: 0.9968 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9975 - acc: 0.5161 - val_loss: 0.9986 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5019762849619266\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 1.0715 - acc: 0.4313 - val_loss: 1.0176 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0400 - acc: 0.4687 - val_loss: 1.0070 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0248 - acc: 0.4958 - val_loss: 0.9996 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0145 - acc: 0.5044 - val_loss: 1.0104 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0025 - acc: 0.5120 - val_loss: 0.9952 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.49723320191085574\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 150s 15ms/step - loss: 1.0754 - acc: 0.4249 - val_loss: 1.0149 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0399 - acc: 0.4780 - val_loss: 1.0078 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0292 - acc: 0.4940 - val_loss: 1.0051 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0115 - acc: 0.5025 - val_loss: 1.0066 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0004 - acc: 0.5078 - val_loss: 0.9939 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5106719370887214\n",
      "{5: 0.5122529644975549, 6: 0.5027667984896498, 7: 0.485375494188942, 8: 0.5177865616417685, 9: 0.5106719371358397, 10: 0.5138339924246897, 11: 0.48695652180980786, 12: 0.4861660080229341, 13: 0.507509881752753, 14: 0.5027667987488004, 15: 0.5146245063058001, 16: 0.4980237156977295, 17: 0.4814229252310138, 18: 0.5098814233018476, 19: 0.513833992471808, 20: 0.5019762849619266, 21: 0.49723320191085574, 22: 0.5106719370887214}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0608 - acc: 0.4430 - val_loss: 1.0204 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0423 - acc: 0.4717 - val_loss: 1.0211 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 111s 11ms/step - loss: 1.0301 - acc: 0.4813 - val_loss: 1.0149 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 111s 11ms/step - loss: 1.0202 - acc: 0.4890 - val_loss: 1.0097 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0114 - acc: 0.5017 - val_loss: 1.0062 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.49328063278801354\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0639 - acc: 0.4344 - val_loss: 1.0250 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0409 - acc: 0.4747 - val_loss: 1.0105 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0262 - acc: 0.4927 - val_loss: 1.0202 - val_acc: 0.4938\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0205 - acc: 0.4924 - val_loss: 1.0011 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0107 - acc: 0.5040 - val_loss: 0.9997 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5122529644975549\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0664 - acc: 0.4371 - val_loss: 1.0219 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0408 - acc: 0.4731 - val_loss: 1.0106 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0267 - acc: 0.4875 - val_loss: 1.0075 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0195 - acc: 0.4985 - val_loss: 1.0142 - val_acc: 0.4977\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0092 - acc: 0.5058 - val_loss: 1.0022 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.4893280636180531\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0657 - acc: 0.4360 - val_loss: 1.0219 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0412 - acc: 0.4734 - val_loss: 1.0091 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0287 - acc: 0.4863 - val_loss: 1.0158 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0151 - acc: 0.5009 - val_loss: 0.9974 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0085 - acc: 0.5084 - val_loss: 0.9939 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.509881423207611\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0634 - acc: 0.4439 - val_loss: 1.0297 - val_acc: 0.4868\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0407 - acc: 0.4763 - val_loss: 1.0132 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0270 - acc: 0.4862 - val_loss: 1.0043 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0136 - acc: 0.4999 - val_loss: 1.0265 - val_acc: 0.4992\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0026 - acc: 0.5103 - val_loss: 0.9999 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5043478264167846\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0651 - acc: 0.4319 - val_loss: 1.0257 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0398 - acc: 0.4744 - val_loss: 1.0208 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0249 - acc: 0.4908 - val_loss: 1.0144 - val_acc: 0.4969\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0158 - acc: 0.5011 - val_loss: 1.0120 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0033 - acc: 0.5067 - val_loss: 1.0097 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5090909094678555\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0672 - acc: 0.4363 - val_loss: 1.0192 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0389 - acc: 0.4772 - val_loss: 1.0136 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0245 - acc: 0.4915 - val_loss: 1.0163 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0167 - acc: 0.5022 - val_loss: 1.0020 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0030 - acc: 0.5050 - val_loss: 0.9953 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5201581031437448\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0715 - acc: 0.4332 - val_loss: 1.0210 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0427 - acc: 0.4724 - val_loss: 1.0174 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0286 - acc: 0.4909 - val_loss: 1.0024 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0182 - acc: 0.4985 - val_loss: 0.9991 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0066 - acc: 0.5093 - val_loss: 1.0027 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5027667987959187\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0674 - acc: 0.4362 - val_loss: 1.0122 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0423 - acc: 0.4723 - val_loss: 1.0092 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0276 - acc: 0.4939 - val_loss: 1.0050 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0134 - acc: 0.5033 - val_loss: 0.9959 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0058 - acc: 0.5035 - val_loss: 0.9999 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.4948616601026105\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0692 - acc: 0.4362 - val_loss: 1.0412 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0416 - acc: 0.4820 - val_loss: 1.0151 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0250 - acc: 0.4911 - val_loss: 1.0157 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0145 - acc: 0.5013 - val_loss: 0.9980 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0032 - acc: 0.5090 - val_loss: 1.0134 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5043478264639029\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0734 - acc: 0.4319 - val_loss: 1.0163 - val_acc: 0.4899\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0424 - acc: 0.4731 - val_loss: 1.0235 - val_acc: 0.4922\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 118s 11ms/step - loss: 1.0235 - acc: 0.4924 - val_loss: 1.0012 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0113 - acc: 0.5026 - val_loss: 0.9973 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0039 - acc: 0.5118 - val_loss: 0.9937 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5067193679187609\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0689 - acc: 0.4260 - val_loss: 1.0366 - val_acc: 0.4743\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0422 - acc: 0.4748 - val_loss: 1.0066 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0288 - acc: 0.4878 - val_loss: 1.0012 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0127 - acc: 0.5034 - val_loss: 0.9946 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0049 - acc: 0.5089 - val_loss: 1.0083 - val_acc: 0.5070\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5154150200926739\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0664 - acc: 0.4362 - val_loss: 1.0201 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 1.0393 - acc: 0.4771 - val_loss: 1.0083 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0189 - acc: 0.4987 - val_loss: 0.9985 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0067 - acc: 0.5100 - val_loss: 0.9987 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9970 - acc: 0.5153 - val_loss: 0.9942 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.499604743412832\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0711 - acc: 0.4366 - val_loss: 1.0196 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0364 - acc: 0.4766 - val_loss: 1.0049 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0213 - acc: 0.4958 - val_loss: 1.0057 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0111 - acc: 0.5043 - val_loss: 0.9927 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0004 - acc: 0.5134 - val_loss: 0.9935 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.49723320191085574\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 147s 14ms/step - loss: 1.0712 - acc: 0.4286 - val_loss: 1.0225 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0429 - acc: 0.4731 - val_loss: 1.0137 - val_acc: 0.4992\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0261 - acc: 0.4952 - val_loss: 1.0061 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0150 - acc: 0.5020 - val_loss: 1.0013 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9978 - acc: 0.5105 - val_loss: 0.9983 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5043478264167846\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 149s 15ms/step - loss: 1.0742 - acc: 0.4294 - val_loss: 1.0198 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0396 - acc: 0.4748 - val_loss: 1.0180 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0280 - acc: 0.4864 - val_loss: 1.0091 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0101 - acc: 0.5093 - val_loss: 1.0005 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0007 - acc: 0.5099 - val_loss: 1.0055 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.507509881752753\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 150s 15ms/step - loss: 1.0744 - acc: 0.4289 - val_loss: 1.0279 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0400 - acc: 0.4751 - val_loss: 1.0155 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0241 - acc: 0.4933 - val_loss: 1.0058 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 157s 15ms/step - loss: 1.0108 - acc: 0.5050 - val_loss: 1.0033 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9997 - acc: 0.5122 - val_loss: 1.0019 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5154150200926739\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 151s 15ms/step - loss: 1.0713 - acc: 0.4380 - val_loss: 1.0149 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0422 - acc: 0.4747 - val_loss: 1.0101 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0253 - acc: 0.4943 - val_loss: 1.0031 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0108 - acc: 0.5042 - val_loss: 1.0034 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9974 - acc: 0.5132 - val_loss: 0.9993 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5067193679658791\n",
      "{5: 0.49328063278801354, 6: 0.5122529644975549, 7: 0.4893280636180531, 8: 0.509881423207611, 9: 0.5043478264167846, 10: 0.5090909094678555, 11: 0.5201581031437448, 12: 0.5027667987959187, 13: 0.4948616601026105, 14: 0.5043478264639029, 15: 0.5067193679187609, 16: 0.5154150200926739, 17: 0.499604743412832, 18: 0.49723320191085574, 19: 0.5043478264167846, 20: 0.507509881752753, 21: 0.5154150200926739, 22: 0.5067193679658791}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0652 - acc: 0.4383 - val_loss: 1.0169 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0441 - acc: 0.4652 - val_loss: 1.0133 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0327 - acc: 0.4826 - val_loss: 1.0104 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0219 - acc: 0.4890 - val_loss: 1.0037 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0130 - acc: 0.4996 - val_loss: 1.0065 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5169960477606581\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0615 - acc: 0.4355 - val_loss: 1.0312 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0413 - acc: 0.4695 - val_loss: 1.0231 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0291 - acc: 0.4854 - val_loss: 1.0059 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0201 - acc: 0.4898 - val_loss: 1.0018 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0076 - acc: 0.5077 - val_loss: 1.0007 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5067193679658791\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0622 - acc: 0.4404 - val_loss: 1.0222 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0426 - acc: 0.4706 - val_loss: 1.0368 - val_acc: 0.4657\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0296 - acc: 0.4867 - val_loss: 1.0186 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0189 - acc: 0.4952 - val_loss: 1.0054 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0091 - acc: 0.5078 - val_loss: 1.0024 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5193675889563655\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0613 - acc: 0.4440 - val_loss: 1.0303 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0414 - acc: 0.4695 - val_loss: 1.0210 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0298 - acc: 0.4862 - val_loss: 1.0186 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0184 - acc: 0.4984 - val_loss: 1.0050 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0070 - acc: 0.5064 - val_loss: 1.0072 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5130434783315471\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0626 - acc: 0.4414 - val_loss: 1.0245 - val_acc: 0.4945\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0405 - acc: 0.4732 - val_loss: 1.0084 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0259 - acc: 0.4881 - val_loss: 1.0054 - val_acc: 0.5062\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0167 - acc: 0.4962 - val_loss: 1.0020 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0077 - acc: 0.5051 - val_loss: 1.0180 - val_acc: 0.5070\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.507509881446484\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0602 - acc: 0.4458 - val_loss: 1.0198 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0374 - acc: 0.4778 - val_loss: 1.0328 - val_acc: 0.4844\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0268 - acc: 0.4897 - val_loss: 1.0074 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0150 - acc: 0.4960 - val_loss: 1.0049 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0034 - acc: 0.5101 - val_loss: 1.0072 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5035573122765236\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0640 - acc: 0.4378 - val_loss: 1.0185 - val_acc: 0.4930\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0363 - acc: 0.4805 - val_loss: 1.0081 - val_acc: 0.5202\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0250 - acc: 0.4903 - val_loss: 1.0053 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0130 - acc: 0.5029 - val_loss: 1.0207 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0050 - acc: 0.5075 - val_loss: 0.9941 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5090909094207372\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0670 - acc: 0.4370 - val_loss: 1.0310 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0409 - acc: 0.4738 - val_loss: 1.0132 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0268 - acc: 0.4915 - val_loss: 1.0017 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0160 - acc: 0.5012 - val_loss: 1.0033 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0060 - acc: 0.5081 - val_loss: 0.9995 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5067193679658791\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0713 - acc: 0.4338 - val_loss: 1.0198 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0380 - acc: 0.4810 - val_loss: 1.0154 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0258 - acc: 0.4937 - val_loss: 1.0039 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0124 - acc: 0.5073 - val_loss: 0.9989 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0020 - acc: 0.5112 - val_loss: 0.9986 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.49644268807686365\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0702 - acc: 0.4356 - val_loss: 1.0126 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0379 - acc: 0.4807 - val_loss: 1.0301 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 118s 11ms/step - loss: 1.0274 - acc: 0.4916 - val_loss: 1.0039 - val_acc: 0.5241\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0107 - acc: 0.5040 - val_loss: 1.0032 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 118s 11ms/step - loss: 1.0045 - acc: 0.5095 - val_loss: 0.9936 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5090909093736189\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 138s 13ms/step - loss: 1.0695 - acc: 0.4379 - val_loss: 1.0140 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0367 - acc: 0.4835 - val_loss: 1.0100 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0254 - acc: 0.4915 - val_loss: 1.0055 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0134 - acc: 0.5062 - val_loss: 1.0000 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0028 - acc: 0.5166 - val_loss: 0.9937 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.5035573126299108\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 1.0707 - acc: 0.4351 - val_loss: 1.0290 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0360 - acc: 0.4828 - val_loss: 1.0082 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0190 - acc: 0.4979 - val_loss: 1.0028 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0109 - acc: 0.5043 - val_loss: 0.9961 - val_acc: 0.5280\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 0.9974 - acc: 0.5128 - val_loss: 0.9933 - val_acc: 0.5280\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5043478263696664\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 145s 14ms/step - loss: 1.0696 - acc: 0.4350 - val_loss: 1.0298 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0394 - acc: 0.4765 - val_loss: 1.0232 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0241 - acc: 0.4953 - val_loss: 1.0038 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0126 - acc: 0.5085 - val_loss: 1.0017 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0028 - acc: 0.5124 - val_loss: 0.9971 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.49486166040887947\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 1.0702 - acc: 0.4296 - val_loss: 1.0177 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0411 - acc: 0.4754 - val_loss: 1.0151 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0256 - acc: 0.4969 - val_loss: 1.0012 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0166 - acc: 0.5029 - val_loss: 1.0116 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0066 - acc: 0.5099 - val_loss: 0.9998 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.507509881446484\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 196s 19ms/step - loss: 1.0701 - acc: 0.4346 - val_loss: 1.0262 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0387 - acc: 0.4777 - val_loss: 1.0209 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0250 - acc: 0.4938 - val_loss: 1.0042 - val_acc: 0.5101\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0120 - acc: 0.5033 - val_loss: 0.9987 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9978 - acc: 0.5173 - val_loss: 0.9945 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5106719370416031\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 152s 15ms/step - loss: 1.0685 - acc: 0.4330 - val_loss: 1.0268 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0380 - acc: 0.4801 - val_loss: 1.0182 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0225 - acc: 0.4940 - val_loss: 1.0065 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0065 - acc: 0.5085 - val_loss: 1.0135 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9974 - acc: 0.5183 - val_loss: 0.9954 - val_acc: 0.5273\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5059288540847687\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 151s 15ms/step - loss: 1.0752 - acc: 0.4339 - val_loss: 1.0139 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0433 - acc: 0.4662 - val_loss: 1.0036 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0244 - acc: 0.4873 - val_loss: 1.0069 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0138 - acc: 0.5043 - val_loss: 1.0050 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0057 - acc: 0.5112 - val_loss: 0.9980 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.516205533926666\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 151s 15ms/step - loss: 1.0702 - acc: 0.4346 - val_loss: 1.0392 - val_acc: 0.4502\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0395 - acc: 0.4741 - val_loss: 1.0091 - val_acc: 0.5202\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0239 - acc: 0.4948 - val_loss: 1.0004 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0111 - acc: 0.5039 - val_loss: 0.9974 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9953 - acc: 0.5190 - val_loss: 0.9995 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5090909094207372\n",
      "{5: 0.5169960477606581, 6: 0.5067193679658791, 7: 0.5193675889563655, 8: 0.5130434783315471, 9: 0.507509881446484, 10: 0.5035573122765236, 11: 0.5090909094207372, 12: 0.5067193679658791, 13: 0.49644268807686365, 14: 0.5090909093736189, 15: 0.5035573126299108, 16: 0.5043478263696664, 17: 0.49486166040887947, 18: 0.507509881446484, 19: 0.5106719370416031, 20: 0.5059288540847687, 21: 0.516205533926666, 22: 0.5090909094207372}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0627 - acc: 0.4388 - val_loss: 1.0254 - val_acc: 0.5109\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0431 - acc: 0.4660 - val_loss: 1.0182 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0311 - acc: 0.4810 - val_loss: 1.0139 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0230 - acc: 0.4911 - val_loss: 1.0067 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0111 - acc: 0.5027 - val_loss: 1.0218 - val_acc: 0.4852\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.49249011860063424\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0655 - acc: 0.4345 - val_loss: 1.0229 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0423 - acc: 0.4722 - val_loss: 1.0172 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0308 - acc: 0.4807 - val_loss: 1.0083 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0211 - acc: 0.4954 - val_loss: 1.0026 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0111 - acc: 0.5023 - val_loss: 1.0081 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5241106723137052\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0664 - acc: 0.4257 - val_loss: 1.0214 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0411 - acc: 0.4680 - val_loss: 1.0254 - val_acc: 0.4930\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0299 - acc: 0.4864 - val_loss: 1.0075 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0223 - acc: 0.4928 - val_loss: 1.0401 - val_acc: 0.4790\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0114 - acc: 0.5041 - val_loss: 1.0026 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.513833992471808\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0645 - acc: 0.4326 - val_loss: 1.0180 - val_acc: 0.5171\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0373 - acc: 0.4772 - val_loss: 1.0189 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0276 - acc: 0.4900 - val_loss: 1.0018 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0169 - acc: 0.5030 - val_loss: 1.0055 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0075 - acc: 0.5125 - val_loss: 1.0014 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5122529644975549\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 138s 13ms/step - loss: 1.0674 - acc: 0.4336 - val_loss: 1.0277 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0433 - acc: 0.4696 - val_loss: 1.0123 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0326 - acc: 0.4821 - val_loss: 1.0166 - val_acc: 0.5016\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0215 - acc: 0.4969 - val_loss: 1.0039 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0089 - acc: 0.5055 - val_loss: 0.9985 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5098814229484603\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 135s 13ms/step - loss: 1.0635 - acc: 0.4444 - val_loss: 1.0207 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0390 - acc: 0.4776 - val_loss: 1.0150 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0283 - acc: 0.4847 - val_loss: 1.0062 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0153 - acc: 0.5033 - val_loss: 0.9990 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0071 - acc: 0.5075 - val_loss: 0.9984 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 6s 4ms/step\n",
      "0.5083003956338633\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 301s 29ms/step - loss: 1.0638 - acc: 0.4336 - val_loss: 1.0194 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 287s 28ms/step - loss: 1.0384 - acc: 0.4781 - val_loss: 1.0085 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 224s 22ms/step - loss: 1.0247 - acc: 0.4932 - val_loss: 1.0013 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0123 - acc: 0.5024 - val_loss: 1.0020 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0040 - acc: 0.5055 - val_loss: 0.9993 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.4972332016517051\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0653 - acc: 0.4365 - val_loss: 1.0252 - val_acc: 0.5187\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 138s 13ms/step - loss: 1.0384 - acc: 0.4785 - val_loss: 1.0072 - val_acc: 0.5179\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0262 - acc: 0.4914 - val_loss: 1.0273 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 223s 22ms/step - loss: 1.0151 - acc: 0.5032 - val_loss: 1.0039 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 194s 19ms/step - loss: 1.0008 - acc: 0.5134 - val_loss: 0.9966 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.5051383402507766\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0663 - acc: 0.4366 - val_loss: 1.0269 - val_acc: 0.4945\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 153s 15ms/step - loss: 1.0394 - acc: 0.4680 - val_loss: 1.0171 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0217 - acc: 0.4979 - val_loss: 1.0147 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0157 - acc: 0.4978 - val_loss: 1.0054 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 145s 14ms/step - loss: 1.0016 - acc: 0.5160 - val_loss: 1.0039 - val_acc: 0.5070\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5059288540847687\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 1.0621 - acc: 0.4468 - val_loss: 1.0189 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 135s 13ms/step - loss: 1.0385 - acc: 0.4799 - val_loss: 1.0062 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 1.0222 - acc: 0.4918 - val_loss: 1.0070 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0104 - acc: 0.5072 - val_loss: 1.0203 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 137s 13ms/step - loss: 1.0010 - acc: 0.5136 - val_loss: 1.0050 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 6s 4ms/step\n",
      "0.49960474331859545\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0681 - acc: 0.4329 - val_loss: 1.0277 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 315s 31ms/step - loss: 1.0384 - acc: 0.4757 - val_loss: 1.0256 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 1.0234 - acc: 0.4956 - val_loss: 1.0045 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 135s 13ms/step - loss: 1.0136 - acc: 0.5032 - val_loss: 1.0194 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 133s 13ms/step - loss: 0.9989 - acc: 0.5128 - val_loss: 0.9961 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.5035573122765236\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 150s 15ms/step - loss: 1.0682 - acc: 0.4349 - val_loss: 1.0332 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 133s 13ms/step - loss: 1.0415 - acc: 0.4713 - val_loss: 1.0211 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0238 - acc: 0.4985 - val_loss: 1.0064 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0078 - acc: 0.5099 - val_loss: 1.0010 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 0.9976 - acc: 0.5160 - val_loss: 0.9959 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.5059288537784998\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 155s 15ms/step - loss: 1.0657 - acc: 0.4433 - val_loss: 1.0201 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0376 - acc: 0.4779 - val_loss: 1.0048 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0203 - acc: 0.4988 - val_loss: 0.9988 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 147s 14ms/step - loss: 1.0101 - acc: 0.5098 - val_loss: 0.9994 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 138s 14ms/step - loss: 0.9942 - acc: 0.5199 - val_loss: 1.0289 - val_acc: 0.5016\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.47905138342276865\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 153s 15ms/step - loss: 1.0683 - acc: 0.4322 - val_loss: 1.0215 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0385 - acc: 0.4759 - val_loss: 1.0075 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0216 - acc: 0.5027 - val_loss: 1.0038 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0103 - acc: 0.4992 - val_loss: 1.0095 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 135s 13ms/step - loss: 0.9967 - acc: 0.5125 - val_loss: 1.0043 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5003952572468241\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 157s 15ms/step - loss: 1.0688 - acc: 0.4362 - val_loss: 1.0152 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0384 - acc: 0.4784 - val_loss: 1.0152 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0256 - acc: 0.4947 - val_loss: 0.9993 - val_acc: 0.5249\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0126 - acc: 0.4985 - val_loss: 0.9991 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 0.9989 - acc: 0.5183 - val_loss: 0.9939 - val_acc: 0.5265\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5090909093265006\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 215s 21ms/step - loss: 1.0712 - acc: 0.4373 - val_loss: 1.0186 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0373 - acc: 0.4760 - val_loss: 1.0051 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0232 - acc: 0.4921 - val_loss: 1.0132 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 137s 13ms/step - loss: 1.0077 - acc: 0.5043 - val_loss: 0.9936 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 0.9928 - acc: 0.5170 - val_loss: 0.9943 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.4988142295788399\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0770 - acc: 0.4317 - val_loss: 1.0303 - val_acc: 0.4961\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0416 - acc: 0.4709 - val_loss: 1.0174 - val_acc: 0.4945\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 137s 13ms/step - loss: 1.0239 - acc: 0.4954 - val_loss: 1.0038 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0123 - acc: 0.5000 - val_loss: 1.0056 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9980 - acc: 0.5153 - val_loss: 0.9975 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5122529648038239\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0735 - acc: 0.4315 - val_loss: 1.0252 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0392 - acc: 0.4754 - val_loss: 1.0253 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0254 - acc: 0.4951 - val_loss: 1.0008 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0117 - acc: 0.5029 - val_loss: 1.0064 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0003 - acc: 0.5164 - val_loss: 1.0020 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.518577075381524\n",
      "{5: 0.49249011860063424, 6: 0.5241106723137052, 7: 0.513833992471808, 8: 0.5122529644975549, 9: 0.5098814229484603, 10: 0.5083003956338633, 11: 0.4972332016517051, 12: 0.5051383402507766, 13: 0.5059288540847687, 14: 0.49960474331859545, 15: 0.5035573122765236, 16: 0.5059288537784998, 17: 0.47905138342276865, 18: 0.5003952572468241, 19: 0.5090909093265006, 20: 0.4988142295788399, 21: 0.5122529648038239, 22: 0.518577075381524}\n",
      "CPU times: user 3d 17h 48min 25s, sys: 7h 20min 15s, total: 4d 1h 8min 41s\n",
      "Wall time: 16h 29min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cnn_bert_rounds = [calculate_round(concatenated_bert) for round in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{5: 0.5067193679658791,\n",
       "  6: 0.5162055339737843,\n",
       "  7: 0.5130434782844288,\n",
       "  8: 0.5225296443865705,\n",
       "  9: 0.5098814232547293,\n",
       "  10: 0.505928854131887,\n",
       "  11: 0.5027667987016821,\n",
       "  12: 0.507509881752753,\n",
       "  13: 0.49644268807686365,\n",
       "  14: 0.48458498056698224,\n",
       "  15: 0.507509881752753,\n",
       "  16: 0.4996047431536814,\n",
       "  17: 0.5114624508755952,\n",
       "  18: 0.49802371574484783,\n",
       "  19: 0.5011857710808162,\n",
       "  20: 0.5043478261105157,\n",
       "  21: 0.5209486169777369,\n",
       "  22: 0.516205533926666,\n",
       "  23: 0.49723320191085574,\n",
       "  24: 0.4758893284401875,\n",
       "  25: 0.5098814233018476,\n",
       "  26: 0.5043478264639029,\n",
       "  27: 0.5035573126299108,\n",
       "  28: 0.4877470355966817,\n",
       "  29: 0.5114624509698318,\n",
       "  30: 0.5083003953275944,\n",
       "  31: 0.5209486169306186,\n",
       "  32: 0.5090909094678555,\n",
       "  33: 0.5035573125827925,\n",
       "  34: 0.5169960478077764,\n",
       "  35: 0.5035573125827925},\n",
       " {5: 0.5122529644975549,\n",
       "  6: 0.5027667984896498,\n",
       "  7: 0.485375494188942,\n",
       "  8: 0.5177865616417685,\n",
       "  9: 0.5106719371358397,\n",
       "  10: 0.5138339924246897,\n",
       "  11: 0.48695652180980786,\n",
       "  12: 0.4861660080229341,\n",
       "  13: 0.507509881752753,\n",
       "  14: 0.5027667987488004,\n",
       "  15: 0.5146245063058001,\n",
       "  16: 0.4980237156977295,\n",
       "  17: 0.4814229252310138,\n",
       "  18: 0.5098814233018476,\n",
       "  19: 0.513833992471808,\n",
       "  20: 0.5019762849619266,\n",
       "  21: 0.49723320191085574,\n",
       "  22: 0.5106719370887214,\n",
       "  23: 0.5114624509698318,\n",
       "  24: 0.507509881446484,\n",
       "  25: 0.5122529644504367,\n",
       "  26: 0.5114624509227135,\n",
       "  27: 0.5035573125356742,\n",
       "  28: 0.5177865615475319,\n",
       "  29: 0.5114624509698318,\n",
       "  30: 0.505928854131887,\n",
       "  31: 0.5083003952804761,\n",
       "  32: 0.5090909093736189,\n",
       "  33: 0.5098814233018476,\n",
       "  34: 0.5067193679187609,\n",
       "  35: 0.507509881752753},\n",
       " {5: 0.49328063278801354,\n",
       "  6: 0.5122529644975549,\n",
       "  7: 0.4893280636180531,\n",
       "  8: 0.509881423207611,\n",
       "  9: 0.5043478264167846,\n",
       "  10: 0.5090909094678555,\n",
       "  11: 0.5201581031437448,\n",
       "  12: 0.5027667987959187,\n",
       "  13: 0.4948616601026105,\n",
       "  14: 0.5043478264639029,\n",
       "  15: 0.5067193679187609,\n",
       "  16: 0.5154150200926739,\n",
       "  17: 0.499604743412832,\n",
       "  18: 0.49723320191085574,\n",
       "  19: 0.5043478264167846,\n",
       "  20: 0.507509881752753,\n",
       "  21: 0.5154150200926739,\n",
       "  22: 0.5067193679658791,\n",
       "  23: 0.4980237156977295,\n",
       "  24: 0.505138340297895,\n",
       "  25: 0.4877470359029506,\n",
       "  26: 0.49644268807686365,\n",
       "  27: 0.5019762846085394,\n",
       "  28: 0.5003952572468241,\n",
       "  29: 0.49249011895402145,\n",
       "  30: 0.5043478264639029,\n",
       "  31: 0.4988142296259582,\n",
       "  32: 0.5067193679187609,\n",
       "  33: 0.5098814232547293,\n",
       "  34: 0.5090909091144682,\n",
       "  35: 0.5106719370416031},\n",
       " {5: 0.5169960477606581,\n",
       "  6: 0.5067193679658791,\n",
       "  7: 0.5193675889563655,\n",
       "  8: 0.5130434783315471,\n",
       "  9: 0.507509881446484,\n",
       "  10: 0.5035573122765236,\n",
       "  11: 0.5090909094207372,\n",
       "  12: 0.5067193679658791,\n",
       "  13: 0.49644268807686365,\n",
       "  14: 0.5090909093736189,\n",
       "  15: 0.5035573126299108,\n",
       "  16: 0.5043478263696664,\n",
       "  17: 0.49486166040887947,\n",
       "  18: 0.507509881446484,\n",
       "  19: 0.5106719370416031,\n",
       "  20: 0.5059288540847687,\n",
       "  21: 0.516205533926666,\n",
       "  22: 0.5090909094207372,\n",
       "  23: 0.5019762846085394,\n",
       "  24: 0.49723320160458684,\n",
       "  25: 0.50197628486769,\n",
       "  26: 0.5059288540847687,\n",
       "  27: 0.5083003956338633,\n",
       "  28: 0.49644268807686365,\n",
       "  29: 0.5011857711279345,\n",
       "  30: 0.5138339923775714,\n",
       "  31: 0.513833992471808,\n",
       "  32: 0.5051383399445077,\n",
       "  33: 0.49169960481376046,\n",
       "  34: 0.505928854131887,\n",
       "  35: 0.5083003955867451},\n",
       " {5: 0.49249011860063424,\n",
       "  6: 0.5241106723137052,\n",
       "  7: 0.513833992471808,\n",
       "  8: 0.5122529644975549,\n",
       "  9: 0.5098814229484603,\n",
       "  10: 0.5083003956338633,\n",
       "  11: 0.4972332016517051,\n",
       "  12: 0.5051383402507766,\n",
       "  13: 0.5059288540847687,\n",
       "  14: 0.49960474331859545,\n",
       "  15: 0.5035573122765236,\n",
       "  16: 0.5059288537784998,\n",
       "  17: 0.47905138342276865,\n",
       "  18: 0.5003952572468241,\n",
       "  19: 0.5090909093265006,\n",
       "  20: 0.4988142295788399,\n",
       "  21: 0.5122529648038239,\n",
       "  22: 0.518577075381524,\n",
       "  23: 0.5011857710808162,\n",
       "  24: 0.496442687817713,\n",
       "  25: 0.5059288540847687,\n",
       "  26: 0.5177865612883813,\n",
       "  27: 0.5130434785906977,\n",
       "  28: 0.49802371574484783,\n",
       "  29: 0.5019762849148083,\n",
       "  30: 0.5098814233018476,\n",
       "  31: 0.5154150201397922,\n",
       "  32: 0.513043478496461,\n",
       "  33: 0.507509881446484,\n",
       "  34: 0.5059288537784998,\n",
       "  35: 0.5106719370416031}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_bert_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Round 0",
         "type": "scatter",
         "uid": "2effce92-43eb-4801-88cd-6aefaec50746",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5067193679658791,
          0.5162055339737843,
          0.5130434782844288,
          0.5225296443865705,
          0.5098814232547293,
          0.505928854131887,
          0.5027667987016821,
          0.507509881752753,
          0.49644268807686365,
          0.48458498056698224,
          0.507509881752753,
          0.4996047431536814,
          0.5114624508755952,
          0.49802371574484783,
          0.5011857710808162,
          0.5043478261105157,
          0.5209486169777369,
          0.516205533926666,
          0.49723320191085574,
          0.4758893284401875,
          0.5098814233018476,
          0.5043478264639029,
          0.5035573126299108,
          0.4877470355966817,
          0.5114624509698318,
          0.5083003953275944,
          0.5209486169306186,
          0.5090909094678555,
          0.5035573125827925,
          0.5169960478077764,
          0.5035573125827925
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 1",
         "type": "scatter",
         "uid": "5eaafa57-d4a1-437d-8362-90f888e6fe1c",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5122529644975549,
          0.5027667984896498,
          0.485375494188942,
          0.5177865616417685,
          0.5106719371358397,
          0.5138339924246897,
          0.48695652180980786,
          0.4861660080229341,
          0.507509881752753,
          0.5027667987488004,
          0.5146245063058001,
          0.4980237156977295,
          0.4814229252310138,
          0.5098814233018476,
          0.513833992471808,
          0.5019762849619266,
          0.49723320191085574,
          0.5106719370887214,
          0.5114624509698318,
          0.507509881446484,
          0.5122529644504367,
          0.5114624509227135,
          0.5035573125356742,
          0.5177865615475319,
          0.5114624509698318,
          0.505928854131887,
          0.5083003952804761,
          0.5090909093736189,
          0.5098814233018476,
          0.5067193679187609,
          0.507509881752753
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 2",
         "type": "scatter",
         "uid": "a249a111-582e-4ba0-afbd-c2f50accfcad",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.49328063278801354,
          0.5122529644975549,
          0.4893280636180531,
          0.509881423207611,
          0.5043478264167846,
          0.5090909094678555,
          0.5201581031437448,
          0.5027667987959187,
          0.4948616601026105,
          0.5043478264639029,
          0.5067193679187609,
          0.5154150200926739,
          0.499604743412832,
          0.49723320191085574,
          0.5043478264167846,
          0.507509881752753,
          0.5154150200926739,
          0.5067193679658791,
          0.4980237156977295,
          0.505138340297895,
          0.4877470359029506,
          0.49644268807686365,
          0.5019762846085394,
          0.5003952572468241,
          0.49249011895402145,
          0.5043478264639029,
          0.4988142296259582,
          0.5067193679187609,
          0.5098814232547293,
          0.5090909091144682,
          0.5106719370416031
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 3",
         "type": "scatter",
         "uid": "e914b6ab-8908-42b0-bd36-e737e33414dc",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5169960477606581,
          0.5067193679658791,
          0.5193675889563655,
          0.5130434783315471,
          0.507509881446484,
          0.5035573122765236,
          0.5090909094207372,
          0.5067193679658791,
          0.49644268807686365,
          0.5090909093736189,
          0.5035573126299108,
          0.5043478263696664,
          0.49486166040887947,
          0.507509881446484,
          0.5106719370416031,
          0.5059288540847687,
          0.516205533926666,
          0.5090909094207372,
          0.5019762846085394,
          0.49723320160458684,
          0.50197628486769,
          0.5059288540847687,
          0.5083003956338633,
          0.49644268807686365,
          0.5011857711279345,
          0.5138339923775714,
          0.513833992471808,
          0.5051383399445077,
          0.49169960481376046,
          0.505928854131887,
          0.5083003955867451
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 4",
         "type": "scatter",
         "uid": "49e5acb4-4c1d-4c65-8cf0-e3ba1dc94aa7",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.49249011860063424,
          0.5241106723137052,
          0.513833992471808,
          0.5122529644975549,
          0.5098814229484603,
          0.5083003956338633,
          0.4972332016517051,
          0.5051383402507766,
          0.5059288540847687,
          0.49960474331859545,
          0.5035573122765236,
          0.5059288537784998,
          0.47905138342276865,
          0.5003952572468241,
          0.5090909093265006,
          0.4988142295788399,
          0.5122529648038239,
          0.518577075381524,
          0.5011857710808162,
          0.496442687817713,
          0.5059288540847687,
          0.5177865612883813,
          0.5130434785906977,
          0.49802371574484783,
          0.5019762849148083,
          0.5098814233018476,
          0.5154150201397922,
          0.513043478496461,
          0.507509881446484,
          0.5059288537784998,
          0.5106719370416031
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "CNN test set accuracy of padded BERT dataset with variable maximum lengths"
        }
       }
      },
      "text/html": [
       "<div id=\"35ca2786-cb18-4987-a3df-9973288a05cc\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"35ca2786-cb18-4987-a3df-9973288a05cc\")) {\n",
       "    Plotly.newPlot(\"35ca2786-cb18-4987-a3df-9973288a05cc\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5067193679658791, 0.5162055339737843, 0.5130434782844288, 0.5225296443865705, 0.5098814232547293, 0.505928854131887, 0.5027667987016821, 0.507509881752753, 0.49644268807686365, 0.48458498056698224, 0.507509881752753, 0.4996047431536814, 0.5114624508755952, 0.49802371574484783, 0.5011857710808162, 0.5043478261105157, 0.5209486169777369, 0.516205533926666, 0.49723320191085574, 0.4758893284401875, 0.5098814233018476, 0.5043478264639029, 0.5035573126299108, 0.4877470355966817, 0.5114624509698318, 0.5083003953275944, 0.5209486169306186, 0.5090909094678555, 0.5035573125827925, 0.5169960478077764, 0.5035573125827925], \"type\": \"scatter\", \"uid\": \"2effce92-43eb-4801-88cd-6aefaec50746\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5122529644975549, 0.5027667984896498, 0.485375494188942, 0.5177865616417685, 0.5106719371358397, 0.5138339924246897, 0.48695652180980786, 0.4861660080229341, 0.507509881752753, 0.5027667987488004, 0.5146245063058001, 0.4980237156977295, 0.4814229252310138, 0.5098814233018476, 0.513833992471808, 0.5019762849619266, 0.49723320191085574, 0.5106719370887214, 0.5114624509698318, 0.507509881446484, 0.5122529644504367, 0.5114624509227135, 0.5035573125356742, 0.5177865615475319, 0.5114624509698318, 0.505928854131887, 0.5083003952804761, 0.5090909093736189, 0.5098814233018476, 0.5067193679187609, 0.507509881752753], \"type\": \"scatter\", \"uid\": \"5eaafa57-d4a1-437d-8362-90f888e6fe1c\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49328063278801354, 0.5122529644975549, 0.4893280636180531, 0.509881423207611, 0.5043478264167846, 0.5090909094678555, 0.5201581031437448, 0.5027667987959187, 0.4948616601026105, 0.5043478264639029, 0.5067193679187609, 0.5154150200926739, 0.499604743412832, 0.49723320191085574, 0.5043478264167846, 0.507509881752753, 0.5154150200926739, 0.5067193679658791, 0.4980237156977295, 0.505138340297895, 0.4877470359029506, 0.49644268807686365, 0.5019762846085394, 0.5003952572468241, 0.49249011895402145, 0.5043478264639029, 0.4988142296259582, 0.5067193679187609, 0.5098814232547293, 0.5090909091144682, 0.5106719370416031], \"type\": \"scatter\", \"uid\": \"a249a111-582e-4ba0-afbd-c2f50accfcad\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5169960477606581, 0.5067193679658791, 0.5193675889563655, 0.5130434783315471, 0.507509881446484, 0.5035573122765236, 0.5090909094207372, 0.5067193679658791, 0.49644268807686365, 0.5090909093736189, 0.5035573126299108, 0.5043478263696664, 0.49486166040887947, 0.507509881446484, 0.5106719370416031, 0.5059288540847687, 0.516205533926666, 0.5090909094207372, 0.5019762846085394, 0.49723320160458684, 0.50197628486769, 0.5059288540847687, 0.5083003956338633, 0.49644268807686365, 0.5011857711279345, 0.5138339923775714, 0.513833992471808, 0.5051383399445077, 0.49169960481376046, 0.505928854131887, 0.5083003955867451], \"type\": \"scatter\", \"uid\": \"e914b6ab-8908-42b0-bd36-e737e33414dc\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49249011860063424, 0.5241106723137052, 0.513833992471808, 0.5122529644975549, 0.5098814229484603, 0.5083003956338633, 0.4972332016517051, 0.5051383402507766, 0.5059288540847687, 0.49960474331859545, 0.5035573122765236, 0.5059288537784998, 0.47905138342276865, 0.5003952572468241, 0.5090909093265006, 0.4988142295788399, 0.5122529648038239, 0.518577075381524, 0.5011857710808162, 0.496442687817713, 0.5059288540847687, 0.5177865612883813, 0.5130434785906977, 0.49802371574484783, 0.5019762849148083, 0.5098814233018476, 0.5154150201397922, 0.513043478496461, 0.507509881446484, 0.5059288537784998, 0.5106719370416031], \"type\": \"scatter\", \"uid\": \"49e5acb4-4c1d-4c65-8cf0-e3ba1dc94aa7\"}], {\"title\": {\"text\": \"CNN test set accuracy of padded BERT dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"35ca2786-cb18-4987-a3df-9973288a05cc\")) {window._Plotly.Plots.resize(document.getElementById(\"35ca2786-cb18-4987-a3df-9973288a05cc\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"35ca2786-cb18-4987-a3df-9973288a05cc\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"35ca2786-cb18-4987-a3df-9973288a05cc\")) {\n",
       "    Plotly.newPlot(\"35ca2786-cb18-4987-a3df-9973288a05cc\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5067193679658791, 0.5162055339737843, 0.5130434782844288, 0.5225296443865705, 0.5098814232547293, 0.505928854131887, 0.5027667987016821, 0.507509881752753, 0.49644268807686365, 0.48458498056698224, 0.507509881752753, 0.4996047431536814, 0.5114624508755952, 0.49802371574484783, 0.5011857710808162, 0.5043478261105157, 0.5209486169777369, 0.516205533926666, 0.49723320191085574, 0.4758893284401875, 0.5098814233018476, 0.5043478264639029, 0.5035573126299108, 0.4877470355966817, 0.5114624509698318, 0.5083003953275944, 0.5209486169306186, 0.5090909094678555, 0.5035573125827925, 0.5169960478077764, 0.5035573125827925], \"type\": \"scatter\", \"uid\": \"2effce92-43eb-4801-88cd-6aefaec50746\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5122529644975549, 0.5027667984896498, 0.485375494188942, 0.5177865616417685, 0.5106719371358397, 0.5138339924246897, 0.48695652180980786, 0.4861660080229341, 0.507509881752753, 0.5027667987488004, 0.5146245063058001, 0.4980237156977295, 0.4814229252310138, 0.5098814233018476, 0.513833992471808, 0.5019762849619266, 0.49723320191085574, 0.5106719370887214, 0.5114624509698318, 0.507509881446484, 0.5122529644504367, 0.5114624509227135, 0.5035573125356742, 0.5177865615475319, 0.5114624509698318, 0.505928854131887, 0.5083003952804761, 0.5090909093736189, 0.5098814233018476, 0.5067193679187609, 0.507509881752753], \"type\": \"scatter\", \"uid\": \"5eaafa57-d4a1-437d-8362-90f888e6fe1c\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49328063278801354, 0.5122529644975549, 0.4893280636180531, 0.509881423207611, 0.5043478264167846, 0.5090909094678555, 0.5201581031437448, 0.5027667987959187, 0.4948616601026105, 0.5043478264639029, 0.5067193679187609, 0.5154150200926739, 0.499604743412832, 0.49723320191085574, 0.5043478264167846, 0.507509881752753, 0.5154150200926739, 0.5067193679658791, 0.4980237156977295, 0.505138340297895, 0.4877470359029506, 0.49644268807686365, 0.5019762846085394, 0.5003952572468241, 0.49249011895402145, 0.5043478264639029, 0.4988142296259582, 0.5067193679187609, 0.5098814232547293, 0.5090909091144682, 0.5106719370416031], \"type\": \"scatter\", \"uid\": \"a249a111-582e-4ba0-afbd-c2f50accfcad\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5169960477606581, 0.5067193679658791, 0.5193675889563655, 0.5130434783315471, 0.507509881446484, 0.5035573122765236, 0.5090909094207372, 0.5067193679658791, 0.49644268807686365, 0.5090909093736189, 0.5035573126299108, 0.5043478263696664, 0.49486166040887947, 0.507509881446484, 0.5106719370416031, 0.5059288540847687, 0.516205533926666, 0.5090909094207372, 0.5019762846085394, 0.49723320160458684, 0.50197628486769, 0.5059288540847687, 0.5083003956338633, 0.49644268807686365, 0.5011857711279345, 0.5138339923775714, 0.513833992471808, 0.5051383399445077, 0.49169960481376046, 0.505928854131887, 0.5083003955867451], \"type\": \"scatter\", \"uid\": \"e914b6ab-8908-42b0-bd36-e737e33414dc\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49249011860063424, 0.5241106723137052, 0.513833992471808, 0.5122529644975549, 0.5098814229484603, 0.5083003956338633, 0.4972332016517051, 0.5051383402507766, 0.5059288540847687, 0.49960474331859545, 0.5035573122765236, 0.5059288537784998, 0.47905138342276865, 0.5003952572468241, 0.5090909093265006, 0.4988142295788399, 0.5122529648038239, 0.518577075381524, 0.5011857710808162, 0.496442687817713, 0.5059288540847687, 0.5177865612883813, 0.5130434785906977, 0.49802371574484783, 0.5019762849148083, 0.5098814233018476, 0.5154150201397922, 0.513043478496461, 0.507509881446484, 0.5059288537784998, 0.5106719370416031], \"type\": \"scatter\", \"uid\": \"49e5acb4-4c1d-4c65-8cf0-e3ba1dc94aa7\"}], {\"title\": {\"text\": \"CNN test set accuracy of padded BERT dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"35ca2786-cb18-4987-a3df-9973288a05cc\")) {window._Plotly.Plots.resize(document.getElementById(\"35ca2786-cb18-4987-a3df-9973288a05cc\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traces = cnn_bert_rounds\n",
    "\n",
    "# Create traces\n",
    "def create_scatter(counter):\n",
    "    acc_dict = traces[counter]\n",
    "    \n",
    "    return go.Scatter(\n",
    "        x = list(acc_dict.keys()),\n",
    "        y = list(acc_dict.values()),\n",
    "        mode = 'lines+markers',\n",
    "        name = 'Round ' + str(counter)\n",
    "    )\n",
    "\n",
    "trace_data = [create_scatter(trace) for trace in range(len(traces))]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'CNN test set accuracy of padded BERT dataset with variable maximum lengths',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = trace_data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_elmo = {\n",
    "    dataset: [np.concatenate(np.array(statement)) for statement in elmo[dataset].statement]\n",
    "    for dataset in elmo.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "2019-06-06 17:27:55,650 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "2019-06-06 17:27:55,736 From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-06-06 17:27:55,831 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 148s 14ms/step - loss: 1.0634 - acc: 0.4377 - val_loss: 1.0281 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0388 - acc: 0.4691 - val_loss: 1.0276 - val_acc: 0.4766\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0272 - acc: 0.4848 - val_loss: 1.0075 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0152 - acc: 0.5015 - val_loss: 1.0009 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0032 - acc: 0.5063 - val_loss: 1.0124 - val_acc: 0.4961\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.49565217424287156\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0649 - acc: 0.4289 - val_loss: 1.0213 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0392 - acc: 0.4790 - val_loss: 1.0129 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0273 - acc: 0.4854 - val_loss: 1.0126 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0121 - acc: 0.5003 - val_loss: 1.0068 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 0.9998 - acc: 0.5116 - val_loss: 1.0072 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5114624506164446\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0691 - acc: 0.4293 - val_loss: 1.0227 - val_acc: 0.5210\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0430 - acc: 0.4644 - val_loss: 1.0466 - val_acc: 0.4673\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0256 - acc: 0.4885 - val_loss: 1.0194 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0100 - acc: 0.5038 - val_loss: 1.0070 - val_acc: 0.5257\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0023 - acc: 0.5161 - val_loss: 1.0035 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5169960477606581\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0693 - acc: 0.4295 - val_loss: 1.0191 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0400 - acc: 0.4763 - val_loss: 1.0068 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0247 - acc: 0.4957 - val_loss: 1.0099 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0047 - acc: 0.5105 - val_loss: 0.9980 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 0.9918 - acc: 0.5213 - val_loss: 0.9987 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5122529644504367\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0699 - acc: 0.4253 - val_loss: 1.0297 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0379 - acc: 0.4741 - val_loss: 1.0096 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0247 - acc: 0.4946 - val_loss: 1.0000 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0071 - acc: 0.5088 - val_loss: 1.0081 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 0.9919 - acc: 0.5207 - val_loss: 1.0052 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.49249011860063424\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0633 - acc: 0.4319 - val_loss: 1.0190 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0321 - acc: 0.4764 - val_loss: 1.0080 - val_acc: 0.5202\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0162 - acc: 0.5043 - val_loss: 1.0043 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0044 - acc: 0.5118 - val_loss: 1.0133 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 0.9891 - acc: 0.5230 - val_loss: 1.0007 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.49644268807686365\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0667 - acc: 0.4401 - val_loss: 1.0161 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0364 - acc: 0.4795 - val_loss: 1.0064 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0181 - acc: 0.4974 - val_loss: 0.9965 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0017 - acc: 0.5122 - val_loss: 1.0060 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 0.9865 - acc: 0.5237 - val_loss: 0.9975 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5075098817998712\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0692 - acc: 0.4333 - val_loss: 1.0229 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0358 - acc: 0.4839 - val_loss: 1.0106 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0200 - acc: 0.4957 - val_loss: 1.0067 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0007 - acc: 0.5116 - val_loss: 1.0024 - val_acc: 0.5148\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 115s 11ms/step - loss: 0.9866 - acc: 0.5274 - val_loss: 1.0097 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - ETA:  - 5s 4ms/step\n",
      "0.5185770751223734\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0726 - acc: 0.4361 - val_loss: 1.0170 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0353 - acc: 0.4823 - val_loss: 1.0163 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0192 - acc: 0.4979 - val_loss: 1.0048 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 0.9997 - acc: 0.5165 - val_loss: 1.0008 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 0.9799 - acc: 0.5297 - val_loss: 0.9985 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.507509881752753\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0737 - acc: 0.4322 - val_loss: 1.0191 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0338 - acc: 0.4839 - val_loss: 1.0006 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0174 - acc: 0.5008 - val_loss: 1.0058 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0020 - acc: 0.5121 - val_loss: 0.9981 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 0.9854 - acc: 0.5206 - val_loss: 0.9955 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.4988142295788399\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0713 - acc: 0.4351 - val_loss: 1.0136 - val_acc: 0.5164\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0351 - acc: 0.4792 - val_loss: 1.0058 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0138 - acc: 0.5092 - val_loss: 1.0164 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0035 - acc: 0.5143 - val_loss: 0.9958 - val_acc: 0.5249\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 0.9823 - acc: 0.5255 - val_loss: 0.9933 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5043478264639029\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0705 - acc: 0.4286 - val_loss: 1.0229 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0362 - acc: 0.4822 - val_loss: 1.0044 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0190 - acc: 0.5028 - val_loss: 1.0182 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9985 - acc: 0.5097 - val_loss: 0.9977 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 0.9819 - acc: 0.5269 - val_loss: 0.9979 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5122529647567056\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0759 - acc: 0.4280 - val_loss: 1.0478 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0390 - acc: 0.4815 - val_loss: 1.0072 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0172 - acc: 0.4978 - val_loss: 1.0114 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9981 - acc: 0.5174 - val_loss: 1.0015 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9751 - acc: 0.5318 - val_loss: 0.9941 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5067193679187609\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 137s 13ms/step - loss: 1.0764 - acc: 0.4326 - val_loss: 1.0181 - val_acc: 0.5210\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0421 - acc: 0.4716 - val_loss: 0.9973 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0167 - acc: 0.5044 - val_loss: 1.0025 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0000 - acc: 0.5133 - val_loss: 1.0205 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9775 - acc: 0.5358 - val_loss: 0.9899 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5090909094207372\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0719 - acc: 0.4394 - val_loss: 1.0286 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0357 - acc: 0.4830 - val_loss: 1.0083 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0140 - acc: 0.5021 - val_loss: 1.0049 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9983 - acc: 0.5160 - val_loss: 0.9886 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9788 - acc: 0.5319 - val_loss: 0.9833 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.513833992471808\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 145s 14ms/step - loss: 1.0751 - acc: 0.4305 - val_loss: 1.0125 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0341 - acc: 0.4829 - val_loss: 1.0054 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0157 - acc: 0.5039 - val_loss: 0.9951 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9945 - acc: 0.5168 - val_loss: 0.9904 - val_acc: 0.5312\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9767 - acc: 0.5336 - val_loss: 0.9957 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5059288540847687\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 1.0773 - acc: 0.4323 - val_loss: 1.0292 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0385 - acc: 0.4779 - val_loss: 1.0074 - val_acc: 0.5210\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0181 - acc: 0.5004 - val_loss: 1.0045 - val_acc: 0.5257\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9999 - acc: 0.5172 - val_loss: 0.9987 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9798 - acc: 0.5330 - val_loss: 0.9935 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5209486166243497\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 147s 14ms/step - loss: 1.0693 - acc: 0.4384 - val_loss: 1.0239 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0366 - acc: 0.4782 - val_loss: 1.0084 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0168 - acc: 0.5005 - val_loss: 0.9947 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0007 - acc: 0.5132 - val_loss: 0.9920 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9763 - acc: 0.5312 - val_loss: 0.9994 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.49565217424287156\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 149s 15ms/step - loss: 1.0796 - acc: 0.4376 - val_loss: 1.0234 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0417 - acc: 0.4739 - val_loss: 1.0178 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0220 - acc: 0.5025 - val_loss: 1.0053 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9976 - acc: 0.5126 - val_loss: 0.9961 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9822 - acc: 0.5294 - val_loss: 0.9972 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5122529647567056\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 146s 14ms/step - loss: 1.0904 - acc: 0.4325 - val_loss: 1.0260 - val_acc: 0.5132\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0354 - acc: 0.4827 - val_loss: 1.0065 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0173 - acc: 0.5014 - val_loss: 0.9982 - val_acc: 0.5257\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9964 - acc: 0.5214 - val_loss: 1.0102 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9773 - acc: 0.5305 - val_loss: 1.0002 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5098814229484603\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 152s 15ms/step - loss: 1.0768 - acc: 0.4339 - val_loss: 1.0173 - val_acc: 0.5109\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0356 - acc: 0.4830 - val_loss: 0.9995 - val_acc: 0.5226\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0125 - acc: 0.5016 - val_loss: 1.0006 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9924 - acc: 0.5212 - val_loss: 0.9966 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9746 - acc: 0.5324 - val_loss: 1.0022 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5272727276025554\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 152s 15ms/step - loss: 1.0714 - acc: 0.4396 - val_loss: 1.0252 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0393 - acc: 0.4805 - val_loss: 1.0013 - val_acc: 0.5241\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0177 - acc: 0.4995 - val_loss: 0.9898 - val_acc: 0.5273\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9897 - acc: 0.5266 - val_loss: 0.9968 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9744 - acc: 0.5363 - val_loss: 0.9841 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5225296445514844\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 154s 15ms/step - loss: 1.0737 - acc: 0.4404 - val_loss: 1.0173 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0328 - acc: 0.4850 - val_loss: 1.0014 - val_acc: 0.5265\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0164 - acc: 0.4996 - val_loss: 1.0090 - val_acc: 0.5249\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 0.9990 - acc: 0.5191 - val_loss: 0.9915 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9777 - acc: 0.5314 - val_loss: 0.9893 - val_acc: 0.5382\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5146245063058001\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 154s 15ms/step - loss: 1.0775 - acc: 0.4237 - val_loss: 1.0271 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0421 - acc: 0.4735 - val_loss: 1.0012 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0187 - acc: 0.4957 - val_loss: 1.0042 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9968 - acc: 0.5178 - val_loss: 0.9944 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9790 - acc: 0.5324 - val_loss: 0.9993 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5051383402507766\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0771 - acc: 0.4273 - val_loss: 1.0254 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0375 - acc: 0.4830 - val_loss: 1.0050 - val_acc: 0.5296\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0183 - acc: 0.5065 - val_loss: 1.0021 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9975 - acc: 0.5188 - val_loss: 0.9946 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9721 - acc: 0.5379 - val_loss: 1.0126 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "0.5059288537784998\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 159s 16ms/step - loss: 1.0779 - acc: 0.4306 - val_loss: 1.0238 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0388 - acc: 0.4743 - val_loss: 1.0226 - val_acc: 0.4953\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0193 - acc: 0.5016 - val_loss: 1.0085 - val_acc: 0.5241\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9984 - acc: 0.5187 - val_loss: 0.9919 - val_acc: 0.5265\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9754 - acc: 0.5271 - val_loss: 1.0068 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "0.5169960477606581\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0717 - acc: 0.4354 - val_loss: 1.0235 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 1.0424 - acc: 0.4739 - val_loss: 1.0097 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 1.0114 - acc: 0.5058 - val_loss: 1.0094 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 0.9940 - acc: 0.5231 - val_loss: 0.9956 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 154s 15ms/step - loss: 0.9715 - acc: 0.5389 - val_loss: 0.9930 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5051383402507766\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0714 - acc: 0.4329 - val_loss: 1.0288 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0375 - acc: 0.4790 - val_loss: 1.0199 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0167 - acc: 0.4984 - val_loss: 1.0185 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 0.9985 - acc: 0.5245 - val_loss: 0.9955 - val_acc: 0.5226\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 140s 14ms/step - loss: 0.9791 - acc: 0.5320 - val_loss: 0.9960 - val_acc: 0.5280\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.513833992471808\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0716 - acc: 0.4357 - val_loss: 1.0388 - val_acc: 0.4899\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0345 - acc: 0.4812 - val_loss: 1.0053 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0126 - acc: 0.5059 - val_loss: 1.0064 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 0.9965 - acc: 0.5195 - val_loss: 0.9943 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 0.9783 - acc: 0.5332 - val_loss: 1.0063 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "0.5003952569405552\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 173s 17ms/step - loss: 1.0696 - acc: 0.4360 - val_loss: 1.0258 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0358 - acc: 0.4804 - val_loss: 1.0143 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0112 - acc: 0.5046 - val_loss: 1.0122 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 0.9951 - acc: 0.5194 - val_loss: 1.0022 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 0.9736 - acc: 0.5328 - val_loss: 0.9950 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5106719370887214\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0742 - acc: 0.4359 - val_loss: 1.0247 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0327 - acc: 0.4831 - val_loss: 1.0083 - val_acc: 0.5296\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0171 - acc: 0.5011 - val_loss: 0.9974 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 0.9942 - acc: 0.5208 - val_loss: 0.9937 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 0.9775 - acc: 0.5310 - val_loss: 0.9954 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5019762849148083\n",
      "{5: 0.49565217424287156, 6: 0.5114624506164446, 7: 0.5169960477606581, 8: 0.5122529644504367, 9: 0.49249011860063424, 10: 0.49644268807686365, 11: 0.5075098817998712, 12: 0.5185770751223734, 13: 0.507509881752753, 14: 0.4988142295788399, 15: 0.5043478264639029, 16: 0.5122529647567056, 17: 0.5067193679187609, 18: 0.5090909094207372, 19: 0.513833992471808, 20: 0.5059288540847687, 21: 0.5209486166243497, 22: 0.49565217424287156, 23: 0.5122529647567056, 24: 0.5098814229484603, 25: 0.5272727276025554, 26: 0.5225296445514844, 27: 0.5146245063058001, 28: 0.5051383402507766, 29: 0.5059288537784998, 30: 0.5169960477606581, 31: 0.5051383402507766, 32: 0.513833992471808, 33: 0.5003952569405552, 34: 0.5106719370887214, 35: 0.5019762849148083}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0652 - acc: 0.4305 - val_loss: 1.0246 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0423 - acc: 0.4665 - val_loss: 1.0098 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0296 - acc: 0.4885 - val_loss: 1.0081 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0157 - acc: 0.4983 - val_loss: 1.0033 - val_acc: 0.5249\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0041 - acc: 0.5085 - val_loss: 1.0059 - val_acc: 0.5047\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5043478261105157\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0655 - acc: 0.4277 - val_loss: 1.0398 - val_acc: 0.4502\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0408 - acc: 0.4743 - val_loss: 1.0090 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0253 - acc: 0.4961 - val_loss: 1.0273 - val_acc: 0.4782\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0118 - acc: 0.5019 - val_loss: 0.9998 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 0.9971 - acc: 0.5139 - val_loss: 1.0112 - val_acc: 0.5039\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5114624509698318\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0646 - acc: 0.4406 - val_loss: 1.0206 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0402 - acc: 0.4755 - val_loss: 1.0130 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0265 - acc: 0.4854 - val_loss: 1.0050 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0069 - acc: 0.5093 - val_loss: 1.0009 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 0.9925 - acc: 0.5179 - val_loss: 0.9992 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5304347829385238\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0649 - acc: 0.4339 - val_loss: 1.0195 - val_acc: 0.4899\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0391 - acc: 0.4745 - val_loss: 1.0174 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0234 - acc: 0.4953 - val_loss: 1.0076 - val_acc: 0.5241\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0050 - acc: 0.5123 - val_loss: 1.0105 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 0.9932 - acc: 0.5158 - val_loss: 1.0020 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5067193679658791\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0673 - acc: 0.4343 - val_loss: 1.0137 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0349 - acc: 0.4821 - val_loss: 1.0045 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0192 - acc: 0.4983 - val_loss: 1.0224 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0055 - acc: 0.5086 - val_loss: 1.0017 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 0.9897 - acc: 0.5224 - val_loss: 0.9950 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.499604743412832\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0667 - acc: 0.4310 - val_loss: 1.0251 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0321 - acc: 0.4839 - val_loss: 1.0083 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0170 - acc: 0.5003 - val_loss: 1.0081 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0046 - acc: 0.5121 - val_loss: 1.0029 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 0.9892 - acc: 0.5237 - val_loss: 1.0007 - val_acc: 0.5343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5130434782844288\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0715 - acc: 0.4344 - val_loss: 1.0310 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0360 - acc: 0.4796 - val_loss: 1.0304 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0128 - acc: 0.4983 - val_loss: 1.0062 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0000 - acc: 0.5127 - val_loss: 1.0049 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9813 - acc: 0.5326 - val_loss: 1.0011 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5098814233018476\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0702 - acc: 0.4283 - val_loss: 1.0165 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0367 - acc: 0.4789 - val_loss: 1.0094 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0218 - acc: 0.4922 - val_loss: 1.0019 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0045 - acc: 0.5096 - val_loss: 0.9989 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 0.9896 - acc: 0.5234 - val_loss: 0.9971 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.505138340297895\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0701 - acc: 0.4296 - val_loss: 1.0183 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0371 - acc: 0.4780 - val_loss: 1.0030 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0182 - acc: 0.5046 - val_loss: 1.0007 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0066 - acc: 0.5128 - val_loss: 0.9939 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 0.9836 - acc: 0.5305 - val_loss: 1.0001 - val_acc: 0.5280\n",
      "1265/1265 [==============================] - 6s 4ms/step\n",
      "0.5011857707745473\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0733 - acc: 0.4265 - val_loss: 1.0283 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0395 - acc: 0.4773 - val_loss: 1.0056 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0173 - acc: 0.4943 - val_loss: 0.9981 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0022 - acc: 0.5153 - val_loss: 0.9943 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9804 - acc: 0.5315 - val_loss: 1.0018 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5019762849148083\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 133s 13ms/step - loss: 1.0737 - acc: 0.4277 - val_loss: 1.0278 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0367 - acc: 0.4802 - val_loss: 1.0051 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0172 - acc: 0.4986 - val_loss: 0.9933 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 0.9982 - acc: 0.5118 - val_loss: 1.0088 - val_acc: 0.5257\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9814 - acc: 0.5241 - val_loss: 0.9951 - val_acc: 0.5288\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5177865615946502\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 135s 13ms/step - loss: 1.0695 - acc: 0.4354 - val_loss: 1.0219 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0325 - acc: 0.4828 - val_loss: 1.0372 - val_acc: 0.4977\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0162 - acc: 0.4967 - val_loss: 1.0079 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9966 - acc: 0.5160 - val_loss: 1.0001 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9773 - acc: 0.5316 - val_loss: 1.0239 - val_acc: 0.5023\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5011857707745473\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0716 - acc: 0.4315 - val_loss: 1.0233 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0334 - acc: 0.4850 - val_loss: 1.0071 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0126 - acc: 0.5082 - val_loss: 0.9996 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9997 - acc: 0.5186 - val_loss: 0.9976 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9792 - acc: 0.5313 - val_loss: 0.9885 - val_acc: 0.5280\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5201581030966265\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0752 - acc: 0.4352 - val_loss: 1.0196 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0340 - acc: 0.4803 - val_loss: 1.0085 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0147 - acc: 0.5041 - val_loss: 1.0071 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9974 - acc: 0.5189 - val_loss: 1.0050 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9814 - acc: 0.5279 - val_loss: 1.0114 - val_acc: 0.5031\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.505138340297895\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0873 - acc: 0.4337 - val_loss: 1.0157 - val_acc: 0.5171\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0371 - acc: 0.4802 - val_loss: 1.0361 - val_acc: 0.4883\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0165 - acc: 0.4978 - val_loss: 1.0018 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9942 - acc: 0.5162 - val_loss: 0.9908 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9796 - acc: 0.5263 - val_loss: 0.9895 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5280632414365475\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 146s 14ms/step - loss: 1.0741 - acc: 0.4331 - val_loss: 1.0256 - val_acc: 0.4899\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0379 - acc: 0.4699 - val_loss: 1.0109 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0179 - acc: 0.4987 - val_loss: 1.0012 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9962 - acc: 0.5188 - val_loss: 0.9955 - val_acc: 0.5257\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9808 - acc: 0.5311 - val_loss: 0.9912 - val_acc: 0.5312\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5359683797293501\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 1.0796 - acc: 0.4255 - val_loss: 1.0254 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 1.0388 - acc: 0.4805 - val_loss: 1.0205 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0168 - acc: 0.4996 - val_loss: 1.0099 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9987 - acc: 0.5131 - val_loss: 0.9988 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9797 - acc: 0.5264 - val_loss: 1.0140 - val_acc: 0.4977\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5264822138156815\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 149s 15ms/step - loss: 1.0860 - acc: 0.4310 - val_loss: 1.0129 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0361 - acc: 0.4803 - val_loss: 1.0169 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0158 - acc: 0.5022 - val_loss: 1.0009 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9955 - acc: 0.5166 - val_loss: 0.9906 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9765 - acc: 0.5306 - val_loss: 1.0030 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5185770751223734\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 151s 15ms/step - loss: 1.0786 - acc: 0.4292 - val_loss: 1.0279 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0377 - acc: 0.4746 - val_loss: 1.0025 - val_acc: 0.5234\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0197 - acc: 0.4975 - val_loss: 1.0169 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9988 - acc: 0.5204 - val_loss: 0.9869 - val_acc: 0.5265\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9778 - acc: 0.5364 - val_loss: 0.9914 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5114624509698318\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 150s 15ms/step - loss: 1.0789 - acc: 0.4265 - val_loss: 1.0153 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0385 - acc: 0.4787 - val_loss: 1.0147 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0143 - acc: 0.5007 - val_loss: 0.9982 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9941 - acc: 0.5160 - val_loss: 0.9906 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9762 - acc: 0.5320 - val_loss: 1.0090 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.4988142295788399\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 154s 15ms/step - loss: 1.0834 - acc: 0.4346 - val_loss: 1.0243 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0348 - acc: 0.4847 - val_loss: 0.9952 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0116 - acc: 0.5042 - val_loss: 0.9939 - val_acc: 0.5265\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9939 - acc: 0.5234 - val_loss: 0.9943 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9757 - acc: 0.5339 - val_loss: 0.9882 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5288537553176579\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 154s 15ms/step - loss: 1.0781 - acc: 0.4349 - val_loss: 1.0225 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0335 - acc: 0.4877 - val_loss: 1.0090 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0184 - acc: 0.4988 - val_loss: 1.0423 - val_acc: 0.4813\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9953 - acc: 0.5220 - val_loss: 0.9961 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9745 - acc: 0.5301 - val_loss: 0.9968 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5225296445986027\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0732 - acc: 0.4366 - val_loss: 1.0230 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0413 - acc: 0.4677 - val_loss: 1.0191 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0174 - acc: 0.5036 - val_loss: 0.9985 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9994 - acc: 0.5177 - val_loss: 1.0105 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9797 - acc: 0.5313 - val_loss: 0.9927 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "0.5201581030966265\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 159s 16ms/step - loss: 1.0736 - acc: 0.4310 - val_loss: 1.0223 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0414 - acc: 0.4830 - val_loss: 1.0027 - val_acc: 0.5234\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0179 - acc: 0.5027 - val_loss: 0.9963 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9941 - acc: 0.5195 - val_loss: 1.0047 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9806 - acc: 0.5297 - val_loss: 0.9914 - val_acc: 0.5265\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5154150200455556\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 157s 15ms/step - loss: 1.0844 - acc: 0.4213 - val_loss: 1.0286 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0476 - acc: 0.4691 - val_loss: 1.0124 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0171 - acc: 0.4999 - val_loss: 1.0115 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9954 - acc: 0.5165 - val_loss: 0.9967 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9792 - acc: 0.5280 - val_loss: 1.0188 - val_acc: 0.4945\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5027667987488004\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0748 - acc: 0.4325 - val_loss: 1.0279 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0427 - acc: 0.4744 - val_loss: 1.0019 - val_acc: 0.5226\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0183 - acc: 0.4999 - val_loss: 0.9959 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 0.9961 - acc: 0.5178 - val_loss: 1.0036 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9823 - acc: 0.5275 - val_loss: 0.9874 - val_acc: 0.5187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5217391307646106\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0778 - acc: 0.4259 - val_loss: 1.0365 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 1.0353 - acc: 0.4819 - val_loss: 1.0078 - val_acc: 0.5335\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 1.0154 - acc: 0.5017 - val_loss: 0.9910 - val_acc: 0.5249\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 133s 13ms/step - loss: 0.9936 - acc: 0.5215 - val_loss: 0.9856 - val_acc: 0.5304\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 0.9709 - acc: 0.5379 - val_loss: 0.9889 - val_acc: 0.5265\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5083003955867451\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0694 - acc: 0.4379 - val_loss: 1.0315 - val_acc: 0.4961\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 138s 14ms/step - loss: 1.0329 - acc: 0.4873 - val_loss: 1.0019 - val_acc: 0.5202\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0103 - acc: 0.5083 - val_loss: 0.9971 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 138s 14ms/step - loss: 0.9943 - acc: 0.5207 - val_loss: 0.9905 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 0.9766 - acc: 0.5339 - val_loss: 0.9982 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.505138339991626\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0729 - acc: 0.4310 - val_loss: 1.0224 - val_acc: 0.4961\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0308 - acc: 0.4896 - val_loss: 1.0073 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0116 - acc: 0.5071 - val_loss: 1.0161 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 0.9950 - acc: 0.5156 - val_loss: 0.9963 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 0.9723 - acc: 0.5340 - val_loss: 0.9942 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.513043478637816\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 173s 17ms/step - loss: 1.0706 - acc: 0.4281 - val_loss: 1.0262 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 1.0368 - acc: 0.4755 - val_loss: 1.0069 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 1.0128 - acc: 0.5053 - val_loss: 1.0082 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 0.9888 - acc: 0.5264 - val_loss: 0.9944 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 0.9721 - acc: 0.5344 - val_loss: 0.9884 - val_acc: 0.5312\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5201581031437448\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 175s 17ms/step - loss: 1.0684 - acc: 0.4382 - val_loss: 1.0237 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0360 - acc: 0.4774 - val_loss: 1.0100 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 1.0136 - acc: 0.5034 - val_loss: 1.0048 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 0.9915 - acc: 0.5244 - val_loss: 0.9953 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 145s 14ms/step - loss: 0.9696 - acc: 0.5326 - val_loss: 1.0130 - val_acc: 0.5016\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5051383402507766\n",
      "{5: 0.5043478261105157, 6: 0.5114624509698318, 7: 0.5304347829385238, 8: 0.5067193679658791, 9: 0.499604743412832, 10: 0.5130434782844288, 11: 0.5098814233018476, 12: 0.505138340297895, 13: 0.5011857707745473, 14: 0.5019762849148083, 15: 0.5177865615946502, 16: 0.5011857707745473, 17: 0.5201581030966265, 18: 0.505138340297895, 19: 0.5280632414365475, 20: 0.5359683797293501, 21: 0.5264822138156815, 22: 0.5185770751223734, 23: 0.5114624509698318, 24: 0.4988142295788399, 25: 0.5288537553176579, 26: 0.5225296445986027, 27: 0.5201581030966265, 28: 0.5154150200455556, 29: 0.5027667987488004, 30: 0.5217391307646106, 31: 0.5083003955867451, 32: 0.505138339991626, 33: 0.513043478637816, 34: 0.5201581031437448, 35: 0.5051383402507766}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0678 - acc: 0.4340 - val_loss: 1.0200 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 11ms/step - loss: 1.0400 - acc: 0.4672 - val_loss: 1.0166 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0276 - acc: 0.4880 - val_loss: 1.0361 - val_acc: 0.4650\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0154 - acc: 0.4983 - val_loss: 1.0041 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0015 - acc: 0.5106 - val_loss: 0.9976 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5067193679658791\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0645 - acc: 0.4312 - val_loss: 1.0239 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0413 - acc: 0.4690 - val_loss: 1.0115 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0270 - acc: 0.4848 - val_loss: 1.0058 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0153 - acc: 0.4997 - val_loss: 1.0228 - val_acc: 0.4821\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 0.9993 - acc: 0.5105 - val_loss: 1.0004 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5019762849148083\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0702 - acc: 0.4270 - val_loss: 1.0205 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0438 - acc: 0.4717 - val_loss: 1.0067 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0252 - acc: 0.4913 - val_loss: 1.0096 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0099 - acc: 0.5081 - val_loss: 1.0301 - val_acc: 0.4868\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 0.9996 - acc: 0.5096 - val_loss: 0.9979 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5090909094678555\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0664 - acc: 0.4345 - val_loss: 1.0246 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0389 - acc: 0.4739 - val_loss: 1.0096 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0207 - acc: 0.4931 - val_loss: 1.0207 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0071 - acc: 0.5008 - val_loss: 1.0028 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 0.9924 - acc: 0.5169 - val_loss: 1.0164 - val_acc: 0.5023\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.507509881446484\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0698 - acc: 0.4312 - val_loss: 1.0189 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0402 - acc: 0.4736 - val_loss: 1.0097 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0240 - acc: 0.4912 - val_loss: 1.0100 - val_acc: 0.5241\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0037 - acc: 0.5102 - val_loss: 1.0027 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 0.9899 - acc: 0.5189 - val_loss: 0.9985 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5177865616417685\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0680 - acc: 0.4350 - val_loss: 1.0286 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0381 - acc: 0.4765 - val_loss: 1.0203 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0237 - acc: 0.4938 - val_loss: 1.0061 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0078 - acc: 0.5045 - val_loss: 1.0086 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 0.9905 - acc: 0.5159 - val_loss: 1.0046 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5003952569405552\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0629 - acc: 0.4398 - val_loss: 1.0153 - val_acc: 0.5171\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0349 - acc: 0.4867 - val_loss: 1.0084 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0169 - acc: 0.5020 - val_loss: 1.0043 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0032 - acc: 0.5153 - val_loss: 0.9984 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9838 - acc: 0.5199 - val_loss: 1.0043 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.507509881752753\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0694 - acc: 0.4308 - val_loss: 1.0191 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0397 - acc: 0.4705 - val_loss: 1.0165 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0196 - acc: 0.4964 - val_loss: 0.9976 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0028 - acc: 0.5110 - val_loss: 0.9911 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9910 - acc: 0.5191 - val_loss: 0.9909 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5011857710336979\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 135s 13ms/step - loss: 1.0683 - acc: 0.4361 - val_loss: 1.0294 - val_acc: 0.4945\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0376 - acc: 0.4784 - val_loss: 1.0094 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 118s 11ms/step - loss: 1.0183 - acc: 0.5010 - val_loss: 1.0075 - val_acc: 0.5226\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0048 - acc: 0.5080 - val_loss: 1.0084 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 0.9854 - acc: 0.5283 - val_loss: 0.9923 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5090909093736189\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0735 - acc: 0.4329 - val_loss: 1.0142 - val_acc: 0.5195\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0364 - acc: 0.4797 - val_loss: 0.9998 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0191 - acc: 0.4962 - val_loss: 1.0049 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 0.9980 - acc: 0.5165 - val_loss: 1.0052 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 0.9810 - acc: 0.5296 - val_loss: 1.0012 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5027667987959187\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0757 - acc: 0.4325 - val_loss: 1.0187 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0371 - acc: 0.4793 - val_loss: 1.0076 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0183 - acc: 0.5030 - val_loss: 0.9952 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0022 - acc: 0.5171 - val_loss: 1.0179 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9824 - acc: 0.5267 - val_loss: 0.9928 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.507509881752753\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 1.0692 - acc: 0.4402 - val_loss: 1.0194 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0351 - acc: 0.4836 - val_loss: 1.0239 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0146 - acc: 0.4995 - val_loss: 1.0107 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0010 - acc: 0.5177 - val_loss: 0.9926 - val_acc: 0.5249\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9797 - acc: 0.5271 - val_loss: 0.9865 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.5185770754286423\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0750 - acc: 0.4319 - val_loss: 1.0284 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0326 - acc: 0.4886 - val_loss: 1.0112 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0138 - acc: 0.4992 - val_loss: 1.0154 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9952 - acc: 0.5159 - val_loss: 0.9973 - val_acc: 0.5257\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9768 - acc: 0.5329 - val_loss: 0.9999 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5122529648038239\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0722 - acc: 0.4338 - val_loss: 1.0504 - val_acc: 0.4595\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0431 - acc: 0.4727 - val_loss: 1.0016 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0129 - acc: 0.5036 - val_loss: 1.0032 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 0.9974 - acc: 0.5120 - val_loss: 0.9982 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9790 - acc: 0.5305 - val_loss: 0.9972 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5169960477606581\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 145s 14ms/step - loss: 1.0779 - acc: 0.4319 - val_loss: 1.0233 - val_acc: 0.5109\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0344 - acc: 0.4863 - val_loss: 1.0031 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0134 - acc: 0.5027 - val_loss: 1.0040 - val_acc: 0.5257\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0003 - acc: 0.5110 - val_loss: 0.9931 - val_acc: 0.5241\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9789 - acc: 0.5323 - val_loss: 0.9885 - val_acc: 0.5366\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.516205533926666\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 150s 15ms/step - loss: 1.0774 - acc: 0.4278 - val_loss: 1.0172 - val_acc: 0.4961\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0355 - acc: 0.4829 - val_loss: 1.0008 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0155 - acc: 0.5041 - val_loss: 0.9969 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9964 - acc: 0.5170 - val_loss: 1.0042 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9801 - acc: 0.5288 - val_loss: 0.9998 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5035573125827925\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 151s 15ms/step - loss: 1.0777 - acc: 0.4285 - val_loss: 1.0236 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0401 - acc: 0.4780 - val_loss: 1.0283 - val_acc: 0.4782\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0202 - acc: 0.5017 - val_loss: 1.0109 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9973 - acc: 0.5206 - val_loss: 0.9946 - val_acc: 0.5288\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9781 - acc: 0.5264 - val_loss: 0.9935 - val_acc: 0.5312\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5051383402507766\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 148s 14ms/step - loss: 1.0733 - acc: 0.4295 - val_loss: 1.0170 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0373 - acc: 0.4768 - val_loss: 0.9971 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0165 - acc: 0.4970 - val_loss: 0.9970 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9925 - acc: 0.5206 - val_loss: 0.9958 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9821 - acc: 0.5279 - val_loss: 1.0080 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5201581031437448\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 153s 15ms/step - loss: 1.0757 - acc: 0.4307 - val_loss: 1.0227 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0410 - acc: 0.4799 - val_loss: 1.0059 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0186 - acc: 0.4998 - val_loss: 0.9968 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9988 - acc: 0.5145 - val_loss: 0.9925 - val_acc: 0.5288\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9771 - acc: 0.5240 - val_loss: 0.9939 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5154150200926739\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 152s 15ms/step - loss: 1.0797 - acc: 0.4391 - val_loss: 1.0159 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0368 - acc: 0.4810 - val_loss: 1.0009 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0170 - acc: 0.5010 - val_loss: 0.9998 - val_acc: 0.5234\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 0.9950 - acc: 0.5177 - val_loss: 1.0024 - val_acc: 0.5257\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9746 - acc: 0.5359 - val_loss: 0.9905 - val_acc: 0.5312\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5122529647567056\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 159s 15ms/step - loss: 1.0730 - acc: 0.4275 - val_loss: 1.0250 - val_acc: 0.5195\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0358 - acc: 0.4816 - val_loss: 1.0235 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0193 - acc: 0.4980 - val_loss: 1.0056 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0000 - acc: 0.5132 - val_loss: 1.0043 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9812 - acc: 0.5290 - val_loss: 1.0128 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.4940711462686184\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 157s 15ms/step - loss: 1.0728 - acc: 0.4360 - val_loss: 1.0226 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0392 - acc: 0.4735 - val_loss: 0.9988 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0196 - acc: 0.4923 - val_loss: 1.0011 - val_acc: 0.5226\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9971 - acc: 0.5162 - val_loss: 0.9940 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9768 - acc: 0.5311 - val_loss: 0.9865 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.516205533926666\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0755 - acc: 0.4316 - val_loss: 1.0305 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0408 - acc: 0.4754 - val_loss: 1.0063 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0164 - acc: 0.4992 - val_loss: 1.0000 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9978 - acc: 0.5214 - val_loss: 1.0198 - val_acc: 0.5008\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9754 - acc: 0.5360 - val_loss: 0.9963 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5193675889563655\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0743 - acc: 0.4270 - val_loss: 1.0404 - val_acc: 0.4650\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0407 - acc: 0.4774 - val_loss: 1.0228 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0173 - acc: 0.5006 - val_loss: 1.0058 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9949 - acc: 0.5212 - val_loss: 1.0146 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9763 - acc: 0.5323 - val_loss: 1.0418 - val_acc: 0.4759\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.4830039526398474\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0771 - acc: 0.4252 - val_loss: 1.0210 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0373 - acc: 0.4782 - val_loss: 1.0115 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0168 - acc: 0.5003 - val_loss: 0.9971 - val_acc: 0.5312\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0000 - acc: 0.5188 - val_loss: 0.9941 - val_acc: 0.5241\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9764 - acc: 0.5331 - val_loss: 1.0009 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5114624509227135\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0850 - acc: 0.4295 - val_loss: 1.0199 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0409 - acc: 0.4747 - val_loss: 1.0107 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0112 - acc: 0.5056 - val_loss: 0.9975 - val_acc: 0.5234\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9952 - acc: 0.5201 - val_loss: 1.0069 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 0.9766 - acc: 0.5332 - val_loss: 0.9895 - val_acc: 0.5265\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5035573125827925\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0743 - acc: 0.4315 - val_loss: 1.0204 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 1.0358 - acc: 0.4819 - val_loss: 1.0081 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0122 - acc: 0.5012 - val_loss: 0.9908 - val_acc: 0.5288\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 138s 13ms/step - loss: 0.9919 - acc: 0.5184 - val_loss: 0.9894 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 137s 13ms/step - loss: 0.9699 - acc: 0.5349 - val_loss: 1.0170 - val_acc: 0.5055\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5098814229484603\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0714 - acc: 0.4284 - val_loss: 1.0387 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 137s 13ms/step - loss: 1.0360 - acc: 0.4759 - val_loss: 1.0142 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 138s 14ms/step - loss: 1.0198 - acc: 0.4980 - val_loss: 1.0093 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 138s 13ms/step - loss: 0.9957 - acc: 0.5163 - val_loss: 1.0058 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 138s 14ms/step - loss: 0.9796 - acc: 0.5308 - val_loss: 0.9960 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5043478264167846\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0702 - acc: 0.4350 - val_loss: 1.0337 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0358 - acc: 0.4762 - val_loss: 1.0168 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0141 - acc: 0.5052 - val_loss: 0.9962 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 0.9960 - acc: 0.5134 - val_loss: 0.9971 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 0.9768 - acc: 0.5321 - val_loss: 0.9926 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5090909094207372\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0727 - acc: 0.4287 - val_loss: 1.0224 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0352 - acc: 0.4806 - val_loss: 1.0105 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0135 - acc: 0.5011 - val_loss: 0.9959 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 0.9937 - acc: 0.5201 - val_loss: 0.9888 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 0.9765 - acc: 0.5314 - val_loss: 0.9876 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5090909094207372\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0682 - acc: 0.4339 - val_loss: 1.0258 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0358 - acc: 0.4786 - val_loss: 1.0010 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 150s 15ms/step - loss: 1.0155 - acc: 0.5012 - val_loss: 0.9934 - val_acc: 0.5241\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 148s 14ms/step - loss: 0.9942 - acc: 0.5211 - val_loss: 0.9946 - val_acc: 0.5304\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 0.9767 - acc: 0.5286 - val_loss: 0.9898 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5090909094678555\n",
      "{5: 0.5067193679658791, 6: 0.5019762849148083, 7: 0.5090909094678555, 8: 0.507509881446484, 9: 0.5177865616417685, 10: 0.5003952569405552, 11: 0.507509881752753, 12: 0.5011857710336979, 13: 0.5090909093736189, 14: 0.5027667987959187, 15: 0.507509881752753, 16: 0.5185770754286423, 17: 0.5122529648038239, 18: 0.5169960477606581, 19: 0.516205533926666, 20: 0.5035573125827925, 21: 0.5051383402507766, 22: 0.5201581031437448, 23: 0.5154150200926739, 24: 0.5122529647567056, 25: 0.4940711462686184, 26: 0.516205533926666, 27: 0.5193675889563655, 28: 0.4830039526398474, 29: 0.5114624509227135, 30: 0.5035573125827925, 31: 0.5098814229484603, 32: 0.5043478264167846, 33: 0.5090909094207372, 34: 0.5090909094207372, 35: 0.5090909094678555}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0654 - acc: 0.4361 - val_loss: 1.0202 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0423 - acc: 0.4671 - val_loss: 1.0103 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0280 - acc: 0.4844 - val_loss: 1.0103 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0155 - acc: 0.4998 - val_loss: 1.0477 - val_acc: 0.4603\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0039 - acc: 0.5078 - val_loss: 0.9986 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5098814232547293\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0667 - acc: 0.4394 - val_loss: 1.0172 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0417 - acc: 0.4696 - val_loss: 1.0068 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0262 - acc: 0.4911 - val_loss: 1.0109 - val_acc: 0.5265\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0144 - acc: 0.5052 - val_loss: 1.0102 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0009 - acc: 0.5098 - val_loss: 0.9970 - val_acc: 0.5312\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5114624509698318\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0671 - acc: 0.4354 - val_loss: 1.0286 - val_acc: 0.4992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0411 - acc: 0.4689 - val_loss: 1.0262 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0268 - acc: 0.4912 - val_loss: 1.0182 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0152 - acc: 0.5000 - val_loss: 1.0038 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 0.9988 - acc: 0.5113 - val_loss: 0.9992 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5098814232547293\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0691 - acc: 0.4301 - val_loss: 1.0168 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0389 - acc: 0.4739 - val_loss: 1.0149 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 1.0221 - acc: 0.4914 - val_loss: 0.9999 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0050 - acc: 0.5096 - val_loss: 1.0007 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 0.9958 - acc: 0.5166 - val_loss: 1.0067 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5114624506635628\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0735 - acc: 0.4324 - val_loss: 1.0268 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0352 - acc: 0.4798 - val_loss: 1.0117 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0218 - acc: 0.4997 - val_loss: 1.0045 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0045 - acc: 0.5109 - val_loss: 0.9985 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 0.9921 - acc: 0.5199 - val_loss: 1.0044 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.49802371543857893\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0633 - acc: 0.4378 - val_loss: 1.0206 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0362 - acc: 0.4761 - val_loss: 1.0206 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0211 - acc: 0.4997 - val_loss: 1.0126 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0033 - acc: 0.5098 - val_loss: 1.0134 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9860 - acc: 0.5232 - val_loss: 1.0191 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.4980237155328155\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0673 - acc: 0.4292 - val_loss: 1.0222 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0317 - acc: 0.4863 - val_loss: 1.0076 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0181 - acc: 0.4976 - val_loss: 1.0106 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0006 - acc: 0.5127 - val_loss: 1.0219 - val_acc: 0.4992\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 0.9858 - acc: 0.5210 - val_loss: 1.0005 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.505928854131887\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0711 - acc: 0.4381 - val_loss: 1.0262 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0364 - acc: 0.4777 - val_loss: 1.0192 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0182 - acc: 0.5019 - val_loss: 1.0053 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0022 - acc: 0.5116 - val_loss: 1.0059 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 0.9819 - acc: 0.5227 - val_loss: 0.9992 - val_acc: 0.5280\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5122529647567056\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0685 - acc: 0.4365 - val_loss: 1.0262 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0377 - acc: 0.4754 - val_loss: 1.0121 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0158 - acc: 0.5023 - val_loss: 1.0010 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0004 - acc: 0.5124 - val_loss: 1.0022 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9841 - acc: 0.5285 - val_loss: 1.0056 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5146245062586818\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 137s 13ms/step - loss: 1.0708 - acc: 0.4400 - val_loss: 1.0132 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0362 - acc: 0.4799 - val_loss: 1.0092 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0136 - acc: 0.5036 - val_loss: 1.0169 - val_acc: 0.4984\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0009 - acc: 0.5139 - val_loss: 1.0125 - val_acc: 0.4984\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 0.9856 - acc: 0.5258 - val_loss: 0.9929 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.505138340297895\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 185s 18ms/step - loss: 1.0695 - acc: 0.4292 - val_loss: 1.0262 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0381 - acc: 0.4790 - val_loss: 1.0247 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0152 - acc: 0.5036 - val_loss: 0.9971 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0001 - acc: 0.5145 - val_loss: 0.9946 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9818 - acc: 0.5285 - val_loss: 0.9907 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5146245062586818\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 145s 14ms/step - loss: 1.0754 - acc: 0.4335 - val_loss: 1.0156 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0379 - acc: 0.4760 - val_loss: 1.0047 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0198 - acc: 0.5026 - val_loss: 1.0004 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0020 - acc: 0.5150 - val_loss: 1.0002 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9824 - acc: 0.5271 - val_loss: 0.9913 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5154150200926739\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 145s 14ms/step - loss: 1.0729 - acc: 0.4377 - val_loss: 1.0090 - val_acc: 0.5109\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0334 - acc: 0.4878 - val_loss: 0.9961 - val_acc: 0.5218\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0139 - acc: 0.5066 - val_loss: 0.9927 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9940 - acc: 0.5202 - val_loss: 0.9938 - val_acc: 0.5249\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9771 - acc: 0.5288 - val_loss: 1.0009 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.5193675893097527\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 1.0753 - acc: 0.4353 - val_loss: 1.0169 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0341 - acc: 0.4889 - val_loss: 1.0019 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0087 - acc: 0.5127 - val_loss: 0.9983 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9956 - acc: 0.5200 - val_loss: 0.9920 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9733 - acc: 0.5325 - val_loss: 0.9964 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5130434785906977\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 148s 15ms/step - loss: 1.0721 - acc: 0.4382 - val_loss: 1.0232 - val_acc: 0.5140\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0384 - acc: 0.4754 - val_loss: 1.0015 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0203 - acc: 0.4994 - val_loss: 0.9980 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0008 - acc: 0.5192 - val_loss: 1.0012 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 151s 15ms/step - loss: 0.9817 - acc: 0.5226 - val_loss: 0.9903 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5169960477135398\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 148s 14ms/step - loss: 1.0737 - acc: 0.4362 - val_loss: 1.0199 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0309 - acc: 0.4882 - val_loss: 1.0067 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 1.0102 - acc: 0.5082 - val_loss: 0.9994 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 0.9950 - acc: 0.5206 - val_loss: 0.9986 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9747 - acc: 0.5329 - val_loss: 0.9905 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5146245062115635\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 152s 15ms/step - loss: 1.0744 - acc: 0.4311 - val_loss: 1.0232 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 133s 13ms/step - loss: 1.0369 - acc: 0.4763 - val_loss: 1.0051 - val_acc: 0.5179\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 137s 13ms/step - loss: 1.0120 - acc: 0.5070 - val_loss: 1.0055 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 0.9934 - acc: 0.5191 - val_loss: 0.9910 - val_acc: 0.5241\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9706 - acc: 0.5340 - val_loss: 1.0046 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.49565217424287156\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 155s 15ms/step - loss: 1.0860 - acc: 0.4297 - val_loss: 1.0179 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0338 - acc: 0.4869 - val_loss: 1.0233 - val_acc: 0.4875\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0144 - acc: 0.5043 - val_loss: 1.0033 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9907 - acc: 0.5230 - val_loss: 1.0114 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9698 - acc: 0.5324 - val_loss: 0.9827 - val_acc: 0.5327\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5375494074444526\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 155s 15ms/step - loss: 1.0774 - acc: 0.4332 - val_loss: 1.0220 - val_acc: 0.5179\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0389 - acc: 0.4758 - val_loss: 1.0111 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0140 - acc: 0.5040 - val_loss: 1.0063 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9999 - acc: 0.5138 - val_loss: 0.9967 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9806 - acc: 0.5273 - val_loss: 0.9934 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5185770754757606\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 155s 15ms/step - loss: 1.0740 - acc: 0.4270 - val_loss: 1.0286 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0395 - acc: 0.4783 - val_loss: 1.0143 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0177 - acc: 0.5018 - val_loss: 0.9966 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9998 - acc: 0.5168 - val_loss: 0.9940 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9799 - acc: 0.5308 - val_loss: 0.9960 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5106719370887214\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 159s 16ms/step - loss: 1.0712 - acc: 0.4321 - val_loss: 1.0369 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0371 - acc: 0.4781 - val_loss: 1.0052 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0152 - acc: 0.5045 - val_loss: 1.0020 - val_acc: 0.5288\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9947 - acc: 0.5240 - val_loss: 1.0102 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9813 - acc: 0.5320 - val_loss: 1.0026 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5114624508755952\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0732 - acc: 0.4398 - val_loss: 1.0213 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0377 - acc: 0.4834 - val_loss: 1.0155 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0158 - acc: 0.5044 - val_loss: 1.0256 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9946 - acc: 0.5191 - val_loss: 1.0067 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9777 - acc: 0.5322 - val_loss: 0.9975 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5154150200926739\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 165s 16ms/step - loss: 1.0796 - acc: 0.4235 - val_loss: 1.0189 - val_acc: 0.4938\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0389 - acc: 0.4823 - val_loss: 1.0070 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0159 - acc: 0.5061 - val_loss: 0.9986 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9977 - acc: 0.5173 - val_loss: 0.9886 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9811 - acc: 0.5289 - val_loss: 0.9926 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "0.5083003956338633\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0747 - acc: 0.4308 - val_loss: 1.0229 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0381 - acc: 0.4708 - val_loss: 1.0035 - val_acc: 0.5234\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0169 - acc: 0.5021 - val_loss: 0.9934 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9955 - acc: 0.5210 - val_loss: 0.9979 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9741 - acc: 0.5358 - val_loss: 0.9919 - val_acc: 0.5273\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5241106722665869\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 196s 19ms/step - loss: 1.0753 - acc: 0.4281 - val_loss: 1.0229 - val_acc: 0.5171\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0385 - acc: 0.4783 - val_loss: 1.0087 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0183 - acc: 0.5027 - val_loss: 1.0022 - val_acc: 0.5280\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9966 - acc: 0.5160 - val_loss: 0.9927 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 0.9744 - acc: 0.5343 - val_loss: 0.9887 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "0.5090909093736189\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0759 - acc: 0.4331 - val_loss: 1.0534 - val_acc: 0.4237\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0387 - acc: 0.4722 - val_loss: 1.0075 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0165 - acc: 0.5031 - val_loss: 1.0005 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9988 - acc: 0.5173 - val_loss: 1.0099 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9764 - acc: 0.5351 - val_loss: 0.9929 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5169960477606581\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0759 - acc: 0.4273 - val_loss: 1.0337 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 1.0404 - acc: 0.4793 - val_loss: 1.0032 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 133s 13ms/step - loss: 1.0165 - acc: 0.5051 - val_loss: 0.9970 - val_acc: 0.5226\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 0.9900 - acc: 0.5268 - val_loss: 0.9905 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 0.9788 - acc: 0.5278 - val_loss: 0.9899 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5177865615475319\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 173s 17ms/step - loss: 1.0691 - acc: 0.4359 - val_loss: 1.0194 - val_acc: 0.4930\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 137s 13ms/step - loss: 1.0333 - acc: 0.4801 - val_loss: 1.0175 - val_acc: 0.4977\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 1.0143 - acc: 0.5032 - val_loss: 0.9986 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 0.9945 - acc: 0.5230 - val_loss: 0.9901 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 0.9766 - acc: 0.5314 - val_loss: 0.9912 - val_acc: 0.5265\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5264822135094126\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0709 - acc: 0.4284 - val_loss: 1.0261 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0390 - acc: 0.4775 - val_loss: 1.0049 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0145 - acc: 0.5007 - val_loss: 1.0131 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 0.9983 - acc: 0.5193 - val_loss: 1.0029 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 201s 20ms/step - loss: 0.9748 - acc: 0.5334 - val_loss: 0.9979 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5217391307646106\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 178s 17ms/step - loss: 1.0681 - acc: 0.4370 - val_loss: 1.0290 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0389 - acc: 0.4702 - val_loss: 1.0081 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0194 - acc: 0.4938 - val_loss: 1.0134 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 0.9941 - acc: 0.5182 - val_loss: 1.0030 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 0.9775 - acc: 0.5304 - val_loss: 0.9932 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5217391307646106\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 179s 18ms/step - loss: 1.0729 - acc: 0.4276 - val_loss: 1.0328 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 1.0341 - acc: 0.4858 - val_loss: 1.0009 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0145 - acc: 0.5063 - val_loss: 1.0051 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 0.9939 - acc: 0.5235 - val_loss: 0.9908 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 0.9712 - acc: 0.5411 - val_loss: 0.9874 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5217391307174923\n",
      "{5: 0.5098814232547293, 6: 0.5114624509698318, 7: 0.5098814232547293, 8: 0.5114624506635628, 9: 0.49802371543857893, 10: 0.4980237155328155, 11: 0.505928854131887, 12: 0.5122529647567056, 13: 0.5146245062586818, 14: 0.505138340297895, 15: 0.5146245062586818, 16: 0.5154150200926739, 17: 0.5193675893097527, 18: 0.5130434785906977, 19: 0.5169960477135398, 20: 0.5146245062115635, 21: 0.49565217424287156, 22: 0.5375494074444526, 23: 0.5185770754757606, 24: 0.5106719370887214, 25: 0.5114624508755952, 26: 0.5154150200926739, 27: 0.5083003956338633, 28: 0.5241106722665869, 29: 0.5090909093736189, 30: 0.5169960477606581, 31: 0.5177865615475319, 32: 0.5264822135094126, 33: 0.5217391307646106, 34: 0.5217391307646106, 35: 0.5217391307174923}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0647 - acc: 0.4319 - val_loss: 1.0267 - val_acc: 0.5171\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0407 - acc: 0.4661 - val_loss: 1.0173 - val_acc: 0.5156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0300 - acc: 0.4841 - val_loss: 1.0144 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0172 - acc: 0.5008 - val_loss: 1.0105 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0076 - acc: 0.5005 - val_loss: 1.0121 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.507509881446484\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0650 - acc: 0.4336 - val_loss: 1.0224 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0398 - acc: 0.4695 - val_loss: 1.0067 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0227 - acc: 0.4938 - val_loss: 1.0032 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 11ms/step - loss: 1.0130 - acc: 0.5009 - val_loss: 0.9992 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 0.9999 - acc: 0.5171 - val_loss: 1.0042 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5083003955867451\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0679 - acc: 0.4332 - val_loss: 1.0201 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0369 - acc: 0.4726 - val_loss: 1.0280 - val_acc: 0.4977\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0261 - acc: 0.4917 - val_loss: 1.0075 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0098 - acc: 0.5085 - val_loss: 1.0044 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 0.9985 - acc: 0.5162 - val_loss: 1.0035 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5233201581263259\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0669 - acc: 0.4302 - val_loss: 1.0134 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0355 - acc: 0.4781 - val_loss: 1.0036 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0172 - acc: 0.5007 - val_loss: 1.0271 - val_acc: 0.4914\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0051 - acc: 0.5053 - val_loss: 0.9947 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 0.9904 - acc: 0.5240 - val_loss: 0.9936 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.513043478637816\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0689 - acc: 0.4317 - val_loss: 1.0207 - val_acc: 0.5132\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0393 - acc: 0.4703 - val_loss: 1.0160 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0216 - acc: 0.4954 - val_loss: 1.0062 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0087 - acc: 0.5049 - val_loss: 1.0168 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9947 - acc: 0.5158 - val_loss: 1.0090 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5019762846085394\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0669 - acc: 0.4410 - val_loss: 1.0191 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0347 - acc: 0.4798 - val_loss: 1.0300 - val_acc: 0.4868\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0170 - acc: 0.5028 - val_loss: 1.0003 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0043 - acc: 0.5130 - val_loss: 1.0072 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9900 - acc: 0.5177 - val_loss: 1.0061 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 6s 4ms/step\n",
      "0.5122529644504367\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0649 - acc: 0.4408 - val_loss: 1.0190 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0349 - acc: 0.4782 - val_loss: 1.0070 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0176 - acc: 0.4992 - val_loss: 1.0090 - val_acc: 0.5265\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0036 - acc: 0.5139 - val_loss: 0.9998 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9855 - acc: 0.5251 - val_loss: 0.9921 - val_acc: 0.5304\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5011857711279345\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0699 - acc: 0.4266 - val_loss: 1.0289 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0363 - acc: 0.4801 - val_loss: 1.0094 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0194 - acc: 0.5048 - val_loss: 1.0183 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0070 - acc: 0.5085 - val_loss: 1.0018 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9904 - acc: 0.5223 - val_loss: 1.0002 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5027667987488004\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0711 - acc: 0.4387 - val_loss: 1.0263 - val_acc: 0.5202\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0361 - acc: 0.4779 - val_loss: 1.0049 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0159 - acc: 0.5003 - val_loss: 1.0102 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9984 - acc: 0.5207 - val_loss: 1.0042 - val_acc: 0.5241\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9814 - acc: 0.5273 - val_loss: 1.0036 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5114624509698318\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0691 - acc: 0.4325 - val_loss: 1.0169 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 1.0354 - acc: 0.4812 - val_loss: 1.0204 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 1.0191 - acc: 0.5005 - val_loss: 0.9959 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 133s 13ms/step - loss: 1.0000 - acc: 0.5156 - val_loss: 1.0105 - val_acc: 0.5241\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 0.9823 - acc: 0.5255 - val_loss: 0.9999 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5011857711279345\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 150s 15ms/step - loss: 1.0708 - acc: 0.4258 - val_loss: 1.0245 - val_acc: 0.5148\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0353 - acc: 0.4818 - val_loss: 1.0220 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0166 - acc: 0.5057 - val_loss: 1.0084 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9986 - acc: 0.5137 - val_loss: 0.9945 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9814 - acc: 0.5318 - val_loss: 1.0236 - val_acc: 0.4883\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.49644268777059475\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0706 - acc: 0.4369 - val_loss: 1.0145 - val_acc: 0.5202\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0351 - acc: 0.4783 - val_loss: 1.0246 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0141 - acc: 0.5061 - val_loss: 0.9990 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9984 - acc: 0.5194 - val_loss: 0.9926 - val_acc: 0.5327\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9793 - acc: 0.5315 - val_loss: 0.9889 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.5122529647567056\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 147s 14ms/step - loss: 1.0658 - acc: 0.4402 - val_loss: 1.0286 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0335 - acc: 0.4843 - val_loss: 1.0174 - val_acc: 0.5234\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0106 - acc: 0.5089 - val_loss: 1.0075 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9950 - acc: 0.5190 - val_loss: 0.9943 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9816 - acc: 0.5269 - val_loss: 1.0053 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.505138340297895\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 151s 15ms/step - loss: 1.0711 - acc: 0.4313 - val_loss: 1.0171 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0379 - acc: 0.4776 - val_loss: 1.0078 - val_acc: 0.5210\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0160 - acc: 0.4971 - val_loss: 1.0036 - val_acc: 0.5312\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9956 - acc: 0.5187 - val_loss: 1.0031 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9812 - acc: 0.5325 - val_loss: 0.9902 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5098814232547293\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 151s 15ms/step - loss: 1.0746 - acc: 0.4310 - val_loss: 1.0270 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0356 - acc: 0.4850 - val_loss: 1.0067 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0156 - acc: 0.5014 - val_loss: 1.0004 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9976 - acc: 0.5191 - val_loss: 0.9961 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9800 - acc: 0.5329 - val_loss: 1.0077 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5185770754286423\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 154s 15ms/step - loss: 1.0708 - acc: 0.4355 - val_loss: 1.0190 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0340 - acc: 0.4836 - val_loss: 1.0014 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0170 - acc: 0.5043 - val_loss: 1.0017 - val_acc: 0.5280\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9955 - acc: 0.5111 - val_loss: 0.9943 - val_acc: 0.5241\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9791 - acc: 0.5320 - val_loss: 0.9910 - val_acc: 0.5273\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5209486169306186\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 158s 15ms/step - loss: 1.0713 - acc: 0.4340 - val_loss: 1.0186 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0369 - acc: 0.4847 - val_loss: 1.0105 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0158 - acc: 0.5047 - val_loss: 0.9980 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9949 - acc: 0.5190 - val_loss: 0.9919 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9795 - acc: 0.5292 - val_loss: 0.9916 - val_acc: 0.5296\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5256916999816894\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 158s 15ms/step - loss: 1.0789 - acc: 0.4265 - val_loss: 1.0158 - val_acc: 0.5210\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0315 - acc: 0.4893 - val_loss: 1.0033 - val_acc: 0.5226\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0094 - acc: 0.5079 - val_loss: 1.0031 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9920 - acc: 0.5216 - val_loss: 0.9941 - val_acc: 0.5249\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9725 - acc: 0.5370 - val_loss: 0.9929 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5201581030966265\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 153s 15ms/step - loss: 1.0711 - acc: 0.4304 - val_loss: 1.0264 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0378 - acc: 0.4791 - val_loss: 1.0123 - val_acc: 0.5179\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0146 - acc: 0.5020 - val_loss: 0.9989 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9955 - acc: 0.5242 - val_loss: 0.9945 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9743 - acc: 0.5394 - val_loss: 1.0041 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5177865615946502\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 157s 15ms/step - loss: 1.0751 - acc: 0.4240 - val_loss: 1.0271 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0370 - acc: 0.4796 - val_loss: 1.0054 - val_acc: 0.5179\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0166 - acc: 0.5014 - val_loss: 0.9995 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9972 - acc: 0.5173 - val_loss: 1.0044 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9757 - acc: 0.5276 - val_loss: 0.9903 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5193675892155161\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0754 - acc: 0.4275 - val_loss: 1.0248 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 145s 14ms/step - loss: 1.0395 - acc: 0.4757 - val_loss: 1.0068 - val_acc: 0.5187\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0178 - acc: 0.5014 - val_loss: 0.9965 - val_acc: 0.5249\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0000 - acc: 0.5150 - val_loss: 0.9935 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9782 - acc: 0.5321 - val_loss: 0.9930 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "0.5138339924246897\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 165s 16ms/step - loss: 1.0795 - acc: 0.4297 - val_loss: 1.0287 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0383 - acc: 0.4809 - val_loss: 1.0086 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0171 - acc: 0.5005 - val_loss: 0.9995 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9985 - acc: 0.5206 - val_loss: 0.9990 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9822 - acc: 0.5305 - val_loss: 1.0025 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5185770754286423\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0737 - acc: 0.4288 - val_loss: 1.0243 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0372 - acc: 0.4862 - val_loss: 1.0178 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0158 - acc: 0.5031 - val_loss: 1.0030 - val_acc: 0.5304\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9980 - acc: 0.5177 - val_loss: 0.9912 - val_acc: 0.5257\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9774 - acc: 0.5272 - val_loss: 0.9932 - val_acc: 0.5288\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5335968382744921\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 165s 16ms/step - loss: 1.0728 - acc: 0.4370 - val_loss: 1.0200 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0311 - acc: 0.4868 - val_loss: 1.0074 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0160 - acc: 0.5034 - val_loss: 1.0008 - val_acc: 0.5234\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9930 - acc: 0.5243 - val_loss: 0.9982 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9767 - acc: 0.5309 - val_loss: 1.0011 - val_acc: 0.5343\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "0.5154150200926739\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0778 - acc: 0.4282 - val_loss: 1.0248 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0367 - acc: 0.4793 - val_loss: 1.0061 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0145 - acc: 0.5042 - val_loss: 0.9965 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9959 - acc: 0.5162 - val_loss: 0.9914 - val_acc: 0.5304\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 0.9773 - acc: 0.5295 - val_loss: 0.9961 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5320158105593896\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0723 - acc: 0.4341 - val_loss: 1.0203 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 149s 15ms/step - loss: 1.0336 - acc: 0.4847 - val_loss: 0.9990 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0133 - acc: 0.5044 - val_loss: 1.0010 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9912 - acc: 0.5234 - val_loss: 0.9964 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 0.9738 - acc: 0.5394 - val_loss: 0.9888 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5169960477606581\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0780 - acc: 0.4322 - val_loss: 1.0405 - val_acc: 0.4657\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 135s 13ms/step - loss: 1.0345 - acc: 0.4834 - val_loss: 1.0103 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 135s 13ms/step - loss: 1.0117 - acc: 0.5103 - val_loss: 1.0030 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 133s 13ms/step - loss: 0.9919 - acc: 0.5237 - val_loss: 0.9990 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 0.9770 - acc: 0.5284 - val_loss: 0.9874 - val_acc: 0.5273\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5146245062586818\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 176s 17ms/step - loss: 1.0744 - acc: 0.4252 - val_loss: 1.0339 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 138s 13ms/step - loss: 1.0399 - acc: 0.4745 - val_loss: 1.0108 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0171 - acc: 0.5003 - val_loss: 1.0155 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 138s 13ms/step - loss: 1.0009 - acc: 0.5128 - val_loss: 1.0072 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 138s 13ms/step - loss: 0.9837 - acc: 0.5323 - val_loss: 0.9927 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5241106722194687\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0689 - acc: 0.4320 - val_loss: 1.0237 - val_acc: 0.4891\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0353 - acc: 0.4788 - val_loss: 1.0109 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 1.0118 - acc: 0.5030 - val_loss: 0.9976 - val_acc: 0.5280\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 0.9930 - acc: 0.5203 - val_loss: 1.0125 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 0.9738 - acc: 0.5330 - val_loss: 0.9889 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5146245062586818\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0692 - acc: 0.4403 - val_loss: 1.0304 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0371 - acc: 0.4786 - val_loss: 1.0069 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0165 - acc: 0.5005 - val_loss: 1.0031 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0013 - acc: 0.5218 - val_loss: 1.0067 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 0.9809 - acc: 0.5304 - val_loss: 0.9910 - val_acc: 0.5312\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "0.5130434785906977\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0725 - acc: 0.4210 - val_loss: 1.0329 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 178s 17ms/step - loss: 1.0384 - acc: 0.4778 - val_loss: 1.0224 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0145 - acc: 0.5018 - val_loss: 0.9994 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 0.9951 - acc: 0.5175 - val_loss: 0.9965 - val_acc: 0.5296\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 158s 15ms/step - loss: 0.9750 - acc: 0.5325 - val_loss: 0.9902 - val_acc: 0.5280\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5146245062115635\n",
      "{5: 0.507509881446484, 6: 0.5083003955867451, 7: 0.5233201581263259, 8: 0.513043478637816, 9: 0.5019762846085394, 10: 0.5122529644504367, 11: 0.5011857711279345, 12: 0.5027667987488004, 13: 0.5114624509698318, 14: 0.5011857711279345, 15: 0.49644268777059475, 16: 0.5122529647567056, 17: 0.505138340297895, 18: 0.5098814232547293, 19: 0.5185770754286423, 20: 0.5209486169306186, 21: 0.5256916999816894, 22: 0.5201581030966265, 23: 0.5177865615946502, 24: 0.5193675892155161, 25: 0.5138339924246897, 26: 0.5185770754286423, 27: 0.5335968382744921, 28: 0.5154150200926739, 29: 0.5320158105593896, 30: 0.5169960477606581, 31: 0.5146245062586818, 32: 0.5241106722194687, 33: 0.5146245062586818, 34: 0.5130434785906977, 35: 0.5146245062115635}\n",
      "CPU times: user 6d 10h 38min 12s, sys: 14h 16min 32s, total: 7d 54min 45s\n",
      "Wall time: 1d 5h 54min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cnn_elmo_rounds = [calculate_round(concatenated_elmo) for round in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{5: 0.49565217424287156,\n",
       "  6: 0.5114624506164446,\n",
       "  7: 0.5169960477606581,\n",
       "  8: 0.5122529644504367,\n",
       "  9: 0.49249011860063424,\n",
       "  10: 0.49644268807686365,\n",
       "  11: 0.5075098817998712,\n",
       "  12: 0.5185770751223734,\n",
       "  13: 0.507509881752753,\n",
       "  14: 0.4988142295788399,\n",
       "  15: 0.5043478264639029,\n",
       "  16: 0.5122529647567056,\n",
       "  17: 0.5067193679187609,\n",
       "  18: 0.5090909094207372,\n",
       "  19: 0.513833992471808,\n",
       "  20: 0.5059288540847687,\n",
       "  21: 0.5209486166243497,\n",
       "  22: 0.49565217424287156,\n",
       "  23: 0.5122529647567056,\n",
       "  24: 0.5098814229484603,\n",
       "  25: 0.5272727276025554,\n",
       "  26: 0.5225296445514844,\n",
       "  27: 0.5146245063058001,\n",
       "  28: 0.5051383402507766,\n",
       "  29: 0.5059288537784998,\n",
       "  30: 0.5169960477606581,\n",
       "  31: 0.5051383402507766,\n",
       "  32: 0.513833992471808,\n",
       "  33: 0.5003952569405552,\n",
       "  34: 0.5106719370887214,\n",
       "  35: 0.5019762849148083},\n",
       " {5: 0.5043478261105157,\n",
       "  6: 0.5114624509698318,\n",
       "  7: 0.5304347829385238,\n",
       "  8: 0.5067193679658791,\n",
       "  9: 0.499604743412832,\n",
       "  10: 0.5130434782844288,\n",
       "  11: 0.5098814233018476,\n",
       "  12: 0.505138340297895,\n",
       "  13: 0.5011857707745473,\n",
       "  14: 0.5019762849148083,\n",
       "  15: 0.5177865615946502,\n",
       "  16: 0.5011857707745473,\n",
       "  17: 0.5201581030966265,\n",
       "  18: 0.505138340297895,\n",
       "  19: 0.5280632414365475,\n",
       "  20: 0.5359683797293501,\n",
       "  21: 0.5264822138156815,\n",
       "  22: 0.5185770751223734,\n",
       "  23: 0.5114624509698318,\n",
       "  24: 0.4988142295788399,\n",
       "  25: 0.5288537553176579,\n",
       "  26: 0.5225296445986027,\n",
       "  27: 0.5201581030966265,\n",
       "  28: 0.5154150200455556,\n",
       "  29: 0.5027667987488004,\n",
       "  30: 0.5217391307646106,\n",
       "  31: 0.5083003955867451,\n",
       "  32: 0.505138339991626,\n",
       "  33: 0.513043478637816,\n",
       "  34: 0.5201581031437448,\n",
       "  35: 0.5051383402507766},\n",
       " {5: 0.5067193679658791,\n",
       "  6: 0.5019762849148083,\n",
       "  7: 0.5090909094678555,\n",
       "  8: 0.507509881446484,\n",
       "  9: 0.5177865616417685,\n",
       "  10: 0.5003952569405552,\n",
       "  11: 0.507509881752753,\n",
       "  12: 0.5011857710336979,\n",
       "  13: 0.5090909093736189,\n",
       "  14: 0.5027667987959187,\n",
       "  15: 0.507509881752753,\n",
       "  16: 0.5185770754286423,\n",
       "  17: 0.5122529648038239,\n",
       "  18: 0.5169960477606581,\n",
       "  19: 0.516205533926666,\n",
       "  20: 0.5035573125827925,\n",
       "  21: 0.5051383402507766,\n",
       "  22: 0.5201581031437448,\n",
       "  23: 0.5154150200926739,\n",
       "  24: 0.5122529647567056,\n",
       "  25: 0.4940711462686184,\n",
       "  26: 0.516205533926666,\n",
       "  27: 0.5193675889563655,\n",
       "  28: 0.4830039526398474,\n",
       "  29: 0.5114624509227135,\n",
       "  30: 0.5035573125827925,\n",
       "  31: 0.5098814229484603,\n",
       "  32: 0.5043478264167846,\n",
       "  33: 0.5090909094207372,\n",
       "  34: 0.5090909094207372,\n",
       "  35: 0.5090909094678555},\n",
       " {5: 0.5098814232547293,\n",
       "  6: 0.5114624509698318,\n",
       "  7: 0.5098814232547293,\n",
       "  8: 0.5114624506635628,\n",
       "  9: 0.49802371543857893,\n",
       "  10: 0.4980237155328155,\n",
       "  11: 0.505928854131887,\n",
       "  12: 0.5122529647567056,\n",
       "  13: 0.5146245062586818,\n",
       "  14: 0.505138340297895,\n",
       "  15: 0.5146245062586818,\n",
       "  16: 0.5154150200926739,\n",
       "  17: 0.5193675893097527,\n",
       "  18: 0.5130434785906977,\n",
       "  19: 0.5169960477135398,\n",
       "  20: 0.5146245062115635,\n",
       "  21: 0.49565217424287156,\n",
       "  22: 0.5375494074444526,\n",
       "  23: 0.5185770754757606,\n",
       "  24: 0.5106719370887214,\n",
       "  25: 0.5114624508755952,\n",
       "  26: 0.5154150200926739,\n",
       "  27: 0.5083003956338633,\n",
       "  28: 0.5241106722665869,\n",
       "  29: 0.5090909093736189,\n",
       "  30: 0.5169960477606581,\n",
       "  31: 0.5177865615475319,\n",
       "  32: 0.5264822135094126,\n",
       "  33: 0.5217391307646106,\n",
       "  34: 0.5217391307646106,\n",
       "  35: 0.5217391307174923},\n",
       " {5: 0.507509881446484,\n",
       "  6: 0.5083003955867451,\n",
       "  7: 0.5233201581263259,\n",
       "  8: 0.513043478637816,\n",
       "  9: 0.5019762846085394,\n",
       "  10: 0.5122529644504367,\n",
       "  11: 0.5011857711279345,\n",
       "  12: 0.5027667987488004,\n",
       "  13: 0.5114624509698318,\n",
       "  14: 0.5011857711279345,\n",
       "  15: 0.49644268777059475,\n",
       "  16: 0.5122529647567056,\n",
       "  17: 0.505138340297895,\n",
       "  18: 0.5098814232547293,\n",
       "  19: 0.5185770754286423,\n",
       "  20: 0.5209486169306186,\n",
       "  21: 0.5256916999816894,\n",
       "  22: 0.5201581030966265,\n",
       "  23: 0.5177865615946502,\n",
       "  24: 0.5193675892155161,\n",
       "  25: 0.5138339924246897,\n",
       "  26: 0.5185770754286423,\n",
       "  27: 0.5335968382744921,\n",
       "  28: 0.5154150200926739,\n",
       "  29: 0.5320158105593896,\n",
       "  30: 0.5169960477606581,\n",
       "  31: 0.5146245062586818,\n",
       "  32: 0.5241106722194687,\n",
       "  33: 0.5146245062586818,\n",
       "  34: 0.5130434785906977,\n",
       "  35: 0.5146245062115635}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_elmo_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Round 0",
         "type": "scatter",
         "uid": "1aed7a30-cb5d-404d-b860-e1ae780ab8e3",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.49565217424287156,
          0.5114624506164446,
          0.5169960477606581,
          0.5122529644504367,
          0.49249011860063424,
          0.49644268807686365,
          0.5075098817998712,
          0.5185770751223734,
          0.507509881752753,
          0.4988142295788399,
          0.5043478264639029,
          0.5122529647567056,
          0.5067193679187609,
          0.5090909094207372,
          0.513833992471808,
          0.5059288540847687,
          0.5209486166243497,
          0.49565217424287156,
          0.5122529647567056,
          0.5098814229484603,
          0.5272727276025554,
          0.5225296445514844,
          0.5146245063058001,
          0.5051383402507766,
          0.5059288537784998,
          0.5169960477606581,
          0.5051383402507766,
          0.513833992471808,
          0.5003952569405552,
          0.5106719370887214,
          0.5019762849148083
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 1",
         "type": "scatter",
         "uid": "fb87cc8e-1753-4561-960d-ccb8e6b19ac8",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5043478261105157,
          0.5114624509698318,
          0.5304347829385238,
          0.5067193679658791,
          0.499604743412832,
          0.5130434782844288,
          0.5098814233018476,
          0.505138340297895,
          0.5011857707745473,
          0.5019762849148083,
          0.5177865615946502,
          0.5011857707745473,
          0.5201581030966265,
          0.505138340297895,
          0.5280632414365475,
          0.5359683797293501,
          0.5264822138156815,
          0.5185770751223734,
          0.5114624509698318,
          0.4988142295788399,
          0.5288537553176579,
          0.5225296445986027,
          0.5201581030966265,
          0.5154150200455556,
          0.5027667987488004,
          0.5217391307646106,
          0.5083003955867451,
          0.505138339991626,
          0.513043478637816,
          0.5201581031437448,
          0.5051383402507766
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 2",
         "type": "scatter",
         "uid": "5f681289-a508-4e60-8ec0-26eb472ba05c",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5067193679658791,
          0.5019762849148083,
          0.5090909094678555,
          0.507509881446484,
          0.5177865616417685,
          0.5003952569405552,
          0.507509881752753,
          0.5011857710336979,
          0.5090909093736189,
          0.5027667987959187,
          0.507509881752753,
          0.5185770754286423,
          0.5122529648038239,
          0.5169960477606581,
          0.516205533926666,
          0.5035573125827925,
          0.5051383402507766,
          0.5201581031437448,
          0.5154150200926739,
          0.5122529647567056,
          0.4940711462686184,
          0.516205533926666,
          0.5193675889563655,
          0.4830039526398474,
          0.5114624509227135,
          0.5035573125827925,
          0.5098814229484603,
          0.5043478264167846,
          0.5090909094207372,
          0.5090909094207372,
          0.5090909094678555
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 3",
         "type": "scatter",
         "uid": "c788682b-204b-4631-9ac6-55369bb5aa96",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5098814232547293,
          0.5114624509698318,
          0.5098814232547293,
          0.5114624506635628,
          0.49802371543857893,
          0.4980237155328155,
          0.505928854131887,
          0.5122529647567056,
          0.5146245062586818,
          0.505138340297895,
          0.5146245062586818,
          0.5154150200926739,
          0.5193675893097527,
          0.5130434785906977,
          0.5169960477135398,
          0.5146245062115635,
          0.49565217424287156,
          0.5375494074444526,
          0.5185770754757606,
          0.5106719370887214,
          0.5114624508755952,
          0.5154150200926739,
          0.5083003956338633,
          0.5241106722665869,
          0.5090909093736189,
          0.5169960477606581,
          0.5177865615475319,
          0.5264822135094126,
          0.5217391307646106,
          0.5217391307646106,
          0.5217391307174923
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 4",
         "type": "scatter",
         "uid": "30faf463-f81b-4a97-ac2d-1d737a08ce24",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.507509881446484,
          0.5083003955867451,
          0.5233201581263259,
          0.513043478637816,
          0.5019762846085394,
          0.5122529644504367,
          0.5011857711279345,
          0.5027667987488004,
          0.5114624509698318,
          0.5011857711279345,
          0.49644268777059475,
          0.5122529647567056,
          0.505138340297895,
          0.5098814232547293,
          0.5185770754286423,
          0.5209486169306186,
          0.5256916999816894,
          0.5201581030966265,
          0.5177865615946502,
          0.5193675892155161,
          0.5138339924246897,
          0.5185770754286423,
          0.5335968382744921,
          0.5154150200926739,
          0.5320158105593896,
          0.5169960477606581,
          0.5146245062586818,
          0.5241106722194687,
          0.5146245062586818,
          0.5130434785906977,
          0.5146245062115635
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "CNN test set accuracy of padded ELMo dataset with variable maximum lengths"
        }
       }
      },
      "text/html": [
       "<div id=\"d207fdd7-4664-45de-9568-7ec7dfbf273c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"d207fdd7-4664-45de-9568-7ec7dfbf273c\")) {\n",
       "    Plotly.newPlot(\"d207fdd7-4664-45de-9568-7ec7dfbf273c\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49565217424287156, 0.5114624506164446, 0.5169960477606581, 0.5122529644504367, 0.49249011860063424, 0.49644268807686365, 0.5075098817998712, 0.5185770751223734, 0.507509881752753, 0.4988142295788399, 0.5043478264639029, 0.5122529647567056, 0.5067193679187609, 0.5090909094207372, 0.513833992471808, 0.5059288540847687, 0.5209486166243497, 0.49565217424287156, 0.5122529647567056, 0.5098814229484603, 0.5272727276025554, 0.5225296445514844, 0.5146245063058001, 0.5051383402507766, 0.5059288537784998, 0.5169960477606581, 0.5051383402507766, 0.513833992471808, 0.5003952569405552, 0.5106719370887214, 0.5019762849148083], \"type\": \"scatter\", \"uid\": \"1aed7a30-cb5d-404d-b860-e1ae780ab8e3\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5043478261105157, 0.5114624509698318, 0.5304347829385238, 0.5067193679658791, 0.499604743412832, 0.5130434782844288, 0.5098814233018476, 0.505138340297895, 0.5011857707745473, 0.5019762849148083, 0.5177865615946502, 0.5011857707745473, 0.5201581030966265, 0.505138340297895, 0.5280632414365475, 0.5359683797293501, 0.5264822138156815, 0.5185770751223734, 0.5114624509698318, 0.4988142295788399, 0.5288537553176579, 0.5225296445986027, 0.5201581030966265, 0.5154150200455556, 0.5027667987488004, 0.5217391307646106, 0.5083003955867451, 0.505138339991626, 0.513043478637816, 0.5201581031437448, 0.5051383402507766], \"type\": \"scatter\", \"uid\": \"fb87cc8e-1753-4561-960d-ccb8e6b19ac8\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5067193679658791, 0.5019762849148083, 0.5090909094678555, 0.507509881446484, 0.5177865616417685, 0.5003952569405552, 0.507509881752753, 0.5011857710336979, 0.5090909093736189, 0.5027667987959187, 0.507509881752753, 0.5185770754286423, 0.5122529648038239, 0.5169960477606581, 0.516205533926666, 0.5035573125827925, 0.5051383402507766, 0.5201581031437448, 0.5154150200926739, 0.5122529647567056, 0.4940711462686184, 0.516205533926666, 0.5193675889563655, 0.4830039526398474, 0.5114624509227135, 0.5035573125827925, 0.5098814229484603, 0.5043478264167846, 0.5090909094207372, 0.5090909094207372, 0.5090909094678555], \"type\": \"scatter\", \"uid\": \"5f681289-a508-4e60-8ec0-26eb472ba05c\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5098814232547293, 0.5114624509698318, 0.5098814232547293, 0.5114624506635628, 0.49802371543857893, 0.4980237155328155, 0.505928854131887, 0.5122529647567056, 0.5146245062586818, 0.505138340297895, 0.5146245062586818, 0.5154150200926739, 0.5193675893097527, 0.5130434785906977, 0.5169960477135398, 0.5146245062115635, 0.49565217424287156, 0.5375494074444526, 0.5185770754757606, 0.5106719370887214, 0.5114624508755952, 0.5154150200926739, 0.5083003956338633, 0.5241106722665869, 0.5090909093736189, 0.5169960477606581, 0.5177865615475319, 0.5264822135094126, 0.5217391307646106, 0.5217391307646106, 0.5217391307174923], \"type\": \"scatter\", \"uid\": \"c788682b-204b-4631-9ac6-55369bb5aa96\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.507509881446484, 0.5083003955867451, 0.5233201581263259, 0.513043478637816, 0.5019762846085394, 0.5122529644504367, 0.5011857711279345, 0.5027667987488004, 0.5114624509698318, 0.5011857711279345, 0.49644268777059475, 0.5122529647567056, 0.505138340297895, 0.5098814232547293, 0.5185770754286423, 0.5209486169306186, 0.5256916999816894, 0.5201581030966265, 0.5177865615946502, 0.5193675892155161, 0.5138339924246897, 0.5185770754286423, 0.5335968382744921, 0.5154150200926739, 0.5320158105593896, 0.5169960477606581, 0.5146245062586818, 0.5241106722194687, 0.5146245062586818, 0.5130434785906977, 0.5146245062115635], \"type\": \"scatter\", \"uid\": \"30faf463-f81b-4a97-ac2d-1d737a08ce24\"}], {\"title\": {\"text\": \"CNN test set accuracy of padded ELMo dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"d207fdd7-4664-45de-9568-7ec7dfbf273c\")) {window._Plotly.Plots.resize(document.getElementById(\"d207fdd7-4664-45de-9568-7ec7dfbf273c\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"d207fdd7-4664-45de-9568-7ec7dfbf273c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"d207fdd7-4664-45de-9568-7ec7dfbf273c\")) {\n",
       "    Plotly.newPlot(\"d207fdd7-4664-45de-9568-7ec7dfbf273c\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49565217424287156, 0.5114624506164446, 0.5169960477606581, 0.5122529644504367, 0.49249011860063424, 0.49644268807686365, 0.5075098817998712, 0.5185770751223734, 0.507509881752753, 0.4988142295788399, 0.5043478264639029, 0.5122529647567056, 0.5067193679187609, 0.5090909094207372, 0.513833992471808, 0.5059288540847687, 0.5209486166243497, 0.49565217424287156, 0.5122529647567056, 0.5098814229484603, 0.5272727276025554, 0.5225296445514844, 0.5146245063058001, 0.5051383402507766, 0.5059288537784998, 0.5169960477606581, 0.5051383402507766, 0.513833992471808, 0.5003952569405552, 0.5106719370887214, 0.5019762849148083], \"type\": \"scatter\", \"uid\": \"1aed7a30-cb5d-404d-b860-e1ae780ab8e3\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5043478261105157, 0.5114624509698318, 0.5304347829385238, 0.5067193679658791, 0.499604743412832, 0.5130434782844288, 0.5098814233018476, 0.505138340297895, 0.5011857707745473, 0.5019762849148083, 0.5177865615946502, 0.5011857707745473, 0.5201581030966265, 0.505138340297895, 0.5280632414365475, 0.5359683797293501, 0.5264822138156815, 0.5185770751223734, 0.5114624509698318, 0.4988142295788399, 0.5288537553176579, 0.5225296445986027, 0.5201581030966265, 0.5154150200455556, 0.5027667987488004, 0.5217391307646106, 0.5083003955867451, 0.505138339991626, 0.513043478637816, 0.5201581031437448, 0.5051383402507766], \"type\": \"scatter\", \"uid\": \"fb87cc8e-1753-4561-960d-ccb8e6b19ac8\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5067193679658791, 0.5019762849148083, 0.5090909094678555, 0.507509881446484, 0.5177865616417685, 0.5003952569405552, 0.507509881752753, 0.5011857710336979, 0.5090909093736189, 0.5027667987959187, 0.507509881752753, 0.5185770754286423, 0.5122529648038239, 0.5169960477606581, 0.516205533926666, 0.5035573125827925, 0.5051383402507766, 0.5201581031437448, 0.5154150200926739, 0.5122529647567056, 0.4940711462686184, 0.516205533926666, 0.5193675889563655, 0.4830039526398474, 0.5114624509227135, 0.5035573125827925, 0.5098814229484603, 0.5043478264167846, 0.5090909094207372, 0.5090909094207372, 0.5090909094678555], \"type\": \"scatter\", \"uid\": \"5f681289-a508-4e60-8ec0-26eb472ba05c\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5098814232547293, 0.5114624509698318, 0.5098814232547293, 0.5114624506635628, 0.49802371543857893, 0.4980237155328155, 0.505928854131887, 0.5122529647567056, 0.5146245062586818, 0.505138340297895, 0.5146245062586818, 0.5154150200926739, 0.5193675893097527, 0.5130434785906977, 0.5169960477135398, 0.5146245062115635, 0.49565217424287156, 0.5375494074444526, 0.5185770754757606, 0.5106719370887214, 0.5114624508755952, 0.5154150200926739, 0.5083003956338633, 0.5241106722665869, 0.5090909093736189, 0.5169960477606581, 0.5177865615475319, 0.5264822135094126, 0.5217391307646106, 0.5217391307646106, 0.5217391307174923], \"type\": \"scatter\", \"uid\": \"c788682b-204b-4631-9ac6-55369bb5aa96\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.507509881446484, 0.5083003955867451, 0.5233201581263259, 0.513043478637816, 0.5019762846085394, 0.5122529644504367, 0.5011857711279345, 0.5027667987488004, 0.5114624509698318, 0.5011857711279345, 0.49644268777059475, 0.5122529647567056, 0.505138340297895, 0.5098814232547293, 0.5185770754286423, 0.5209486169306186, 0.5256916999816894, 0.5201581030966265, 0.5177865615946502, 0.5193675892155161, 0.5138339924246897, 0.5185770754286423, 0.5335968382744921, 0.5154150200926739, 0.5320158105593896, 0.5169960477606581, 0.5146245062586818, 0.5241106722194687, 0.5146245062586818, 0.5130434785906977, 0.5146245062115635], \"type\": \"scatter\", \"uid\": \"30faf463-f81b-4a97-ac2d-1d737a08ce24\"}], {\"title\": {\"text\": \"CNN test set accuracy of padded ELMo dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"d207fdd7-4664-45de-9568-7ec7dfbf273c\")) {window._Plotly.Plots.resize(document.getElementById(\"d207fdd7-4664-45de-9568-7ec7dfbf273c\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traces = cnn_elmo_rounds\n",
    "\n",
    "# Create traces\n",
    "def create_scatter(counter):\n",
    "    acc_dict = traces[counter]\n",
    "    \n",
    "    return go.Scatter(\n",
    "        x = list(acc_dict.keys()),\n",
    "        y = list(acc_dict.values()),\n",
    "        mode = 'lines+markers',\n",
    "        name = 'Round ' + str(counter)\n",
    "    )\n",
    "\n",
    "trace_data = [create_scatter(trace) for trace in range(len(traces))]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'CNN test set accuracy of padded ELMo dataset with variable maximum lengths',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = trace_data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "fill": "tozerox",
         "fillcolor": "rgba(0,100,80,0.2)",
         "line": {
          "color": "rgba(255,255,255,0)"
         },
         "name": "BERT",
         "showlegend": false,
         "type": "scatter",
         "uid": "936d1302-b75b-4d6c-9323-6729b98be482",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          35,
          34,
          33,
          32,
          31,
          30,
          29,
          28,
          27,
          26,
          25,
          24,
          23,
          22,
          21,
          20,
          19,
          18,
          17,
          16,
          15,
          14,
          13,
          12,
          11,
          10,
          9,
          8,
          7,
          6,
          5
         ],
         "y": [
          0.5169960477606581,
          0.5241106723137052,
          0.5193675889563655,
          0.5225296443865705,
          0.5106719371358397,
          0.5138339924246897,
          0.5201581031437448,
          0.507509881752753,
          0.507509881752753,
          0.5090909093736189,
          0.5146245063058001,
          0.5154150200926739,
          0.5114624508755952,
          0.5098814233018476,
          0.513833992471808,
          0.507509881752753,
          0.5209486169777369,
          0.518577075381524,
          0.5114624509698318,
          0.507509881446484,
          0.5122529644504367,
          0.5177865612883813,
          0.5130434785906977,
          0.5177865615475319,
          0.5114624509698318,
          0.5138339923775714,
          0.5209486169306186,
          0.513043478496461,
          0.5098814233018476,
          0.5169960478077764,
          0.5106719370416031,
          0.5035573125827925,
          0.5059288537784998,
          0.49169960481376046,
          0.5051383399445077,
          0.4988142296259582,
          0.5043478264639029,
          0.49249011895402145,
          0.4877470355966817,
          0.5019762846085394,
          0.49644268807686365,
          0.4877470359029506,
          0.4758893284401875,
          0.49723320191085574,
          0.5067193679658791,
          0.49723320191085574,
          0.4988142295788399,
          0.5011857710808162,
          0.49723320191085574,
          0.47905138342276865,
          0.4980237156977295,
          0.5035573122765236,
          0.48458498056698224,
          0.4948616601026105,
          0.4861660080229341,
          0.48695652180980786,
          0.5035573122765236,
          0.5043478264167846,
          0.509881423207611,
          0.485375494188942,
          0.5027667984896498,
          0.49249011860063424
         ]
        },
        {
         "line": {
          "color": "rgb(0,100,80)"
         },
         "mode": "lines+markers",
         "name": "BERT",
         "type": "scatter",
         "uid": "6a88f3dd-1480-4060-8721-323be1666f45",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5043478263225479,
          0.5124110674481146,
          0.5041897235039194,
          0.5150988144130104,
          0.5084584982404595,
          0.5081422927869638,
          0.5032411069455354,
          0.5016600793576523,
          0.5002371544187719,
          0.5000790516943799,
          0.5071936761767497,
          0.5046640318184502,
          0.4932806326702178,
          0.5026086959301719,
          0.5078260872675026,
          0.5037154152977608,
          0.5124110675423512,
          0.5122529647567055,
          0.5019762848535545,
          0.49644268792137325,
          0.5035573125215387,
          0.5071936761673259,
          0.506086956799737,
          0.5000790516425498,
          0.5037154153872855,
          0.5084584983205607,
          0.5114624508897305,
          0.5086166010402409,
          0.5045059290799228,
          0.5089328065502785,
          0.5081422928010995
         ]
        },
        {
         "fill": "tozerox",
         "fillcolor": "rgba(0,176,246,0.2)",
         "line": {
          "color": "rgba(255,255,255,0)"
         },
         "name": "ELMo",
         "showlegend": false,
         "type": "scatter",
         "uid": "eaf15a04-a544-47b4-886d-3525335db530",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          35,
          34,
          33,
          32,
          31,
          30,
          29,
          28,
          27,
          26,
          25,
          24,
          23,
          22,
          21,
          20,
          19,
          18,
          17,
          16,
          15,
          14,
          13,
          12,
          11,
          10,
          9,
          8,
          7,
          6,
          5
         ],
         "y": [
          0.5098814232547293,
          0.5114624509698318,
          0.5304347829385238,
          0.513043478637816,
          0.5177865616417685,
          0.5130434782844288,
          0.5098814233018476,
          0.5185770751223734,
          0.5146245062586818,
          0.505138340297895,
          0.5177865615946502,
          0.5185770754286423,
          0.5201581030966265,
          0.5169960477606581,
          0.5280632414365475,
          0.5359683797293501,
          0.5264822138156815,
          0.5375494074444526,
          0.5185770754757606,
          0.5193675892155161,
          0.5288537553176579,
          0.5225296445986027,
          0.5335968382744921,
          0.5241106722665869,
          0.5320158105593896,
          0.5217391307646106,
          0.5177865615475319,
          0.5264822135094126,
          0.5217391307646106,
          0.5217391307646106,
          0.5217391307174923,
          0.5019762849148083,
          0.5090909094207372,
          0.5003952569405552,
          0.5043478264167846,
          0.5051383402507766,
          0.5035573125827925,
          0.5027667987488004,
          0.4830039526398474,
          0.5083003956338633,
          0.5154150200926739,
          0.4940711462686184,
          0.4988142295788399,
          0.5114624509698318,
          0.49565217424287156,
          0.49565217424287156,
          0.5035573125827925,
          0.513833992471808,
          0.505138340297895,
          0.505138340297895,
          0.5011857707745473,
          0.49644268777059475,
          0.4988142295788399,
          0.5011857707745473,
          0.5011857710336979,
          0.5011857711279345,
          0.49644268807686365,
          0.49249011860063424,
          0.5067193679658791,
          0.5090909094678555,
          0.5019762849148083,
          0.49565217424287156
         ]
        },
        {
         "line": {
          "color": "rgb(0,176,246)"
         },
         "mode": "lines+markers",
         "name": "ELMo",
         "type": "scatter",
         "uid": "d093866d-5026-45df-94de-f46d08853a95",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5048221346040959,
          0.5089328066115323,
          0.5179446643096185,
          0.5101976286328357,
          0.5019762847404706,
          0.5040316206570199,
          0.5064031624228587,
          0.5079841899918944,
          0.5087747038258866,
          0.5019762849430792,
          0.5081422927681165,
          0.511936759161855,
          0.5127272730853718,
          0.5108300398649435,
          0.5187351781954407,
          0.5162055339078186,
          0.5147826089830737,
          0.5184189726100138,
          0.5150988145779245,
          0.5101976287176486,
          0.5150988144978234,
          0.5190513837196139,
          0.5192094864534295,
          0.5086166010590881,
          0.5122529646766044,
          0.5152569173258754,
          0.5111462453184391,
          0.51478260892182,
          0.5117786564044801,
          0.5149407118017024,
          0.5105138343124993
         ]
        }
       ],
       "layout": {
        "paper_bgcolor": "rgb(255,255,255)",
        "plot_bgcolor": "rgb(229,229,229)",
        "title": {
         "text": "CNN test set accuracy of padded datasets with variable maximum lengths"
        },
        "xaxis": {
         "gridcolor": "rgb(255,255,255)",
         "range": [
          5,
          35
         ],
         "showgrid": true,
         "showline": false,
         "showticklabels": true,
         "tickcolor": "rgb(127,127,127)",
         "ticks": "outside",
         "zeroline": false
        },
        "yaxis": {
         "gridcolor": "rgb(255,255,255)",
         "showgrid": true,
         "showline": false,
         "showticklabels": true,
         "tickcolor": "rgb(127,127,127)",
         "ticks": "outside",
         "zeroline": false
        }
       }
      },
      "text/html": [
       "<div id=\"cf678e0a-c9a1-4719-a678-93d3204e34d5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"cf678e0a-c9a1-4719-a678-93d3204e34d5\")) {\n",
       "    Plotly.newPlot(\"cf678e0a-c9a1-4719-a678-93d3204e34d5\", [{\"fill\": \"tozerox\", \"fillcolor\": \"rgba(0,100,80,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"BERT\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.5169960477606581, 0.5241106723137052, 0.5193675889563655, 0.5225296443865705, 0.5106719371358397, 0.5138339924246897, 0.5201581031437448, 0.507509881752753, 0.507509881752753, 0.5090909093736189, 0.5146245063058001, 0.5154150200926739, 0.5114624508755952, 0.5098814233018476, 0.513833992471808, 0.507509881752753, 0.5209486169777369, 0.518577075381524, 0.5114624509698318, 0.507509881446484, 0.5122529644504367, 0.5177865612883813, 0.5130434785906977, 0.5177865615475319, 0.5114624509698318, 0.5138339923775714, 0.5209486169306186, 0.513043478496461, 0.5098814233018476, 0.5169960478077764, 0.5106719370416031, 0.5035573125827925, 0.5059288537784998, 0.49169960481376046, 0.5051383399445077, 0.4988142296259582, 0.5043478264639029, 0.49249011895402145, 0.4877470355966817, 0.5019762846085394, 0.49644268807686365, 0.4877470359029506, 0.4758893284401875, 0.49723320191085574, 0.5067193679658791, 0.49723320191085574, 0.4988142295788399, 0.5011857710808162, 0.49723320191085574, 0.47905138342276865, 0.4980237156977295, 0.5035573122765236, 0.48458498056698224, 0.4948616601026105, 0.4861660080229341, 0.48695652180980786, 0.5035573122765236, 0.5043478264167846, 0.509881423207611, 0.485375494188942, 0.5027667984896498, 0.49249011860063424], \"type\": \"scatter\", \"uid\": \"936d1302-b75b-4d6c-9323-6729b98be482\"}, {\"line\": {\"color\": \"rgb(0,100,80)\"}, \"mode\": \"lines+markers\", \"name\": \"BERT\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5043478263225479, 0.5124110674481146, 0.5041897235039194, 0.5150988144130104, 0.5084584982404595, 0.5081422927869638, 0.5032411069455354, 0.5016600793576523, 0.5002371544187719, 0.5000790516943799, 0.5071936761767497, 0.5046640318184502, 0.4932806326702178, 0.5026086959301719, 0.5078260872675026, 0.5037154152977608, 0.5124110675423512, 0.5122529647567055, 0.5019762848535545, 0.49644268792137325, 0.5035573125215387, 0.5071936761673259, 0.506086956799737, 0.5000790516425498, 0.5037154153872855, 0.5084584983205607, 0.5114624508897305, 0.5086166010402409, 0.5045059290799228, 0.5089328065502785, 0.5081422928010995], \"type\": \"scatter\", \"uid\": \"6a88f3dd-1480-4060-8721-323be1666f45\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(0,176,246,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"ELMo\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.5098814232547293, 0.5114624509698318, 0.5304347829385238, 0.513043478637816, 0.5177865616417685, 0.5130434782844288, 0.5098814233018476, 0.5185770751223734, 0.5146245062586818, 0.505138340297895, 0.5177865615946502, 0.5185770754286423, 0.5201581030966265, 0.5169960477606581, 0.5280632414365475, 0.5359683797293501, 0.5264822138156815, 0.5375494074444526, 0.5185770754757606, 0.5193675892155161, 0.5288537553176579, 0.5225296445986027, 0.5335968382744921, 0.5241106722665869, 0.5320158105593896, 0.5217391307646106, 0.5177865615475319, 0.5264822135094126, 0.5217391307646106, 0.5217391307646106, 0.5217391307174923, 0.5019762849148083, 0.5090909094207372, 0.5003952569405552, 0.5043478264167846, 0.5051383402507766, 0.5035573125827925, 0.5027667987488004, 0.4830039526398474, 0.5083003956338633, 0.5154150200926739, 0.4940711462686184, 0.4988142295788399, 0.5114624509698318, 0.49565217424287156, 0.49565217424287156, 0.5035573125827925, 0.513833992471808, 0.505138340297895, 0.505138340297895, 0.5011857707745473, 0.49644268777059475, 0.4988142295788399, 0.5011857707745473, 0.5011857710336979, 0.5011857711279345, 0.49644268807686365, 0.49249011860063424, 0.5067193679658791, 0.5090909094678555, 0.5019762849148083, 0.49565217424287156], \"type\": \"scatter\", \"uid\": \"eaf15a04-a544-47b4-886d-3525335db530\"}, {\"line\": {\"color\": \"rgb(0,176,246)\"}, \"mode\": \"lines+markers\", \"name\": \"ELMo\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5048221346040959, 0.5089328066115323, 0.5179446643096185, 0.5101976286328357, 0.5019762847404706, 0.5040316206570199, 0.5064031624228587, 0.5079841899918944, 0.5087747038258866, 0.5019762849430792, 0.5081422927681165, 0.511936759161855, 0.5127272730853718, 0.5108300398649435, 0.5187351781954407, 0.5162055339078186, 0.5147826089830737, 0.5184189726100138, 0.5150988145779245, 0.5101976287176486, 0.5150988144978234, 0.5190513837196139, 0.5192094864534295, 0.5086166010590881, 0.5122529646766044, 0.5152569173258754, 0.5111462453184391, 0.51478260892182, 0.5117786564044801, 0.5149407118017024, 0.5105138343124993], \"type\": \"scatter\", \"uid\": \"d093866d-5026-45df-94de-f46d08853a95\"}], {\"paper_bgcolor\": \"rgb(255,255,255)\", \"plot_bgcolor\": \"rgb(229,229,229)\", \"title\": {\"text\": \"CNN test set accuracy of padded datasets with variable maximum lengths\"}, \"xaxis\": {\"gridcolor\": \"rgb(255,255,255)\", \"range\": [5, 35], \"showgrid\": true, \"showline\": false, \"showticklabels\": true, \"tickcolor\": \"rgb(127,127,127)\", \"ticks\": \"outside\", \"zeroline\": false}, \"yaxis\": {\"gridcolor\": \"rgb(255,255,255)\", \"showgrid\": true, \"showline\": false, \"showticklabels\": true, \"tickcolor\": \"rgb(127,127,127)\", \"ticks\": \"outside\", \"zeroline\": false}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"cf678e0a-c9a1-4719-a678-93d3204e34d5\")) {window._Plotly.Plots.resize(document.getElementById(\"cf678e0a-c9a1-4719-a678-93d3204e34d5\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"cf678e0a-c9a1-4719-a678-93d3204e34d5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"cf678e0a-c9a1-4719-a678-93d3204e34d5\")) {\n",
       "    Plotly.newPlot(\"cf678e0a-c9a1-4719-a678-93d3204e34d5\", [{\"fill\": \"tozerox\", \"fillcolor\": \"rgba(0,100,80,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"BERT\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.5169960477606581, 0.5241106723137052, 0.5193675889563655, 0.5225296443865705, 0.5106719371358397, 0.5138339924246897, 0.5201581031437448, 0.507509881752753, 0.507509881752753, 0.5090909093736189, 0.5146245063058001, 0.5154150200926739, 0.5114624508755952, 0.5098814233018476, 0.513833992471808, 0.507509881752753, 0.5209486169777369, 0.518577075381524, 0.5114624509698318, 0.507509881446484, 0.5122529644504367, 0.5177865612883813, 0.5130434785906977, 0.5177865615475319, 0.5114624509698318, 0.5138339923775714, 0.5209486169306186, 0.513043478496461, 0.5098814233018476, 0.5169960478077764, 0.5106719370416031, 0.5035573125827925, 0.5059288537784998, 0.49169960481376046, 0.5051383399445077, 0.4988142296259582, 0.5043478264639029, 0.49249011895402145, 0.4877470355966817, 0.5019762846085394, 0.49644268807686365, 0.4877470359029506, 0.4758893284401875, 0.49723320191085574, 0.5067193679658791, 0.49723320191085574, 0.4988142295788399, 0.5011857710808162, 0.49723320191085574, 0.47905138342276865, 0.4980237156977295, 0.5035573122765236, 0.48458498056698224, 0.4948616601026105, 0.4861660080229341, 0.48695652180980786, 0.5035573122765236, 0.5043478264167846, 0.509881423207611, 0.485375494188942, 0.5027667984896498, 0.49249011860063424], \"type\": \"scatter\", \"uid\": \"936d1302-b75b-4d6c-9323-6729b98be482\"}, {\"line\": {\"color\": \"rgb(0,100,80)\"}, \"mode\": \"lines+markers\", \"name\": \"BERT\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5043478263225479, 0.5124110674481146, 0.5041897235039194, 0.5150988144130104, 0.5084584982404595, 0.5081422927869638, 0.5032411069455354, 0.5016600793576523, 0.5002371544187719, 0.5000790516943799, 0.5071936761767497, 0.5046640318184502, 0.4932806326702178, 0.5026086959301719, 0.5078260872675026, 0.5037154152977608, 0.5124110675423512, 0.5122529647567055, 0.5019762848535545, 0.49644268792137325, 0.5035573125215387, 0.5071936761673259, 0.506086956799737, 0.5000790516425498, 0.5037154153872855, 0.5084584983205607, 0.5114624508897305, 0.5086166010402409, 0.5045059290799228, 0.5089328065502785, 0.5081422928010995], \"type\": \"scatter\", \"uid\": \"6a88f3dd-1480-4060-8721-323be1666f45\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(0,176,246,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"ELMo\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.5098814232547293, 0.5114624509698318, 0.5304347829385238, 0.513043478637816, 0.5177865616417685, 0.5130434782844288, 0.5098814233018476, 0.5185770751223734, 0.5146245062586818, 0.505138340297895, 0.5177865615946502, 0.5185770754286423, 0.5201581030966265, 0.5169960477606581, 0.5280632414365475, 0.5359683797293501, 0.5264822138156815, 0.5375494074444526, 0.5185770754757606, 0.5193675892155161, 0.5288537553176579, 0.5225296445986027, 0.5335968382744921, 0.5241106722665869, 0.5320158105593896, 0.5217391307646106, 0.5177865615475319, 0.5264822135094126, 0.5217391307646106, 0.5217391307646106, 0.5217391307174923, 0.5019762849148083, 0.5090909094207372, 0.5003952569405552, 0.5043478264167846, 0.5051383402507766, 0.5035573125827925, 0.5027667987488004, 0.4830039526398474, 0.5083003956338633, 0.5154150200926739, 0.4940711462686184, 0.4988142295788399, 0.5114624509698318, 0.49565217424287156, 0.49565217424287156, 0.5035573125827925, 0.513833992471808, 0.505138340297895, 0.505138340297895, 0.5011857707745473, 0.49644268777059475, 0.4988142295788399, 0.5011857707745473, 0.5011857710336979, 0.5011857711279345, 0.49644268807686365, 0.49249011860063424, 0.5067193679658791, 0.5090909094678555, 0.5019762849148083, 0.49565217424287156], \"type\": \"scatter\", \"uid\": \"eaf15a04-a544-47b4-886d-3525335db530\"}, {\"line\": {\"color\": \"rgb(0,176,246)\"}, \"mode\": \"lines+markers\", \"name\": \"ELMo\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5048221346040959, 0.5089328066115323, 0.5179446643096185, 0.5101976286328357, 0.5019762847404706, 0.5040316206570199, 0.5064031624228587, 0.5079841899918944, 0.5087747038258866, 0.5019762849430792, 0.5081422927681165, 0.511936759161855, 0.5127272730853718, 0.5108300398649435, 0.5187351781954407, 0.5162055339078186, 0.5147826089830737, 0.5184189726100138, 0.5150988145779245, 0.5101976287176486, 0.5150988144978234, 0.5190513837196139, 0.5192094864534295, 0.5086166010590881, 0.5122529646766044, 0.5152569173258754, 0.5111462453184391, 0.51478260892182, 0.5117786564044801, 0.5149407118017024, 0.5105138343124993], \"type\": \"scatter\", \"uid\": \"d093866d-5026-45df-94de-f46d08853a95\"}], {\"paper_bgcolor\": \"rgb(255,255,255)\", \"plot_bgcolor\": \"rgb(229,229,229)\", \"title\": {\"text\": \"CNN test set accuracy of padded datasets with variable maximum lengths\"}, \"xaxis\": {\"gridcolor\": \"rgb(255,255,255)\", \"range\": [5, 35], \"showgrid\": true, \"showline\": false, \"showticklabels\": true, \"tickcolor\": \"rgb(127,127,127)\", \"ticks\": \"outside\", \"zeroline\": false}, \"yaxis\": {\"gridcolor\": \"rgb(255,255,255)\", \"showgrid\": true, \"showline\": false, \"showticklabels\": true, \"tickcolor\": \"rgb(127,127,127)\", \"ticks\": \"outside\", \"zeroline\": false}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"cf678e0a-c9a1-4719-a678-93d3204e34d5\")) {window._Plotly.Plots.resize(document.getElementById(\"cf678e0a-c9a1-4719-a678-93d3204e34d5\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(5, 36))\n",
    "x_rev = x[::-1]\n",
    "\n",
    "# BERT\n",
    "bert_matrix = np.transpose(np.array([np.array(list(acc_round.values())) for acc_round in cnn_bert_rounds]))\n",
    "bert_y = [np.average(row) for row in bert_matrix]\n",
    "bert_y_upper = [row.max() for row in bert_matrix]\n",
    "bert_y_lower = [row.min() for row in bert_matrix]\n",
    "bert_y_lower = bert_y_lower[::-1]\n",
    "\n",
    "bert1 = go.Scatter(\n",
    "    x = x + x_rev,\n",
    "    y = bert_y_upper + bert_y_lower,\n",
    "    fill = 'tozerox',\n",
    "    fillcolor = 'rgba(0,100,80,0.2)',\n",
    "    line = dict(color = 'rgba(255,255,255,0)'),\n",
    "    showlegend = False,\n",
    "    name = 'BERT',\n",
    ")\n",
    "bert2 = go.Scatter(\n",
    "    x = x,\n",
    "    y = bert_y,\n",
    "    line = dict(color='rgb(0,100,80)'),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'BERT',\n",
    ")\n",
    "\n",
    "# ELMo\n",
    "elmo_matrix = np.transpose(np.array([np.array(list(acc_round.values())) for acc_round in cnn_elmo_rounds]))\n",
    "elmo_y = [np.average(row) for row in elmo_matrix]\n",
    "elmo_y_upper = [row.max() for row in elmo_matrix]\n",
    "elmo_y_lower = [row.min() for row in elmo_matrix]\n",
    "elmo_y_lower = elmo_y_lower[::-1]\n",
    "\n",
    "elmo1 = go.Scatter(\n",
    "    x = x + x_rev,\n",
    "    y = elmo_y_upper + elmo_y_lower,\n",
    "    fill = 'tozerox',\n",
    "    fillcolor = 'rgba(0,176,246,0.2)',\n",
    "    line = dict(color = 'rgba(255,255,255,0)'),\n",
    "    showlegend = False,\n",
    "    name = 'ELMo',\n",
    ")\n",
    "elmo2 = go.Scatter(\n",
    "    x = x,\n",
    "    y = elmo_y,\n",
    "    line = dict(color='rgb(0,176,246)'),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'ELMo',\n",
    ")\n",
    "\n",
    "data = [bert1, bert2, elmo1, elmo2]\n",
    "layout = go.Layout(\n",
    "    title = 'CNN test set accuracy of padded datasets with variable maximum lengths',\n",
    "    paper_bgcolor = 'rgb(255,255,255)',\n",
    "    plot_bgcolor = 'rgb(229,229,229)',\n",
    "    xaxis = dict(\n",
    "        gridcolor = 'rgb(255,255,255)',\n",
    "        range = [5,35],\n",
    "        showgrid = True,\n",
    "        showline = False,\n",
    "        showticklabels = True,\n",
    "        tickcolor = 'rgb(127,127,127)',\n",
    "        ticks = 'outside',\n",
    "        zeroline = False\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        gridcolor='rgb(255,255,255)',\n",
    "        showgrid = True,\n",
    "        showline = False,\n",
    "        showticklabels = True,\n",
    "        tickcolor = 'rgb(127,127,127)',\n",
    "        ticks = 'outside',\n",
    "        zeroline = False\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### References\n",
    "\n",
    "```\n",
    "@article{DBLP:journals/corr/Wang17j,\n",
    "  author    = {William Yang Wang},\n",
    "  title     = {\"Liar, Liar Pants on Fire\": {A} New Benchmark Dataset for Fake News\n",
    "               Detection},\n",
    "  journal   = {CoRR},\n",
    "  volume    = {abs/1705.00648},\n",
    "  year      = {2017},\n",
    "  url       = {http://arxiv.org/abs/1705.00648},\n",
    "  archivePrefix = {arXiv},\n",
    "  eprint    = {1705.00648},\n",
    "  timestamp = {Mon, 13 Aug 2018 16:48:58 +0200},\n",
    "  biburl    = {https://dblp.org/rec/bib/journals/corr/Wang17j},\n",
    "  bibsource = {dblp computer science bibliography, https://dblp.org}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
