{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using neural classification\n",
    "As has been proven by [Wang (2017)](https://arxiv.org/abs/1705.00648), neural classifiers carry better results than non-neural classifiers when detecting fake news. However, it is unknown how well neural networks classify fake news when using previously mentioned text embeddings. \n",
    "In this notebook, the second research question will be answered: *how well do neural network architecture classify fake news compared to non-neural classification algorithms?*\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Reshape, Conv1D, Flatten\n",
    "\n",
    "# Set offline mode for plotly\n",
    "init_notebook_mode(connected = True)\n",
    "\n",
    "# The DataLoader class gives access to pretrained vectors from the Liar dataset\n",
    "from data_loader import DataLoader\n",
    "data = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "general = data.get_dfs()\n",
    "\n",
    "# Recode labels from 6 to 3\n",
    "def recode(label):\n",
    "    if label == 'false' or label == 'pants-fire' or label == 'barely-true':\n",
    "        return 0\n",
    "    elif label == 'true' or label == 'mostly-true':\n",
    "        return 2\n",
    "    elif label == 'half-true':\n",
    "        return 1\n",
    "\n",
    "for dataset in general.keys():\n",
    "    general[dataset]['label'] = general[dataset]['label'].apply(lambda label: recode(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Bidirectional LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = data.get_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max-pooled BERT embeddings from RQ1\n",
    "def max_pool(statement):\n",
    "    if len(statement) > 1:\n",
    "        return [row.max() for row in np.transpose([[token_row.max() for token_row in np.transpose(np.array(sentence))] for sentence in statement])]\n",
    "    else:\n",
    "        return [token_row.max() for token_row in np.transpose(statement[0])]\n",
    "\n",
    "max_pooled_bert = {\n",
    "    dataset: pd.DataFrame(list(bert[dataset].statement.apply(lambda statement: max_pool(statement)).values))\n",
    "    for dataset in bert.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_score(X_train, X_test, X_validation, y_train = general['train']['label'], y_test = general['test']['label'], y_validation = general['validation']['label'], reshape = True):\n",
    "    # Rearrange data types    \n",
    "    params = locals().copy()    \n",
    "    inputs = {\n",
    "        dataset: np.array(params[dataset])\n",
    "        for dataset in params.keys()\n",
    "    }\n",
    "    \n",
    "    for dataset in inputs.keys():\n",
    "        if dataset[0:1] == 'X' and reshape:\n",
    "            # Reshape datasets from 2D to 3D\n",
    "            inputs[dataset] = np.reshape(inputs[dataset], (inputs[dataset].shape[0], inputs[dataset].shape[1], 1))\n",
    "        elif dataset[0:1] == 'y':\n",
    "            inputs[dataset] = np_utils.to_categorical(np.array(inputs[dataset]), 3)\n",
    "    \n",
    "    # Set model parameters\n",
    "    epochs = 5\n",
    "    batch_size = 32\n",
    "    input_shape = X_train.shape\n",
    "\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, input_shape = input_shape)))\n",
    "    model.add(Dropout(0.8))\n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "    model.compile('sgd', 'categorical_crossentropy', metrics = ['accuracy']) \n",
    "    \n",
    "    # Fit the training set over the model and correct on the validation set\n",
    "    model.fit(inputs['X_train'], inputs['y_train'],\n",
    "            batch_size = batch_size,\n",
    "            epochs = epochs,\n",
    "            validation_data = (inputs['X_validation'], inputs['y_validation']))\n",
    "    \n",
    "    # Get score over the test set\n",
    "    score, acc = model.evaluate(inputs['X_test'], inputs['y_test'])\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "2019-05-21 12:40:37,047 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "2019-05-21 12:40:37,448 From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-05-21 12:40:37,537 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 829s 81ms/step - loss: 1.0743 - acc: 0.4087 - val_loss: 1.0423 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 844s 82ms/step - loss: 1.0632 - acc: 0.4300 - val_loss: 1.0392 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 838s 82ms/step - loss: 1.0590 - acc: 0.4354 - val_loss: 1.0408 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 840s 82ms/step - loss: 1.0592 - acc: 0.4354 - val_loss: 1.0432 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 809s 79ms/step - loss: 1.0591 - acc: 0.4337 - val_loss: 1.0395 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 16s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.43715415052745654"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bilstm_score(max_pooled_bert['train'], max_pooled_bert['test'], max_pooled_bert['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, the condensed datasets from RQ1 do not perform well when using a neural classifier. The next step is trying out a padding approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.1184 - acc: 0.4145 - val_loss: 1.0170 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0597 - acc: 0.4532 - val_loss: 1.0196 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0432 - acc: 0.4707 - val_loss: 1.0158 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0362 - acc: 0.4775 - val_loss: 1.0134 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0279 - acc: 0.4821 - val_loss: 1.0089 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.1094 - acc: 0.4181 - val_loss: 1.0184 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0558 - acc: 0.4556 - val_loss: 1.0125 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0428 - acc: 0.4693 - val_loss: 1.0125 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0332 - acc: 0.4863 - val_loss: 1.0140 - val_acc: 0.5008\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0303 - acc: 0.4824 - val_loss: 1.0051 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 1s 594us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.1187 - acc: 0.4151 - val_loss: 1.0248 - val_acc: 0.5109\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0541 - acc: 0.4529 - val_loss: 1.0166 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0422 - acc: 0.4718 - val_loss: 1.0122 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0347 - acc: 0.4808 - val_loss: 1.0118 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0292 - acc: 0.4889 - val_loss: 1.0056 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 1s 969us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.1133 - acc: 0.4260 - val_loss: 1.0178 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0581 - acc: 0.4538 - val_loss: 1.0119 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0404 - acc: 0.4719 - val_loss: 1.0130 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0340 - acc: 0.4828 - val_loss: 1.0065 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0276 - acc: 0.4863 - val_loss: 1.0027 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 1s 799us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.1089 - acc: 0.4181 - val_loss: 1.0162 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0551 - acc: 0.4594 - val_loss: 1.0202 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0381 - acc: 0.4771 - val_loss: 1.0091 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0320 - acc: 0.4829 - val_loss: 1.0067 - val_acc: 0.5000\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0247 - acc: 0.4908 - val_loss: 1.0029 - val_acc: 0.5016\n",
      "1265/1265 [==============================] - 1s 846us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.1186 - acc: 0.4209 - val_loss: 1.0138 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0519 - acc: 0.4614 - val_loss: 1.0098 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0398 - acc: 0.4782 - val_loss: 1.0044 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0280 - acc: 0.4896 - val_loss: 1.0080 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0241 - acc: 0.4943 - val_loss: 1.0050 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 1s 929us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.1218 - acc: 0.4208 - val_loss: 1.0134 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0544 - acc: 0.4600 - val_loss: 1.0069 - val_acc: 0.5202\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0398 - acc: 0.4773 - val_loss: 1.0043 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0296 - acc: 0.4862 - val_loss: 1.0027 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0265 - acc: 0.4872 - val_loss: 0.9985 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 1.1138 - acc: 0.4279 - val_loss: 1.0142 - val_acc: 0.5109\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0477 - acc: 0.4682 - val_loss: 1.0073 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0368 - acc: 0.4805 - val_loss: 1.0111 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0281 - acc: 0.4867 - val_loss: 1.0011 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0280 - acc: 0.4947 - val_loss: 1.0041 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 80s 8ms/step - loss: 1.1205 - acc: 0.4236 - val_loss: 1.0263 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0541 - acc: 0.4649 - val_loss: 1.0081 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0391 - acc: 0.4769 - val_loss: 1.0083 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0296 - acc: 0.4863 - val_loss: 1.0060 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0239 - acc: 0.4927 - val_loss: 1.0026 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 86s 8ms/step - loss: 1.1171 - acc: 0.4152 - val_loss: 1.0107 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0496 - acc: 0.4637 - val_loss: 1.0054 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0413 - acc: 0.4780 - val_loss: 1.0082 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0297 - acc: 0.4904 - val_loss: 1.0004 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0234 - acc: 0.4949 - val_loss: 0.9993 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 92s 9ms/step - loss: 1.1165 - acc: 0.4206 - val_loss: 1.0214 - val_acc: 0.5148\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0553 - acc: 0.4656 - val_loss: 1.0178 - val_acc: 0.5109\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0350 - acc: 0.4852 - val_loss: 1.0104 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0312 - acc: 0.4890 - val_loss: 1.0051 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0229 - acc: 0.4910 - val_loss: 1.0055 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 91s 9ms/step - loss: 1.1168 - acc: 0.4210 - val_loss: 1.0127 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0508 - acc: 0.4668 - val_loss: 1.0166 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0364 - acc: 0.4793 - val_loss: 1.0032 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0281 - acc: 0.4893 - val_loss: 1.0000 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0228 - acc: 0.4973 - val_loss: 1.0013 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 98s 10ms/step - loss: 1.1218 - acc: 0.4131 - val_loss: 1.0302 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0546 - acc: 0.4616 - val_loss: 1.0226 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 36s 4ms/step - loss: 1.0387 - acc: 0.4726 - val_loss: 1.0115 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0314 - acc: 0.4861 - val_loss: 1.0049 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0245 - acc: 0.4940 - val_loss: 1.0055 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 103s 10ms/step - loss: 1.1113 - acc: 0.4235 - val_loss: 1.0184 - val_acc: 0.5234\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0534 - acc: 0.4643 - val_loss: 1.0104 - val_acc: 0.5226\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 36s 3ms/step - loss: 1.0409 - acc: 0.4802 - val_loss: 1.0079 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 36s 4ms/step - loss: 1.0321 - acc: 0.4882 - val_loss: 1.0040 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0239 - acc: 0.5000 - val_loss: 1.0034 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 101s 10ms/step - loss: 1.1158 - acc: 0.4191 - val_loss: 1.0152 - val_acc: 0.5226\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0550 - acc: 0.4584 - val_loss: 1.0119 - val_acc: 0.5249\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0378 - acc: 0.4787 - val_loss: 1.0057 - val_acc: 0.5296\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0310 - acc: 0.4812 - val_loss: 1.0030 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0227 - acc: 0.4948 - val_loss: 0.9970 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 109s 11ms/step - loss: 1.1228 - acc: 0.4128 - val_loss: 1.0199 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0554 - acc: 0.4577 - val_loss: 1.0189 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0387 - acc: 0.4784 - val_loss: 1.0061 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0341 - acc: 0.4836 - val_loss: 1.0074 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0242 - acc: 0.4958 - val_loss: 1.0025 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 111s 11ms/step - loss: 1.1078 - acc: 0.4219 - val_loss: 1.0287 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0534 - acc: 0.4610 - val_loss: 1.0140 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0422 - acc: 0.4721 - val_loss: 1.0134 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0265 - acc: 0.4914 - val_loss: 1.0109 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0268 - acc: 0.4949 - val_loss: 1.0093 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.1146 - acc: 0.4152 - val_loss: 1.0207 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0577 - acc: 0.4632 - val_loss: 1.0113 - val_acc: 0.5249\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0401 - acc: 0.4759 - val_loss: 1.0133 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0335 - acc: 0.4819 - val_loss: 1.0071 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0266 - acc: 0.4901 - val_loss: 1.0044 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 6s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 110s 11ms/step - loss: 1.1089 - acc: 0.4237 - val_loss: 1.0265 - val_acc: 0.5140\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0583 - acc: 0.4500 - val_loss: 1.0147 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0430 - acc: 0.4778 - val_loss: 1.0108 - val_acc: 0.5241\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0318 - acc: 0.4808 - val_loss: 1.0067 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0303 - acc: 0.4843 - val_loss: 1.0078 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.1008 - acc: 0.4262 - val_loss: 1.0131 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0570 - acc: 0.4583 - val_loss: 1.0094 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0434 - acc: 0.4738 - val_loss: 1.0143 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0325 - acc: 0.4833 - val_loss: 1.0063 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0260 - acc: 0.4900 - val_loss: 1.0022 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.1036 - acc: 0.4241 - val_loss: 1.0248 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0551 - acc: 0.4624 - val_loss: 1.0112 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0423 - acc: 0.4752 - val_loss: 1.0161 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0296 - acc: 0.4879 - val_loss: 1.0038 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0270 - acc: 0.4869 - val_loss: 1.0063 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.1012 - acc: 0.4247 - val_loss: 1.0202 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0566 - acc: 0.4576 - val_loss: 1.0107 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0400 - acc: 0.4775 - val_loss: 1.0086 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0354 - acc: 0.4865 - val_loss: 1.0027 - val_acc: 0.5265\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0266 - acc: 0.4919 - val_loss: 1.0065 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0999 - acc: 0.4246 - val_loss: 1.0188 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0588 - acc: 0.4596 - val_loss: 1.0145 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0415 - acc: 0.4750 - val_loss: 1.0089 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0319 - acc: 0.4848 - val_loss: 1.0036 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0260 - acc: 0.4872 - val_loss: 1.0034 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 133s 13ms/step - loss: 1.1086 - acc: 0.4064 - val_loss: 1.0186 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0563 - acc: 0.4585 - val_loss: 1.0152 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0411 - acc: 0.4723 - val_loss: 1.0108 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0383 - acc: 0.4797 - val_loss: 1.0096 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0304 - acc: 0.4826 - val_loss: 1.0036 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0960 - acc: 0.4262 - val_loss: 1.0261 - val_acc: 0.4774\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0572 - acc: 0.4566 - val_loss: 1.0126 - val_acc: 0.5202\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0404 - acc: 0.4806 - val_loss: 1.0154 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0341 - acc: 0.4777 - val_loss: 1.0074 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0242 - acc: 0.4919 - val_loss: 1.0010 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 1.0877 - acc: 0.4269 - val_loss: 1.0218 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0545 - acc: 0.4550 - val_loss: 1.0150 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0455 - acc: 0.4722 - val_loss: 1.0101 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0369 - acc: 0.4793 - val_loss: 1.0025 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0328 - acc: 0.4806 - val_loss: 1.0035 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 1.0966 - acc: 0.4192 - val_loss: 1.0159 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0535 - acc: 0.4648 - val_loss: 1.0140 - val_acc: 0.5179\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0407 - acc: 0.4737 - val_loss: 1.0057 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0344 - acc: 0.4890 - val_loss: 1.0030 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0281 - acc: 0.4929 - val_loss: 1.0092 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0952 - acc: 0.4262 - val_loss: 1.0169 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.0548 - acc: 0.4621 - val_loss: 1.0139 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0452 - acc: 0.4678 - val_loss: 1.0068 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0353 - acc: 0.4783 - val_loss: 1.0042 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0292 - acc: 0.4875 - val_loss: 0.9996 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 138s 14ms/step - loss: 1.0938 - acc: 0.4192 - val_loss: 1.0201 - val_acc: 0.5156\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 1.0523 - acc: 0.4641 - val_loss: 1.0134 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0419 - acc: 0.4745 - val_loss: 1.0075 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0364 - acc: 0.4815 - val_loss: 1.0043 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0280 - acc: 0.4910 - val_loss: 1.0054 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 1.1040 - acc: 0.4251 - val_loss: 1.0204 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 77s 8ms/step - loss: 1.0522 - acc: 0.4651 - val_loss: 1.0147 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0452 - acc: 0.4717 - val_loss: 1.0112 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0349 - acc: 0.4837 - val_loss: 1.0142 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0292 - acc: 0.4872 - val_loss: 0.9990 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 147s 14ms/step - loss: 1.0870 - acc: 0.4226 - val_loss: 1.0187 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 79s 8ms/step - loss: 1.0529 - acc: 0.4603 - val_loss: 1.0116 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.0456 - acc: 0.4740 - val_loss: 1.0122 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0368 - acc: 0.4828 - val_loss: 1.0076 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.0324 - acc: 0.4843 - val_loss: 1.0060 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "CPU times: user 8h 57s, sys: 2h 14min 29s, total: 10h 15min 26s\n",
      "Wall time: 2h 44min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Store accuracies\n",
    "accuracies = {\n",
    "    padding_len: 0.0 for padding_len in list(range(5,36))\n",
    "}\n",
    "\n",
    "concatenated_bert = {\n",
    "    dataset: [np.concatenate(np.array(statement)) for statement in bert[dataset].statement]\n",
    "    for dataset in bert.keys()\n",
    "}\n",
    "\n",
    "for max_len in accuracies.keys():\n",
    "    padded_bert = {\n",
    "        dataset: sequence.pad_sequences(concatenated_bert[dataset], maxlen = max_len, dtype = float)\n",
    "        for dataset in concatenated_bert.keys()\n",
    "    }\n",
    "    \n",
    "    accuracies[max_len] = get_bilstm_score(padded_bert['train'], padded_bert['test'], padded_bert['validation'], reshape = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{5: 0.513043478637816,\n",
       "  6: 0.5122529648038239,\n",
       "  7: 0.5075098817998712,\n",
       "  8: 0.5083003953747127,\n",
       "  9: 0.49407114631573673,\n",
       "  10: 0.5098814233018476,\n",
       "  11: 0.507509881752753,\n",
       "  12: 0.5035573126299108,\n",
       "  13: 0.4996047434599503,\n",
       "  14: 0.5027667987488004,\n",
       "  15: 0.507509881752753,\n",
       "  16: 0.5090909091144682,\n",
       "  17: 0.5067193679187609,\n",
       "  18: 0.5114624509227135,\n",
       "  19: 0.5098814233018476,\n",
       "  20: 0.5162055336203971,\n",
       "  21: 0.5193675893097527,\n",
       "  22: 0.5193675893097527,\n",
       "  23: 0.5233201584797131,\n",
       "  24: 0.5217391304583417,\n",
       "  25: 0.5106719371358397,\n",
       "  26: 0.5106719367824524,\n",
       "  27: 0.5177865616417685,\n",
       "  28: 0.5075098817998712,\n",
       "  29: 0.513043478637816,\n",
       "  30: 0.5043478264639029,\n",
       "  31: 0.5162055336203971,\n",
       "  32: 0.5075098817998712,\n",
       "  33: 0.5114624509698318,\n",
       "  34: 0.513833992471808,\n",
       "  35: 0.5075098817998712},\n",
       " {5: 0.5098814233018476,\n",
       "  6: 0.5098814229484603,\n",
       "  7: 0.49723320160458684,\n",
       "  8: 0.5090909091144682,\n",
       "  9: 0.5051383399445077,\n",
       "  10: 0.505928854131887,\n",
       "  11: 0.498814229272571,\n",
       "  12: 0.5114624509698318,\n",
       "  13: 0.4940711466220057,\n",
       "  14: 0.49169960512002936,\n",
       "  15: 0.4996047431065631,\n",
       "  16: 0.5027667984425315,\n",
       "  17: 0.4956521739366026,\n",
       "  18: 0.5067193679658791,\n",
       "  19: 0.5067193676124919,\n",
       "  20: 0.5043478264167846,\n",
       "  21: 0.5146245063058001,\n",
       "  22: 0.5256916999816894,\n",
       "  23: 0.5201581031437448,\n",
       "  24: 0.515415019786405,\n",
       "  25: 0.5169960478077764,\n",
       "  26: 0.521739130811729,\n",
       "  27: 0.5185770751223734,\n",
       "  28: 0.5154150201397922,\n",
       "  29: 0.5209486169777369,\n",
       "  30: 0.5027667987959187,\n",
       "  31: 0.5011857711279345,\n",
       "  32: 0.5154150201397922,\n",
       "  33: 0.5083003956338633,\n",
       "  34: 0.5075098817998712,\n",
       "  35: 0.5185770754286423},\n",
       " {5: 0.5035573122765236,\n",
       "  6: 0.5027667984425315,\n",
       "  7: 0.5090909094207372,\n",
       "  8: 0.49090909128603727,\n",
       "  9: 0.5098814233018476,\n",
       "  10: 0.505928854131887,\n",
       "  11: 0.5075098817998712,\n",
       "  12: 0.5035573123236419,\n",
       "  13: 0.5003952572468241,\n",
       "  14: 0.488537549784061,\n",
       "  15: 0.5075098817998712,\n",
       "  16: 0.5035573122765236,\n",
       "  17: 0.5003952572468241,\n",
       "  18: 0.5122529648038239,\n",
       "  19: 0.5051383402507766,\n",
       "  20: 0.5177865616417685,\n",
       "  21: 0.5106719371358397,\n",
       "  22: 0.5019762849619266,\n",
       "  23: 0.5106719368295707,\n",
       "  24: 0.5083003956338633,\n",
       "  25: 0.5146245063058001,\n",
       "  26: 0.5146245063058001,\n",
       "  27: 0.5177865616417685,\n",
       "  28: 0.5067193676596102,\n",
       "  29: 0.5098814233018476,\n",
       "  30: 0.5146245063058001,\n",
       "  31: 0.5138339921184208,\n",
       "  32: 0.5043478264639029,\n",
       "  33: 0.507509881446484,\n",
       "  34: 0.5154150201397922,\n",
       "  35: 0.5067193679658791},\n",
       " {5: 0.49249011895402145,\n",
       "  6: 0.5098814229484603,\n",
       "  7: 0.5019762849148083,\n",
       "  8: 0.505928854131887,\n",
       "  9: 0.5059288537784998,\n",
       "  10: 0.4996047434599503,\n",
       "  11: 0.5035573126299108,\n",
       "  12: 0.4996047434599503,\n",
       "  13: 0.5003952572468241,\n",
       "  14: 0.4893280636180531,\n",
       "  15: 0.5035573126299108,\n",
       "  16: 0.49802371543857893,\n",
       "  17: 0.513043478637816,\n",
       "  18: 0.5019762846085394,\n",
       "  19: 0.5106719371358397,\n",
       "  20: 0.5114624509227135,\n",
       "  21: 0.5122529648038239,\n",
       "  22: 0.515415019786405,\n",
       "  23: 0.5154150201397922,\n",
       "  24: 0.522529644645721,\n",
       "  25: 0.505138340297895,\n",
       "  26: 0.5169960478077764,\n",
       "  27: 0.5201581031437448,\n",
       "  28: 0.5106719371358397,\n",
       "  29: 0.4996047434599503,\n",
       "  30: 0.5169960478077764,\n",
       "  31: 0.5067193676124919,\n",
       "  32: 0.5146245063058001,\n",
       "  33: 0.5043478264639029,\n",
       "  34: 0.5043478261105157,\n",
       "  35: 0.5106719367824524},\n",
       " {5: 0.505138340297895,\n",
       "  6: 0.505928854131887,\n",
       "  7: 0.4948616604559978,\n",
       "  8: 0.5051383399445077,\n",
       "  9: 0.498814229272571,\n",
       "  10: 0.5122529647567056,\n",
       "  11: 0.49723320195797405,\n",
       "  12: 0.5146245063058001,\n",
       "  13: 0.505928854131887,\n",
       "  14: 0.4940711466220057,\n",
       "  15: 0.5106719368295707,\n",
       "  16: 0.5106719367824524,\n",
       "  17: 0.5027667987959187,\n",
       "  18: 0.5059288537784998,\n",
       "  19: 0.5083003956338633,\n",
       "  20: 0.5098814233018476,\n",
       "  21: 0.5138339921184208,\n",
       "  22: 0.5027667987959187,\n",
       "  23: 0.5146245059995312,\n",
       "  24: 0.5059288540847687,\n",
       "  25: 0.5130434783315471,\n",
       "  26: 0.5169960478077764,\n",
       "  27: 0.5114624509698318,\n",
       "  28: 0.505928854131887,\n",
       "  29: 0.505928854131887,\n",
       "  30: 0.5098814233018476,\n",
       "  31: 0.5114624509698318,\n",
       "  32: 0.5059288537784998,\n",
       "  33: 0.5130434783315471,\n",
       "  34: 0.49723320195797405,\n",
       "  35: 0.5114624509698318}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Round 0",
         "type": "scatter",
         "uid": "36b39775-a328-4c54-a3ca-4455c6dc01d6",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.513043478637816,
          0.5122529648038239,
          0.5075098817998712,
          0.5083003953747127,
          0.49407114631573673,
          0.5098814233018476,
          0.507509881752753,
          0.5035573126299108,
          0.4996047434599503,
          0.5027667987488004,
          0.507509881752753,
          0.5090909091144682,
          0.5067193679187609,
          0.5114624509227135,
          0.5098814233018476,
          0.5162055336203971,
          0.5193675893097527,
          0.5193675893097527,
          0.5233201584797131,
          0.5217391304583417,
          0.5106719371358397,
          0.5106719367824524,
          0.5177865616417685,
          0.5075098817998712,
          0.513043478637816,
          0.5043478264639029,
          0.5162055336203971,
          0.5075098817998712,
          0.5114624509698318,
          0.513833992471808,
          0.5075098817998712
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 1",
         "type": "scatter",
         "uid": "05ff3bde-811d-43d5-8421-2f3acd734853",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5098814233018476,
          0.5098814229484603,
          0.49723320160458684,
          0.5090909091144682,
          0.5051383399445077,
          0.505928854131887,
          0.498814229272571,
          0.5114624509698318,
          0.4940711466220057,
          0.49169960512002936,
          0.4996047431065631,
          0.5027667984425315,
          0.4956521739366026,
          0.5067193679658791,
          0.5067193676124919,
          0.5043478264167846,
          0.5146245063058001,
          0.5256916999816894,
          0.5201581031437448,
          0.515415019786405,
          0.5169960478077764,
          0.521739130811729,
          0.5185770751223734,
          0.5154150201397922,
          0.5209486169777369,
          0.5027667987959187,
          0.5011857711279345,
          0.5154150201397922,
          0.5083003956338633,
          0.5075098817998712,
          0.5185770754286423
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 2",
         "type": "scatter",
         "uid": "332e414a-c977-4eeb-894a-69644b06f03d",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5035573122765236,
          0.5027667984425315,
          0.5090909094207372,
          0.49090909128603727,
          0.5098814233018476,
          0.505928854131887,
          0.5075098817998712,
          0.5035573123236419,
          0.5003952572468241,
          0.488537549784061,
          0.5075098817998712,
          0.5035573122765236,
          0.5003952572468241,
          0.5122529648038239,
          0.5051383402507766,
          0.5177865616417685,
          0.5106719371358397,
          0.5019762849619266,
          0.5106719368295707,
          0.5083003956338633,
          0.5146245063058001,
          0.5146245063058001,
          0.5177865616417685,
          0.5067193676596102,
          0.5098814233018476,
          0.5146245063058001,
          0.5138339921184208,
          0.5043478264639029,
          0.507509881446484,
          0.5154150201397922,
          0.5067193679658791
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 3",
         "type": "scatter",
         "uid": "0b216ed9-0130-4d9d-837c-77b49dbb95d4",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.49249011895402145,
          0.5098814229484603,
          0.5019762849148083,
          0.505928854131887,
          0.5059288537784998,
          0.4996047434599503,
          0.5035573126299108,
          0.4996047434599503,
          0.5003952572468241,
          0.4893280636180531,
          0.5035573126299108,
          0.49802371543857893,
          0.513043478637816,
          0.5019762846085394,
          0.5106719371358397,
          0.5114624509227135,
          0.5122529648038239,
          0.515415019786405,
          0.5154150201397922,
          0.522529644645721,
          0.505138340297895,
          0.5169960478077764,
          0.5201581031437448,
          0.5106719371358397,
          0.4996047434599503,
          0.5169960478077764,
          0.5067193676124919,
          0.5146245063058001,
          0.5043478264639029,
          0.5043478261105157,
          0.5106719367824524
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 4",
         "type": "scatter",
         "uid": "2f9230fa-3bf5-47ea-b643-5e2650adb502",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.505138340297895,
          0.505928854131887,
          0.4948616604559978,
          0.5051383399445077,
          0.498814229272571,
          0.5122529647567056,
          0.49723320195797405,
          0.5146245063058001,
          0.505928854131887,
          0.4940711466220057,
          0.5106719368295707,
          0.5106719367824524,
          0.5027667987959187,
          0.5059288537784998,
          0.5083003956338633,
          0.5098814233018476,
          0.5138339921184208,
          0.5027667987959187,
          0.5146245059995312,
          0.5059288540847687,
          0.5130434783315471,
          0.5169960478077764,
          0.5114624509698318,
          0.505928854131887,
          0.505928854131887,
          0.5098814233018476,
          0.5114624509698318,
          0.5059288537784998,
          0.5130434783315471,
          0.49723320195797405,
          0.5114624509698318
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "Test set accuracy of padded BERT dataset with variable maximum lengths"
        }
       }
      },
      "text/html": [
       "<div id=\"6142f664-ca53-4b83-9386-37ca9a91d9a5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\")) {\n",
       "    Plotly.newPlot(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.513043478637816, 0.5122529648038239, 0.5075098817998712, 0.5083003953747127, 0.49407114631573673, 0.5098814233018476, 0.507509881752753, 0.5035573126299108, 0.4996047434599503, 0.5027667987488004, 0.507509881752753, 0.5090909091144682, 0.5067193679187609, 0.5114624509227135, 0.5098814233018476, 0.5162055336203971, 0.5193675893097527, 0.5193675893097527, 0.5233201584797131, 0.5217391304583417, 0.5106719371358397, 0.5106719367824524, 0.5177865616417685, 0.5075098817998712, 0.513043478637816, 0.5043478264639029, 0.5162055336203971, 0.5075098817998712, 0.5114624509698318, 0.513833992471808, 0.5075098817998712], \"type\": \"scatter\", \"uid\": \"36b39775-a328-4c54-a3ca-4455c6dc01d6\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5098814233018476, 0.5098814229484603, 0.49723320160458684, 0.5090909091144682, 0.5051383399445077, 0.505928854131887, 0.498814229272571, 0.5114624509698318, 0.4940711466220057, 0.49169960512002936, 0.4996047431065631, 0.5027667984425315, 0.4956521739366026, 0.5067193679658791, 0.5067193676124919, 0.5043478264167846, 0.5146245063058001, 0.5256916999816894, 0.5201581031437448, 0.515415019786405, 0.5169960478077764, 0.521739130811729, 0.5185770751223734, 0.5154150201397922, 0.5209486169777369, 0.5027667987959187, 0.5011857711279345, 0.5154150201397922, 0.5083003956338633, 0.5075098817998712, 0.5185770754286423], \"type\": \"scatter\", \"uid\": \"05ff3bde-811d-43d5-8421-2f3acd734853\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5035573122765236, 0.5027667984425315, 0.5090909094207372, 0.49090909128603727, 0.5098814233018476, 0.505928854131887, 0.5075098817998712, 0.5035573123236419, 0.5003952572468241, 0.488537549784061, 0.5075098817998712, 0.5035573122765236, 0.5003952572468241, 0.5122529648038239, 0.5051383402507766, 0.5177865616417685, 0.5106719371358397, 0.5019762849619266, 0.5106719368295707, 0.5083003956338633, 0.5146245063058001, 0.5146245063058001, 0.5177865616417685, 0.5067193676596102, 0.5098814233018476, 0.5146245063058001, 0.5138339921184208, 0.5043478264639029, 0.507509881446484, 0.5154150201397922, 0.5067193679658791], \"type\": \"scatter\", \"uid\": \"332e414a-c977-4eeb-894a-69644b06f03d\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49249011895402145, 0.5098814229484603, 0.5019762849148083, 0.505928854131887, 0.5059288537784998, 0.4996047434599503, 0.5035573126299108, 0.4996047434599503, 0.5003952572468241, 0.4893280636180531, 0.5035573126299108, 0.49802371543857893, 0.513043478637816, 0.5019762846085394, 0.5106719371358397, 0.5114624509227135, 0.5122529648038239, 0.515415019786405, 0.5154150201397922, 0.522529644645721, 0.505138340297895, 0.5169960478077764, 0.5201581031437448, 0.5106719371358397, 0.4996047434599503, 0.5169960478077764, 0.5067193676124919, 0.5146245063058001, 0.5043478264639029, 0.5043478261105157, 0.5106719367824524], \"type\": \"scatter\", \"uid\": \"0b216ed9-0130-4d9d-837c-77b49dbb95d4\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.505138340297895, 0.505928854131887, 0.4948616604559978, 0.5051383399445077, 0.498814229272571, 0.5122529647567056, 0.49723320195797405, 0.5146245063058001, 0.505928854131887, 0.4940711466220057, 0.5106719368295707, 0.5106719367824524, 0.5027667987959187, 0.5059288537784998, 0.5083003956338633, 0.5098814233018476, 0.5138339921184208, 0.5027667987959187, 0.5146245059995312, 0.5059288540847687, 0.5130434783315471, 0.5169960478077764, 0.5114624509698318, 0.505928854131887, 0.505928854131887, 0.5098814233018476, 0.5114624509698318, 0.5059288537784998, 0.5130434783315471, 0.49723320195797405, 0.5114624509698318], \"type\": \"scatter\", \"uid\": \"2f9230fa-3bf5-47ea-b643-5e2650adb502\"}], {\"title\": {\"text\": \"Test set accuracy of padded BERT dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\")) {window._Plotly.Plots.resize(document.getElementById(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"6142f664-ca53-4b83-9386-37ca9a91d9a5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\")) {\n",
       "    Plotly.newPlot(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.513043478637816, 0.5122529648038239, 0.5075098817998712, 0.5083003953747127, 0.49407114631573673, 0.5098814233018476, 0.507509881752753, 0.5035573126299108, 0.4996047434599503, 0.5027667987488004, 0.507509881752753, 0.5090909091144682, 0.5067193679187609, 0.5114624509227135, 0.5098814233018476, 0.5162055336203971, 0.5193675893097527, 0.5193675893097527, 0.5233201584797131, 0.5217391304583417, 0.5106719371358397, 0.5106719367824524, 0.5177865616417685, 0.5075098817998712, 0.513043478637816, 0.5043478264639029, 0.5162055336203971, 0.5075098817998712, 0.5114624509698318, 0.513833992471808, 0.5075098817998712], \"type\": \"scatter\", \"uid\": \"36b39775-a328-4c54-a3ca-4455c6dc01d6\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5098814233018476, 0.5098814229484603, 0.49723320160458684, 0.5090909091144682, 0.5051383399445077, 0.505928854131887, 0.498814229272571, 0.5114624509698318, 0.4940711466220057, 0.49169960512002936, 0.4996047431065631, 0.5027667984425315, 0.4956521739366026, 0.5067193679658791, 0.5067193676124919, 0.5043478264167846, 0.5146245063058001, 0.5256916999816894, 0.5201581031437448, 0.515415019786405, 0.5169960478077764, 0.521739130811729, 0.5185770751223734, 0.5154150201397922, 0.5209486169777369, 0.5027667987959187, 0.5011857711279345, 0.5154150201397922, 0.5083003956338633, 0.5075098817998712, 0.5185770754286423], \"type\": \"scatter\", \"uid\": \"05ff3bde-811d-43d5-8421-2f3acd734853\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5035573122765236, 0.5027667984425315, 0.5090909094207372, 0.49090909128603727, 0.5098814233018476, 0.505928854131887, 0.5075098817998712, 0.5035573123236419, 0.5003952572468241, 0.488537549784061, 0.5075098817998712, 0.5035573122765236, 0.5003952572468241, 0.5122529648038239, 0.5051383402507766, 0.5177865616417685, 0.5106719371358397, 0.5019762849619266, 0.5106719368295707, 0.5083003956338633, 0.5146245063058001, 0.5146245063058001, 0.5177865616417685, 0.5067193676596102, 0.5098814233018476, 0.5146245063058001, 0.5138339921184208, 0.5043478264639029, 0.507509881446484, 0.5154150201397922, 0.5067193679658791], \"type\": \"scatter\", \"uid\": \"332e414a-c977-4eeb-894a-69644b06f03d\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49249011895402145, 0.5098814229484603, 0.5019762849148083, 0.505928854131887, 0.5059288537784998, 0.4996047434599503, 0.5035573126299108, 0.4996047434599503, 0.5003952572468241, 0.4893280636180531, 0.5035573126299108, 0.49802371543857893, 0.513043478637816, 0.5019762846085394, 0.5106719371358397, 0.5114624509227135, 0.5122529648038239, 0.515415019786405, 0.5154150201397922, 0.522529644645721, 0.505138340297895, 0.5169960478077764, 0.5201581031437448, 0.5106719371358397, 0.4996047434599503, 0.5169960478077764, 0.5067193676124919, 0.5146245063058001, 0.5043478264639029, 0.5043478261105157, 0.5106719367824524], \"type\": \"scatter\", \"uid\": \"0b216ed9-0130-4d9d-837c-77b49dbb95d4\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.505138340297895, 0.505928854131887, 0.4948616604559978, 0.5051383399445077, 0.498814229272571, 0.5122529647567056, 0.49723320195797405, 0.5146245063058001, 0.505928854131887, 0.4940711466220057, 0.5106719368295707, 0.5106719367824524, 0.5027667987959187, 0.5059288537784998, 0.5083003956338633, 0.5098814233018476, 0.5138339921184208, 0.5027667987959187, 0.5146245059995312, 0.5059288540847687, 0.5130434783315471, 0.5169960478077764, 0.5114624509698318, 0.505928854131887, 0.505928854131887, 0.5098814233018476, 0.5114624509698318, 0.5059288537784998, 0.5130434783315471, 0.49723320195797405, 0.5114624509698318], \"type\": \"scatter\", \"uid\": \"2f9230fa-3bf5-47ea-b643-5e2650adb502\"}], {\"title\": {\"text\": \"Test set accuracy of padded BERT dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\")) {window._Plotly.Plots.resize(document.getElementById(\"6142f664-ca53-4b83-9386-37ca9a91d9a5\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traces = [round1, round2, round3, round4, round5]\n",
    "\n",
    "# Create traces\n",
    "bert_trace = go.Scatter(\n",
    "    x = list(round1.keys()),\n",
    "    y = list(round1.values()),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'BERT'\n",
    ")\n",
    "\n",
    "def create_scatter(counter):\n",
    "    acc_dict = traces[counter]\n",
    "    \n",
    "    return go.Scatter(\n",
    "        x = list(acc_dict.keys()),\n",
    "        y = list(acc_dict.values()),\n",
    "        mode = 'lines+markers',\n",
    "        name = 'Round ' + str(counter)\n",
    "    )\n",
    "\n",
    "trace_data = [create_scatter(trace) for trace in range(len(traces))]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'Test set accuracy of padded BERT dataset with variable maximum lengths',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = trace_data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = data.get_elmo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_round(dataset):\n",
    "    # Store accuracies\n",
    "    accuracies = {\n",
    "        padding_len: 0.0 for padding_len in list(range(5,36))\n",
    "    }\n",
    "\n",
    "    for max_len in accuracies.keys():\n",
    "        padded_dataset = {\n",
    "            fold: sequence.pad_sequences(dataset[fold], maxlen = max_len, dtype = float)\n",
    "            for fold in dataset.keys()\n",
    "        }\n",
    "\n",
    "        accuracies[max_len] = get_bilstm_score(padded_dataset['train'], padded_dataset['test'], padded_dataset['validation'], reshape = False)\n",
    "\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "2019-06-04 08:49:28,085 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "2019-06-04 08:49:29,218 From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-06-04 08:49:29,336 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.1231 - acc: 0.4109 - val_loss: 1.0209 - val_acc: 0.5132\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0595 - acc: 0.4525 - val_loss: 1.0150 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0383 - acc: 0.4803 - val_loss: 1.0079 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0308 - acc: 0.4804 - val_loss: 1.0084 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0214 - acc: 0.4905 - val_loss: 1.0035 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 1s 976us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.1167 - acc: 0.4189 - val_loss: 1.0217 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0523 - acc: 0.4639 - val_loss: 1.0156 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0370 - acc: 0.4768 - val_loss: 1.0143 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0299 - acc: 0.4821 - val_loss: 1.0149 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0195 - acc: 0.4913 - val_loss: 1.0048 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.1217 - acc: 0.4253 - val_loss: 1.0176 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0519 - acc: 0.4686 - val_loss: 1.0084 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0349 - acc: 0.4787 - val_loss: 1.0108 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0223 - acc: 0.4999 - val_loss: 1.0038 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0190 - acc: 0.5024 - val_loss: 1.0035 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 1s 603us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.1229 - acc: 0.4177 - val_loss: 1.0027 - val_acc: 0.5234\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0514 - acc: 0.4736 - val_loss: 1.0046 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0353 - acc: 0.4785 - val_loss: 1.0000 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0236 - acc: 0.4977 - val_loss: 1.0011 - val_acc: 0.5265\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0156 - acc: 0.5013 - val_loss: 0.9955 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 1s 693us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.1301 - acc: 0.4194 - val_loss: 1.0173 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0536 - acc: 0.4634 - val_loss: 1.0070 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0346 - acc: 0.4853 - val_loss: 1.0047 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0252 - acc: 0.4909 - val_loss: 1.0040 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0197 - acc: 0.4994 - val_loss: 1.0040 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 1s 736us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.1315 - acc: 0.4200 - val_loss: 1.0127 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0574 - acc: 0.4640 - val_loss: 1.0057 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0345 - acc: 0.4799 - val_loss: 1.0039 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0257 - acc: 0.4910 - val_loss: 1.0002 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0188 - acc: 0.4982 - val_loss: 1.0004 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 1s 889us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.1205 - acc: 0.4319 - val_loss: 1.0081 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0486 - acc: 0.4710 - val_loss: 1.0146 - val_acc: 0.5218\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0332 - acc: 0.4878 - val_loss: 1.0067 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0219 - acc: 0.4874 - val_loss: 1.0054 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0164 - acc: 0.5048 - val_loss: 0.9998 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 1s 861us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.1176 - acc: 0.4283 - val_loss: 1.0073 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0519 - acc: 0.4675 - val_loss: 1.0026 - val_acc: 0.5202\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0326 - acc: 0.4869 - val_loss: 1.0003 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0246 - acc: 0.4937 - val_loss: 1.0000 - val_acc: 0.5249\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0157 - acc: 0.5025 - val_loss: 0.9957 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 1s 961us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.1207 - acc: 0.4320 - val_loss: 1.0211 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0517 - acc: 0.4702 - val_loss: 1.0160 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0308 - acc: 0.4884 - val_loss: 1.0031 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0219 - acc: 0.4962 - val_loss: 1.0021 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0132 - acc: 0.5085 - val_loss: 0.9967 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.1081 - acc: 0.4299 - val_loss: 1.0038 - val_acc: 0.5148\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0480 - acc: 0.4731 - val_loss: 1.0029 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0338 - acc: 0.4838 - val_loss: 0.9986 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0252 - acc: 0.4940 - val_loss: 0.9948 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0173 - acc: 0.5001 - val_loss: 0.9952 - val_acc: 0.5304\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.1159 - acc: 0.4307 - val_loss: 1.0145 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0539 - acc: 0.4660 - val_loss: 1.0119 - val_acc: 0.5241\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0376 - acc: 0.4810 - val_loss: 1.0023 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0254 - acc: 0.4942 - val_loss: 1.0029 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0151 - acc: 0.5005 - val_loss: 0.9986 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.1116 - acc: 0.4280 - val_loss: 1.0146 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0522 - acc: 0.4614 - val_loss: 1.0047 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0323 - acc: 0.4829 - val_loss: 1.0021 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0226 - acc: 0.4975 - val_loss: 0.9980 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0145 - acc: 0.5009 - val_loss: 0.9958 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.1100 - acc: 0.4297 - val_loss: 1.0106 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0491 - acc: 0.4758 - val_loss: 1.0045 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0337 - acc: 0.4851 - val_loss: 1.0016 - val_acc: 0.5257\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0218 - acc: 0.4977 - val_loss: 1.0015 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0155 - acc: 0.5094 - val_loss: 0.9978 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.1046 - acc: 0.4262 - val_loss: 1.0209 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0444 - acc: 0.4710 - val_loss: 1.0108 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0373 - acc: 0.4827 - val_loss: 1.0073 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0255 - acc: 0.4903 - val_loss: 0.9997 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0189 - acc: 0.5031 - val_loss: 1.0030 - val_acc: 0.5273\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.1119 - acc: 0.4246 - val_loss: 1.0191 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0542 - acc: 0.4693 - val_loss: 1.0102 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0323 - acc: 0.4845 - val_loss: 1.0067 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0226 - acc: 0.4977 - val_loss: 1.0000 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0150 - acc: 0.5075 - val_loss: 0.9978 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.1130 - acc: 0.4248 - val_loss: 1.0219 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0533 - acc: 0.4693 - val_loss: 1.0114 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0386 - acc: 0.4756 - val_loss: 1.0107 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0249 - acc: 0.4981 - val_loss: 1.0015 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0178 - acc: 0.5035 - val_loss: 0.9976 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.1114 - acc: 0.4328 - val_loss: 1.0196 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0550 - acc: 0.4624 - val_loss: 1.0083 - val_acc: 0.5210\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0401 - acc: 0.4788 - val_loss: 1.0085 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0260 - acc: 0.4947 - val_loss: 1.0104 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0154 - acc: 0.5027 - val_loss: 1.0011 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.1098 - acc: 0.4213 - val_loss: 1.0179 - val_acc: 0.5187\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0559 - acc: 0.4659 - val_loss: 1.0179 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0370 - acc: 0.4813 - val_loss: 1.0063 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0263 - acc: 0.4923 - val_loss: 1.0022 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0179 - acc: 0.5044 - val_loss: 1.0010 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 77s 8ms/step - loss: 1.1053 - acc: 0.4298 - val_loss: 1.0166 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0537 - acc: 0.4683 - val_loss: 1.0147 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0386 - acc: 0.4815 - val_loss: 1.0042 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0252 - acc: 0.4949 - val_loss: 1.0019 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0171 - acc: 0.5017 - val_loss: 1.0018 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 6s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.1049 - acc: 0.4234 - val_loss: 1.0253 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0562 - acc: 0.4676 - val_loss: 1.0136 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0383 - acc: 0.4799 - val_loss: 1.0103 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0291 - acc: 0.4906 - val_loss: 1.0088 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0189 - acc: 0.4974 - val_loss: 0.9991 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 82s 8ms/step - loss: 1.0997 - acc: 0.4202 - val_loss: 1.0207 - val_acc: 0.5171\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0539 - acc: 0.4663 - val_loss: 1.0153 - val_acc: 0.5226\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0407 - acc: 0.4841 - val_loss: 1.0067 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0305 - acc: 0.4888 - val_loss: 1.0041 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0242 - acc: 0.4967 - val_loss: 1.0020 - val_acc: 0.5273\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 92s 9ms/step - loss: 1.0929 - acc: 0.4319 - val_loss: 1.0242 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.0557 - acc: 0.4656 - val_loss: 1.0122 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0428 - acc: 0.4768 - val_loss: 1.0064 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0296 - acc: 0.4952 - val_loss: 0.9998 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0219 - acc: 0.4967 - val_loss: 0.9976 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 89s 9ms/step - loss: 1.0916 - acc: 0.4263 - val_loss: 1.0179 - val_acc: 0.5234\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.0553 - acc: 0.4643 - val_loss: 1.0136 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0419 - acc: 0.4820 - val_loss: 1.0032 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0267 - acc: 0.4969 - val_loss: 1.0052 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0238 - acc: 0.4944 - val_loss: 1.0009 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 96s 9ms/step - loss: 1.1015 - acc: 0.4337 - val_loss: 1.0205 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.0565 - acc: 0.4613 - val_loss: 1.0087 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0414 - acc: 0.4827 - val_loss: 1.0037 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0279 - acc: 0.4897 - val_loss: 0.9966 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0232 - acc: 0.4973 - val_loss: 0.9984 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 98s 10ms/step - loss: 1.0877 - acc: 0.4276 - val_loss: 1.0195 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.0520 - acc: 0.4664 - val_loss: 1.0108 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.0377 - acc: 0.4810 - val_loss: 1.0109 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0299 - acc: 0.4877 - val_loss: 1.0029 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0245 - acc: 0.4924 - val_loss: 1.0048 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 99s 10ms/step - loss: 1.0832 - acc: 0.4339 - val_loss: 1.0227 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 82s 8ms/step - loss: 1.0560 - acc: 0.4565 - val_loss: 1.0177 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0434 - acc: 0.4694 - val_loss: 1.0077 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0374 - acc: 0.4791 - val_loss: 1.0058 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0250 - acc: 0.4925 - val_loss: 1.0009 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0894 - acc: 0.4293 - val_loss: 1.0238 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 82s 8ms/step - loss: 1.0533 - acc: 0.4647 - val_loss: 1.0107 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0378 - acc: 0.4809 - val_loss: 1.0060 - val_acc: 0.5241\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0297 - acc: 0.4920 - val_loss: 1.0018 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0235 - acc: 0.4967 - val_loss: 0.9994 - val_acc: 0.5312\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 105s 10ms/step - loss: 1.0882 - acc: 0.4199 - val_loss: 1.0226 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 83s 8ms/step - loss: 1.0509 - acc: 0.4627 - val_loss: 1.0152 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0414 - acc: 0.4792 - val_loss: 1.0036 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0336 - acc: 0.4829 - val_loss: 1.0052 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0206 - acc: 0.4984 - val_loss: 0.9965 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 109s 11ms/step - loss: 1.0867 - acc: 0.4291 - val_loss: 1.0143 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 88s 9ms/step - loss: 1.0541 - acc: 0.4648 - val_loss: 1.0082 - val_acc: 0.5226\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.0381 - acc: 0.4800 - val_loss: 1.0098 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.0359 - acc: 0.4850 - val_loss: 1.0044 - val_acc: 0.5195\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.0263 - acc: 0.4929 - val_loss: 0.9962 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0812 - acc: 0.4300 - val_loss: 1.0226 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 89s 9ms/step - loss: 1.0550 - acc: 0.4587 - val_loss: 1.0114 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.0377 - acc: 0.4820 - val_loss: 1.0068 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.0326 - acc: 0.4815 - val_loss: 1.0002 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.0230 - acc: 0.5020 - val_loss: 1.0016 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0779 - acc: 0.4358 - val_loss: 1.0203 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 94s 9ms/step - loss: 1.0488 - acc: 0.4633 - val_loss: 1.0100 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.0379 - acc: 0.4818 - val_loss: 1.0051 - val_acc: 0.5249\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.0333 - acc: 0.4880 - val_loss: 1.0019 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.0242 - acc: 0.4993 - val_loss: 0.9981 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "CPU times: user 9h 15min 19s, sys: 2h 16min 35s, total: 11h 31min 54s\n",
      "Wall time: 2h 32min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "concatenated_elmo = {\n",
    "    fold: [np.concatenate(np.array(statement)) for statement in elmo[fold]['statement']]\n",
    "    for fold in elmo.keys()\n",
    "}\n",
    "\n",
    "elmo_rounds = [calculate_round(concatenated_elmo) for round in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{5: 0.5177865616417685,\n",
       "  6: 0.5241106722665869,\n",
       "  7: 0.5114624509698318,\n",
       "  8: 0.5067193676124919,\n",
       "  9: 0.5067193679187609,\n",
       "  10: 0.5059288540847687,\n",
       "  11: 0.49802371574484783,\n",
       "  12: 0.5011857710808162,\n",
       "  13: 0.49169960507291105,\n",
       "  14: 0.49723320191085574,\n",
       "  15: 0.4932806327408953,\n",
       "  16: 0.5169960478077764,\n",
       "  17: 0.5154150200455556,\n",
       "  18: 0.513043478637816,\n",
       "  19: 0.5083003955396268,\n",
       "  20: 0.5185770754286423,\n",
       "  21: 0.5067193679187609,\n",
       "  22: 0.5146245063058001,\n",
       "  23: 0.5011857710808162,\n",
       "  24: 0.5106719370887214,\n",
       "  25: 0.5169960477606581,\n",
       "  26: 0.5193675893097527,\n",
       "  27: 0.5114624509227135,\n",
       "  28: 0.5130434785906977,\n",
       "  29: 0.5241106723137052,\n",
       "  30: 0.5169960477606581,\n",
       "  31: 0.5169960477606581,\n",
       "  32: 0.5146245063058001,\n",
       "  33: 0.5185770754286423,\n",
       "  34: 0.5051383402507766,\n",
       "  35: 0.5035573125827925},\n",
       " {5: 0.5122529647567056,\n",
       "  6: 0.5169960478077764,\n",
       "  7: 0.5146245062586818,\n",
       "  8: 0.5233201584325948,\n",
       "  9: 0.5138339924246897,\n",
       "  10: 0.4996047434599503,\n",
       "  11: 0.49090909123891896,\n",
       "  12: 0.49090909123891896,\n",
       "  13: 0.5011857710808162,\n",
       "  14: 0.5051383402036584,\n",
       "  15: 0.5067193679187609,\n",
       "  16: 0.5122529648038239,\n",
       "  17: 0.5146245062586818,\n",
       "  18: 0.5146245062586818,\n",
       "  19: 0.516205533926666,\n",
       "  20: 0.5083003955867451,\n",
       "  21: 0.5304347829385238,\n",
       "  22: 0.5217391307646106,\n",
       "  23: 0.5106719370887214,\n",
       "  24: 0.5138339924246897,\n",
       "  25: 0.5209486169306186,\n",
       "  26: 0.5233201584325948,\n",
       "  27: 0.505138340297895,\n",
       "  28: 0.5201581030966265,\n",
       "  29: 0.5225296445986027,\n",
       "  30: 0.5098814233018476,\n",
       "  31: 0.5090909094207372,\n",
       "  32: 0.5122529647567056,\n",
       "  33: 0.5130434785906977,\n",
       "  34: 0.499604743412832,\n",
       "  35: 0.5154150200926739},\n",
       " {5: 0.516205533926666,\n",
       "  6: 0.5264822137685633,\n",
       "  7: 0.5146245062586818,\n",
       "  8: 0.5067193679187609,\n",
       "  9: 0.505928854131887,\n",
       "  10: 0.5035573125827925,\n",
       "  11: 0.4988142295788399,\n",
       "  12: 0.5146245063058001,\n",
       "  13: 0.4940711465748874,\n",
       "  14: 0.5035573125827925,\n",
       "  15: 0.5083003955867451,\n",
       "  16: 0.5067193679187609,\n",
       "  17: 0.5098814232547293,\n",
       "  18: 0.5177865616417685,\n",
       "  19: 0.5059288540847687,\n",
       "  20: 0.513043478637816,\n",
       "  21: 0.5067193679187609,\n",
       "  22: 0.5312252967725158,\n",
       "  23: 0.5209486169306186,\n",
       "  24: 0.5122529647567056,\n",
       "  25: 0.5138339924246897,\n",
       "  26: 0.5185770754757606,\n",
       "  27: 0.5169960478077764,\n",
       "  28: 0.5177865616417685,\n",
       "  29: 0.5193675892626344,\n",
       "  30: 0.5185770754286423,\n",
       "  31: 0.5043478264167846,\n",
       "  32: 0.5090909094678555,\n",
       "  33: 0.5177865616417685,\n",
       "  34: 0.5138339924246897,\n",
       "  35: 0.5098814232547293},\n",
       " {5: 0.505928854131887,\n",
       "  6: 0.5241106723137052,\n",
       "  7: 0.5003952572468241,\n",
       "  8: 0.5154150201397922,\n",
       "  9: 0.4996047431065631,\n",
       "  10: 0.5019762849148083,\n",
       "  11: 0.49802371543857893,\n",
       "  12: 0.5011857710808162,\n",
       "  13: 0.5003952572468241,\n",
       "  14: 0.5122529648038239,\n",
       "  15: 0.5083003955867451,\n",
       "  16: 0.5114624509227135,\n",
       "  17: 0.5169960477606581,\n",
       "  18: 0.5090909094207372,\n",
       "  19: 0.516205533926666,\n",
       "  20: 0.49802371574484783,\n",
       "  21: 0.5177865615946502,\n",
       "  22: 0.5106719370887214,\n",
       "  23: 0.5177865612883813,\n",
       "  24: 0.5193675892626344,\n",
       "  25: 0.5177865616417685,\n",
       "  26: 0.5122529648038239,\n",
       "  27: 0.5154150200926739,\n",
       "  28: 0.5122529648038239,\n",
       "  29: 0.4988142295788399,\n",
       "  30: 0.516205533926666,\n",
       "  31: 0.5035573126299108,\n",
       "  32: 0.5106719370887214,\n",
       "  33: 0.5098814232547293,\n",
       "  34: 0.513833992471808,\n",
       "  35: 0.5225296445986027},\n",
       " {5: 0.5114624509698318,\n",
       "  6: 0.5130434785906977,\n",
       "  7: 0.5019762849148083,\n",
       "  8: 0.5154150200926739,\n",
       "  9: 0.5154150201397922,\n",
       "  10: 0.5075098817998712,\n",
       "  11: 0.5027667987488004,\n",
       "  12: 0.49090909128603727,\n",
       "  13: 0.5027667987488004,\n",
       "  14: 0.5059288540376505,\n",
       "  15: 0.5090909094207372,\n",
       "  16: 0.524901186100579,\n",
       "  17: 0.507509881752753,\n",
       "  18: 0.5177865615475319,\n",
       "  19: 0.5067193679187609,\n",
       "  20: 0.507509881752753,\n",
       "  21: 0.5272727276025554,\n",
       "  22: 0.5264822138156815,\n",
       "  23: 0.5217391307646106,\n",
       "  24: 0.5106719371358397,\n",
       "  25: 0.5146245062586818,\n",
       "  26: 0.5146245062586818,\n",
       "  27: 0.507509881752753,\n",
       "  28: 0.5162055339737843,\n",
       "  29: 0.5138339924246897,\n",
       "  30: 0.5209486169306186,\n",
       "  31: 0.5233201584325948,\n",
       "  32: 0.5177865615946502,\n",
       "  33: 0.5090909094207372,\n",
       "  34: 0.5130434785906977,\n",
       "  35: 0.516205533926666}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Round 0",
         "type": "scatter",
         "uid": "ee5219cc-6129-477f-abcd-fa309019fae0",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5177865616417685,
          0.5241106722665869,
          0.5114624509698318,
          0.5067193676124919,
          0.5067193679187609,
          0.5059288540847687,
          0.49802371574484783,
          0.5011857710808162,
          0.49169960507291105,
          0.49723320191085574,
          0.4932806327408953,
          0.5169960478077764,
          0.5154150200455556,
          0.513043478637816,
          0.5083003955396268,
          0.5185770754286423,
          0.5067193679187609,
          0.5146245063058001,
          0.5011857710808162,
          0.5106719370887214,
          0.5169960477606581,
          0.5193675893097527,
          0.5114624509227135,
          0.5130434785906977,
          0.5241106723137052,
          0.5169960477606581,
          0.5169960477606581,
          0.5146245063058001,
          0.5185770754286423,
          0.5051383402507766,
          0.5035573125827925
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 1",
         "type": "scatter",
         "uid": "2d68eaf6-378a-449b-ae52-b4c0c62e34f0",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5122529647567056,
          0.5169960478077764,
          0.5146245062586818,
          0.5233201584325948,
          0.5138339924246897,
          0.4996047434599503,
          0.49090909123891896,
          0.49090909123891896,
          0.5011857710808162,
          0.5051383402036584,
          0.5067193679187609,
          0.5122529648038239,
          0.5146245062586818,
          0.5146245062586818,
          0.516205533926666,
          0.5083003955867451,
          0.5304347829385238,
          0.5217391307646106,
          0.5106719370887214,
          0.5138339924246897,
          0.5209486169306186,
          0.5233201584325948,
          0.505138340297895,
          0.5201581030966265,
          0.5225296445986027,
          0.5098814233018476,
          0.5090909094207372,
          0.5122529647567056,
          0.5130434785906977,
          0.499604743412832,
          0.5154150200926739
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 2",
         "type": "scatter",
         "uid": "93305cef-80fb-4afa-8a42-a15e5ddb2480",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.516205533926666,
          0.5264822137685633,
          0.5146245062586818,
          0.5067193679187609,
          0.505928854131887,
          0.5035573125827925,
          0.4988142295788399,
          0.5146245063058001,
          0.4940711465748874,
          0.5035573125827925,
          0.5083003955867451,
          0.5067193679187609,
          0.5098814232547293,
          0.5177865616417685,
          0.5059288540847687,
          0.513043478637816,
          0.5067193679187609,
          0.5312252967725158,
          0.5209486169306186,
          0.5122529647567056,
          0.5138339924246897,
          0.5185770754757606,
          0.5169960478077764,
          0.5177865616417685,
          0.5193675892626344,
          0.5185770754286423,
          0.5043478264167846,
          0.5090909094678555,
          0.5177865616417685,
          0.5138339924246897,
          0.5098814232547293
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 3",
         "type": "scatter",
         "uid": "89320afe-f430-4aed-90a3-59edc22c4188",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.505928854131887,
          0.5241106723137052,
          0.5003952572468241,
          0.5154150201397922,
          0.4996047431065631,
          0.5019762849148083,
          0.49802371543857893,
          0.5011857710808162,
          0.5003952572468241,
          0.5122529648038239,
          0.5083003955867451,
          0.5114624509227135,
          0.5169960477606581,
          0.5090909094207372,
          0.516205533926666,
          0.49802371574484783,
          0.5177865615946502,
          0.5106719370887214,
          0.5177865612883813,
          0.5193675892626344,
          0.5177865616417685,
          0.5122529648038239,
          0.5154150200926739,
          0.5122529648038239,
          0.4988142295788399,
          0.516205533926666,
          0.5035573126299108,
          0.5106719370887214,
          0.5098814232547293,
          0.513833992471808,
          0.5225296445986027
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 4",
         "type": "scatter",
         "uid": "3d6c90f8-65ab-433c-b117-b1e8d64c6712",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5114624509698318,
          0.5130434785906977,
          0.5019762849148083,
          0.5154150200926739,
          0.5154150201397922,
          0.5075098817998712,
          0.5027667987488004,
          0.49090909128603727,
          0.5027667987488004,
          0.5059288540376505,
          0.5090909094207372,
          0.524901186100579,
          0.507509881752753,
          0.5177865615475319,
          0.5067193679187609,
          0.507509881752753,
          0.5272727276025554,
          0.5264822138156815,
          0.5217391307646106,
          0.5106719371358397,
          0.5146245062586818,
          0.5146245062586818,
          0.507509881752753,
          0.5162055339737843,
          0.5138339924246897,
          0.5209486169306186,
          0.5233201584325948,
          0.5177865615946502,
          0.5090909094207372,
          0.5130434785906977,
          0.516205533926666
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "Test set accuracy of padded ELMo dataset with variable maximum lengths"
        }
       }
      },
      "text/html": [
       "<div id=\"99ad5de9-afa7-4181-8346-a581665b0d58\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"99ad5de9-afa7-4181-8346-a581665b0d58\")) {\n",
       "    Plotly.newPlot(\"99ad5de9-afa7-4181-8346-a581665b0d58\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5177865616417685, 0.5241106722665869, 0.5114624509698318, 0.5067193676124919, 0.5067193679187609, 0.5059288540847687, 0.49802371574484783, 0.5011857710808162, 0.49169960507291105, 0.49723320191085574, 0.4932806327408953, 0.5169960478077764, 0.5154150200455556, 0.513043478637816, 0.5083003955396268, 0.5185770754286423, 0.5067193679187609, 0.5146245063058001, 0.5011857710808162, 0.5106719370887214, 0.5169960477606581, 0.5193675893097527, 0.5114624509227135, 0.5130434785906977, 0.5241106723137052, 0.5169960477606581, 0.5169960477606581, 0.5146245063058001, 0.5185770754286423, 0.5051383402507766, 0.5035573125827925], \"type\": \"scatter\", \"uid\": \"ee5219cc-6129-477f-abcd-fa309019fae0\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5122529647567056, 0.5169960478077764, 0.5146245062586818, 0.5233201584325948, 0.5138339924246897, 0.4996047434599503, 0.49090909123891896, 0.49090909123891896, 0.5011857710808162, 0.5051383402036584, 0.5067193679187609, 0.5122529648038239, 0.5146245062586818, 0.5146245062586818, 0.516205533926666, 0.5083003955867451, 0.5304347829385238, 0.5217391307646106, 0.5106719370887214, 0.5138339924246897, 0.5209486169306186, 0.5233201584325948, 0.505138340297895, 0.5201581030966265, 0.5225296445986027, 0.5098814233018476, 0.5090909094207372, 0.5122529647567056, 0.5130434785906977, 0.499604743412832, 0.5154150200926739], \"type\": \"scatter\", \"uid\": \"2d68eaf6-378a-449b-ae52-b4c0c62e34f0\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.516205533926666, 0.5264822137685633, 0.5146245062586818, 0.5067193679187609, 0.505928854131887, 0.5035573125827925, 0.4988142295788399, 0.5146245063058001, 0.4940711465748874, 0.5035573125827925, 0.5083003955867451, 0.5067193679187609, 0.5098814232547293, 0.5177865616417685, 0.5059288540847687, 0.513043478637816, 0.5067193679187609, 0.5312252967725158, 0.5209486169306186, 0.5122529647567056, 0.5138339924246897, 0.5185770754757606, 0.5169960478077764, 0.5177865616417685, 0.5193675892626344, 0.5185770754286423, 0.5043478264167846, 0.5090909094678555, 0.5177865616417685, 0.5138339924246897, 0.5098814232547293], \"type\": \"scatter\", \"uid\": \"93305cef-80fb-4afa-8a42-a15e5ddb2480\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.505928854131887, 0.5241106723137052, 0.5003952572468241, 0.5154150201397922, 0.4996047431065631, 0.5019762849148083, 0.49802371543857893, 0.5011857710808162, 0.5003952572468241, 0.5122529648038239, 0.5083003955867451, 0.5114624509227135, 0.5169960477606581, 0.5090909094207372, 0.516205533926666, 0.49802371574484783, 0.5177865615946502, 0.5106719370887214, 0.5177865612883813, 0.5193675892626344, 0.5177865616417685, 0.5122529648038239, 0.5154150200926739, 0.5122529648038239, 0.4988142295788399, 0.516205533926666, 0.5035573126299108, 0.5106719370887214, 0.5098814232547293, 0.513833992471808, 0.5225296445986027], \"type\": \"scatter\", \"uid\": \"89320afe-f430-4aed-90a3-59edc22c4188\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5114624509698318, 0.5130434785906977, 0.5019762849148083, 0.5154150200926739, 0.5154150201397922, 0.5075098817998712, 0.5027667987488004, 0.49090909128603727, 0.5027667987488004, 0.5059288540376505, 0.5090909094207372, 0.524901186100579, 0.507509881752753, 0.5177865615475319, 0.5067193679187609, 0.507509881752753, 0.5272727276025554, 0.5264822138156815, 0.5217391307646106, 0.5106719371358397, 0.5146245062586818, 0.5146245062586818, 0.507509881752753, 0.5162055339737843, 0.5138339924246897, 0.5209486169306186, 0.5233201584325948, 0.5177865615946502, 0.5090909094207372, 0.5130434785906977, 0.516205533926666], \"type\": \"scatter\", \"uid\": \"3d6c90f8-65ab-433c-b117-b1e8d64c6712\"}], {\"title\": {\"text\": \"Test set accuracy of padded ELMo dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"99ad5de9-afa7-4181-8346-a581665b0d58\")) {window._Plotly.Plots.resize(document.getElementById(\"99ad5de9-afa7-4181-8346-a581665b0d58\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"99ad5de9-afa7-4181-8346-a581665b0d58\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"99ad5de9-afa7-4181-8346-a581665b0d58\")) {\n",
       "    Plotly.newPlot(\"99ad5de9-afa7-4181-8346-a581665b0d58\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5177865616417685, 0.5241106722665869, 0.5114624509698318, 0.5067193676124919, 0.5067193679187609, 0.5059288540847687, 0.49802371574484783, 0.5011857710808162, 0.49169960507291105, 0.49723320191085574, 0.4932806327408953, 0.5169960478077764, 0.5154150200455556, 0.513043478637816, 0.5083003955396268, 0.5185770754286423, 0.5067193679187609, 0.5146245063058001, 0.5011857710808162, 0.5106719370887214, 0.5169960477606581, 0.5193675893097527, 0.5114624509227135, 0.5130434785906977, 0.5241106723137052, 0.5169960477606581, 0.5169960477606581, 0.5146245063058001, 0.5185770754286423, 0.5051383402507766, 0.5035573125827925], \"type\": \"scatter\", \"uid\": \"ee5219cc-6129-477f-abcd-fa309019fae0\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5122529647567056, 0.5169960478077764, 0.5146245062586818, 0.5233201584325948, 0.5138339924246897, 0.4996047434599503, 0.49090909123891896, 0.49090909123891896, 0.5011857710808162, 0.5051383402036584, 0.5067193679187609, 0.5122529648038239, 0.5146245062586818, 0.5146245062586818, 0.516205533926666, 0.5083003955867451, 0.5304347829385238, 0.5217391307646106, 0.5106719370887214, 0.5138339924246897, 0.5209486169306186, 0.5233201584325948, 0.505138340297895, 0.5201581030966265, 0.5225296445986027, 0.5098814233018476, 0.5090909094207372, 0.5122529647567056, 0.5130434785906977, 0.499604743412832, 0.5154150200926739], \"type\": \"scatter\", \"uid\": \"2d68eaf6-378a-449b-ae52-b4c0c62e34f0\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.516205533926666, 0.5264822137685633, 0.5146245062586818, 0.5067193679187609, 0.505928854131887, 0.5035573125827925, 0.4988142295788399, 0.5146245063058001, 0.4940711465748874, 0.5035573125827925, 0.5083003955867451, 0.5067193679187609, 0.5098814232547293, 0.5177865616417685, 0.5059288540847687, 0.513043478637816, 0.5067193679187609, 0.5312252967725158, 0.5209486169306186, 0.5122529647567056, 0.5138339924246897, 0.5185770754757606, 0.5169960478077764, 0.5177865616417685, 0.5193675892626344, 0.5185770754286423, 0.5043478264167846, 0.5090909094678555, 0.5177865616417685, 0.5138339924246897, 0.5098814232547293], \"type\": \"scatter\", \"uid\": \"93305cef-80fb-4afa-8a42-a15e5ddb2480\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.505928854131887, 0.5241106723137052, 0.5003952572468241, 0.5154150201397922, 0.4996047431065631, 0.5019762849148083, 0.49802371543857893, 0.5011857710808162, 0.5003952572468241, 0.5122529648038239, 0.5083003955867451, 0.5114624509227135, 0.5169960477606581, 0.5090909094207372, 0.516205533926666, 0.49802371574484783, 0.5177865615946502, 0.5106719370887214, 0.5177865612883813, 0.5193675892626344, 0.5177865616417685, 0.5122529648038239, 0.5154150200926739, 0.5122529648038239, 0.4988142295788399, 0.516205533926666, 0.5035573126299108, 0.5106719370887214, 0.5098814232547293, 0.513833992471808, 0.5225296445986027], \"type\": \"scatter\", \"uid\": \"89320afe-f430-4aed-90a3-59edc22c4188\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5114624509698318, 0.5130434785906977, 0.5019762849148083, 0.5154150200926739, 0.5154150201397922, 0.5075098817998712, 0.5027667987488004, 0.49090909128603727, 0.5027667987488004, 0.5059288540376505, 0.5090909094207372, 0.524901186100579, 0.507509881752753, 0.5177865615475319, 0.5067193679187609, 0.507509881752753, 0.5272727276025554, 0.5264822138156815, 0.5217391307646106, 0.5106719371358397, 0.5146245062586818, 0.5146245062586818, 0.507509881752753, 0.5162055339737843, 0.5138339924246897, 0.5209486169306186, 0.5233201584325948, 0.5177865615946502, 0.5090909094207372, 0.5130434785906977, 0.516205533926666], \"type\": \"scatter\", \"uid\": \"3d6c90f8-65ab-433c-b117-b1e8d64c6712\"}], {\"title\": {\"text\": \"Test set accuracy of padded ELMo dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"99ad5de9-afa7-4181-8346-a581665b0d58\")) {window._Plotly.Plots.resize(document.getElementById(\"99ad5de9-afa7-4181-8346-a581665b0d58\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traces = elmo_rounds\n",
    "\n",
    "# Create traces\n",
    "def create_scatter(counter):\n",
    "    acc_dict = traces[counter]\n",
    "    \n",
    "    return go.Scatter(\n",
    "        x = list(acc_dict.keys()),\n",
    "        y = list(acc_dict.values()),\n",
    "        mode = 'lines+markers',\n",
    "        name = 'Round ' + str(counter)\n",
    "    )\n",
    "\n",
    "trace_data = [create_scatter(trace) for trace in range(len(traces))]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'Test set accuracy of padded ELMo dataset with variable maximum lengths',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = trace_data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0872 - acc: 0.3999 - val_loss: 1.0340 - val_acc: 0.4836\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 6s 553us/step - loss: 1.0666 - acc: 0.4330 - val_loss: 1.0242 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 6s 563us/step - loss: 1.0573 - acc: 0.4496 - val_loss: 1.0170 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 6s 563us/step - loss: 1.0508 - acc: 0.4609 - val_loss: 1.0161 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 6s 545us/step - loss: 1.0456 - acc: 0.4652 - val_loss: 1.0134 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 0s 267us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0840 - acc: 0.4108 - val_loss: 1.0314 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 6s 586us/step - loss: 1.0624 - acc: 0.4487 - val_loss: 1.0226 - val_acc: 0.4992\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 6s 610us/step - loss: 1.0585 - acc: 0.4487 - val_loss: 1.0159 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 6s 607us/step - loss: 1.0479 - acc: 0.4602 - val_loss: 1.0123 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 6s 611us/step - loss: 1.0428 - acc: 0.4714 - val_loss: 1.0099 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 0s 291us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0876 - acc: 0.4073 - val_loss: 1.0358 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 7s 637us/step - loss: 1.0669 - acc: 0.4343 - val_loss: 1.0276 - val_acc: 0.4945\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 7s 650us/step - loss: 1.0574 - acc: 0.4502 - val_loss: 1.0220 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 7s 648us/step - loss: 1.0522 - acc: 0.4592 - val_loss: 1.0170 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 7s 652us/step - loss: 1.0456 - acc: 0.4664 - val_loss: 1.0156 - val_acc: 0.5047\n",
      "1265/1265 [==============================] - 0s 317us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0946 - acc: 0.4157 - val_loss: 1.0327 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 7s 690us/step - loss: 1.0692 - acc: 0.4412 - val_loss: 1.0214 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 7s 688us/step - loss: 1.0581 - acc: 0.4561 - val_loss: 1.0161 - val_acc: 0.4984\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 7s 688us/step - loss: 1.0483 - acc: 0.4691 - val_loss: 1.0155 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 7s 684us/step - loss: 1.0439 - acc: 0.4743 - val_loss: 1.0119 - val_acc: 0.5055\n",
      "1265/1265 [==============================] - 0s 349us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0877 - acc: 0.4090 - val_loss: 1.0294 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 8s 735us/step - loss: 1.0642 - acc: 0.4395 - val_loss: 1.0199 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 8s 735us/step - loss: 1.0554 - acc: 0.4612 - val_loss: 1.0157 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 8s 733us/step - loss: 1.0491 - acc: 0.4644 - val_loss: 1.0126 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 7s 732us/step - loss: 1.0456 - acc: 0.4705 - val_loss: 1.0089 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 0s 357us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0840 - acc: 0.4138 - val_loss: 1.0305 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 8s 767us/step - loss: 1.0606 - acc: 0.4489 - val_loss: 1.0205 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 8s 770us/step - loss: 1.0556 - acc: 0.4511 - val_loss: 1.0196 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 8s 767us/step - loss: 1.0470 - acc: 0.4656 - val_loss: 1.0140 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 8s 769us/step - loss: 1.0392 - acc: 0.4738 - val_loss: 1.0066 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 0s 383us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0941 - acc: 0.4107 - val_loss: 1.0314 - val_acc: 0.4836\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 8s 828us/step - loss: 1.0668 - acc: 0.4415 - val_loss: 1.0180 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 8s 822us/step - loss: 1.0565 - acc: 0.4626 - val_loss: 1.0133 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 8s 819us/step - loss: 1.0480 - acc: 0.4704 - val_loss: 1.0114 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 8s 825us/step - loss: 1.0457 - acc: 0.4722 - val_loss: 1.0094 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 0s 391us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0900 - acc: 0.4162 - val_loss: 1.0294 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 9s 850us/step - loss: 1.0693 - acc: 0.4425 - val_loss: 1.0219 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 9s 845us/step - loss: 1.0598 - acc: 0.4505 - val_loss: 1.0187 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 9s 844us/step - loss: 1.0477 - acc: 0.4723 - val_loss: 1.0158 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 9s 840us/step - loss: 1.0461 - acc: 0.4720 - val_loss: 1.0119 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 1s 406us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0896 - acc: 0.4107 - val_loss: 1.0275 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 9s 895us/step - loss: 1.0690 - acc: 0.4442 - val_loss: 1.0171 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 9s 894us/step - loss: 1.0541 - acc: 0.4641 - val_loss: 1.0156 - val_acc: 0.4992\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 9s 892us/step - loss: 1.0502 - acc: 0.4668 - val_loss: 1.0105 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 9s 895us/step - loss: 1.0438 - acc: 0.4718 - val_loss: 1.0103 - val_acc: 0.5008\n",
      "1265/1265 [==============================] - 1s 434us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0864 - acc: 0.4138 - val_loss: 1.0286 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 10s 937us/step - loss: 1.0657 - acc: 0.4451 - val_loss: 1.0191 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 10s 938us/step - loss: 1.0554 - acc: 0.4565 - val_loss: 1.0115 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 10s 936us/step - loss: 1.0470 - acc: 0.4647 - val_loss: 1.0071 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 10s 935us/step - loss: 1.0462 - acc: 0.4678 - val_loss: 1.0058 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 1s 467us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0835 - acc: 0.4085 - val_loss: 1.0320 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 10s 984us/step - loss: 1.0700 - acc: 0.4376 - val_loss: 1.0262 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 10s 986us/step - loss: 1.0586 - acc: 0.4506 - val_loss: 1.0187 - val_acc: 0.5016\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 10s 983us/step - loss: 1.0502 - acc: 0.4652 - val_loss: 1.0146 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 10s 983us/step - loss: 1.0451 - acc: 0.4700 - val_loss: 1.0089 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 1s 488us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0916 - acc: 0.3986 - val_loss: 1.0365 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0686 - acc: 0.4376 - val_loss: 1.0242 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0536 - acc: 0.4590 - val_loss: 1.0173 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0440 - acc: 0.4720 - val_loss: 1.0122 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0412 - acc: 0.4752 - val_loss: 1.0102 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 1s 509us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0859 - acc: 0.4166 - val_loss: 1.0274 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0675 - acc: 0.4423 - val_loss: 1.0221 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0561 - acc: 0.4584 - val_loss: 1.0156 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0523 - acc: 0.4630 - val_loss: 1.0065 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0429 - acc: 0.4754 - val_loss: 1.0050 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 1s 541us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0877 - acc: 0.4094 - val_loss: 1.0422 - val_acc: 0.4743\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0698 - acc: 0.4408 - val_loss: 1.0313 - val_acc: 0.4914\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0567 - acc: 0.4575 - val_loss: 1.0231 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0503 - acc: 0.4629 - val_loss: 1.0153 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0419 - acc: 0.4727 - val_loss: 1.0142 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 1s 558us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0860 - acc: 0.4091 - val_loss: 1.0297 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0667 - acc: 0.4384 - val_loss: 1.0229 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0530 - acc: 0.4527 - val_loss: 1.0169 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0500 - acc: 0.4628 - val_loss: 1.0111 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0434 - acc: 0.4665 - val_loss: 1.0090 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 1s 573us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0831 - acc: 0.4164 - val_loss: 1.0360 - val_acc: 0.4961\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0644 - acc: 0.4425 - val_loss: 1.0284 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0532 - acc: 0.4543 - val_loss: 1.0197 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0481 - acc: 0.4609 - val_loss: 1.0159 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0432 - acc: 0.4725 - val_loss: 1.0103 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 1s 596us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0791 - acc: 0.4267 - val_loss: 1.0308 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0605 - acc: 0.4517 - val_loss: 1.0257 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0537 - acc: 0.4578 - val_loss: 1.0178 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0497 - acc: 0.4665 - val_loss: 1.0110 - val_acc: 0.5257\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0428 - acc: 0.4735 - val_loss: 1.0113 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 1s 614us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0846 - acc: 0.4160 - val_loss: 1.0345 - val_acc: 0.4868\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0644 - acc: 0.4386 - val_loss: 1.0230 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0560 - acc: 0.4534 - val_loss: 1.0189 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0469 - acc: 0.4642 - val_loss: 1.0134 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0408 - acc: 0.4715 - val_loss: 1.0085 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 1s 652us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0762 - acc: 0.4180 - val_loss: 1.0325 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0580 - acc: 0.4432 - val_loss: 1.0229 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0511 - acc: 0.4603 - val_loss: 1.0187 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0444 - acc: 0.4683 - val_loss: 1.0148 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0433 - acc: 0.4722 - val_loss: 1.0123 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 1s 672us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0856 - acc: 0.4113 - val_loss: 1.0368 - val_acc: 0.4751\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0652 - acc: 0.4342 - val_loss: 1.0289 - val_acc: 0.4883\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0550 - acc: 0.4453 - val_loss: 1.0238 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0506 - acc: 0.4598 - val_loss: 1.0185 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0403 - acc: 0.4661 - val_loss: 1.0165 - val_acc: 0.5031\n",
      "1265/1265 [==============================] - 1s 681us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0821 - acc: 0.4076 - val_loss: 1.0369 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0635 - acc: 0.4383 - val_loss: 1.0281 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0579 - acc: 0.4483 - val_loss: 1.0229 - val_acc: 0.5148\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0522 - acc: 0.4574 - val_loss: 1.0185 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0479 - acc: 0.4644 - val_loss: 1.0142 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 1s 708us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0773 - acc: 0.4165 - val_loss: 1.0360 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0589 - acc: 0.4489 - val_loss: 1.0265 - val_acc: 0.4938\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0528 - acc: 0.4490 - val_loss: 1.0209 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0476 - acc: 0.4656 - val_loss: 1.0170 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0407 - acc: 0.4710 - val_loss: 1.0130 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 1s 919us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0800 - acc: 0.4063 - val_loss: 1.0359 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 15s 2ms/step - loss: 1.0679 - acc: 0.4323 - val_loss: 1.0293 - val_acc: 0.4852\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0571 - acc: 0.4442 - val_loss: 1.0285 - val_acc: 0.4992\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0534 - acc: 0.4537 - val_loss: 1.0175 - val_acc: 0.4945\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 2ms/step - loss: 1.0442 - acc: 0.4693 - val_loss: 1.0163 - val_acc: 0.5008\n",
      "1265/1265 [==============================] - 1s 726us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0807 - acc: 0.4138 - val_loss: 1.0343 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0622 - acc: 0.4425 - val_loss: 1.0282 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0531 - acc: 0.4531 - val_loss: 1.0237 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0475 - acc: 0.4691 - val_loss: 1.0216 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0433 - acc: 0.4704 - val_loss: 1.0128 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 1s 770us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0824 - acc: 0.4057 - val_loss: 1.0334 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0645 - acc: 0.4329 - val_loss: 1.0247 - val_acc: 0.4883\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0535 - acc: 0.4533 - val_loss: 1.0211 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0466 - acc: 0.4640 - val_loss: 1.0156 - val_acc: 0.4945\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0419 - acc: 0.4752 - val_loss: 1.0112 - val_acc: 0.5023: 2s - loss: 1.0437 - acc:  - ETA: 2\n",
      "1265/1265 [==============================] - 1s 792us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0771 - acc: 0.4145 - val_loss: 1.0339 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0640 - acc: 0.4353 - val_loss: 1.0286 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0558 - acc: 0.4515 - val_loss: 1.0230 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0502 - acc: 0.4603 - val_loss: 1.0201 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0449 - acc: 0.4673 - val_loss: 1.0135 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 1s 918us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0792 - acc: 0.4104 - val_loss: 1.0373 - val_acc: 0.4930\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0651 - acc: 0.4317 - val_loss: 1.0315 - val_acc: 0.4977\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0578 - acc: 0.4430 - val_loss: 1.0262 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0531 - acc: 0.4528 - val_loss: 1.0223 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0502 - acc: 0.4601 - val_loss: 1.0174 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 1s 861us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0758 - acc: 0.4131 - val_loss: 1.0266 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0554 - acc: 0.4489 - val_loss: 1.0188 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0506 - acc: 0.4613 - val_loss: 1.0145 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0420 - acc: 0.4720 - val_loss: 1.0076 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0386 - acc: 0.4742 - val_loss: 1.0027 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 1s 868us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0752 - acc: 0.4167 - val_loss: 1.0322 - val_acc: 0.4945\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0612 - acc: 0.4369 - val_loss: 1.0247 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0540 - acc: 0.4543 - val_loss: 1.0207 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0504 - acc: 0.4645 - val_loss: 1.0183 - val_acc: 0.5031\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0453 - acc: 0.4663 - val_loss: 1.0126 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 1s 915us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0788 - acc: 0.4147 - val_loss: 1.0304 - val_acc: 0.4868\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0623 - acc: 0.4454 - val_loss: 1.0246 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0532 - acc: 0.4556 - val_loss: 1.0192 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0470 - acc: 0.4625 - val_loss: 1.0180 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0438 - acc: 0.4709 - val_loss: 1.0122 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0758 - acc: 0.4172 - val_loss: 1.0376 - val_acc: 0.4743\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0609 - acc: 0.4446 - val_loss: 1.0311 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0509 - acc: 0.4585 - val_loss: 1.0205 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0469 - acc: 0.4717 - val_loss: 1.0182 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0428 - acc: 0.4710 - val_loss: 1.0190 - val_acc: 0.5031\n",
      "1265/1265 [==============================] - 1s 947us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0810 - acc: 0.4070 - val_loss: 1.0313 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 6s 611us/step - loss: 1.0669 - acc: 0.4326 - val_loss: 1.0239 - val_acc: 0.4945\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 6s 613us/step - loss: 1.0553 - acc: 0.4525 - val_loss: 1.0190 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 6s 602us/step - loss: 1.0504 - acc: 0.4576 - val_loss: 1.0154 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 6s 599us/step - loss: 1.0473 - acc: 0.4636 - val_loss: 1.0127 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 0s 296us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0829 - acc: 0.4065 - val_loss: 1.0336 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 7s 658us/step - loss: 1.0656 - acc: 0.4375 - val_loss: 1.0268 - val_acc: 0.4914\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 7s 664us/step - loss: 1.0590 - acc: 0.4503 - val_loss: 1.0230 - val_acc: 0.4891\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 6s 627us/step - loss: 1.0521 - acc: 0.4636 - val_loss: 1.0162 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 6s 629us/step - loss: 1.0481 - acc: 0.4647 - val_loss: 1.0127 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 0s 323us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0893 - acc: 0.4042 - val_loss: 1.0404 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 7s 648us/step - loss: 1.0676 - acc: 0.4322 - val_loss: 1.0279 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 7s 662us/step - loss: 1.0571 - acc: 0.4571 - val_loss: 1.0228 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 7s 695us/step - loss: 1.0498 - acc: 0.4634 - val_loss: 1.0175 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 7s 707us/step - loss: 1.0473 - acc: 0.4773 - val_loss: 1.0163 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 0s 348us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0891 - acc: 0.4031 - val_loss: 1.0306 - val_acc: 0.4945\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 7s 726us/step - loss: 1.0643 - acc: 0.4453 - val_loss: 1.0176 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 7s 676us/step - loss: 1.0575 - acc: 0.4504 - val_loss: 1.0143 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 7s 697us/step - loss: 1.0528 - acc: 0.4597 - val_loss: 1.0084 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 7s 728us/step - loss: 1.0458 - acc: 0.4678 - val_loss: 1.0077 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 0s 367us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0869 - acc: 0.4132 - val_loss: 1.0304 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 8s 791us/step - loss: 1.0668 - acc: 0.4468 - val_loss: 1.0215 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 8s 788us/step - loss: 1.0554 - acc: 0.4567 - val_loss: 1.0158 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 8s 788us/step - loss: 1.0512 - acc: 0.4658 - val_loss: 1.0140 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 8s 784us/step - loss: 1.0461 - acc: 0.4722 - val_loss: 1.0110 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 0s 392us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0855 - acc: 0.4076 - val_loss: 1.0303 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 8s 824us/step - loss: 1.0636 - acc: 0.4391 - val_loss: 1.0219 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 8s 826us/step - loss: 1.0561 - acc: 0.4528 - val_loss: 1.0152 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 8s 825us/step - loss: 1.0514 - acc: 0.4593 - val_loss: 1.0125 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 8s 825us/step - loss: 1.0471 - acc: 0.4703 - val_loss: 1.0117 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 1s 418us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0860 - acc: 0.4198 - val_loss: 1.0236 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 8s 817us/step - loss: 1.0641 - acc: 0.4491 - val_loss: 1.0167 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 9s 862us/step - loss: 1.0523 - acc: 0.4650 - val_loss: 1.0159 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 9s 870us/step - loss: 1.0450 - acc: 0.4751 - val_loss: 1.0138 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 9s 870us/step - loss: 1.0400 - acc: 0.4787 - val_loss: 1.0071 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 1s 439us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0841 - acc: 0.4140 - val_loss: 1.0318 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 9s 913us/step - loss: 1.0660 - acc: 0.4424 - val_loss: 1.0247 - val_acc: 0.4992\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 9s 912us/step - loss: 1.0533 - acc: 0.4533 - val_loss: 1.0166 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 9s 912us/step - loss: 1.0495 - acc: 0.4661 - val_loss: 1.0127 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 9s 913us/step - loss: 1.0436 - acc: 0.4753 - val_loss: 1.0112 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 1s 462us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0828 - acc: 0.4157 - val_loss: 1.0342 - val_acc: 0.4782\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 9s 928us/step - loss: 1.0661 - acc: 0.4329 - val_loss: 1.0224 - val_acc: 0.4977\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 9s 908us/step - loss: 1.0555 - acc: 0.4516 - val_loss: 1.0190 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 10s 930us/step - loss: 1.0467 - acc: 0.4689 - val_loss: 1.0102 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 10s 938us/step - loss: 1.0426 - acc: 0.4704 - val_loss: 1.0089 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 1s 483us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0898 - acc: 0.4060 - val_loss: 1.0350 - val_acc: 0.4836\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 10s 977us/step - loss: 1.0715 - acc: 0.4371 - val_loss: 1.0227 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 10s 979us/step - loss: 1.0561 - acc: 0.4558 - val_loss: 1.0142 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 10s 978us/step - loss: 1.0522 - acc: 0.4635 - val_loss: 1.0104 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 10s 978us/step - loss: 1.0402 - acc: 0.4733 - val_loss: 1.0070 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 1s 513us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0854 - acc: 0.4115 - val_loss: 1.0278 - val_acc: 0.5047\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0677 - acc: 0.4385 - val_loss: 1.0190 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 10s 1ms/step - loss: 1.0584 - acc: 0.4524 - val_loss: 1.0123 - val_acc: 0.5280\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0469 - acc: 0.4668 - val_loss: 1.0093 - val_acc: 0.5257\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0451 - acc: 0.4716 - val_loss: 1.0043 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 1s 539us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0939 - acc: 0.3998 - val_loss: 1.0315 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0625 - acc: 0.4452 - val_loss: 1.0232 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0603 - acc: 0.4498 - val_loss: 1.0150 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0504 - acc: 0.4647 - val_loss: 1.0107 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0458 - acc: 0.4739 - val_loss: 1.0036 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 1s 551us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0892 - acc: 0.4144 - val_loss: 1.0375 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0663 - acc: 0.4426 - val_loss: 1.0276 - val_acc: 0.4945\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0551 - acc: 0.4512 - val_loss: 1.0194 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0477 - acc: 0.4628 - val_loss: 1.0141 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0445 - acc: 0.4729 - val_loss: 1.0122 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 1s 568us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0864 - acc: 0.4157 - val_loss: 1.0344 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0673 - acc: 0.4401 - val_loss: 1.0258 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0568 - acc: 0.4518 - val_loss: 1.0204 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0469 - acc: 0.4636 - val_loss: 1.0133 - val_acc: 0.5195s: 1.\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0447 - acc: 0.4721 - val_loss: 1.0095 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 1s 599us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0847 - acc: 0.4088 - val_loss: 1.0340 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0652 - acc: 0.4399 - val_loss: 1.0229 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0546 - acc: 0.4571 - val_loss: 1.0190 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0486 - acc: 0.4605 - val_loss: 1.0143 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0447 - acc: 0.4715 - val_loss: 1.0094 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 1s 609us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0857 - acc: 0.4017 - val_loss: 1.0362 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0648 - acc: 0.4386 - val_loss: 1.0266 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0588 - acc: 0.4505 - val_loss: 1.0195 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0522 - acc: 0.4579 - val_loss: 1.0159 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0443 - acc: 0.4679 - val_loss: 1.0097 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 1s 643us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0842 - acc: 0.4125 - val_loss: 1.0331 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0627 - acc: 0.4400 - val_loss: 1.0216 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0600 - acc: 0.4481 - val_loss: 1.0168 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0478 - acc: 0.4614 - val_loss: 1.0122 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0444 - acc: 0.4679 - val_loss: 1.0062 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 1s 683us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0796 - acc: 0.4158 - val_loss: 1.0311 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0603 - acc: 0.4468 - val_loss: 1.0236 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0551 - acc: 0.4535 - val_loss: 1.0186 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0462 - acc: 0.4701 - val_loss: 1.0147 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0398 - acc: 0.4743 - val_loss: 1.0083 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 1s 684us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0748 - acc: 0.4236 - val_loss: 1.0347 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0580 - acc: 0.4474 - val_loss: 1.0255 - val_acc: 0.4914\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0526 - acc: 0.4559 - val_loss: 1.0217 - val_acc: 0.4969\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0434 - acc: 0.4673 - val_loss: 1.0136 - val_acc: 0.5008\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0366 - acc: 0.4795 - val_loss: 1.0121 - val_acc: 0.5023\n",
      "1265/1265 [==============================] - 1s 664us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0806 - acc: 0.4113 - val_loss: 1.0342 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0655 - acc: 0.4402 - val_loss: 1.0268 - val_acc: 0.4977\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0556 - acc: 0.4456 - val_loss: 1.0221 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0465 - acc: 0.4601 - val_loss: 1.0173 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0488 - acc: 0.4655 - val_loss: 1.0156 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 1s 710us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0800 - acc: 0.4166 - val_loss: 1.0383 - val_acc: 0.4727\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 15s 2ms/step - loss: 1.0657 - acc: 0.4293 - val_loss: 1.0273 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 15s 2ms/step - loss: 1.0550 - acc: 0.4527 - val_loss: 1.0247 - val_acc: 0.4969\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 15s 2ms/step - loss: 1.0496 - acc: 0.4575 - val_loss: 1.0157 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 2ms/step - loss: 1.0434 - acc: 0.4693 - val_loss: 1.0198 - val_acc: 0.4922\n",
      "1265/1265 [==============================] - 1s 752us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0854 - acc: 0.4140 - val_loss: 1.0350 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 15s 2ms/step - loss: 1.0664 - acc: 0.4354 - val_loss: 1.0237 - val_acc: 0.4860\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 15s 2ms/step - loss: 1.0554 - acc: 0.4492 - val_loss: 1.0175 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 15s 2ms/step - loss: 1.0483 - acc: 0.4642 - val_loss: 1.0137 - val_acc: 0.5008\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0410 - acc: 0.4620 - val_loss: 1.0119 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 1s 765us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 36s 4ms/step - loss: 1.0823 - acc: 0.4066 - val_loss: 1.0295 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0622 - acc: 0.4433 - val_loss: 1.0210 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0523 - acc: 0.4585 - val_loss: 1.0157 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0429 - acc: 0.4714 - val_loss: 1.0100 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0396 - acc: 0.4765 - val_loss: 1.0083 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 1s 775us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0804 - acc: 0.4126 - val_loss: 1.0335 - val_acc: 0.4836\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0618 - acc: 0.4415 - val_loss: 1.0271 - val_acc: 0.4829 ETA: 2s - loss:  - ETA: 0s - loss: 1.0618 - a\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0540 - acc: 0.4578 - val_loss: 1.0223 - val_acc: 0.4891\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0486 - acc: 0.4592 - val_loss: 1.0169 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0430 - acc: 0.4707 - val_loss: 1.0159 - val_acc: 0.5055\n",
      "1265/1265 [==============================] - 1s 869us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0746 - acc: 0.4190 - val_loss: 1.0272 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0559 - acc: 0.4501 - val_loss: 1.0241 - val_acc: 0.4992\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0512 - acc: 0.4561 - val_loss: 1.0145 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0462 - acc: 0.4689 - val_loss: 1.0151 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0410 - acc: 0.4761 - val_loss: 1.0075 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 1s 810us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0781 - acc: 0.4125 - val_loss: 1.0358 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0587 - acc: 0.4430 - val_loss: 1.0252 - val_acc: 0.4992\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0525 - acc: 0.4551 - val_loss: 1.0221 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0459 - acc: 0.4649 - val_loss: 1.0178 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0420 - acc: 0.4680 - val_loss: 1.0141 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 1s 889us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0708 - acc: 0.4284 - val_loss: 1.0359 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0562 - acc: 0.4497 - val_loss: 1.0253 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0472 - acc: 0.4610 - val_loss: 1.0147 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0417 - acc: 0.4748 - val_loss: 1.0106 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0405 - acc: 0.4728 - val_loss: 1.0071 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 1s 958us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0796 - acc: 0.4112 - val_loss: 1.0352 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0633 - acc: 0.4360 - val_loss: 1.0287 - val_acc: 0.4899\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0538 - acc: 0.4552 - val_loss: 1.0213 - val_acc: 0.4977\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0498 - acc: 0.4608 - val_loss: 1.0185 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0483 - acc: 0.4609 - val_loss: 1.0132 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0742 - acc: 0.4207 - val_loss: 1.0390 - val_acc: 0.4673\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0598 - acc: 0.4389 - val_loss: 1.0346 - val_acc: 0.4938\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0555 - acc: 0.4489 - val_loss: 1.0211 - val_acc: 0.4984\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0488 - acc: 0.4515 - val_loss: 1.0194 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0454 - acc: 0.4616 - val_loss: 1.0146 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0763 - acc: 0.4126 - val_loss: 1.0356 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0599 - acc: 0.4426 - val_loss: 1.0262 - val_acc: 0.4992\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0492 - acc: 0.4557 - val_loss: 1.0184 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0461 - acc: 0.4685 - val_loss: 1.0169 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0410 - acc: 0.4772 - val_loss: 1.0100 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 1s 999us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0773 - acc: 0.4170 - val_loss: 1.0312 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0585 - acc: 0.4484 - val_loss: 1.0262 - val_acc: 0.4992\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0515 - acc: 0.4577 - val_loss: 1.0174 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0446 - acc: 0.4725 - val_loss: 1.0133 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0396 - acc: 0.4788 - val_loss: 1.0102 - val_acc: 0.5117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0831 - acc: 0.4091 - val_loss: 1.0285 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 6s 634us/step - loss: 1.0634 - acc: 0.4335 - val_loss: 1.0223 - val_acc: 0.4907\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 6s 629us/step - loss: 1.0562 - acc: 0.4504 - val_loss: 1.0180 - val_acc: 0.4977\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 7s 654us/step - loss: 1.0491 - acc: 0.4616 - val_loss: 1.0148 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 7s 654us/step - loss: 1.0450 - acc: 0.4654 - val_loss: 1.0150 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 0s 324us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0916 - acc: 0.4017 - val_loss: 1.0304 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 7s 702us/step - loss: 1.0671 - acc: 0.4391 - val_loss: 1.0175 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 7s 694us/step - loss: 1.0548 - acc: 0.4552 - val_loss: 1.0127 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 7s 697us/step - loss: 1.0531 - acc: 0.4572 - val_loss: 1.0063 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 7s 694us/step - loss: 1.0445 - acc: 0.4664 - val_loss: 1.0056 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 0s 348us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0856 - acc: 0.4077 - val_loss: 1.0290 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 8s 738us/step - loss: 1.0602 - acc: 0.4460 - val_loss: 1.0205 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 8s 741us/step - loss: 1.0545 - acc: 0.4567 - val_loss: 1.0141 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 8s 739us/step - loss: 1.0470 - acc: 0.4649 - val_loss: 1.0116 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 8s 741us/step - loss: 1.0461 - acc: 0.4685 - val_loss: 1.0099 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 0s 373us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0963 - acc: 0.4048 - val_loss: 1.0264 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 8s 781us/step - loss: 1.0671 - acc: 0.4436 - val_loss: 1.0178 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 8s 780us/step - loss: 1.0536 - acc: 0.4560 - val_loss: 1.0130 - val_acc: 0.5226\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 8s 782us/step - loss: 1.0499 - acc: 0.4591 - val_loss: 1.0073 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 8s 782us/step - loss: 1.0436 - acc: 0.4702 - val_loss: 1.0054 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 1s 399us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0919 - acc: 0.4112 - val_loss: 1.0360 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 9s 837us/step - loss: 1.0672 - acc: 0.4373 - val_loss: 1.0247 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 9s 838us/step - loss: 1.0539 - acc: 0.4551 - val_loss: 1.0224 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 9s 836us/step - loss: 1.0515 - acc: 0.4631 - val_loss: 1.0170 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 9s 838us/step - loss: 1.0455 - acc: 0.4696 - val_loss: 1.0137 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 1s 420us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0884 - acc: 0.4093 - val_loss: 1.0381 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 9s 878us/step - loss: 1.0684 - acc: 0.4341 - val_loss: 1.0232 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 9s 877us/step - loss: 1.0556 - acc: 0.4604 - val_loss: 1.0183 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 9s 875us/step - loss: 1.0497 - acc: 0.4633 - val_loss: 1.0144 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 9s 876us/step - loss: 1.0460 - acc: 0.4686 - val_loss: 1.0138 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 1s 444us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0823 - acc: 0.4197 - val_loss: 1.0280 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 9s 914us/step - loss: 1.0621 - acc: 0.4444 - val_loss: 1.0229 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 9s 913us/step - loss: 1.0574 - acc: 0.4537 - val_loss: 1.0149 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 9s 915us/step - loss: 1.0466 - acc: 0.4663 - val_loss: 1.0117 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 9s 912us/step - loss: 1.0436 - acc: 0.4741 - val_loss: 1.0095 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 1s 465us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0875 - acc: 0.4094 - val_loss: 1.0290 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 10s 952us/step - loss: 1.0640 - acc: 0.4455 - val_loss: 1.0192 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 10s 953us/step - loss: 1.0548 - acc: 0.4548 - val_loss: 1.0138 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 10s 957us/step - loss: 1.0487 - acc: 0.4689 - val_loss: 1.0130 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 10s 953us/step - loss: 1.0464 - acc: 0.4645 - val_loss: 1.0105 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 1s 493us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0857 - acc: 0.4060 - val_loss: 1.0270 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 10s 964us/step - loss: 1.0607 - acc: 0.4487 - val_loss: 1.0185 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 10s 970us/step - loss: 1.0536 - acc: 0.4611 - val_loss: 1.0110 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 10s 990us/step - loss: 1.0453 - acc: 0.4729 - val_loss: 1.0059 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 10s 991us/step - loss: 1.0409 - acc: 0.4797 - val_loss: 1.0021 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 1s 512us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0949 - acc: 0.4027 - val_loss: 1.0332 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 10s 1ms/step - loss: 1.0657 - acc: 0.4388 - val_loss: 1.0211 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0555 - acc: 0.4533 - val_loss: 1.0169 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0502 - acc: 0.4612 - val_loss: 1.0119 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0436 - acc: 0.4695 - val_loss: 1.0077 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 1s 544us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0929 - acc: 0.4019 - val_loss: 1.0350 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0734 - acc: 0.4251 - val_loss: 1.0244 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0592 - acc: 0.4495 - val_loss: 1.0160 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0504 - acc: 0.4611 - val_loss: 1.0143 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0453 - acc: 0.4739 - val_loss: 1.0085 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 1s 568us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 36s 3ms/step - loss: 1.0884 - acc: 0.4093 - val_loss: 1.0320 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0701 - acc: 0.4325 - val_loss: 1.0183 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0560 - acc: 0.4574 - val_loss: 1.0112 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0467 - acc: 0.4639 - val_loss: 1.0074 - val_acc: 0.5257\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0466 - acc: 0.4674 - val_loss: 1.0034 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 1s 600us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 36s 4ms/step - loss: 1.0897 - acc: 0.4085 - val_loss: 1.0294 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0652 - acc: 0.4424 - val_loss: 1.0202 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0527 - acc: 0.4541 - val_loss: 1.0158 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0471 - acc: 0.4634 - val_loss: 1.0054 - val_acc: 0.5265\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0455 - acc: 0.4730 - val_loss: 1.0039 - val_acc: 0.5265\n",
      "1265/1265 [==============================] - 1s 547us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0849 - acc: 0.4211 - val_loss: 1.0371 - val_acc: 0.4836\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0660 - acc: 0.4410 - val_loss: 1.0238 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0569 - acc: 0.4473 - val_loss: 1.0164 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0484 - acc: 0.4618 - val_loss: 1.0144 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0430 - acc: 0.4711 - val_loss: 1.0134 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 1s 640us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0777 - acc: 0.4207 - val_loss: 1.0310 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0629 - acc: 0.4466 - val_loss: 1.0257 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0538 - acc: 0.4595 - val_loss: 1.0198 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0471 - acc: 0.4660 - val_loss: 1.0134 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0402 - acc: 0.4771 - val_loss: 1.0086 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 1s 677us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0909 - acc: 0.4127 - val_loss: 1.0393 - val_acc: 0.4844\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0660 - acc: 0.4408 - val_loss: 1.0275 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0588 - acc: 0.4482 - val_loss: 1.0209 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0483 - acc: 0.4678 - val_loss: 1.0162 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0444 - acc: 0.4709 - val_loss: 1.0098 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 1s 680us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0793 - acc: 0.4260 - val_loss: 1.0329 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0618 - acc: 0.4499 - val_loss: 1.0211 - val_acc: 0.4977\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0528 - acc: 0.4574 - val_loss: 1.0149 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0469 - acc: 0.4667 - val_loss: 1.0119 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0389 - acc: 0.4738 - val_loss: 1.0107 - val_acc: 0.5070\n",
      "1265/1265 [==============================] - 1s 696us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0786 - acc: 0.4118 - val_loss: 1.0285 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0613 - acc: 0.4410 - val_loss: 1.0207 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0555 - acc: 0.4522 - val_loss: 1.0147 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0515 - acc: 0.4594 - val_loss: 1.0091 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0448 - acc: 0.4681 - val_loss: 1.0063 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 1s 801us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0821 - acc: 0.4170 - val_loss: 1.0333 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0703 - acc: 0.4308 - val_loss: 1.0299 - val_acc: 0.4938\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0528 - acc: 0.4553 - val_loss: 1.0200 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0489 - acc: 0.4620 - val_loss: 1.0159 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0483 - acc: 0.4633 - val_loss: 1.0105 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 1s 706us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0820 - acc: 0.4222 - val_loss: 1.0347 - val_acc: 0.4930\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0636 - acc: 0.4395 - val_loss: 1.0244 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0489 - acc: 0.4604 - val_loss: 1.0193 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0462 - acc: 0.4662 - val_loss: 1.0133 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0458 - acc: 0.4699 - val_loss: 1.0130 - val_acc: 0.5039\n",
      "1265/1265 [==============================] - 1s 796us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0829 - acc: 0.4114 - val_loss: 1.0353 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0607 - acc: 0.4365 - val_loss: 1.0243 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0547 - acc: 0.4484 - val_loss: 1.0173 - val_acc: 0.5101\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0457 - acc: 0.4611 - val_loss: 1.0121 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0416 - acc: 0.4742 - val_loss: 1.0103 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 1s 999us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0827 - acc: 0.4063 - val_loss: 1.0299 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0660 - acc: 0.4348 - val_loss: 1.0217 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0561 - acc: 0.4497 - val_loss: 1.0161 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0468 - acc: 0.4668 - val_loss: 1.0114 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0446 - acc: 0.4638 - val_loss: 1.0061 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 1s 845us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0843 - acc: 0.4113 - val_loss: 1.0395 - val_acc: 0.4774\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0642 - acc: 0.4386 - val_loss: 1.0302 - val_acc: 0.4922\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0545 - acc: 0.4487 - val_loss: 1.0243 - val_acc: 0.4938\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0467 - acc: 0.4554 - val_loss: 1.0205 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0448 - acc: 0.4664 - val_loss: 1.0189 - val_acc: 0.5000\n",
      "1265/1265 [==============================] - 1s 831us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0864 - acc: 0.4051 - val_loss: 1.0373 - val_acc: 0.48910s - loss: 1.0931 - - E - ETA: 10s - loss: 1.0841 - a - ETA: 9s  - \n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0640 - acc: 0.4401 - val_loss: 1.0280 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0560 - acc: 0.4580 - val_loss: 1.0203 - val_acc: 0.5156s - loss: 1.0549 - a - ETA: 2s - loss: 1.0549 - acc: 0.4 - ETA: 2s - loss: 1.0544 - ac - ETA: 2s - loss: 1.0551 - acc: 0. - ETA: 1s - loss: 1.0548 - acc: 0.459 - ETA: 1s - loss: - ETA: 0s - loss: 1.0560 - \n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0492 - acc: 0.4623 - val_loss: 1.0159 - val_acc: 0.5179: 5s - loss: 1.0516 - acc: 0. - ETA: 5s - loss: - ETA: 4s - loss: 1.0500 - acc:  - ETA: 3s - l - ETA: 0s - loss: 1.0489 - acc:\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0397 - acc: 0.4769 - val_loss: 1.0100 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 1s 900us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0809 - acc: 0.4129 - val_loss: 1.0353 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0600 - acc: 0.4407 - val_loss: 1.0274 - val_acc: 0.4961\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0556 - acc: 0.4476 - val_loss: 1.0237 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0490 - acc: 0.4622 - val_loss: 1.0197 - val_acc: 0.4984\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0451 - acc: 0.4672 - val_loss: 1.0156 - val_acc: 0.5031\n",
      "1265/1265 [==============================] - 1s 867us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0797 - acc: 0.4167 - val_loss: 1.0311 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0661 - acc: 0.4370 - val_loss: 1.0243 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0527 - acc: 0.4574 - val_loss: 1.0170 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0430 - acc: 0.4669 - val_loss: 1.0173 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0406 - acc: 0.4773 - val_loss: 1.0099 - val_acc: 0.5039\n",
      "1265/1265 [==============================] - 1s 995us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0734 - acc: 0.4146 - val_loss: 1.0363 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0630 - acc: 0.4399 - val_loss: 1.0317 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0564 - acc: 0.4436 - val_loss: 1.0254 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0496 - acc: 0.4578 - val_loss: 1.0204 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0463 - acc: 0.4655 - val_loss: 1.0162 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 1s 920us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0793 - acc: 0.4119 - val_loss: 1.0337 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0615 - acc: 0.4346 - val_loss: 1.0272 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0511 - acc: 0.4610 - val_loss: 1.0231 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0473 - acc: 0.4715 - val_loss: 1.0169 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0458 - acc: 0.4680 - val_loss: 1.0133 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0752 - acc: 0.4129 - val_loss: 1.0369 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0626 - acc: 0.4292 - val_loss: 1.0269 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0548 - acc: 0.4513 - val_loss: 1.0233 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0474 - acc: 0.4657 - val_loss: 1.0171 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0430 - acc: 0.4721 - val_loss: 1.0108 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0766 - acc: 0.4096 - val_loss: 1.0373 - val_acc: 0.4751\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0620 - acc: 0.4347 - val_loss: 1.0318 - val_acc: 0.4938\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0564 - acc: 0.4510 - val_loss: 1.0240 - val_acc: 0.4938\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0488 - acc: 0.4591 - val_loss: 1.0217 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0462 - acc: 0.4656 - val_loss: 1.0155 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0751 - acc: 0.4156 - val_loss: 1.0308 - val_acc: 0.4930\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0558 - acc: 0.4487 - val_loss: 1.0228 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0485 - acc: 0.4630 - val_loss: 1.0168 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0463 - acc: 0.4712 - val_loss: 1.0117 - val_acc: 0.5031\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0417 - acc: 0.4738 - val_loss: 1.0071 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0832 - acc: 0.4105 - val_loss: 1.0304 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 7s 686us/step - loss: 1.0659 - acc: 0.4289 - val_loss: 1.0241 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 7s 678us/step - loss: 1.0564 - acc: 0.4539 - val_loss: 1.0231 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 7s 702us/step - loss: 1.0496 - acc: 0.4639 - val_loss: 1.0157 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 7s 697us/step - loss: 1.0448 - acc: 0.4675 - val_loss: 1.0128 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 0s 288us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 36s 4ms/step - loss: 1.0833 - acc: 0.4091 - val_loss: 1.0325 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 8s 736us/step - loss: 1.0690 - acc: 0.4334 - val_loss: 1.0249 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 7s 727us/step - loss: 1.0539 - acc: 0.4523 - val_loss: 1.0166 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 7s 721us/step - loss: 1.0492 - acc: 0.4597 - val_loss: 1.0159 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 7s 700us/step - loss: 1.0427 - acc: 0.4691 - val_loss: 1.0111 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 0s 355us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0888 - acc: 0.4056 - val_loss: 1.0294 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 8s 777us/step - loss: 1.0695 - acc: 0.4373 - val_loss: 1.0228 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 8s 779us/step - loss: 1.0592 - acc: 0.4542 - val_loss: 1.0171 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 8s 779us/step - loss: 1.0482 - acc: 0.4622 - val_loss: 1.0104 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 8s 790us/step - loss: 1.0479 - acc: 0.4675 - val_loss: 1.0119 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 1s 404us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0832 - acc: 0.4121 - val_loss: 1.0266 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 8s 821us/step - loss: 1.0629 - acc: 0.4501 - val_loss: 1.0189 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 8s 822us/step - loss: 1.0510 - acc: 0.4632 - val_loss: 1.0115 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 8s 824us/step - loss: 1.0474 - acc: 0.4657 - val_loss: 1.0098 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 8s 825us/step - loss: 1.0436 - acc: 0.4760 - val_loss: 1.0063 - val_acc: 0.5070\n",
      "1265/1265 [==============================] - 1s 429us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0872 - acc: 0.4129 - val_loss: 1.0247 - val_acc: 0.5164\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 9s 887us/step - loss: 1.0651 - acc: 0.4415 - val_loss: 1.0186 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 9s 889us/step - loss: 1.0544 - acc: 0.4577 - val_loss: 1.0133 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 9s 892us/step - loss: 1.0494 - acc: 0.4654 - val_loss: 1.0108 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 9s 889us/step - loss: 1.0420 - acc: 0.4723 - val_loss: 1.0063 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 1s 451us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0900 - acc: 0.4133 - val_loss: 1.0340 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 9s 925us/step - loss: 1.0698 - acc: 0.4372 - val_loss: 1.0202 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 9s 889us/step - loss: 1.0560 - acc: 0.4537 - val_loss: 1.0123 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 9s 862us/step - loss: 1.0503 - acc: 0.4645 - val_loss: 1.0110 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 9s 862us/step - loss: 1.0462 - acc: 0.4669 - val_loss: 1.0082 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 1s 472us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0866 - acc: 0.4124 - val_loss: 1.0374 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 10s 954us/step - loss: 1.0662 - acc: 0.4347 - val_loss: 1.0253 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 10s 952us/step - loss: 1.0500 - acc: 0.4637 - val_loss: 1.0199 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 10s 960us/step - loss: 1.0467 - acc: 0.4665 - val_loss: 1.0131 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 10s 965us/step - loss: 1.0452 - acc: 0.4716 - val_loss: 1.0101 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 1s 497us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0808 - acc: 0.4175 - val_loss: 1.0272 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 10s 999us/step - loss: 1.0641 - acc: 0.4447 - val_loss: 1.0217 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 10s 1ms/step - loss: 1.0545 - acc: 0.4574 - val_loss: 1.0156 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 10s 1ms/step - loss: 1.0531 - acc: 0.4598 - val_loss: 1.0104 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 10s 982us/step - loss: 1.0427 - acc: 0.4725 - val_loss: 1.0106 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 1s 534us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0914 - acc: 0.4063 - val_loss: 1.0327 - val_acc: 0.4836\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0697 - acc: 0.4375 - val_loss: 1.0226 - val_acc: 0.4875\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0558 - acc: 0.4653 - val_loss: 1.0167 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0498 - acc: 0.4649 - val_loss: 1.0125 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0442 - acc: 0.4753 - val_loss: 1.0074 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 1s 555us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0949 - acc: 0.4014 - val_loss: 1.0324 - val_acc: 0.4844\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0692 - acc: 0.4358 - val_loss: 1.0224 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0556 - acc: 0.4511 - val_loss: 1.0188 - val_acc: 0.5132\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0538 - acc: 0.4595 - val_loss: 1.0136 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0414 - acc: 0.4698 - val_loss: 1.0080 - val_acc: 0.5164A: 8s - loss: 1.\n",
      "1265/1265 [==============================] - 1s 584us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0803 - acc: 0.4150 - val_loss: 1.0278 - val_acc: 0.5031TA: 1:59 - loss: 1.0948 -  - ETA: 1:22\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0605 - acc: 0.4460 - val_loss: 1.0207 - val_acc: 0.5249: 1 - ETA: 2s - loss: 1.0 - ETA: 1s \n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0543 - acc: 0.4579 - val_loss: 1.0174 - val_acc: 0.5109 8s - loss: 1.0526 - - ETA: 7s - los - ETA: 6s - loss: 1. - ETA: 4s - loss: 1.0569 - a  - ETA: 1s - loss: 1.0533 - acc: 0 - ETA: 1s - loss: 1\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0470 - acc: 0.4653 - val_loss: 1.0090 - val_acc: 0.5187.0518 - acc: 0. - ETA: 7s - loss: 1.0507 - ETA: 6s - loss: 1.0489 - acc: - ETA: 5s - loss: 1.0521 - acc: 0. - ETA: 5s - loss: 1.0507 - acc:  - ETA: 4s - loss - ETA: 3s - loss: 1. - ETA: 2s - loss: 1.0 - ETA: 1s - loss: 1.04\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0374 - acc: 0.4746 - val_loss: 1.0084 - val_acc: 0.51401.0 - ETA: 0s - loss: 1.0392\n",
      "1265/1265 [==============================] - 1s 603us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0884 - acc: 0.4070 - val_loss: 1.0340 - val_acc: 0.4774\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0658 - acc: 0.4410 - val_loss: 1.0233 - val_acc: 0.4907 ETA: \n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0564 - acc: 0.4541 - val_loss: 1.0178 - val_acc: 0.4961\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0478 - acc: 0.4620 - val_loss: 1.0152 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0440 - acc: 0.4699 - val_loss: 1.0087 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 1s 615us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0808 - acc: 0.4113 - val_loss: 1.0290 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0641 - acc: 0.4426 - val_loss: 1.0209 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0566 - acc: 0.4572 - val_loss: 1.0151 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0512 - acc: 0.4635 - val_loss: 1.0094 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0467 - acc: 0.4663 - val_loss: 1.0060 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 1s 639us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0804 - acc: 0.4205 - val_loss: 1.0351 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0654 - acc: 0.4397 - val_loss: 1.0238 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0535 - acc: 0.4503 - val_loss: 1.0187 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0495 - acc: 0.4606 - val_loss: 1.0151 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0448 - acc: 0.4685 - val_loss: 1.0106 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 1s 670us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0901 - acc: 0.4061 - val_loss: 1.0367 - val_acc: 0.4891\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0659 - acc: 0.4454 - val_loss: 1.0265 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0598 - acc: 0.4505 - val_loss: 1.0180 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0520 - acc: 0.4574 - val_loss: 1.0149 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0443 - acc: 0.4692 - val_loss: 1.0102 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 1s 786us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0843 - acc: 0.4111 - val_loss: 1.0342 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0740 - acc: 0.4277 - val_loss: 1.0266 - val_acc: 0.4930\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0563 - acc: 0.4553 - val_loss: 1.0207 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0520 - acc: 0.4629 - val_loss: 1.0156 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0499 - acc: 0.4624 - val_loss: 1.0137 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 1s 727us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0820 - acc: 0.4149 - val_loss: 1.0365 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0627 - acc: 0.4440 - val_loss: 1.0275 - val_acc: 0.4953\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0536 - acc: 0.4586 - val_loss: 1.0236 - val_acc: 0.4961\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0462 - acc: 0.4677 - val_loss: 1.0168 - val_acc: 0.5031\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0440 - acc: 0.4696 - val_loss: 1.0131 - val_acc: 0.5016\n",
      "1265/1265 [==============================] - 1s 812us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0831 - acc: 0.4110 - val_loss: 1.0342 - val_acc: 0.4782\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0652 - acc: 0.4397 - val_loss: 1.0269 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0554 - acc: 0.4517 - val_loss: 1.0162 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0462 - acc: 0.4657 - val_loss: 1.0130 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0408 - acc: 0.4682 - val_loss: 1.0103 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 1s 791us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0806 - acc: 0.4150 - val_loss: 1.0364 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0656 - acc: 0.4370 - val_loss: 1.0278 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0526 - acc: 0.4537 - val_loss: 1.0215 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0463 - acc: 0.4628 - val_loss: 1.0152 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0399 - acc: 0.4715 - val_loss: 1.0132 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 1s 959us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0864 - acc: 0.4086 - val_loss: 1.0380 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0626 - acc: 0.4386 - val_loss: 1.0274 - val_acc: 0.4969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0549 - acc: 0.4559 - val_loss: 1.0240 - val_acc: 0.4805\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0476 - acc: 0.4659 - val_loss: 1.0200 - val_acc: 0.4852\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0417 - acc: 0.4728 - val_loss: 1.0147 - val_acc: 0.4930\n",
      "1265/1265 [==============================] - 1s 832us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0820 - acc: 0.4090 - val_loss: 1.0389 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0637 - acc: 0.4387 - val_loss: 1.0330 - val_acc: 0.4961\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0553 - acc: 0.4542 - val_loss: 1.0264 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0511 - acc: 0.4607 - val_loss: 1.0211 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0437 - acc: 0.4623 - val_loss: 1.0216 - val_acc: 0.4992\n",
      "1265/1265 [==============================] - 1s 860us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0802 - acc: 0.4168 - val_loss: 1.0307 - val_acc: 0.4891: \n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0589 - acc: 0.4505 - val_loss: 1.0225 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0542 - acc: 0.4596 - val_loss: 1.0163 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0454 - acc: 0.4675 - val_loss: 1.0131 - val_acc: 0.5179 - loss:  - ETA: 9s - loss: 1. - ETA: 5s - loss: 1.0506 - acc: 0 - ETA: 5s - loss: 1.0 - ETA: 1s - loss: 1.0473 - acc: 0.465 - ETA: 1s\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0365 - acc: 0.4752 - val_loss: 1.0101 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 1s 892us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0843 - acc: 0.4050 - val_loss: 1.0364 - val_acc: 0.4891\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0649 - acc: 0.4351 - val_loss: 1.0309 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0553 - acc: 0.4468 - val_loss: 1.0240 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0475 - acc: 0.4618 - val_loss: 1.0199 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0442 - acc: 0.4649 - val_loss: 1.0153 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0792 - acc: 0.4121 - val_loss: 1.0331 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0629 - acc: 0.4394 - val_loss: 1.0237 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0558 - acc: 0.4536 - val_loss: 1.0201 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0473 - acc: 0.4676 - val_loss: 1.0162 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0444 - acc: 0.4682 - val_loss: 1.0135 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 1s 912us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0759 - acc: 0.4252 - val_loss: 1.0316 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0588 - acc: 0.4466 - val_loss: 1.0263 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0504 - acc: 0.4574 - val_loss: 1.0179 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0430 - acc: 0.4718 - val_loss: 1.0120 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0384 - acc: 0.4773 - val_loss: 1.0114 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 1s 957us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0780 - acc: 0.4102 - val_loss: 1.0378 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0622 - acc: 0.4348 - val_loss: 1.0296 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0522 - acc: 0.4502 - val_loss: 1.0243 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0476 - acc: 0.4572 - val_loss: 1.0193 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0440 - acc: 0.4671 - val_loss: 1.0174 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0721 - acc: 0.4231 - val_loss: 1.0334 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0599 - acc: 0.4463 - val_loss: 1.0237 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0494 - acc: 0.4605 - val_loss: 1.0166 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0439 - acc: 0.4760 - val_loss: 1.0164 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0389 - acc: 0.4823 - val_loss: 1.0160 - val_acc: 0.5047\n",
      "1265/1265 [==============================] - 1s 981us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0776 - acc: 0.4182 - val_loss: 1.0312 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0602 - acc: 0.4449 - val_loss: 1.0241 - val_acc: 0.4914\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0525 - acc: 0.4584 - val_loss: 1.0227 - val_acc: 0.4953\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0477 - acc: 0.4609 - val_loss: 1.0159 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0408 - acc: 0.4691 - val_loss: 1.0106 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 1s 968us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0792 - acc: 0.4093 - val_loss: 1.0362 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0641 - acc: 0.4382 - val_loss: 1.0286 - val_acc: 0.4977\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0556 - acc: 0.4492 - val_loss: 1.0226 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0464 - acc: 0.4612 - val_loss: 1.0189 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0414 - acc: 0.4710 - val_loss: 1.0165 - val_acc: 0.5031\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0779 - acc: 0.4146 - val_loss: 1.0365 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0596 - acc: 0.4455 - val_loss: 1.0275 - val_acc: 0.4953\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0511 - acc: 0.4622 - val_loss: 1.0220 - val_acc: 0.4984\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0464 - acc: 0.4600 - val_loss: 1.0178 - val_acc: 0.4953\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0415 - acc: 0.4723 - val_loss: 1.0135 - val_acc: 0.5023\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0739 - acc: 0.4214 - val_loss: 1.0336 - val_acc: 0.4984: 1.0739 - \n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0578 - acc: 0.4465 - val_loss: 1.0255 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0524 - acc: 0.4562 - val_loss: 1.0188 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0441 - acc: 0.4715 - val_loss: 1.0157 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0430 - acc: 0.4728 - val_loss: 1.0115 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0781 - acc: 0.4138 - val_loss: 1.0321 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 7s 727us/step - loss: 1.0621 - acc: 0.4370 - val_loss: 1.0250 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 7s 726us/step - loss: 1.0548 - acc: 0.4520 - val_loss: 1.0231 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 7s 727us/step - loss: 1.0459 - acc: 0.4646 - val_loss: 1.0202 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 7s 654us/step - loss: 1.0445 - acc: 0.4644 - val_loss: 1.0173 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 0s 307us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0822 - acc: 0.4059 - val_loss: 1.0374 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 8s 771us/step - loss: 1.0633 - acc: 0.4429 - val_loss: 1.0285 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 8s 775us/step - loss: 1.0562 - acc: 0.4544 - val_loss: 1.0234 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 8s 754us/step - loss: 1.0519 - acc: 0.4595 - val_loss: 1.0200 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 8s 770us/step - loss: 1.0472 - acc: 0.4678 - val_loss: 1.0145 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 0s 329us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0911 - acc: 0.4049 - val_loss: 1.0345 - val_acc: 0.4735\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 8s 818us/step - loss: 1.0733 - acc: 0.4333 - val_loss: 1.0309 - val_acc: 0.4852\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 8s 821us/step - loss: 1.0591 - acc: 0.4571 - val_loss: 1.0189 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 8s 818us/step - loss: 1.0531 - acc: 0.4607 - val_loss: 1.0176 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 8s 820us/step - loss: 1.0484 - acc: 0.4632 - val_loss: 1.0119 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 1s 430us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0905 - acc: 0.4029 - val_loss: 1.0295 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 8s 781us/step - loss: 1.0696 - acc: 0.4340 - val_loss: 1.0206 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 9s 869us/step - loss: 1.0537 - acc: 0.4490 - val_loss: 1.0148 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 9s 863us/step - loss: 1.0497 - acc: 0.4593 - val_loss: 1.0091 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 9s 877us/step - loss: 1.0457 - acc: 0.4668 - val_loss: 1.0114 - val_acc: 0.5016\n",
      "1265/1265 [==============================] - 1s 462us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0838 - acc: 0.4149 - val_loss: 1.0282 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 9s 871us/step - loss: 1.0641 - acc: 0.4494 - val_loss: 1.0208 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 9s 928us/step - loss: 1.0564 - acc: 0.4533 - val_loss: 1.0160 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 9s 899us/step - loss: 1.0506 - acc: 0.4633 - val_loss: 1.0116 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 9s 832us/step - loss: 1.0450 - acc: 0.4751 - val_loss: 1.0126 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 1s 406us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0928 - acc: 0.4083 - val_loss: 1.0371 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 10s 982us/step - loss: 1.0641 - acc: 0.4479 - val_loss: 1.0246 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 10s 945us/step - loss: 1.0552 - acc: 0.4588 - val_loss: 1.0204 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 10s 982us/step - loss: 1.0453 - acc: 0.4695 - val_loss: 1.0142 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 10s 982us/step - loss: 1.0454 - acc: 0.4728 - val_loss: 1.0155 - val_acc: 0.5000\n",
      "1265/1265 [==============================] - 1s 507us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0953 - acc: 0.4085 - val_loss: 1.0309 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 10s 1ms/step - loss: 1.0734 - acc: 0.4352 - val_loss: 1.0211 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 10s 1ms/step - loss: 1.0588 - acc: 0.4565 - val_loss: 1.0154 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 10s 1ms/step - loss: 1.0522 - acc: 0.4635 - val_loss: 1.0096 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 10s 1ms/step - loss: 1.0468 - acc: 0.4741 - val_loss: 1.0072 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 1s 529us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0846 - acc: 0.4180 - val_loss: 1.0329 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0651 - acc: 0.4484 - val_loss: 1.0247 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 10s 984us/step - loss: 1.0520 - acc: 0.4587 - val_loss: 1.0170 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0553 - acc: 0.4620 - val_loss: 1.0147 - val_acc: 0.5008\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0453 - acc: 0.4646 - val_loss: 1.0099 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 1s 561us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0885 - acc: 0.4190 - val_loss: 1.0285 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0674 - acc: 0.4407 - val_loss: 1.0180 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0582 - acc: 0.4550 - val_loss: 1.0141 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0477 - acc: 0.4667 - val_loss: 1.0075 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0449 - acc: 0.4743 - val_loss: 1.0059 - val_acc: 0.5078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265/1265 [==============================] - 1s 579us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0882 - acc: 0.4040 - val_loss: 1.0298 - val_acc: 0.4945\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0679 - acc: 0.4375 - val_loss: 1.0174 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0560 - acc: 0.4543 - val_loss: 1.0140 - val_acc: 0.5055ETA: 5s  - ETA: 3s - loss: 1.\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0494 - acc: 0.4612 - val_loss: 1.0065 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0418 - acc: 0.4740 - val_loss: 1.0047 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 1s 610us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0908 - acc: 0.4089 - val_loss: 1.0296 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0679 - acc: 0.4362 - val_loss: 1.0236 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0597 - acc: 0.4471 - val_loss: 1.0122 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0437 - acc: 0.4716 - val_loss: 1.0067 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0456 - acc: 0.4721 - val_loss: 1.0039 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 1s 640us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0859 - acc: 0.4079 - val_loss: 1.0301 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0690 - acc: 0.4368 - val_loss: 1.0236 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0546 - acc: 0.4521 - val_loss: 1.0148 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0520 - acc: 0.4601 - val_loss: 1.0109 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0484 - acc: 0.4652 - val_loss: 1.0108 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 1s 649us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0846 - acc: 0.4196 - val_loss: 1.0311 - val_acc: 0.4961\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0689 - acc: 0.4437 - val_loss: 1.0237 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0495 - acc: 0.4650 - val_loss: 1.0163 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0459 - acc: 0.4701 - val_loss: 1.0111 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0465 - acc: 0.4718 - val_loss: 1.0066 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 1s 694us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0898 - acc: 0.4109 - val_loss: 1.0352 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0687 - acc: 0.4425 - val_loss: 1.0225 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0583 - acc: 0.4541 - val_loss: 1.0177 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0457 - acc: 0.4666 - val_loss: 1.0129 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0419 - acc: 0.4715 - val_loss: 1.0094 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 1s 706us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0898 - acc: 0.4073 - val_loss: 1.0364 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0666 - acc: 0.4375 - val_loss: 1.0252 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0575 - acc: 0.4484 - val_loss: 1.0169 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0538 - acc: 0.4535 - val_loss: 1.0146 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0433 - acc: 0.4724 - val_loss: 1.0071 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 1s 770us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0848 - acc: 0.4071 - val_loss: 1.0282 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0667 - acc: 0.4434 - val_loss: 1.0217 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0570 - acc: 0.4516 - val_loss: 1.0167 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0506 - acc: 0.4635 - val_loss: 1.0096 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0420 - acc: 0.4746 - val_loss: 1.0071 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 1s 766us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0817 - acc: 0.4121 - val_loss: 1.0339 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0640 - acc: 0.4435 - val_loss: 1.0267 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0539 - acc: 0.4578 - val_loss: 1.0197 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0465 - acc: 0.4682 - val_loss: 1.0148 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0439 - acc: 0.4705 - val_loss: 1.0090 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 1s 806us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0894 - acc: 0.4110 - val_loss: 1.0415 - val_acc: 0.4657\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0644 - acc: 0.4369 - val_loss: 1.0309 - val_acc: 0.4813\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0565 - acc: 0.4536 - val_loss: 1.0266 - val_acc: 0.4907\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0504 - acc: 0.4617 - val_loss: 1.0214 - val_acc: 0.4969\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0434 - acc: 0.4701 - val_loss: 1.0145 - val_acc: 0.5000\n",
      "1265/1265 [==============================] - 1s 819us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0807 - acc: 0.4131 - val_loss: 1.0357 - val_acc: 0.4899\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0643 - acc: 0.4405 - val_loss: 1.0280 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0533 - acc: 0.4551 - val_loss: 1.0177 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0446 - acc: 0.4694 - val_loss: 1.0117 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0396 - acc: 0.4775 - val_loss: 1.0083 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 1s 845us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0832 - acc: 0.4015 - val_loss: 1.0382 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0633 - acc: 0.4378 - val_loss: 1.0280 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0563 - acc: 0.4512 - val_loss: 1.0221 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0482 - acc: 0.4654 - val_loss: 1.0167 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0468 - acc: 0.4674 - val_loss: 1.0170 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 1s 876us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0860 - acc: 0.4022 - val_loss: 1.0311 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0669 - acc: 0.4305 - val_loss: 1.0224 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0556 - acc: 0.4499 - val_loss: 1.0164 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0484 - acc: 0.4650 - val_loss: 1.0129 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0398 - acc: 0.4736 - val_loss: 1.0100 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 1s 897us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0801 - acc: 0.4151 - val_loss: 1.0369 - val_acc: 0.4735\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0651 - acc: 0.4394 - val_loss: 1.0295 - val_acc: 0.4883s -  - ETA: 6s - loss: 1.0657 - a  - ETA: 1s - loss - ETA: 0s - loss: 1.0649 - acc:  - ETA: 0s - loss: 1.0647 - acc:\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0537 - acc: 0.4465 - val_loss: 1.0214 - val_acc: 0.4945 1.0528 - acc: - ETA: 1s - loss: \n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0483 - acc: 0.4671 - val_loss: 1.0149 - val_acc: 0.5023\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0428 - acc: 0.4708 - val_loss: 1.0114 - val_acc: 0.5070 -  - ETA: \n",
      "1265/1265 [==============================] - 1s 902us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0847 - acc: 0.4100 - val_loss: 1.0356 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0676 - acc: 0.4345 - val_loss: 1.0276 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0544 - acc: 0.4514 - val_loss: 1.0277 - val_acc: 0.4992\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0474 - acc: 0.4673 - val_loss: 1.0170 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0429 - acc: 0.4735 - val_loss: 1.0163 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.0819 - acc: 0.4050 - val_loss: 1.0384 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0657 - acc: 0.4295 - val_loss: 1.0306 - val_acc: 0.4961\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0522 - acc: 0.4468 - val_loss: 1.0265 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0502 - acc: 0.4550 - val_loss: 1.0213 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0460 - acc: 0.4680 - val_loss: 1.0176 - val_acc: 0.5031\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0782 - acc: 0.4199 - val_loss: 1.0364 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0621 - acc: 0.4453 - val_loss: 1.0242 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0532 - acc: 0.4568 - val_loss: 1.0197 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0465 - acc: 0.4653 - val_loss: 1.0179 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0438 - acc: 0.4662 - val_loss: 1.0130 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0770 - acc: 0.4126 - val_loss: 1.0353 - val_acc: 0.4751\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0615 - acc: 0.4401 - val_loss: 1.0287 - val_acc: 0.4914\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0565 - acc: 0.4467 - val_loss: 1.0247 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0478 - acc: 0.4634 - val_loss: 1.0171 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0437 - acc: 0.4720 - val_loss: 1.0141 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0797 - acc: 0.4122 - val_loss: 1.0374 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0623 - acc: 0.4411 - val_loss: 1.0263 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0554 - acc: 0.4511 - val_loss: 1.0211 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0502 - acc: 0.4562 - val_loss: 1.0175 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0444 - acc: 0.4662 - val_loss: 1.0108 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0803 - acc: 0.4051 - val_loss: 1.0339 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0618 - acc: 0.4321 - val_loss: 1.0265 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0571 - acc: 0.4449 - val_loss: 1.0217 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0503 - acc: 0.4587 - val_loss: 1.0177 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0447 - acc: 0.4676 - val_loss: 1.0150 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.0794 - acc: 0.4042 - val_loss: 1.0336 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0632 - acc: 0.4302 - val_loss: 1.0290 - val_acc: 0.4938\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0534 - acc: 0.4530 - val_loss: 1.0224 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0463 - acc: 0.4638 - val_loss: 1.0172 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0415 - acc: 0.4722 - val_loss: 1.0134 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0779 - acc: 0.4069 - val_loss: 1.0368 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0631 - acc: 0.4350 - val_loss: 1.0290 - val_acc: 0.4922\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0530 - acc: 0.4514 - val_loss: 1.0234 - val_acc: 0.4992\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0516 - acc: 0.4546 - val_loss: 1.0193 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0452 - acc: 0.4677 - val_loss: 1.0146 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.0791 - acc: 0.4097 - val_loss: 1.0350 - val_acc: 0.4868\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0628 - acc: 0.4352 - val_loss: 1.0289 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0518 - acc: 0.4520 - val_loss: 1.0221 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0512 - acc: 0.4537 - val_loss: 1.0158 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0424 - acc: 0.4706 - val_loss: 1.0124 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "CPU times: user 15h 1min 47s, sys: 3h 10min 24s, total: 18h 12min 11s\n",
      "Wall time: 4h 16min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "transformerxl_rounds = [calculate_round(concatenated_transformerxl) for round in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{5: 0.5003952572468241,\n",
       "  6: 0.474308300725085,\n",
       "  7: 0.4996047434599503,\n",
       "  8: 0.5106719370416031,\n",
       "  9: 0.49802371574484783,\n",
       "  10: 0.4853754944009743,\n",
       "  11: 0.47905138372903755,\n",
       "  12: 0.49249011895402145,\n",
       "  13: 0.49249011895402145,\n",
       "  14: 0.49090909128603727,\n",
       "  15: 0.4940711466220057,\n",
       "  16: 0.49169960507291105,\n",
       "  17: 0.4893280635709348,\n",
       "  18: 0.4956521739366026,\n",
       "  19: 0.4940711466220057,\n",
       "  20: 0.49802371579196614,\n",
       "  21: 0.49802371574484783,\n",
       "  22: 0.49486166040887947,\n",
       "  23: 0.4948616604559978,\n",
       "  24: 0.4988142296259582,\n",
       "  25: 0.49723320191085574,\n",
       "  26: 0.4877470359029506,\n",
       "  27: 0.49644268807686365,\n",
       "  28: 0.4940711465748874,\n",
       "  29: 0.49486166040887947,\n",
       "  30: 0.49090909123891896,\n",
       "  31: 0.4735177868910929,\n",
       "  32: 0.4924901189069032,\n",
       "  33: 0.4869565220689585,\n",
       "  34: 0.4877470358558323,\n",
       "  35: 0.49802371579196614},\n",
       " {5: 0.49090909123891896,\n",
       "  6: 0.4845849802607133,\n",
       "  7: 0.48616600792869746,\n",
       "  8: 0.49565217428998987,\n",
       "  9: 0.49169960512002936,\n",
       "  10: 0.49090909123891896,\n",
       "  11: 0.5019762849619266,\n",
       "  12: 0.49723320191085574,\n",
       "  13: 0.4869565217626896,\n",
       "  14: 0.488537549784061,\n",
       "  15: 0.47984189761014795,\n",
       "  16: 0.4893280636180531,\n",
       "  17: 0.49090909093265006,\n",
       "  18: 0.4861660082820847,\n",
       "  19: 0.49723320191085574,\n",
       "  20: 0.5090909094678555,\n",
       "  21: 0.5059288540847687,\n",
       "  22: 0.4861660082820847,\n",
       "  23: 0.49328063278801354,\n",
       "  24: 0.4766798422270613,\n",
       "  25: 0.47984189751591133,\n",
       "  26: 0.4932806327408953,\n",
       "  27: 0.49169960507291105,\n",
       "  28: 0.49565217424287156,\n",
       "  29: 0.49565217419575325,\n",
       "  30: 0.4877470359029506,\n",
       "  31: 0.48379446673299015,\n",
       "  32: 0.4822134391121242,\n",
       "  33: 0.4869565221160768,\n",
       "  34: 0.4948616604559978,\n",
       "  35: 0.49644268807686365},\n",
       " {5: 0.49644268807686365,\n",
       "  6: 0.4869565221160768,\n",
       "  7: 0.49169960512002936,\n",
       "  8: 0.4869565221160768,\n",
       "  9: 0.4885375497369427,\n",
       "  10: 0.49169960512002936,\n",
       "  11: 0.48458498056698224,\n",
       "  12: 0.47984189730387905,\n",
       "  13: 0.49090909093265006,\n",
       "  14: 0.4901185774520452,\n",
       "  15: 0.49723320191085574,\n",
       "  16: 0.49723320191085574,\n",
       "  17: 0.47826086994216377,\n",
       "  18: 0.488537549784061,\n",
       "  19: 0.48142292527813213,\n",
       "  20: 0.49249011860063424,\n",
       "  21: 0.4956521739366026,\n",
       "  22: 0.4885375497369427,\n",
       "  23: 0.4893280635709348,\n",
       "  24: 0.48458498061410055,\n",
       "  25: 0.4885375497369427,\n",
       "  26: 0.48063241144414004,\n",
       "  27: 0.4877470359500689,\n",
       "  28: 0.499604743412832,\n",
       "  29: 0.4885375497369427,\n",
       "  30: 0.4869565221160768,\n",
       "  31: 0.5043478264167846,\n",
       "  32: 0.4988142296259582,\n",
       "  33: 0.48458498061410055,\n",
       "  34: 0.4893280635709348,\n",
       "  35: 0.4924901189069032},\n",
       " {5: 0.49249011895402145,\n",
       "  6: 0.49090909123891896,\n",
       "  7: 0.49960474336571375,\n",
       "  8: 0.4948616604559978,\n",
       "  9: 0.4766798422270613,\n",
       "  10: 0.4893280635709348,\n",
       "  11: 0.4869565220689585,\n",
       "  12: 0.4861660082820847,\n",
       "  13: 0.49565217428998987,\n",
       "  14: 0.49486166040887947,\n",
       "  15: 0.4948616604559978,\n",
       "  16: 0.4885375497369427,\n",
       "  17: 0.4893280635709348,\n",
       "  18: 0.4837944664267212,\n",
       "  19: 0.49249011895402145,\n",
       "  20: 0.48063241144414004,\n",
       "  21: 0.49249011860063424,\n",
       "  22: 0.499604743412832,\n",
       "  23: 0.4893280636180531,\n",
       "  24: 0.4940711466220057,\n",
       "  25: 0.4861660082349664,\n",
       "  26: 0.4988142295788399,\n",
       "  27: 0.4893280636180531,\n",
       "  28: 0.4924901189069032,\n",
       "  29: 0.49249011860063424,\n",
       "  30: 0.4948616604559978,\n",
       "  31: 0.4822134391121242,\n",
       "  32: 0.5011857710336979,\n",
       "  33: 0.5051383399445077,\n",
       "  34: 0.4901185774520452,\n",
       "  35: 0.5027667987488004},\n",
       " {5: 0.49486166040887947,\n",
       "  6: 0.4940711466220057,\n",
       "  7: 0.4948616604559978,\n",
       "  8: 0.4948616601026105,\n",
       "  9: 0.49169960507291105,\n",
       "  10: 0.49169960507291105,\n",
       "  11: 0.4924901189069032,\n",
       "  12: 0.4924901189069032,\n",
       "  13: 0.4869565221160768,\n",
       "  14: 0.4988142296259582,\n",
       "  15: 0.47905138377615586,\n",
       "  16: 0.4988142296259582,\n",
       "  17: 0.4877470359029506,\n",
       "  18: 0.4940711466220057,\n",
       "  19: 0.5027667987488004,\n",
       "  20: 0.4948616604559978,\n",
       "  21: 0.4893280636180531,\n",
       "  22: 0.4932806327408953,\n",
       "  23: 0.488537549784061,\n",
       "  24: 0.4869565217626896,\n",
       "  25: 0.49011857740492687,\n",
       "  26: 0.49802371574484783,\n",
       "  27: 0.5011857710808162,\n",
       "  28: 0.49486166040887947,\n",
       "  29: 0.49802371579196614,\n",
       "  30: 0.49486166040887947,\n",
       "  31: 0.4853754944009743,\n",
       "  32: 0.4940711462686184,\n",
       "  33: 0.4901185773578086,\n",
       "  34: 0.4853754944009743,\n",
       "  35: 0.49169960512002936}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformerxl_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Round 0",
         "type": "scatter",
         "uid": "23e2ed92-97c8-4984-aab3-819416ad104f",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5003952572468241,
          0.474308300725085,
          0.4996047434599503,
          0.5106719370416031,
          0.49802371574484783,
          0.4853754944009743,
          0.47905138372903755,
          0.49249011895402145,
          0.49249011895402145,
          0.49090909128603727,
          0.4940711466220057,
          0.49169960507291105,
          0.4893280635709348,
          0.4956521739366026,
          0.4940711466220057,
          0.49802371579196614,
          0.49802371574484783,
          0.49486166040887947,
          0.4948616604559978,
          0.4988142296259582,
          0.49723320191085574,
          0.4877470359029506,
          0.49644268807686365,
          0.4940711465748874,
          0.49486166040887947,
          0.49090909123891896,
          0.4735177868910929,
          0.4924901189069032,
          0.4869565220689585,
          0.4877470358558323,
          0.49802371579196614
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 1",
         "type": "scatter",
         "uid": "971b44c3-c87f-4bb6-bb10-0544b1c96825",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.49090909123891896,
          0.4845849802607133,
          0.48616600792869746,
          0.49565217428998987,
          0.49169960512002936,
          0.49090909123891896,
          0.5019762849619266,
          0.49723320191085574,
          0.4869565217626896,
          0.488537549784061,
          0.47984189761014795,
          0.4893280636180531,
          0.49090909093265006,
          0.4861660082820847,
          0.49723320191085574,
          0.5090909094678555,
          0.5059288540847687,
          0.4861660082820847,
          0.49328063278801354,
          0.4766798422270613,
          0.47984189751591133,
          0.4932806327408953,
          0.49169960507291105,
          0.49565217424287156,
          0.49565217419575325,
          0.4877470359029506,
          0.48379446673299015,
          0.4822134391121242,
          0.4869565221160768,
          0.4948616604559978,
          0.49644268807686365
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 2",
         "type": "scatter",
         "uid": "7bb971fd-1ee3-4283-8e04-05e118fcf5aa",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.49644268807686365,
          0.4869565221160768,
          0.49169960512002936,
          0.4869565221160768,
          0.4885375497369427,
          0.49169960512002936,
          0.48458498056698224,
          0.47984189730387905,
          0.49090909093265006,
          0.4901185774520452,
          0.49723320191085574,
          0.49723320191085574,
          0.47826086994216377,
          0.488537549784061,
          0.48142292527813213,
          0.49249011860063424,
          0.4956521739366026,
          0.4885375497369427,
          0.4893280635709348,
          0.48458498061410055,
          0.4885375497369427,
          0.48063241144414004,
          0.4877470359500689,
          0.499604743412832,
          0.4885375497369427,
          0.4869565221160768,
          0.5043478264167846,
          0.4988142296259582,
          0.48458498061410055,
          0.4893280635709348,
          0.4924901189069032
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 3",
         "type": "scatter",
         "uid": "15b3d872-ffd2-4402-b83d-76528edc068a",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.49249011895402145,
          0.49090909123891896,
          0.49960474336571375,
          0.4948616604559978,
          0.4766798422270613,
          0.4893280635709348,
          0.4869565220689585,
          0.4861660082820847,
          0.49565217428998987,
          0.49486166040887947,
          0.4948616604559978,
          0.4885375497369427,
          0.4893280635709348,
          0.4837944664267212,
          0.49249011895402145,
          0.48063241144414004,
          0.49249011860063424,
          0.499604743412832,
          0.4893280636180531,
          0.4940711466220057,
          0.4861660082349664,
          0.4988142295788399,
          0.4893280636180531,
          0.4924901189069032,
          0.49249011860063424,
          0.4948616604559978,
          0.4822134391121242,
          0.5011857710336979,
          0.5051383399445077,
          0.4901185774520452,
          0.5027667987488004
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 4",
         "type": "scatter",
         "uid": "e2c8c786-bd9c-49d4-9bd7-0ded00c348e5",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.49486166040887947,
          0.4940711466220057,
          0.4948616604559978,
          0.4948616601026105,
          0.49169960507291105,
          0.49169960507291105,
          0.4924901189069032,
          0.4924901189069032,
          0.4869565221160768,
          0.4988142296259582,
          0.47905138377615586,
          0.4988142296259582,
          0.4877470359029506,
          0.4940711466220057,
          0.5027667987488004,
          0.4948616604559978,
          0.4893280636180531,
          0.4932806327408953,
          0.488537549784061,
          0.4869565217626896,
          0.49011857740492687,
          0.49802371574484783,
          0.5011857710808162,
          0.49486166040887947,
          0.49802371579196614,
          0.49486166040887947,
          0.4853754944009743,
          0.4940711462686184,
          0.4901185773578086,
          0.4853754944009743,
          0.49169960512002936
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "Test set accuracy of padded Transformer-XL dataset with variable maximum lengths"
        }
       }
      },
      "text/html": [
       "<div id=\"9cc5c671-4685-4162-8b88-037416b33282\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"9cc5c671-4685-4162-8b88-037416b33282\")) {\n",
       "    Plotly.newPlot(\"9cc5c671-4685-4162-8b88-037416b33282\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5003952572468241, 0.474308300725085, 0.4996047434599503, 0.5106719370416031, 0.49802371574484783, 0.4853754944009743, 0.47905138372903755, 0.49249011895402145, 0.49249011895402145, 0.49090909128603727, 0.4940711466220057, 0.49169960507291105, 0.4893280635709348, 0.4956521739366026, 0.4940711466220057, 0.49802371579196614, 0.49802371574484783, 0.49486166040887947, 0.4948616604559978, 0.4988142296259582, 0.49723320191085574, 0.4877470359029506, 0.49644268807686365, 0.4940711465748874, 0.49486166040887947, 0.49090909123891896, 0.4735177868910929, 0.4924901189069032, 0.4869565220689585, 0.4877470358558323, 0.49802371579196614], \"type\": \"scatter\", \"uid\": \"23e2ed92-97c8-4984-aab3-819416ad104f\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49090909123891896, 0.4845849802607133, 0.48616600792869746, 0.49565217428998987, 0.49169960512002936, 0.49090909123891896, 0.5019762849619266, 0.49723320191085574, 0.4869565217626896, 0.488537549784061, 0.47984189761014795, 0.4893280636180531, 0.49090909093265006, 0.4861660082820847, 0.49723320191085574, 0.5090909094678555, 0.5059288540847687, 0.4861660082820847, 0.49328063278801354, 0.4766798422270613, 0.47984189751591133, 0.4932806327408953, 0.49169960507291105, 0.49565217424287156, 0.49565217419575325, 0.4877470359029506, 0.48379446673299015, 0.4822134391121242, 0.4869565221160768, 0.4948616604559978, 0.49644268807686365], \"type\": \"scatter\", \"uid\": \"971b44c3-c87f-4bb6-bb10-0544b1c96825\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49644268807686365, 0.4869565221160768, 0.49169960512002936, 0.4869565221160768, 0.4885375497369427, 0.49169960512002936, 0.48458498056698224, 0.47984189730387905, 0.49090909093265006, 0.4901185774520452, 0.49723320191085574, 0.49723320191085574, 0.47826086994216377, 0.488537549784061, 0.48142292527813213, 0.49249011860063424, 0.4956521739366026, 0.4885375497369427, 0.4893280635709348, 0.48458498061410055, 0.4885375497369427, 0.48063241144414004, 0.4877470359500689, 0.499604743412832, 0.4885375497369427, 0.4869565221160768, 0.5043478264167846, 0.4988142296259582, 0.48458498061410055, 0.4893280635709348, 0.4924901189069032], \"type\": \"scatter\", \"uid\": \"7bb971fd-1ee3-4283-8e04-05e118fcf5aa\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49249011895402145, 0.49090909123891896, 0.49960474336571375, 0.4948616604559978, 0.4766798422270613, 0.4893280635709348, 0.4869565220689585, 0.4861660082820847, 0.49565217428998987, 0.49486166040887947, 0.4948616604559978, 0.4885375497369427, 0.4893280635709348, 0.4837944664267212, 0.49249011895402145, 0.48063241144414004, 0.49249011860063424, 0.499604743412832, 0.4893280636180531, 0.4940711466220057, 0.4861660082349664, 0.4988142295788399, 0.4893280636180531, 0.4924901189069032, 0.49249011860063424, 0.4948616604559978, 0.4822134391121242, 0.5011857710336979, 0.5051383399445077, 0.4901185774520452, 0.5027667987488004], \"type\": \"scatter\", \"uid\": \"15b3d872-ffd2-4402-b83d-76528edc068a\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49486166040887947, 0.4940711466220057, 0.4948616604559978, 0.4948616601026105, 0.49169960507291105, 0.49169960507291105, 0.4924901189069032, 0.4924901189069032, 0.4869565221160768, 0.4988142296259582, 0.47905138377615586, 0.4988142296259582, 0.4877470359029506, 0.4940711466220057, 0.5027667987488004, 0.4948616604559978, 0.4893280636180531, 0.4932806327408953, 0.488537549784061, 0.4869565217626896, 0.49011857740492687, 0.49802371574484783, 0.5011857710808162, 0.49486166040887947, 0.49802371579196614, 0.49486166040887947, 0.4853754944009743, 0.4940711462686184, 0.4901185773578086, 0.4853754944009743, 0.49169960512002936], \"type\": \"scatter\", \"uid\": \"e2c8c786-bd9c-49d4-9bd7-0ded00c348e5\"}], {\"title\": {\"text\": \"Test set accuracy of padded Transformer-XL dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"9cc5c671-4685-4162-8b88-037416b33282\")) {window._Plotly.Plots.resize(document.getElementById(\"9cc5c671-4685-4162-8b88-037416b33282\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"9cc5c671-4685-4162-8b88-037416b33282\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"9cc5c671-4685-4162-8b88-037416b33282\")) {\n",
       "    Plotly.newPlot(\"9cc5c671-4685-4162-8b88-037416b33282\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5003952572468241, 0.474308300725085, 0.4996047434599503, 0.5106719370416031, 0.49802371574484783, 0.4853754944009743, 0.47905138372903755, 0.49249011895402145, 0.49249011895402145, 0.49090909128603727, 0.4940711466220057, 0.49169960507291105, 0.4893280635709348, 0.4956521739366026, 0.4940711466220057, 0.49802371579196614, 0.49802371574484783, 0.49486166040887947, 0.4948616604559978, 0.4988142296259582, 0.49723320191085574, 0.4877470359029506, 0.49644268807686365, 0.4940711465748874, 0.49486166040887947, 0.49090909123891896, 0.4735177868910929, 0.4924901189069032, 0.4869565220689585, 0.4877470358558323, 0.49802371579196614], \"type\": \"scatter\", \"uid\": \"23e2ed92-97c8-4984-aab3-819416ad104f\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49090909123891896, 0.4845849802607133, 0.48616600792869746, 0.49565217428998987, 0.49169960512002936, 0.49090909123891896, 0.5019762849619266, 0.49723320191085574, 0.4869565217626896, 0.488537549784061, 0.47984189761014795, 0.4893280636180531, 0.49090909093265006, 0.4861660082820847, 0.49723320191085574, 0.5090909094678555, 0.5059288540847687, 0.4861660082820847, 0.49328063278801354, 0.4766798422270613, 0.47984189751591133, 0.4932806327408953, 0.49169960507291105, 0.49565217424287156, 0.49565217419575325, 0.4877470359029506, 0.48379446673299015, 0.4822134391121242, 0.4869565221160768, 0.4948616604559978, 0.49644268807686365], \"type\": \"scatter\", \"uid\": \"971b44c3-c87f-4bb6-bb10-0544b1c96825\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49644268807686365, 0.4869565221160768, 0.49169960512002936, 0.4869565221160768, 0.4885375497369427, 0.49169960512002936, 0.48458498056698224, 0.47984189730387905, 0.49090909093265006, 0.4901185774520452, 0.49723320191085574, 0.49723320191085574, 0.47826086994216377, 0.488537549784061, 0.48142292527813213, 0.49249011860063424, 0.4956521739366026, 0.4885375497369427, 0.4893280635709348, 0.48458498061410055, 0.4885375497369427, 0.48063241144414004, 0.4877470359500689, 0.499604743412832, 0.4885375497369427, 0.4869565221160768, 0.5043478264167846, 0.4988142296259582, 0.48458498061410055, 0.4893280635709348, 0.4924901189069032], \"type\": \"scatter\", \"uid\": \"7bb971fd-1ee3-4283-8e04-05e118fcf5aa\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49249011895402145, 0.49090909123891896, 0.49960474336571375, 0.4948616604559978, 0.4766798422270613, 0.4893280635709348, 0.4869565220689585, 0.4861660082820847, 0.49565217428998987, 0.49486166040887947, 0.4948616604559978, 0.4885375497369427, 0.4893280635709348, 0.4837944664267212, 0.49249011895402145, 0.48063241144414004, 0.49249011860063424, 0.499604743412832, 0.4893280636180531, 0.4940711466220057, 0.4861660082349664, 0.4988142295788399, 0.4893280636180531, 0.4924901189069032, 0.49249011860063424, 0.4948616604559978, 0.4822134391121242, 0.5011857710336979, 0.5051383399445077, 0.4901185774520452, 0.5027667987488004], \"type\": \"scatter\", \"uid\": \"15b3d872-ffd2-4402-b83d-76528edc068a\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49486166040887947, 0.4940711466220057, 0.4948616604559978, 0.4948616601026105, 0.49169960507291105, 0.49169960507291105, 0.4924901189069032, 0.4924901189069032, 0.4869565221160768, 0.4988142296259582, 0.47905138377615586, 0.4988142296259582, 0.4877470359029506, 0.4940711466220057, 0.5027667987488004, 0.4948616604559978, 0.4893280636180531, 0.4932806327408953, 0.488537549784061, 0.4869565217626896, 0.49011857740492687, 0.49802371574484783, 0.5011857710808162, 0.49486166040887947, 0.49802371579196614, 0.49486166040887947, 0.4853754944009743, 0.4940711462686184, 0.4901185773578086, 0.4853754944009743, 0.49169960512002936], \"type\": \"scatter\", \"uid\": \"e2c8c786-bd9c-49d4-9bd7-0ded00c348e5\"}], {\"title\": {\"text\": \"Test set accuracy of padded Transformer-XL dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"9cc5c671-4685-4162-8b88-037416b33282\")) {window._Plotly.Plots.resize(document.getElementById(\"9cc5c671-4685-4162-8b88-037416b33282\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traces = transformerxl_rounds\n",
    "\n",
    "# Create traces\n",
    "def create_scatter(counter):\n",
    "    acc_dict = traces[counter]\n",
    "    \n",
    "    return go.Scatter(\n",
    "        x = list(acc_dict.keys()),\n",
    "        y = list(acc_dict.values()),\n",
    "        mode = 'lines+markers',\n",
    "        name = 'Round ' + str(counter)\n",
    "    )\n",
    "\n",
    "trace_data = [create_scatter(trace) for trace in range(len(traces))]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'Test set accuracy of padded Transformer-XL dataset with variable maximum lengths',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = trace_data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair = data.get_flair()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "2019-06-13 20:08:53,109 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "2019-06-13 20:08:54,567 From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-06-13 20:08:55,021 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0680 - acc: 0.4275 - val_loss: 1.0403 - val_acc: 0.4603\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0575 - acc: 0.4473 - val_loss: 1.0191 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0510 - acc: 0.4521 - val_loss: 1.0195 - val_acc: 0.5016\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0477 - acc: 0.4693 - val_loss: 1.0135 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0415 - acc: 0.4692 - val_loss: 1.0169 - val_acc: 0.5023\n",
      "1265/1265 [==============================] - 1s 687us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0639 - acc: 0.4275 - val_loss: 1.0273 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0539 - acc: 0.4540 - val_loss: 1.0346 - val_acc: 0.4938\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0500 - acc: 0.4542 - val_loss: 1.0293 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0439 - acc: 0.4711 - val_loss: 1.0177 - val_acc: 0.5031\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0438 - acc: 0.4716 - val_loss: 1.0155 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 1s 580us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0692 - acc: 0.4237 - val_loss: 1.0301 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0549 - acc: 0.4496 - val_loss: 1.0202 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0496 - acc: 0.4562 - val_loss: 1.0194 - val_acc: 0.4992\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0466 - acc: 0.4653 - val_loss: 1.0125 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0423 - acc: 0.4712 - val_loss: 1.0151 - val_acc: 0.5047\n",
      "1265/1265 [==============================] - 1s 665us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0663 - acc: 0.4331 - val_loss: 1.0239 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0495 - acc: 0.4612 - val_loss: 1.0173 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0463 - acc: 0.4665 - val_loss: 1.0125 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0423 - acc: 0.4718 - val_loss: 1.0096 - val_acc: 0.5023\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0407 - acc: 0.4789 - val_loss: 1.0131 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 1s 723us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0664 - acc: 0.4295 - val_loss: 1.0320 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0519 - acc: 0.4574 - val_loss: 1.0193 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0421 - acc: 0.4690 - val_loss: 1.0143 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0422 - acc: 0.4720 - val_loss: 1.0248 - val_acc: 0.4984\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0373 - acc: 0.4815 - val_loss: 1.0060 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 1s 801us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0638 - acc: 0.4323 - val_loss: 1.0218 - val_acc: 0.4961\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0503 - acc: 0.4630 - val_loss: 1.0141 - val_acc: 0.4945\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0435 - acc: 0.4745 - val_loss: 1.0123 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0407 - acc: 0.4774 - val_loss: 1.0108 - val_acc: 0.4992\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0378 - acc: 0.4734 - val_loss: 1.0057 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 1s 974us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0690 - acc: 0.4285 - val_loss: 1.0295 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0543 - acc: 0.4599 - val_loss: 1.0128 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0444 - acc: 0.4732 - val_loss: 1.0152 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0410 - acc: 0.4777 - val_loss: 1.0070 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0391 - acc: 0.4793 - val_loss: 1.0078 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0645 - acc: 0.4365 - val_loss: 1.0260 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0536 - acc: 0.4563 - val_loss: 1.0210 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0419 - acc: 0.4799 - val_loss: 1.0112 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0407 - acc: 0.4843 - val_loss: 1.0167 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0339 - acc: 0.4819 - val_loss: 1.0072 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0727 - acc: 0.4228 - val_loss: 1.0356 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0522 - acc: 0.4590 - val_loss: 1.0278 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0477 - acc: 0.4678 - val_loss: 1.0164 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0410 - acc: 0.4739 - val_loss: 1.0156 - val_acc: 0.5000\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0369 - acc: 0.4761 - val_loss: 1.0079 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0661 - acc: 0.4329 - val_loss: 1.0256 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0515 - acc: 0.4691 - val_loss: 1.0147 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0448 - acc: 0.4751 - val_loss: 1.0104 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0403 - acc: 0.4828 - val_loss: 1.0067 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0374 - acc: 0.4793 - val_loss: 1.0093 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0710 - acc: 0.4277 - val_loss: 1.0296 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0517 - acc: 0.4577 - val_loss: 1.0156 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0433 - acc: 0.4784 - val_loss: 1.0093 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0388 - acc: 0.4762 - val_loss: 1.0044 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0397 - acc: 0.4742 - val_loss: 1.0092 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0667 - acc: 0.4303 - val_loss: 1.0329 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0524 - acc: 0.4519 - val_loss: 1.0161 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0465 - acc: 0.4692 - val_loss: 1.0124 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0432 - acc: 0.4695 - val_loss: 1.0143 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0383 - acc: 0.4748 - val_loss: 1.0090 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0693 - acc: 0.4234 - val_loss: 1.0259 - val_acc: 0.5148\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0490 - acc: 0.4587 - val_loss: 1.0148 - val_acc: 0.5202\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0443 - acc: 0.4678 - val_loss: 1.0100 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0403 - acc: 0.4785 - val_loss: 1.0064 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0373 - acc: 0.4791 - val_loss: 1.0043 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0708 - acc: 0.4290 - val_loss: 1.0251 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0531 - acc: 0.4570 - val_loss: 1.0158 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0446 - acc: 0.4692 - val_loss: 1.0093 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0391 - acc: 0.4784 - val_loss: 1.0070 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0374 - acc: 0.4790 - val_loss: 1.0066 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 1.0696 - acc: 0.4212 - val_loss: 1.0226 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 56s 6ms/step - loss: 1.0525 - acc: 0.4568 - val_loss: 1.0175 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0428 - acc: 0.4697 - val_loss: 1.0070 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0384 - acc: 0.4756 - val_loss: 1.0110 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0332 - acc: 0.4847 - val_loss: 1.0059 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 6s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 88s 9ms/step - loss: 1.0671 - acc: 0.4247 - val_loss: 1.0279 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0505 - acc: 0.4493 - val_loss: 1.0173 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0482 - acc: 0.4628 - val_loss: 1.0125 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0409 - acc: 0.4711 - val_loss: 1.0116 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0371 - acc: 0.4816 - val_loss: 1.0088 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 84s 8ms/step - loss: 1.0679 - acc: 0.4239 - val_loss: 1.0234 - val_acc: 0.4930\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0538 - acc: 0.4492 - val_loss: 1.0170 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0428 - acc: 0.4684 - val_loss: 1.0116 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0383 - acc: 0.4739 - val_loss: 1.0072 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0357 - acc: 0.4748 - val_loss: 1.0041 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 80s 8ms/step - loss: 1.0636 - acc: 0.4414 - val_loss: 1.0246 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0520 - acc: 0.4512 - val_loss: 1.0186 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0445 - acc: 0.4652 - val_loss: 1.0118 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0417 - acc: 0.4731 - val_loss: 1.0066 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 88s 9ms/step - loss: 1.0399 - acc: 0.4758 - val_loss: 1.0035 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 91s 9ms/step - loss: 1.0677 - acc: 0.4299 - val_loss: 1.0309 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0539 - acc: 0.4477 - val_loss: 1.0236 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0448 - acc: 0.4660 - val_loss: 1.0122 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0407 - acc: 0.4771 - val_loss: 1.0133 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0378 - acc: 0.4770 - val_loss: 1.0120 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 93s 9ms/step - loss: 1.0677 - acc: 0.4270 - val_loss: 1.0244 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.0513 - acc: 0.4566 - val_loss: 1.0191 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0458 - acc: 0.4617 - val_loss: 1.0137 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 56s 6ms/step - loss: 1.0416 - acc: 0.4713 - val_loss: 1.0080 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 56s 6ms/step - loss: 1.0387 - acc: 0.4749 - val_loss: 1.0114 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 94s 9ms/step - loss: 1.0663 - acc: 0.4238 - val_loss: 1.0289 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.0546 - acc: 0.4449 - val_loss: 1.0231 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0473 - acc: 0.4574 - val_loss: 1.0155 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0418 - acc: 0.4580 - val_loss: 1.0160 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0409 - acc: 0.4733 - val_loss: 1.0144 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 96s 9ms/step - loss: 1.0687 - acc: 0.4254 - val_loss: 1.0322 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 1.0529 - acc: 0.4503 - val_loss: 1.0225 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0473 - acc: 0.4570 - val_loss: 1.0207 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0422 - acc: 0.4676 - val_loss: 1.0156 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0382 - acc: 0.4750 - val_loss: 1.0100 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 103s 10ms/step - loss: 1.0697 - acc: 0.4218 - val_loss: 1.0406 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.0549 - acc: 0.4531 - val_loss: 1.0210 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0496 - acc: 0.4615 - val_loss: 1.0164 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0436 - acc: 0.4631 - val_loss: 1.0121 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0386 - acc: 0.4732 - val_loss: 1.0055 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0666 - acc: 0.4254 - val_loss: 1.0315 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 82s 8ms/step - loss: 1.0539 - acc: 0.4508 - val_loss: 1.0166 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0470 - acc: 0.4680 - val_loss: 1.0155 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0409 - acc: 0.4711 - val_loss: 1.0073 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0385 - acc: 0.4776 - val_loss: 1.0051 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0697 - acc: 0.4221 - val_loss: 1.0279 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 87s 9ms/step - loss: 1.0510 - acc: 0.4520 - val_loss: 1.0198 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0478 - acc: 0.4608 - val_loss: 1.0198 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0427 - acc: 0.4702 - val_loss: 1.0102 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0390 - acc: 0.4729 - val_loss: 1.0100 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0655 - acc: 0.4201 - val_loss: 1.0261 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 87s 8ms/step - loss: 1.0503 - acc: 0.4516 - val_loss: 1.0235 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0453 - acc: 0.4626 - val_loss: 1.0169 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0430 - acc: 0.4660 - val_loss: 1.0083 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0367 - acc: 0.4820 - val_loss: 1.0127 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0656 - acc: 0.4266 - val_loss: 1.0307 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 92s 9ms/step - loss: 1.0533 - acc: 0.4483 - val_loss: 1.0195 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.0480 - acc: 0.4613 - val_loss: 1.0201 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.0426 - acc: 0.4620 - val_loss: 1.0108 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.0394 - acc: 0.4707 - val_loss: 1.0181 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 150s 15ms/step - loss: 1.0674 - acc: 0.4265 - val_loss: 1.0315 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0550 - acc: 0.4492 - val_loss: 1.0243 - val_acc: 0.4977\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.0439 - acc: 0.4634 - val_loss: 1.0226 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.0422 - acc: 0.4669 - val_loss: 1.0195 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 1.0388 - acc: 0.4769 - val_loss: 1.0078 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 11s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0687 - acc: 0.4236 - val_loss: 1.0386 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 106s 10ms/step - loss: 1.0549 - acc: 0.4398 - val_loss: 1.0250 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 84s 8ms/step - loss: 1.0463 - acc: 0.4610 - val_loss: 1.0155 - val_acc: 0.4992\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 84s 8ms/step - loss: 1.0444 - acc: 0.4664 - val_loss: 1.0086 - val_acc: 0.5047\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.0366 - acc: 0.4771 - val_loss: 1.0116 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 11s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0662 - acc: 0.4265 - val_loss: 1.0314 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 103s 10ms/step - loss: 1.0521 - acc: 0.4462 - val_loss: 1.0248 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 90s 9ms/step - loss: 1.0467 - acc: 0.4631 - val_loss: 1.0153 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 86s 8ms/step - loss: 1.0430 - acc: 0.4677 - val_loss: 1.0158 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 86s 8ms/step - loss: 1.0397 - acc: 0.4790 - val_loss: 1.0084 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 1.0646 - acc: 0.4357 - val_loss: 1.0242 - val_acc: 0.5132\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 109s 11ms/step - loss: 1.0479 - acc: 0.4601 - val_loss: 1.0210 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 94s 9ms/step - loss: 1.0423 - acc: 0.4687 - val_loss: 1.0193 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 99s 10ms/step - loss: 1.0414 - acc: 0.4796 - val_loss: 1.0054 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 101s 10ms/step - loss: 1.0390 - acc: 0.4755 - val_loss: 1.0044 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0687 - acc: 0.4189 - val_loss: 1.0362 - val_acc: 0.4774\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0547 - acc: 0.4456 - val_loss: 1.0355 - val_acc: 0.4914\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0511 - acc: 0.4486 - val_loss: 1.0218 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0452 - acc: 0.4624 - val_loss: 1.0220 - val_acc: 0.4992\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0460 - acc: 0.4667 - val_loss: 1.0207 - val_acc: 0.5047\n",
      "1265/1265 [==============================] - 1s 535us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0686 - acc: 0.4281 - val_loss: 1.0208 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0524 - acc: 0.4515 - val_loss: 1.0186 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0494 - acc: 0.4550 - val_loss: 1.0140 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0430 - acc: 0.4720 - val_loss: 1.0149 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0433 - acc: 0.4695 - val_loss: 1.0129 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 1s 612us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0673 - acc: 0.4315 - val_loss: 1.0287 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0552 - acc: 0.4550 - val_loss: 1.0285 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0483 - acc: 0.4640 - val_loss: 1.0321 - val_acc: 0.4844\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0471 - acc: 0.4691 - val_loss: 1.0168 - val_acc: 0.5031\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0411 - acc: 0.4715 - val_loss: 1.0152 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 1s 707us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0671 - acc: 0.4304 - val_loss: 1.0365 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0522 - acc: 0.4541 - val_loss: 1.0235 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0445 - acc: 0.4617 - val_loss: 1.0170 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0454 - acc: 0.4663 - val_loss: 1.0176 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0411 - acc: 0.4750 - val_loss: 1.0142 - val_acc: 0.5023\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0685 - acc: 0.4273 - val_loss: 1.0259 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0523 - acc: 0.4603 - val_loss: 1.0161 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0453 - acc: 0.4646 - val_loss: 1.0131 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0422 - acc: 0.4752 - val_loss: 1.0170 - val_acc: 0.5031\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0390 - acc: 0.4803 - val_loss: 1.0063 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0644 - acc: 0.4409 - val_loss: 1.0232 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0505 - acc: 0.4605 - val_loss: 1.0166 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0434 - acc: 0.4666 - val_loss: 1.0172 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0438 - acc: 0.4753 - val_loss: 1.0074 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0373 - acc: 0.4777 - val_loss: 1.0067 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0701 - acc: 0.4283 - val_loss: 1.0201 - val_acc: 0.4961\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0545 - acc: 0.4583 - val_loss: 1.0190 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0470 - acc: 0.4662 - val_loss: 1.0135 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0396 - acc: 0.4787 - val_loss: 1.0074 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0378 - acc: 0.4845 - val_loss: 1.0050 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0678 - acc: 0.4297 - val_loss: 1.0253 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0539 - acc: 0.4580 - val_loss: 1.0171 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0458 - acc: 0.4717 - val_loss: 1.0122 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0409 - acc: 0.4735 - val_loss: 1.0068 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0355 - acc: 0.4829 - val_loss: 1.0084 - val_acc: 0.5055\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0721 - acc: 0.4319 - val_loss: 1.0230 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0509 - acc: 0.4596 - val_loss: 1.0232 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0440 - acc: 0.4772 - val_loss: 1.0105 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0397 - acc: 0.4790 - val_loss: 1.0064 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 36s 4ms/step - loss: 1.0373 - acc: 0.4782 - val_loss: 1.0063 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0650 - acc: 0.4338 - val_loss: 1.0261 - val_acc: 0.4945\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0497 - acc: 0.4638 - val_loss: 1.0183 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0446 - acc: 0.4725 - val_loss: 1.0099 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0379 - acc: 0.4778 - val_loss: 1.0112 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0370 - acc: 0.4816 - val_loss: 1.0082 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0669 - acc: 0.4282 - val_loss: 1.0235 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0497 - acc: 0.4613 - val_loss: 1.0219 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0452 - acc: 0.4689 - val_loss: 1.0194 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0406 - acc: 0.4785 - val_loss: 1.0077 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0352 - acc: 0.4823 - val_loss: 1.0062 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.0661 - acc: 0.4334 - val_loss: 1.0295 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0514 - acc: 0.4671 - val_loss: 1.0118 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0419 - acc: 0.4746 - val_loss: 1.0121 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0396 - acc: 0.4742 - val_loss: 1.0099 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0366 - acc: 0.4769 - val_loss: 1.0170 - val_acc: 0.5039\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.0752 - acc: 0.4235 - val_loss: 1.0297 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0561 - acc: 0.4519 - val_loss: 1.0171 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0433 - acc: 0.4692 - val_loss: 1.0110 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0402 - acc: 0.4732 - val_loss: 1.0102 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0350 - acc: 0.4796 - val_loss: 1.0086 - val_acc: 0.5055\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 84s 8ms/step - loss: 1.0672 - acc: 0.4335 - val_loss: 1.0230 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0521 - acc: 0.4574 - val_loss: 1.0145 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0447 - acc: 0.4691 - val_loss: 1.0133 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0386 - acc: 0.4820 - val_loss: 1.0104 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0334 - acc: 0.4866 - val_loss: 1.0017 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 85s 8ms/step - loss: 1.0719 - acc: 0.4228 - val_loss: 1.0231 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0543 - acc: 0.4571 - val_loss: 1.0164 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0427 - acc: 0.4744 - val_loss: 1.0112 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0371 - acc: 0.4777 - val_loss: 1.0075 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0357 - acc: 0.4803 - val_loss: 1.0107 - val_acc: 0.5031\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 86s 8ms/step - loss: 1.0661 - acc: 0.4384 - val_loss: 1.0297 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0509 - acc: 0.4579 - val_loss: 1.0248 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0427 - acc: 0.4660 - val_loss: 1.0276 - val_acc: 0.4969\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0377 - acc: 0.4799 - val_loss: 1.0098 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0359 - acc: 0.4823 - val_loss: 1.0065 - val_acc: 0.5070\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 91s 9ms/step - loss: 1.0708 - acc: 0.4248 - val_loss: 1.0283 - val_acc: 0.4945\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0523 - acc: 0.4507 - val_loss: 1.0170 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0450 - acc: 0.4706 - val_loss: 1.0180 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0417 - acc: 0.4715 - val_loss: 1.0052 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0378 - acc: 0.4792 - val_loss: 1.0042 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 101s 10ms/step - loss: 1.0664 - acc: 0.4273 - val_loss: 1.0294 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0555 - acc: 0.4500 - val_loss: 1.0201 - val_acc: 0.4992\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0474 - acc: 0.4619 - val_loss: 1.0235 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0440 - acc: 0.4716 - val_loss: 1.0129 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0362 - acc: 0.4857 - val_loss: 1.0059 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 98s 10ms/step - loss: 1.0674 - acc: 0.4321 - val_loss: 1.0293 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0545 - acc: 0.4518 - val_loss: 1.0265 - val_acc: 0.5125\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0467 - acc: 0.4590 - val_loss: 1.0154 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0439 - acc: 0.4744 - val_loss: 1.0146 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0404 - acc: 0.4732 - val_loss: 1.0091 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 102s 10ms/step - loss: 1.0660 - acc: 0.4313 - val_loss: 1.0307 - val_acc: 0.4836\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.0515 - acc: 0.4547 - val_loss: 1.0283 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0456 - acc: 0.4636 - val_loss: 1.0170 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0421 - acc: 0.4673 - val_loss: 1.0120 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0398 - acc: 0.4772 - val_loss: 1.0098 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 109s 11ms/step - loss: 1.0610 - acc: 0.4388 - val_loss: 1.0285 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.0496 - acc: 0.4535 - val_loss: 1.0186 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0466 - acc: 0.4630 - val_loss: 1.0116 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0403 - acc: 0.4776 - val_loss: 1.0077 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0367 - acc: 0.4818 - val_loss: 1.0157 - val_acc: 0.5047\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0634 - acc: 0.4278 - val_loss: 1.0326 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 79s 8ms/step - loss: 1.0531 - acc: 0.4515 - val_loss: 1.0214 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0443 - acc: 0.4668 - val_loss: 1.0127 - val_acc: 0.4977\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0413 - acc: 0.4716 - val_loss: 1.0113 - val_acc: 0.5280\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0367 - acc: 0.4792 - val_loss: 1.0047 - val_acc: 0.5312\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 111s 11ms/step - loss: 1.0640 - acc: 0.4210 - val_loss: 1.0410 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 79s 8ms/step - loss: 1.0516 - acc: 0.4524 - val_loss: 1.0194 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0466 - acc: 0.4595 - val_loss: 1.0247 - val_acc: 0.4992\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0425 - acc: 0.4647 - val_loss: 1.0153 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0380 - acc: 0.4786 - val_loss: 1.0122 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0707 - acc: 0.4211 - val_loss: 1.0367 - val_acc: 0.4961\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 86s 8ms/step - loss: 1.0543 - acc: 0.4443 - val_loss: 1.0301 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0483 - acc: 0.4553 - val_loss: 1.0233 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0445 - acc: 0.4629 - val_loss: 1.0216 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0417 - acc: 0.4705 - val_loss: 1.0127 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0629 - acc: 0.4353 - val_loss: 1.0302 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 87s 8ms/step - loss: 1.0510 - acc: 0.4586 - val_loss: 1.0192 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0476 - acc: 0.4619 - val_loss: 1.0207 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0418 - acc: 0.4687 - val_loss: 1.0126 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0391 - acc: 0.4762 - val_loss: 1.0110 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0651 - acc: 0.4287 - val_loss: 1.0342 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 89s 9ms/step - loss: 1.0530 - acc: 0.4499 - val_loss: 1.0169 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0492 - acc: 0.4589 - val_loss: 1.0103 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0407 - acc: 0.4707 - val_loss: 1.0093 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0387 - acc: 0.4750 - val_loss: 1.0105 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0662 - acc: 0.4285 - val_loss: 1.0296 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 95s 9ms/step - loss: 1.0506 - acc: 0.4523 - val_loss: 1.0232 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.0440 - acc: 0.4641 - val_loss: 1.0181 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.0420 - acc: 0.4697 - val_loss: 1.0086 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.0366 - acc: 0.4763 - val_loss: 1.0106 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0682 - acc: 0.4240 - val_loss: 1.0278 - val_acc: 0.4844\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 98s 10ms/step - loss: 1.0519 - acc: 0.4457 - val_loss: 1.0232 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.0479 - acc: 0.4555 - val_loss: 1.0178 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.0435 - acc: 0.4675 - val_loss: 1.0146 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.0397 - acc: 0.4680 - val_loss: 1.0095 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0659 - acc: 0.4279 - val_loss: 1.0295 - val_acc: 0.4868\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 108s 11ms/step - loss: 1.0537 - acc: 0.4485 - val_loss: 1.0211 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 80s 8ms/step - loss: 1.0468 - acc: 0.4589 - val_loss: 1.0136 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.0404 - acc: 0.4720 - val_loss: 1.0140 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.0406 - acc: 0.4709 - val_loss: 1.0115 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 11s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0656 - acc: 0.4237 - val_loss: 1.0281 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0532 - acc: 0.4464 - val_loss: 1.0202 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 86s 8ms/step - loss: 1.0474 - acc: 0.4610 - val_loss: 1.0165 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 84s 8ms/step - loss: 1.0398 - acc: 0.4723 - val_loss: 1.0166 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 87s 9ms/step - loss: 1.0365 - acc: 0.4801 - val_loss: 1.0067 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 150s 15ms/step - loss: 1.0651 - acc: 0.4300 - val_loss: 1.0258 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0533 - acc: 0.4531 - val_loss: 1.0182 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 93s 9ms/step - loss: 1.0469 - acc: 0.4667 - val_loss: 1.0113 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 91s 9ms/step - loss: 1.0407 - acc: 0.4712 - val_loss: 1.0071 - val_acc: 0.5023\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 99s 10ms/step - loss: 1.0377 - acc: 0.4786 - val_loss: 1.0071 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 12s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 36s 4ms/step - loss: 1.0633 - acc: 0.4263 - val_loss: 1.0267 - val_acc: 0.4930\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0542 - acc: 0.4516 - val_loss: 1.0236 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0498 - acc: 0.4578 - val_loss: 1.0179 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0461 - acc: 0.4607 - val_loss: 1.0135 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0444 - acc: 0.4611 - val_loss: 1.0143 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 1s 569us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0680 - acc: 0.4255 - val_loss: 1.0338 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0521 - acc: 0.4516 - val_loss: 1.0233 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0498 - acc: 0.4573 - val_loss: 1.0206 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0457 - acc: 0.4657 - val_loss: 1.0219 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0403 - acc: 0.4730 - val_loss: 1.0161 - val_acc: 0.5039\n",
      "1265/1265 [==============================] - 1s 646us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0663 - acc: 0.4296 - val_loss: 1.0284 - val_acc: 0.4945\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0552 - acc: 0.4507 - val_loss: 1.0217 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0461 - acc: 0.4707 - val_loss: 1.0172 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0440 - acc: 0.4708 - val_loss: 1.0121 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0387 - acc: 0.4799 - val_loss: 1.0109 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0642 - acc: 0.4280 - val_loss: 1.0265 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0491 - acc: 0.4596 - val_loss: 1.0289 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0466 - acc: 0.4663 - val_loss: 1.0181 - val_acc: 0.5016\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0434 - acc: 0.4694 - val_loss: 1.0177 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0414 - acc: 0.4734 - val_loss: 1.0084 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 1s 932us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0649 - acc: 0.4303 - val_loss: 1.0248 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0527 - acc: 0.4625 - val_loss: 1.0170 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0457 - acc: 0.4668 - val_loss: 1.0196 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0421 - acc: 0.4745 - val_loss: 1.0105 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0405 - acc: 0.4776 - val_loss: 1.0138 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0679 - acc: 0.4271 - val_loss: 1.0283 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0516 - acc: 0.4556 - val_loss: 1.0221 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0467 - acc: 0.4668 - val_loss: 1.0116 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0424 - acc: 0.4783 - val_loss: 1.0093 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0378 - acc: 0.4862 - val_loss: 1.0073 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - ETA:  - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0707 - acc: 0.4256 - val_loss: 1.0235 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0514 - acc: 0.4616 - val_loss: 1.0118 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0456 - acc: 0.4696 - val_loss: 1.0140 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0391 - acc: 0.4836 - val_loss: 1.0084 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0352 - acc: 0.4809 - val_loss: 1.0033 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0638 - acc: 0.4372 - val_loss: 1.0264 - val_acc: 0.4930\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0532 - acc: 0.4606 - val_loss: 1.0155 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0429 - acc: 0.4772 - val_loss: 1.0148 - val_acc: 0.4984\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0391 - acc: 0.4785 - val_loss: 1.0111 - val_acc: 0.5031\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0359 - acc: 0.4830 - val_loss: 1.0055 - val_acc: 0.5055\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0651 - acc: 0.4373 - val_loss: 1.0255 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0511 - acc: 0.4601 - val_loss: 1.0177 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0432 - acc: 0.4741 - val_loss: 1.0134 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0390 - acc: 0.4793 - val_loss: 1.0143 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0362 - acc: 0.4809 - val_loss: 1.0050 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 77s 7ms/step - loss: 1.0674 - acc: 0.4337 - val_loss: 1.0259 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0490 - acc: 0.4640 - val_loss: 1.0148 - val_acc: 0.4945\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0435 - acc: 0.4720 - val_loss: 1.0104 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0396 - acc: 0.4796 - val_loss: 1.0099 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0364 - acc: 0.4825 - val_loss: 1.0146 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 81s 8ms/step - loss: 1.0690 - acc: 0.4245 - val_loss: 1.0256 - val_acc: 0.4945\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0536 - acc: 0.4580 - val_loss: 1.0215 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0470 - acc: 0.4694 - val_loss: 1.0099 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0408 - acc: 0.4764 - val_loss: 1.0082 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0404 - acc: 0.4769 - val_loss: 1.0062 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 84s 8ms/step - loss: 1.0686 - acc: 0.4308 - val_loss: 1.0237 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0510 - acc: 0.4616 - val_loss: 1.0139 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0458 - acc: 0.4704 - val_loss: 1.0175 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0400 - acc: 0.4777 - val_loss: 1.0044 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0345 - acc: 0.4801 - val_loss: 1.0060 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 94s 9ms/step - loss: 1.0694 - acc: 0.4319 - val_loss: 1.0239 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0520 - acc: 0.4559 - val_loss: 1.0147 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0416 - acc: 0.4729 - val_loss: 1.0132 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0419 - acc: 0.4726 - val_loss: 1.0086 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0347 - acc: 0.4829 - val_loss: 1.0116 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 90s 9ms/step - loss: 1.0713 - acc: 0.4292 - val_loss: 1.0259 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0544 - acc: 0.4571 - val_loss: 1.0244 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0465 - acc: 0.4675 - val_loss: 1.0217 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0414 - acc: 0.4771 - val_loss: 1.0158 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0362 - acc: 0.4824 - val_loss: 1.0155 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 6s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 101s 10ms/step - loss: 1.0678 - acc: 0.4297 - val_loss: 1.0318 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0506 - acc: 0.4586 - val_loss: 1.0197 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0435 - acc: 0.4680 - val_loss: 1.0131 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0408 - acc: 0.4725 - val_loss: 1.0134 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0377 - acc: 0.4820 - val_loss: 1.0066 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 99s 10ms/step - loss: 1.0690 - acc: 0.4302 - val_loss: 1.0351 - val_acc: 0.5132\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0528 - acc: 0.4569 - val_loss: 1.0208 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0432 - acc: 0.4698 - val_loss: 1.0099 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0403 - acc: 0.4792 - val_loss: 1.0059 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0357 - acc: 0.4785 - val_loss: 1.0058 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 102s 10ms/step - loss: 1.0665 - acc: 0.4300 - val_loss: 1.0283 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0527 - acc: 0.4596 - val_loss: 1.0166 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0428 - acc: 0.4653 - val_loss: 1.0137 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0388 - acc: 0.4777 - val_loss: 1.0062 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0340 - acc: 0.4842 - val_loss: 1.0100 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 106s 10ms/step - loss: 1.0677 - acc: 0.4250 - val_loss: 1.0280 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0542 - acc: 0.4490 - val_loss: 1.0172 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0459 - acc: 0.4620 - val_loss: 1.0269 - val_acc: 0.4907\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0425 - acc: 0.4673 - val_loss: 1.0072 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0395 - acc: 0.4734 - val_loss: 1.0092 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0668 - acc: 0.4246 - val_loss: 1.0265 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0537 - acc: 0.4550 - val_loss: 1.0194 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0451 - acc: 0.4630 - val_loss: 1.0169 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0425 - acc: 0.4697 - val_loss: 1.0072 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0379 - acc: 0.4749 - val_loss: 1.0098 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0672 - acc: 0.4243 - val_loss: 1.0470 - val_acc: 0.4626\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0527 - acc: 0.4518 - val_loss: 1.0278 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0443 - acc: 0.4658 - val_loss: 1.0131 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0412 - acc: 0.4745 - val_loss: 1.0095 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0394 - acc: 0.4752 - val_loss: 1.0080 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0649 - acc: 0.4368 - val_loss: 1.0304 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.0515 - acc: 0.4518 - val_loss: 1.0297 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0459 - acc: 0.4647 - val_loss: 1.0223 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0431 - acc: 0.4724 - val_loss: 1.0241 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0367 - acc: 0.4815 - val_loss: 1.0060 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0674 - acc: 0.4225 - val_loss: 1.0271 - val_acc: 0.4930\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 85s 8ms/step - loss: 1.0548 - acc: 0.4478 - val_loss: 1.0214 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0458 - acc: 0.4607 - val_loss: 1.0132 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0417 - acc: 0.4690 - val_loss: 1.0094 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0374 - acc: 0.4739 - val_loss: 1.0051 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0670 - acc: 0.4215 - val_loss: 1.0323 - val_acc: 0.4930\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.0555 - acc: 0.4457 - val_loss: 1.0241 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0474 - acc: 0.4604 - val_loss: 1.0187 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0428 - acc: 0.4659 - val_loss: 1.0106 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0380 - acc: 0.4740 - val_loss: 1.0078 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0663 - acc: 0.4282 - val_loss: 1.0271 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 83s 8ms/step - loss: 1.0510 - acc: 0.4529 - val_loss: 1.0170 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0429 - acc: 0.4690 - val_loss: 1.0114 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0422 - acc: 0.4727 - val_loss: 1.0090 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0348 - acc: 0.4851 - val_loss: 1.0035 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 1.0652 - acc: 0.4317 - val_loss: 1.0236 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 90s 9ms/step - loss: 1.0503 - acc: 0.4541 - val_loss: 1.0197 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0427 - acc: 0.4672 - val_loss: 1.0082 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0400 - acc: 0.4739 - val_loss: 1.0063 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0376 - acc: 0.4745 - val_loss: 1.0083 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 137s 13ms/step - loss: 1.0666 - acc: 0.4308 - val_loss: 1.0309 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 94s 9ms/step - loss: 1.0539 - acc: 0.4533 - val_loss: 1.0186 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.0454 - acc: 0.4614 - val_loss: 1.0162 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.0430 - acc: 0.4647 - val_loss: 1.0091 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.0404 - acc: 0.4738 - val_loss: 1.0097 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0670 - acc: 0.4239 - val_loss: 1.0368 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 93s 9ms/step - loss: 1.0530 - acc: 0.4511 - val_loss: 1.0257 - val_acc: 0.4961\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 1.0475 - acc: 0.4609 - val_loss: 1.0197 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.0426 - acc: 0.4683 - val_loss: 1.0241 - val_acc: 0.4977\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.0391 - acc: 0.4716 - val_loss: 1.0175 - val_acc: 0.5055\n",
      "1265/1265 [==============================] - 12s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 146s 14ms/step - loss: 1.0676 - acc: 0.4246 - val_loss: 1.0301 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 100s 10ms/step - loss: 1.0520 - acc: 0.4550 - val_loss: 1.0174 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 1.0454 - acc: 0.4668 - val_loss: 1.0107 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 77s 7ms/step - loss: 1.0442 - acc: 0.4725 - val_loss: 1.0065 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.0378 - acc: 0.4801 - val_loss: 1.0079 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 152s 15ms/step - loss: 1.0661 - acc: 0.4282 - val_loss: 1.0280 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0556 - acc: 0.4466 - val_loss: 1.0253 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 84s 8ms/step - loss: 1.0483 - acc: 0.4644 - val_loss: 1.0160 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 80s 8ms/step - loss: 1.0420 - acc: 0.4687 - val_loss: 1.0152 - val_acc: 0.5140\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 80s 8ms/step - loss: 1.0406 - acc: 0.4734 - val_loss: 1.0064 - val_acc: 0.5055\n",
      "1265/1265 [==============================] - 12s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0641 - acc: 0.4269 - val_loss: 1.0335 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0511 - acc: 0.4468 - val_loss: 1.0205 - val_acc: 0.4961\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 91s 9ms/step - loss: 1.0475 - acc: 0.4570 - val_loss: 1.0149 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 87s 9ms/step - loss: 1.0423 - acc: 0.4685 - val_loss: 1.0136 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 87s 8ms/step - loss: 1.0387 - acc: 0.4758 - val_loss: 1.0096 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0627 - acc: 0.4311 - val_loss: 1.0303 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0510 - acc: 0.4531 - val_loss: 1.0260 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 93s 9ms/step - loss: 1.0455 - acc: 0.4634 - val_loss: 1.0142 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 87s 9ms/step - loss: 1.0401 - acc: 0.4682 - val_loss: 1.0164 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 100s 10ms/step - loss: 1.0395 - acc: 0.4747 - val_loss: 1.0056 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0691 - acc: 0.4230 - val_loss: 1.0280 - val_acc: 0.4961\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0535 - acc: 0.4493 - val_loss: 1.0219 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0520 - acc: 0.4515 - val_loss: 1.0174 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0463 - acc: 0.4602 - val_loss: 1.0167 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0430 - acc: 0.4684 - val_loss: 1.0140 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 1s 611us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0668 - acc: 0.4238 - val_loss: 1.0337 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0527 - acc: 0.4484 - val_loss: 1.0212 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0465 - acc: 0.4588 - val_loss: 1.0166 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0444 - acc: 0.4696 - val_loss: 1.0149 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0426 - acc: 0.4697 - val_loss: 1.0120 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 1s 695us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0672 - acc: 0.4325 - val_loss: 1.0261 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0539 - acc: 0.4514 - val_loss: 1.0253 - val_acc: 0.4930\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0498 - acc: 0.4608 - val_loss: 1.0204 - val_acc: 0.4992\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0445 - acc: 0.4690 - val_loss: 1.0157 - val_acc: 0.4961\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0428 - acc: 0.4731 - val_loss: 1.0189 - val_acc: 0.4992\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0647 - acc: 0.4352 - val_loss: 1.0200 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0492 - acc: 0.4586 - val_loss: 1.0128 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0425 - acc: 0.4703 - val_loss: 1.0212 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0423 - acc: 0.4737 - val_loss: 1.0176 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0399 - acc: 0.4807 - val_loss: 1.0157 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0663 - acc: 0.4313 - val_loss: 1.0275 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0548 - acc: 0.4501 - val_loss: 1.0221 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0455 - acc: 0.4695 - val_loss: 1.0237 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0419 - acc: 0.4735 - val_loss: 1.0129 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0379 - acc: 0.4796 - val_loss: 1.0101 - val_acc: 0.5039\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0701 - acc: 0.4296 - val_loss: 1.0213 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0548 - acc: 0.4511 - val_loss: 1.0171 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0479 - acc: 0.4649 - val_loss: 1.0141 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0399 - acc: 0.4856 - val_loss: 1.0075 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0369 - acc: 0.4795 - val_loss: 1.0187 - val_acc: 0.4961\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0664 - acc: 0.4289 - val_loss: 1.0231 - val_acc: 0.4961\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0508 - acc: 0.4615 - val_loss: 1.0161 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0433 - acc: 0.4715 - val_loss: 1.0169 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0403 - acc: 0.4778 - val_loss: 1.0149 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0379 - acc: 0.4830 - val_loss: 1.0084 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 84s 8ms/step - loss: 1.0693 - acc: 0.4300 - val_loss: 1.0232 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0484 - acc: 0.4634 - val_loss: 1.0177 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0446 - acc: 0.4700 - val_loss: 1.0167 - val_acc: 0.4992\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0386 - acc: 0.4774 - val_loss: 1.0131 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0364 - acc: 0.4842 - val_loss: 1.0126 - val_acc: 0.5023\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 81s 8ms/step - loss: 1.0659 - acc: 0.4314 - val_loss: 1.0280 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 35s 3ms/step - loss: 1.0510 - acc: 0.4565 - val_loss: 1.0194 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0443 - acc: 0.4664 - val_loss: 1.0123 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0414 - acc: 0.4755 - val_loss: 1.0165 - val_acc: 0.4984\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0395 - acc: 0.4788 - val_loss: 1.0093 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 89s 9ms/step - loss: 1.0664 - acc: 0.4353 - val_loss: 1.0240 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0496 - acc: 0.4636 - val_loss: 1.0159 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0445 - acc: 0.4724 - val_loss: 1.0100 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 36s 4ms/step - loss: 1.0379 - acc: 0.4794 - val_loss: 1.0073 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 36s 3ms/step - loss: 1.0353 - acc: 0.4817 - val_loss: 1.0024 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 96s 9ms/step - loss: 1.0683 - acc: 0.4376 - val_loss: 1.0199 - val_acc: 0.5164\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0507 - acc: 0.4665 - val_loss: 1.0137 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0423 - acc: 0.4801 - val_loss: 1.0099 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0379 - acc: 0.4788 - val_loss: 1.0066 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0361 - acc: 0.4854 - val_loss: 1.0087 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 102s 10ms/step - loss: 1.0670 - acc: 0.4428 - val_loss: 1.0289 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0519 - acc: 0.4607 - val_loss: 1.0178 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0442 - acc: 0.4712 - val_loss: 1.0135 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0401 - acc: 0.4749 - val_loss: 1.0056 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0356 - acc: 0.4887 - val_loss: 1.0060 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 102s 10ms/step - loss: 1.0663 - acc: 0.4323 - val_loss: 1.0246 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0531 - acc: 0.4615 - val_loss: 1.0144 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0439 - acc: 0.4666 - val_loss: 1.0097 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0386 - acc: 0.4797 - val_loss: 1.0047 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0349 - acc: 0.4801 - val_loss: 1.0028 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 102s 10ms/step - loss: 1.0674 - acc: 0.4307 - val_loss: 1.0293 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0496 - acc: 0.4618 - val_loss: 1.0187 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0428 - acc: 0.4743 - val_loss: 1.0165 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0371 - acc: 0.4806 - val_loss: 1.0094 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0355 - acc: 0.4855 - val_loss: 1.0030 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 102s 10ms/step - loss: 1.0722 - acc: 0.4192 - val_loss: 1.0280 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0549 - acc: 0.4525 - val_loss: 1.0190 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0448 - acc: 0.4675 - val_loss: 1.0140 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0408 - acc: 0.4735 - val_loss: 1.0185 - val_acc: 0.5008\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0382 - acc: 0.4753 - val_loss: 1.0041 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 6s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 109s 11ms/step - loss: 1.0694 - acc: 0.4285 - val_loss: 1.0231 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0514 - acc: 0.4567 - val_loss: 1.0181 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0433 - acc: 0.4699 - val_loss: 1.0104 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0394 - acc: 0.4716 - val_loss: 1.0127 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0372 - acc: 0.4771 - val_loss: 1.0043 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0659 - acc: 0.4284 - val_loss: 1.0277 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0539 - acc: 0.4509 - val_loss: 1.0159 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0462 - acc: 0.4590 - val_loss: 1.0133 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0424 - acc: 0.4746 - val_loss: 1.0076 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0375 - acc: 0.4798 - val_loss: 1.0041 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0623 - acc: 0.4376 - val_loss: 1.0241 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0548 - acc: 0.4553 - val_loss: 1.0153 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0435 - acc: 0.4681 - val_loss: 1.0138 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0386 - acc: 0.4749 - val_loss: 1.0060 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0357 - acc: 0.4789 - val_loss: 1.0028 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0668 - acc: 0.4327 - val_loss: 1.0239 - val_acc: 0.4961\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0514 - acc: 0.4549 - val_loss: 1.0234 - val_acc: 0.5078\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0414 - acc: 0.4688 - val_loss: 1.0107 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0408 - acc: 0.4745 - val_loss: 1.0058 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0356 - acc: 0.4835 - val_loss: 1.0080 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0655 - acc: 0.4343 - val_loss: 1.0218 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.0524 - acc: 0.4453 - val_loss: 1.0167 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0472 - acc: 0.4607 - val_loss: 1.0092 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0403 - acc: 0.4672 - val_loss: 1.0063 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0347 - acc: 0.4821 - val_loss: 1.0074 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0664 - acc: 0.4238 - val_loss: 1.0340 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 79s 8ms/step - loss: 1.0523 - acc: 0.4562 - val_loss: 1.0221 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0441 - acc: 0.4621 - val_loss: 1.0113 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0420 - acc: 0.4746 - val_loss: 1.0098 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0371 - acc: 0.4771 - val_loss: 1.0063 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 133s 13ms/step - loss: 1.0640 - acc: 0.4276 - val_loss: 1.0304 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0527 - acc: 0.4549 - val_loss: 1.0202 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0417 - acc: 0.4697 - val_loss: 1.0119 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0418 - acc: 0.4755 - val_loss: 1.0111 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0342 - acc: 0.4878 - val_loss: 1.0178 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0674 - acc: 0.4313 - val_loss: 1.0343 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 77s 8ms/step - loss: 1.0536 - acc: 0.4444 - val_loss: 1.0224 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0462 - acc: 0.4641 - val_loss: 1.0186 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0429 - acc: 0.4660 - val_loss: 1.0135 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0358 - acc: 0.4788 - val_loss: 1.0067 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0654 - acc: 0.4332 - val_loss: 1.0289 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 81s 8ms/step - loss: 1.0518 - acc: 0.4505 - val_loss: 1.0242 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0466 - acc: 0.4609 - val_loss: 1.0172 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0418 - acc: 0.4704 - val_loss: 1.0106 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0360 - acc: 0.4740 - val_loss: 1.0134 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 146s 14ms/step - loss: 1.0657 - acc: 0.4359 - val_loss: 1.0260 - val_acc: 0.4891\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 92s 9ms/step - loss: 1.0489 - acc: 0.4584 - val_loss: 1.0182 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0445 - acc: 0.4677 - val_loss: 1.0135 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0392 - acc: 0.4739 - val_loss: 1.0126 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0361 - acc: 0.4828 - val_loss: 1.0030 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 151s 15ms/step - loss: 1.0661 - acc: 0.4238 - val_loss: 1.0338 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 92s 9ms/step - loss: 1.0542 - acc: 0.4469 - val_loss: 1.0247 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.0454 - acc: 0.4650 - val_loss: 1.0147 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.0416 - acc: 0.4760 - val_loss: 1.0123 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.0380 - acc: 0.4788 - val_loss: 1.0117 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 179s 17ms/step - loss: 1.0653 - acc: 0.4304 - val_loss: 1.0296 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 96s 9ms/step - loss: 1.0507 - acc: 0.4515 - val_loss: 1.0169 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.0427 - acc: 0.4648 - val_loss: 1.0113 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 1.0401 - acc: 0.4779 - val_loss: 1.0059 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.0371 - acc: 0.4790 - val_loss: 1.0087 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0631 - acc: 0.4277 - val_loss: 1.0279 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 103s 10ms/step - loss: 1.0538 - acc: 0.4550 - val_loss: 1.0182 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.0449 - acc: 0.4671 - val_loss: 1.0116 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 77s 8ms/step - loss: 1.0414 - acc: 0.4674 - val_loss: 1.0163 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.0363 - acc: 0.4746 - val_loss: 1.0127 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 159s 15ms/step - loss: 1.0666 - acc: 0.4311 - val_loss: 1.0304 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 105s 10ms/step - loss: 1.0554 - acc: 0.4414 - val_loss: 1.0211 - val_acc: 0.4953\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 77s 8ms/step - loss: 1.0474 - acc: 0.4570 - val_loss: 1.0153 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.0417 - acc: 0.4663 - val_loss: 1.0084 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.0381 - acc: 0.4781 - val_loss: 1.0070 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0666 - acc: 0.4256 - val_loss: 1.0345 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0547 - acc: 0.4449 - val_loss: 1.0260 - val_acc: 0.4961\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 87s 9ms/step - loss: 1.0471 - acc: 0.4626 - val_loss: 1.0180 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 86s 8ms/step - loss: 1.0396 - acc: 0.4702 - val_loss: 1.0133 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 88s 9ms/step - loss: 1.0380 - acc: 0.4780 - val_loss: 1.0175 - val_acc: 0.5039\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "10235/10235 [==============================] - 88s 9ms/step - loss: 1.0468 - acc: 0.4624 - val_loss: 1.0121 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 97s 9ms/step - loss: 1.0389 - acc: 0.4700 - val_loss: 1.0072 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0663 - acc: 0.4211 - val_loss: 1.0241 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0560 - acc: 0.4448 - val_loss: 1.0226 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0489 - acc: 0.4579 - val_loss: 1.0175 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0460 - acc: 0.4646 - val_loss: 1.0151 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0443 - acc: 0.4663 - val_loss: 1.0104 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 1s 899us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0695 - acc: 0.4223 - val_loss: 1.0228 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0536 - acc: 0.4480 - val_loss: 1.0165 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0505 - acc: 0.4551 - val_loss: 1.0165 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0460 - acc: 0.4651 - val_loss: 1.0104 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0408 - acc: 0.4751 - val_loss: 1.0119 - val_acc: 0.5070\n",
      "1265/1265 [==============================] - 1s 776us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0663 - acc: 0.4273 - val_loss: 1.0270 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0542 - acc: 0.4552 - val_loss: 1.0207 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0453 - acc: 0.4675 - val_loss: 1.0183 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0436 - acc: 0.4634 - val_loss: 1.0168 - val_acc: 0.4984\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0432 - acc: 0.4750 - val_loss: 1.0141 - val_acc: 0.5023\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0647 - acc: 0.4339 - val_loss: 1.0256 - val_acc: 0.4945\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0510 - acc: 0.4553 - val_loss: 1.0192 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0469 - acc: 0.4631 - val_loss: 1.0216 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0438 - acc: 0.4737 - val_loss: 1.0128 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0378 - acc: 0.4764 - val_loss: 1.0117 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.0658 - acc: 0.4292 - val_loss: 1.0337 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0528 - acc: 0.4531 - val_loss: 1.0211 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0449 - acc: 0.4669 - val_loss: 1.0194 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0411 - acc: 0.4669 - val_loss: 1.0162 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0398 - acc: 0.4731 - val_loss: 1.0101 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.0656 - acc: 0.4273 - val_loss: 1.0360 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0535 - acc: 0.4509 - val_loss: 1.0168 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0449 - acc: 0.4719 - val_loss: 1.0142 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0401 - acc: 0.4782 - val_loss: 1.0135 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0417 - acc: 0.4765 - val_loss: 1.0114 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 81s 8ms/step - loss: 1.0679 - acc: 0.4281 - val_loss: 1.0231 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0514 - acc: 0.4619 - val_loss: 1.0176 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0451 - acc: 0.4670 - val_loss: 1.0205 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0369 - acc: 0.4810 - val_loss: 1.0115 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0368 - acc: 0.4832 - val_loss: 1.0097 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 97s 9ms/step - loss: 1.0642 - acc: 0.4329 - val_loss: 1.0259 - val_acc: 0.4891\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0525 - acc: 0.4568 - val_loss: 1.0169 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 33s 3ms/step - loss: 1.0442 - acc: 0.4711 - val_loss: 1.0119 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0395 - acc: 0.4787 - val_loss: 1.0177 - val_acc: 0.4930\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 34s 3ms/step - loss: 1.0346 - acc: 0.4841 - val_loss: 1.0110 - val_acc: 0.5047\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 103s 10ms/step - loss: 1.0661 - acc: 0.4373 - val_loss: 1.0258 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0522 - acc: 0.4619 - val_loss: 1.0178 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 36s 4ms/step - loss: 1.0445 - acc: 0.4766 - val_loss: 1.0127 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 36s 4ms/step - loss: 1.0385 - acc: 0.4753 - val_loss: 1.0071 - val_acc: 0.5047\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 36s 4ms/step - loss: 1.0345 - acc: 0.4884 - val_loss: 1.0080 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 103s 10ms/step - loss: 1.0666 - acc: 0.4277 - val_loss: 1.0225 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0471 - acc: 0.4655 - val_loss: 1.0131 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0434 - acc: 0.4729 - val_loss: 1.0177 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0377 - acc: 0.4797 - val_loss: 1.0060 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0352 - acc: 0.4830 - val_loss: 1.0124 - val_acc: 0.5008\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 106s 10ms/step - loss: 1.0676 - acc: 0.4260 - val_loss: 1.0328 - val_acc: 0.4899\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0500 - acc: 0.4627 - val_loss: 1.0149 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0446 - acc: 0.4721 - val_loss: 1.0111 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0391 - acc: 0.4787 - val_loss: 1.0108 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0357 - acc: 0.4841 - val_loss: 1.0113 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 107s 10ms/step - loss: 1.0655 - acc: 0.4277 - val_loss: 1.0320 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0527 - acc: 0.4567 - val_loss: 1.0179 - val_acc: 0.4992\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0447 - acc: 0.4710 - val_loss: 1.0148 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0375 - acc: 0.4823 - val_loss: 1.0117 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0368 - acc: 0.4797 - val_loss: 1.0084 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0646 - acc: 0.4313 - val_loss: 1.0271 - val_acc: 0.4891\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0518 - acc: 0.4549 - val_loss: 1.0174 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0424 - acc: 0.4728 - val_loss: 1.0153 - val_acc: 0.5016\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0406 - acc: 0.4784 - val_loss: 1.0097 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0338 - acc: 0.4848 - val_loss: 1.0052 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 6s 4ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0704 - acc: 0.4329 - val_loss: 1.0363 - val_acc: 0.4727\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0497 - acc: 0.4636 - val_loss: 1.0140 - val_acc: 0.5179\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0440 - acc: 0.4705 - val_loss: 1.0121 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0383 - acc: 0.4803 - val_loss: 1.0069 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0346 - acc: 0.4838 - val_loss: 1.0059 - val_acc: 0.5070\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0715 - acc: 0.4196 - val_loss: 1.0335 - val_acc: 0.5132\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0500 - acc: 0.4634 - val_loss: 1.0181 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0437 - acc: 0.4683 - val_loss: 1.0109 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0348 - acc: 0.4740 - val_loss: 1.0083 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0377 - acc: 0.4758 - val_loss: 1.0045 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0683 - acc: 0.4260 - val_loss: 1.0409 - val_acc: 0.4696\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0516 - acc: 0.4572 - val_loss: 1.0173 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0427 - acc: 0.4702 - val_loss: 1.0120 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0386 - acc: 0.4776 - val_loss: 1.0121 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0360 - acc: 0.4824 - val_loss: 1.0031 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0665 - acc: 0.4301 - val_loss: 1.0342 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0524 - acc: 0.4501 - val_loss: 1.0162 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0461 - acc: 0.4643 - val_loss: 1.0122 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0384 - acc: 0.4755 - val_loss: 1.0151 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0338 - acc: 0.4767 - val_loss: 1.0035 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 135s 13ms/step - loss: 1.0660 - acc: 0.4227 - val_loss: 1.0274 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0518 - acc: 0.4548 - val_loss: 1.0207 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0452 - acc: 0.4621 - val_loss: 1.0180 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0381 - acc: 0.4736 - val_loss: 1.0091 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0344 - acc: 0.4763 - val_loss: 1.0137 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0695 - acc: 0.4253 - val_loss: 1.0314 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.0536 - acc: 0.4534 - val_loss: 1.0193 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0425 - acc: 0.4718 - val_loss: 1.0131 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0417 - acc: 0.4699 - val_loss: 1.0073 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0373 - acc: 0.4780 - val_loss: 1.0044 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0638 - acc: 0.4384 - val_loss: 1.0259 - val_acc: 0.4930\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.0509 - acc: 0.4559 - val_loss: 1.0180 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0474 - acc: 0.4628 - val_loss: 1.0130 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0375 - acc: 0.4745 - val_loss: 1.0118 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0364 - acc: 0.4759 - val_loss: 1.0054 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0694 - acc: 0.4297 - val_loss: 1.0275 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.0521 - acc: 0.4516 - val_loss: 1.0271 - val_acc: 0.4930\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0480 - acc: 0.4602 - val_loss: 1.0174 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0381 - acc: 0.4781 - val_loss: 1.0107 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0365 - acc: 0.4800 - val_loss: 1.0070 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 152s 15ms/step - loss: 1.0693 - acc: 0.4265 - val_loss: 1.0302 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 84s 8ms/step - loss: 1.0544 - acc: 0.4448 - val_loss: 1.0224 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0482 - acc: 0.4588 - val_loss: 1.0138 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0421 - acc: 0.4695 - val_loss: 1.0190 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0385 - acc: 0.4743 - val_loss: 1.0178 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 165s 16ms/step - loss: 1.0672 - acc: 0.4345 - val_loss: 1.0309 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 87s 9ms/step - loss: 1.0519 - acc: 0.4467 - val_loss: 1.0275 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0447 - acc: 0.4663 - val_loss: 1.0137 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0396 - acc: 0.4685 - val_loss: 1.0089 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0373 - acc: 0.4827 - val_loss: 1.0129 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0655 - acc: 0.4276 - val_loss: 1.0300 - val_acc: 0.4836\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 88s 9ms/step - loss: 1.0563 - acc: 0.4448 - val_loss: 1.0244 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0466 - acc: 0.4574 - val_loss: 1.0236 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0434 - acc: 0.4665 - val_loss: 1.0117 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0382 - acc: 0.4801 - val_loss: 1.0142 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0638 - acc: 0.4265 - val_loss: 1.0289 - val_acc: 0.4868\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 101s 10ms/step - loss: 1.0546 - acc: 0.4460 - val_loss: 1.0234 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0447 - acc: 0.4652 - val_loss: 1.0198 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0429 - acc: 0.4729 - val_loss: 1.0126 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0392 - acc: 0.4725 - val_loss: 1.0070 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 186s 18ms/step - loss: 1.0645 - acc: 0.4300 - val_loss: 1.0300 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 101s 10ms/step - loss: 1.0547 - acc: 0.4447 - val_loss: 1.0199 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.0458 - acc: 0.4630 - val_loss: 1.0153 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.0414 - acc: 0.4737 - val_loss: 1.0147 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.0385 - acc: 0.4786 - val_loss: 1.0169 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0686 - acc: 0.4272 - val_loss: 1.0330 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 105s 10ms/step - loss: 1.0516 - acc: 0.4537 - val_loss: 1.0178 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 1.0475 - acc: 0.4609 - val_loss: 1.0136 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.0426 - acc: 0.4685 - val_loss: 1.0145 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.0381 - acc: 0.4758 - val_loss: 1.0102 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0643 - acc: 0.4328 - val_loss: 1.0300 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0540 - acc: 0.4470 - val_loss: 1.0198 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 83s 8ms/step - loss: 1.0475 - acc: 0.4537 - val_loss: 1.0122 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.0422 - acc: 0.4688 - val_loss: 1.0126 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 1.0391 - acc: 0.4737 - val_loss: 1.0079 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 12s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 186s 18ms/step - loss: 1.0665 - acc: 0.4240 - val_loss: 1.0385 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0533 - acc: 0.4437 - val_loss: 1.0182 - val_acc: 0.4914\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 83s 8ms/step - loss: 1.0433 - acc: 0.4628 - val_loss: 1.0204 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 82s 8ms/step - loss: 1.0418 - acc: 0.4668 - val_loss: 1.0095 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 83s 8ms/step - loss: 1.0386 - acc: 0.4701 - val_loss: 1.0066 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 12s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0664 - acc: 0.4291 - val_loss: 1.0408 - val_acc: 0.4759\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 111s 11ms/step - loss: 1.0527 - acc: 0.4565 - val_loss: 1.0192 - val_acc: 0.5016\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 87s 9ms/step - loss: 1.0464 - acc: 0.4638 - val_loss: 1.0158 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 84s 8ms/step - loss: 1.0408 - acc: 0.4721 - val_loss: 1.0101 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 82s 8ms/step - loss: 1.0381 - acc: 0.4797 - val_loss: 1.0091 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 185s 18ms/step - loss: 1.0663 - acc: 0.4155 - val_loss: 1.0333 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0545 - acc: 0.4381 - val_loss: 1.0271 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 95s 9ms/step - loss: 1.0486 - acc: 0.4590 - val_loss: 1.0217 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 92s 9ms/step - loss: 1.0440 - acc: 0.4680 - val_loss: 1.0127 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 89s 9ms/step - loss: 1.0381 - acc: 0.4726 - val_loss: 1.0073 - val_acc: 0.5047\n",
      "1265/1265 [==============================] - 12s 9ms/step\n",
      "CPU times: user 2d 5h 22min 2s, sys: 13h 41min 34s, total: 2d 19h 3min 36s\n",
      "Wall time: 16h 32min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "concatenated_flair = {\n",
    "    fold: [np.concatenate(np.array(statement)) for statement in flair[fold]['statement']]\n",
    "    for fold in flair.keys()\n",
    "}\n",
    "\n",
    "flair_rounds = [calculate_round(concatenated_flair) for round in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{5: 0.4901185771457763,\n",
       "  6: 0.49090909093265006,\n",
       "  7: 0.49644268777059475,\n",
       "  8: 0.4940711462686184,\n",
       "  9: 0.4885375494306738,\n",
       "  10: 0.4996047431065631,\n",
       "  11: 0.4956521739366026,\n",
       "  12: 0.49328063243462633,\n",
       "  13: 0.4877470359029506,\n",
       "  14: 0.4940711465748874,\n",
       "  15: 0.5090909094207372,\n",
       "  16: 0.505138340297895,\n",
       "  17: 0.5122529647567056,\n",
       "  18: 0.4940711466220057,\n",
       "  19: 0.4988142296259582,\n",
       "  20: 0.499604743412832,\n",
       "  21: 0.49802371574484783,\n",
       "  22: 0.505928854131887,\n",
       "  23: 0.5090909091615865,\n",
       "  24: 0.4940711462686184,\n",
       "  25: 0.49249011860063424,\n",
       "  26: 0.49644268777059475,\n",
       "  27: 0.49090909123891896,\n",
       "  28: 0.5035573125827925,\n",
       "  29: 0.5019762846556577,\n",
       "  30: 0.4988142296259582,\n",
       "  31: 0.48774703569091826,\n",
       "  32: 0.49723320195797405,\n",
       "  33: 0.507509881446484,\n",
       "  34: 0.49802371543857893,\n",
       "  35: 0.5098814232547293},\n",
       " {5: 0.488537549784061,\n",
       "  6: 0.4885375494306738,\n",
       "  7: 0.4940711462686184,\n",
       "  8: 0.49249011860063424,\n",
       "  9: 0.49169960507291105,\n",
       "  10: 0.4893280636180531,\n",
       "  11: 0.49802371543857893,\n",
       "  12: 0.4940711462686184,\n",
       "  13: 0.4924901189069032,\n",
       "  14: 0.49328063243462633,\n",
       "  15: 0.49486166040887947,\n",
       "  16: 0.4996047431065631,\n",
       "  17: 0.5122529647095873,\n",
       "  18: 0.49723320191085574,\n",
       "  19: 0.5035573122765236,\n",
       "  20: 0.5019762849619266,\n",
       "  21: 0.5019762849148083,\n",
       "  22: 0.49565217424287156,\n",
       "  23: 0.5035573122765236,\n",
       "  24: 0.5035573126299108,\n",
       "  25: 0.4948616601497288,\n",
       "  26: 0.5011857710808162,\n",
       "  27: 0.49169960476664215,\n",
       "  28: 0.49723320195797405,\n",
       "  29: 0.4996047431536814,\n",
       "  30: 0.4948616601026105,\n",
       "  31: 0.498814229272571,\n",
       "  32: 0.4956521739366026,\n",
       "  33: 0.4814229249247449,\n",
       "  34: 0.49644268812398196,\n",
       "  35: 0.5003952569405552},\n",
       " {5: 0.4885375494306738,\n",
       "  6: 0.4901185774520452,\n",
       "  7: 0.4940711462686184,\n",
       "  8: 0.48458498061410055,\n",
       "  9: 0.49644268777059475,\n",
       "  10: 0.4996047434599503,\n",
       "  11: 0.4940711466220057,\n",
       "  12: 0.49486166040887947,\n",
       "  13: 0.49169960507291105,\n",
       "  14: 0.49565217428998987,\n",
       "  15: 0.49565217424287156,\n",
       "  16: 0.5114624509227135,\n",
       "  17: 0.5043478264167846,\n",
       "  18: 0.4956521739837209,\n",
       "  19: 0.4885375497369427,\n",
       "  20: 0.5083003955867451,\n",
       "  21: 0.5011857711279345,\n",
       "  22: 0.5067193676596102,\n",
       "  23: 0.5019762849619266,\n",
       "  24: 0.4996047434599503,\n",
       "  25: 0.5027667987959187,\n",
       "  26: 0.49802371543857893,\n",
       "  27: 0.4996047434599503,\n",
       "  28: 0.4988142295788399,\n",
       "  29: 0.5059288538256181,\n",
       "  30: 0.5067193676596102,\n",
       "  31: 0.4980237154856972,\n",
       "  32: 0.49802371543857893,\n",
       "  33: 0.4893280635709348,\n",
       "  34: 0.49644268812398196,\n",
       "  35: 0.49486166040887947},\n",
       " {5: 0.4869565220689585,\n",
       "  6: 0.4893280632646659,\n",
       "  7: 0.49644268777059475,\n",
       "  8: 0.504347826157634,\n",
       "  9: 0.4822134391121242,\n",
       "  10: 0.498814229272571,\n",
       "  11: 0.49169960512002936,\n",
       "  12: 0.4940711462686184,\n",
       "  13: 0.49328063278801354,\n",
       "  14: 0.499604743412832,\n",
       "  15: 0.4996047431536814,\n",
       "  16: 0.507509881752753,\n",
       "  17: 0.5035573125827925,\n",
       "  18: 0.49011857740492687,\n",
       "  19: 0.49090909123891896,\n",
       "  20: 0.5043478264167846,\n",
       "  21: 0.5051383402507766,\n",
       "  22: 0.507509881752753,\n",
       "  23: 0.5090909094678555,\n",
       "  24: 0.5011857707745473,\n",
       "  25: 0.4996047431065631,\n",
       "  26: 0.4980237155799338,\n",
       "  27: 0.49644268777059475,\n",
       "  28: 0.4996047431536814,\n",
       "  29: 0.5035573126299108,\n",
       "  30: 0.4996047434599503,\n",
       "  31: 0.5019762849619266,\n",
       "  32: 0.49802371543857893,\n",
       "  33: 0.4948616601026105,\n",
       "  34: 0.5019762847498943,\n",
       "  35: 0.49802371579196614},\n",
       " {5: 0.48616600792869746,\n",
       "  6: 0.5011857711279345,\n",
       "  7: 0.5035573122765236,\n",
       "  8: 0.5043478261105157,\n",
       "  9: 0.4940711466220057,\n",
       "  10: 0.4885375494306738,\n",
       "  11: 0.4948616601026105,\n",
       "  12: 0.49644268812398196,\n",
       "  13: 0.4940711465748874,\n",
       "  14: 0.48458498061410055,\n",
       "  15: 0.5067193676124919,\n",
       "  16: 0.5083003955867451,\n",
       "  17: 0.4940711465748874,\n",
       "  18: 0.5003952572939424,\n",
       "  19: 0.4924901189069032,\n",
       "  20: 0.5051383402507766,\n",
       "  21: 0.5011857710808162,\n",
       "  22: 0.5027667984896498,\n",
       "  23: 0.5027667987488004,\n",
       "  24: 0.5027667987959187,\n",
       "  25: 0.5075098817998712,\n",
       "  26: 0.48932806335890244,\n",
       "  27: 0.4996047431536814,\n",
       "  28: 0.49249011864775255,\n",
       "  29: 0.4940711466220057,\n",
       "  30: 0.49169960486087877,\n",
       "  31: 0.5035573126299108,\n",
       "  32: 0.49169960512002936,\n",
       "  33: 0.49723320195797405,\n",
       "  34: 0.5003952572939424,\n",
       "  35: 0.4869565221160768}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flair_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Round 0",
         "type": "scatter",
         "uid": "19baac33-ae00-4d48-a7a1-674005b329a9",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.4901185771457763,
          0.49090909093265006,
          0.49644268777059475,
          0.4940711462686184,
          0.4885375494306738,
          0.4996047431065631,
          0.4956521739366026,
          0.49328063243462633,
          0.4877470359029506,
          0.4940711465748874,
          0.5090909094207372,
          0.505138340297895,
          0.5122529647567056,
          0.4940711466220057,
          0.4988142296259582,
          0.499604743412832,
          0.49802371574484783,
          0.505928854131887,
          0.5090909091615865,
          0.4940711462686184,
          0.49249011860063424,
          0.49644268777059475,
          0.49090909123891896,
          0.5035573125827925,
          0.5019762846556577,
          0.4988142296259582,
          0.48774703569091826,
          0.49723320195797405,
          0.507509881446484,
          0.49802371543857893,
          0.5098814232547293
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 1",
         "type": "scatter",
         "uid": "bb1d6ddd-07e2-41c4-be54-28d120787363",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.488537549784061,
          0.4885375494306738,
          0.4940711462686184,
          0.49249011860063424,
          0.49169960507291105,
          0.4893280636180531,
          0.49802371543857893,
          0.4940711462686184,
          0.4924901189069032,
          0.49328063243462633,
          0.49486166040887947,
          0.4996047431065631,
          0.5122529647095873,
          0.49723320191085574,
          0.5035573122765236,
          0.5019762849619266,
          0.5019762849148083,
          0.49565217424287156,
          0.5035573122765236,
          0.5035573126299108,
          0.4948616601497288,
          0.5011857710808162,
          0.49169960476664215,
          0.49723320195797405,
          0.4996047431536814,
          0.4948616601026105,
          0.498814229272571,
          0.4956521739366026,
          0.4814229249247449,
          0.49644268812398196,
          0.5003952569405552
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 2",
         "type": "scatter",
         "uid": "1820c310-cb2a-4e8b-b044-bede880ec652",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.4885375494306738,
          0.4901185774520452,
          0.4940711462686184,
          0.48458498061410055,
          0.49644268777059475,
          0.4996047434599503,
          0.4940711466220057,
          0.49486166040887947,
          0.49169960507291105,
          0.49565217428998987,
          0.49565217424287156,
          0.5114624509227135,
          0.5043478264167846,
          0.4956521739837209,
          0.4885375497369427,
          0.5083003955867451,
          0.5011857711279345,
          0.5067193676596102,
          0.5019762849619266,
          0.4996047434599503,
          0.5027667987959187,
          0.49802371543857893,
          0.4996047434599503,
          0.4988142295788399,
          0.5059288538256181,
          0.5067193676596102,
          0.4980237154856972,
          0.49802371543857893,
          0.4893280635709348,
          0.49644268812398196,
          0.49486166040887947
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 3",
         "type": "scatter",
         "uid": "465184a0-2fac-4b18-a930-eb3a83af0889",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.4869565220689585,
          0.4893280632646659,
          0.49644268777059475,
          0.504347826157634,
          0.4822134391121242,
          0.498814229272571,
          0.49169960512002936,
          0.4940711462686184,
          0.49328063278801354,
          0.499604743412832,
          0.4996047431536814,
          0.507509881752753,
          0.5035573125827925,
          0.49011857740492687,
          0.49090909123891896,
          0.5043478264167846,
          0.5051383402507766,
          0.507509881752753,
          0.5090909094678555,
          0.5011857707745473,
          0.4996047431065631,
          0.4980237155799338,
          0.49644268777059475,
          0.4996047431536814,
          0.5035573126299108,
          0.4996047434599503,
          0.5019762849619266,
          0.49802371543857893,
          0.4948616601026105,
          0.5019762847498943,
          0.49802371579196614
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 4",
         "type": "scatter",
         "uid": "f5522aba-8def-446d-a133-e9df15e62519",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.48616600792869746,
          0.5011857711279345,
          0.5035573122765236,
          0.5043478261105157,
          0.4940711466220057,
          0.4885375494306738,
          0.4948616601026105,
          0.49644268812398196,
          0.4940711465748874,
          0.48458498061410055,
          0.5067193676124919,
          0.5083003955867451,
          0.4940711465748874,
          0.5003952572939424,
          0.4924901189069032,
          0.5051383402507766,
          0.5011857710808162,
          0.5027667984896498,
          0.5027667987488004,
          0.5027667987959187,
          0.5075098817998712,
          0.48932806335890244,
          0.4996047431536814,
          0.49249011864775255,
          0.4940711466220057,
          0.49169960486087877,
          0.5035573126299108,
          0.49169960512002936,
          0.49723320195797405,
          0.5003952572939424,
          0.4869565221160768
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "Test set accuracy of padded Flair dataset with variable maximum lengths"
        }
       }
      },
      "text/html": [
       "<div id=\"2fca2ce3-d94f-4597-9d62-05d6fbecefe3\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"2fca2ce3-d94f-4597-9d62-05d6fbecefe3\")) {\n",
       "    Plotly.newPlot(\"2fca2ce3-d94f-4597-9d62-05d6fbecefe3\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4901185771457763, 0.49090909093265006, 0.49644268777059475, 0.4940711462686184, 0.4885375494306738, 0.4996047431065631, 0.4956521739366026, 0.49328063243462633, 0.4877470359029506, 0.4940711465748874, 0.5090909094207372, 0.505138340297895, 0.5122529647567056, 0.4940711466220057, 0.4988142296259582, 0.499604743412832, 0.49802371574484783, 0.505928854131887, 0.5090909091615865, 0.4940711462686184, 0.49249011860063424, 0.49644268777059475, 0.49090909123891896, 0.5035573125827925, 0.5019762846556577, 0.4988142296259582, 0.48774703569091826, 0.49723320195797405, 0.507509881446484, 0.49802371543857893, 0.5098814232547293], \"type\": \"scatter\", \"uid\": \"19baac33-ae00-4d48-a7a1-674005b329a9\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.488537549784061, 0.4885375494306738, 0.4940711462686184, 0.49249011860063424, 0.49169960507291105, 0.4893280636180531, 0.49802371543857893, 0.4940711462686184, 0.4924901189069032, 0.49328063243462633, 0.49486166040887947, 0.4996047431065631, 0.5122529647095873, 0.49723320191085574, 0.5035573122765236, 0.5019762849619266, 0.5019762849148083, 0.49565217424287156, 0.5035573122765236, 0.5035573126299108, 0.4948616601497288, 0.5011857710808162, 0.49169960476664215, 0.49723320195797405, 0.4996047431536814, 0.4948616601026105, 0.498814229272571, 0.4956521739366026, 0.4814229249247449, 0.49644268812398196, 0.5003952569405552], \"type\": \"scatter\", \"uid\": \"bb1d6ddd-07e2-41c4-be54-28d120787363\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4885375494306738, 0.4901185774520452, 0.4940711462686184, 0.48458498061410055, 0.49644268777059475, 0.4996047434599503, 0.4940711466220057, 0.49486166040887947, 0.49169960507291105, 0.49565217428998987, 0.49565217424287156, 0.5114624509227135, 0.5043478264167846, 0.4956521739837209, 0.4885375497369427, 0.5083003955867451, 0.5011857711279345, 0.5067193676596102, 0.5019762849619266, 0.4996047434599503, 0.5027667987959187, 0.49802371543857893, 0.4996047434599503, 0.4988142295788399, 0.5059288538256181, 0.5067193676596102, 0.4980237154856972, 0.49802371543857893, 0.4893280635709348, 0.49644268812398196, 0.49486166040887947], \"type\": \"scatter\", \"uid\": \"1820c310-cb2a-4e8b-b044-bede880ec652\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4869565220689585, 0.4893280632646659, 0.49644268777059475, 0.504347826157634, 0.4822134391121242, 0.498814229272571, 0.49169960512002936, 0.4940711462686184, 0.49328063278801354, 0.499604743412832, 0.4996047431536814, 0.507509881752753, 0.5035573125827925, 0.49011857740492687, 0.49090909123891896, 0.5043478264167846, 0.5051383402507766, 0.507509881752753, 0.5090909094678555, 0.5011857707745473, 0.4996047431065631, 0.4980237155799338, 0.49644268777059475, 0.4996047431536814, 0.5035573126299108, 0.4996047434599503, 0.5019762849619266, 0.49802371543857893, 0.4948616601026105, 0.5019762847498943, 0.49802371579196614], \"type\": \"scatter\", \"uid\": \"465184a0-2fac-4b18-a930-eb3a83af0889\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.48616600792869746, 0.5011857711279345, 0.5035573122765236, 0.5043478261105157, 0.4940711466220057, 0.4885375494306738, 0.4948616601026105, 0.49644268812398196, 0.4940711465748874, 0.48458498061410055, 0.5067193676124919, 0.5083003955867451, 0.4940711465748874, 0.5003952572939424, 0.4924901189069032, 0.5051383402507766, 0.5011857710808162, 0.5027667984896498, 0.5027667987488004, 0.5027667987959187, 0.5075098817998712, 0.48932806335890244, 0.4996047431536814, 0.49249011864775255, 0.4940711466220057, 0.49169960486087877, 0.5035573126299108, 0.49169960512002936, 0.49723320195797405, 0.5003952572939424, 0.4869565221160768], \"type\": \"scatter\", \"uid\": \"f5522aba-8def-446d-a133-e9df15e62519\"}], {\"title\": {\"text\": \"Test set accuracy of padded Flair dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"2fca2ce3-d94f-4597-9d62-05d6fbecefe3\")) {window._Plotly.Plots.resize(document.getElementById(\"2fca2ce3-d94f-4597-9d62-05d6fbecefe3\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"2fca2ce3-d94f-4597-9d62-05d6fbecefe3\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"2fca2ce3-d94f-4597-9d62-05d6fbecefe3\")) {\n",
       "    Plotly.newPlot(\"2fca2ce3-d94f-4597-9d62-05d6fbecefe3\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4901185771457763, 0.49090909093265006, 0.49644268777059475, 0.4940711462686184, 0.4885375494306738, 0.4996047431065631, 0.4956521739366026, 0.49328063243462633, 0.4877470359029506, 0.4940711465748874, 0.5090909094207372, 0.505138340297895, 0.5122529647567056, 0.4940711466220057, 0.4988142296259582, 0.499604743412832, 0.49802371574484783, 0.505928854131887, 0.5090909091615865, 0.4940711462686184, 0.49249011860063424, 0.49644268777059475, 0.49090909123891896, 0.5035573125827925, 0.5019762846556577, 0.4988142296259582, 0.48774703569091826, 0.49723320195797405, 0.507509881446484, 0.49802371543857893, 0.5098814232547293], \"type\": \"scatter\", \"uid\": \"19baac33-ae00-4d48-a7a1-674005b329a9\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.488537549784061, 0.4885375494306738, 0.4940711462686184, 0.49249011860063424, 0.49169960507291105, 0.4893280636180531, 0.49802371543857893, 0.4940711462686184, 0.4924901189069032, 0.49328063243462633, 0.49486166040887947, 0.4996047431065631, 0.5122529647095873, 0.49723320191085574, 0.5035573122765236, 0.5019762849619266, 0.5019762849148083, 0.49565217424287156, 0.5035573122765236, 0.5035573126299108, 0.4948616601497288, 0.5011857710808162, 0.49169960476664215, 0.49723320195797405, 0.4996047431536814, 0.4948616601026105, 0.498814229272571, 0.4956521739366026, 0.4814229249247449, 0.49644268812398196, 0.5003952569405552], \"type\": \"scatter\", \"uid\": \"bb1d6ddd-07e2-41c4-be54-28d120787363\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4885375494306738, 0.4901185774520452, 0.4940711462686184, 0.48458498061410055, 0.49644268777059475, 0.4996047434599503, 0.4940711466220057, 0.49486166040887947, 0.49169960507291105, 0.49565217428998987, 0.49565217424287156, 0.5114624509227135, 0.5043478264167846, 0.4956521739837209, 0.4885375497369427, 0.5083003955867451, 0.5011857711279345, 0.5067193676596102, 0.5019762849619266, 0.4996047434599503, 0.5027667987959187, 0.49802371543857893, 0.4996047434599503, 0.4988142295788399, 0.5059288538256181, 0.5067193676596102, 0.4980237154856972, 0.49802371543857893, 0.4893280635709348, 0.49644268812398196, 0.49486166040887947], \"type\": \"scatter\", \"uid\": \"1820c310-cb2a-4e8b-b044-bede880ec652\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4869565220689585, 0.4893280632646659, 0.49644268777059475, 0.504347826157634, 0.4822134391121242, 0.498814229272571, 0.49169960512002936, 0.4940711462686184, 0.49328063278801354, 0.499604743412832, 0.4996047431536814, 0.507509881752753, 0.5035573125827925, 0.49011857740492687, 0.49090909123891896, 0.5043478264167846, 0.5051383402507766, 0.507509881752753, 0.5090909094678555, 0.5011857707745473, 0.4996047431065631, 0.4980237155799338, 0.49644268777059475, 0.4996047431536814, 0.5035573126299108, 0.4996047434599503, 0.5019762849619266, 0.49802371543857893, 0.4948616601026105, 0.5019762847498943, 0.49802371579196614], \"type\": \"scatter\", \"uid\": \"465184a0-2fac-4b18-a930-eb3a83af0889\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.48616600792869746, 0.5011857711279345, 0.5035573122765236, 0.5043478261105157, 0.4940711466220057, 0.4885375494306738, 0.4948616601026105, 0.49644268812398196, 0.4940711465748874, 0.48458498061410055, 0.5067193676124919, 0.5083003955867451, 0.4940711465748874, 0.5003952572939424, 0.4924901189069032, 0.5051383402507766, 0.5011857710808162, 0.5027667984896498, 0.5027667987488004, 0.5027667987959187, 0.5075098817998712, 0.48932806335890244, 0.4996047431536814, 0.49249011864775255, 0.4940711466220057, 0.49169960486087877, 0.5035573126299108, 0.49169960512002936, 0.49723320195797405, 0.5003952572939424, 0.4869565221160768], \"type\": \"scatter\", \"uid\": \"f5522aba-8def-446d-a133-e9df15e62519\"}], {\"title\": {\"text\": \"Test set accuracy of padded Flair dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"2fca2ce3-d94f-4597-9d62-05d6fbecefe3\")) {window._Plotly.Plots.resize(document.getElementById(\"2fca2ce3-d94f-4597-9d62-05d6fbecefe3\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traces = flair_rounds\n",
    "\n",
    "# Create traces\n",
    "def create_scatter(counter):\n",
    "    acc_dict = traces[counter]\n",
    "    \n",
    "    return go.Scatter(\n",
    "        x = list(acc_dict.keys()),\n",
    "        y = list(acc_dict.values()),\n",
    "        mode = 'lines+markers',\n",
    "        name = 'Round ' + str(counter)\n",
    "    )\n",
    "\n",
    "trace_data = [create_scatter(trace) for trace in range(len(traces))]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'Test set accuracy of padded Flair dataset with variable maximum lengths',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = trace_data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = data.get_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.1045 - acc: 0.4055 - val_loss: 1.0437 - val_acc: 0.4494\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 8s 747us/step - loss: 1.0744 - acc: 0.4264 - val_loss: 1.0432 - val_acc: 0.4626\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 8s 736us/step - loss: 1.0680 - acc: 0.4327 - val_loss: 1.0300 - val_acc: 0.4907\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 8s 751us/step - loss: 1.0632 - acc: 0.4293 - val_loss: 1.0429 - val_acc: 0.4751\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 8s 783us/step - loss: 1.0601 - acc: 0.4325 - val_loss: 1.0306 - val_acc: 0.4875\n",
      "1265/1265 [==============================] - 0s 333us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.1024 - acc: 0.4074 - val_loss: 1.0434 - val_acc: 0.4509\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 8s 812us/step - loss: 1.0755 - acc: 0.4197 - val_loss: 1.0297 - val_acc: 0.4821\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 9s 841us/step - loss: 1.0666 - acc: 0.4333 - val_loss: 1.0347 - val_acc: 0.4860\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 9s 835us/step - loss: 1.0625 - acc: 0.4366 - val_loss: 1.0289 - val_acc: 0.4891\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 8s 816us/step - loss: 1.0590 - acc: 0.4435 - val_loss: 1.0269 - val_acc: 0.4899\n",
      "1265/1265 [==============================] - 0s 366us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.1142 - acc: 0.4003 - val_loss: 1.0351 - val_acc: 0.4735\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 9s 873us/step - loss: 1.0781 - acc: 0.4258 - val_loss: 1.0356 - val_acc: 0.4657\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 9s 881us/step - loss: 1.0663 - acc: 0.4374 - val_loss: 1.0300 - val_acc: 0.4821\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 9s 922us/step - loss: 1.0645 - acc: 0.4311 - val_loss: 1.0316 - val_acc: 0.4821\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 10s 933us/step - loss: 1.0578 - acc: 0.4424 - val_loss: 1.0349 - val_acc: 0.4681\n",
      "1265/1265 [==============================] - 1s 419us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.1180 - acc: 0.3976 - val_loss: 1.0369 - val_acc: 0.4751\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 10s 986us/step - loss: 1.0738 - acc: 0.4227 - val_loss: 1.0329 - val_acc: 0.4751\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 10s 1000us/step - loss: 1.0677 - acc: 0.4344 - val_loss: 1.0294 - val_acc: 0.4743\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 10s 957us/step - loss: 1.0589 - acc: 0.4380 - val_loss: 1.0352 - val_acc: 0.4743\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 10s 968us/step - loss: 1.0576 - acc: 0.4391 - val_loss: 1.0353 - val_acc: 0.4782\n",
      "1265/1265 [==============================] - 1s 447us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.1113 - acc: 0.4088 - val_loss: 1.0461 - val_acc: 0.4455\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0752 - acc: 0.4205 - val_loss: 1.0382 - val_acc: 0.4868\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0624 - acc: 0.4404 - val_loss: 1.0274 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 10s 1ms/step - loss: 1.0602 - acc: 0.4408 - val_loss: 1.0386 - val_acc: 0.4704\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 10s 995us/step - loss: 1.0571 - acc: 0.4443 - val_loss: 1.0320 - val_acc: 0.4977\n",
      "1265/1265 [==============================] - 1s 818us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.1047 - acc: 0.4098 - val_loss: 1.0291 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0732 - acc: 0.4362 - val_loss: 1.0337 - val_acc: 0.4883\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0604 - acc: 0.4368 - val_loss: 1.0406 - val_acc: 0.4805\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0593 - acc: 0.4396 - val_loss: 1.0278 - val_acc: 0.4891\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0517 - acc: 0.4545 - val_loss: 1.0349 - val_acc: 0.4657\n",
      "1265/1265 [==============================] - 1s 512us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.1114 - acc: 0.4057 - val_loss: 1.0338 - val_acc: 0.4782\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0726 - acc: 0.4343 - val_loss: 1.0416 - val_acc: 0.4369\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0597 - acc: 0.4495 - val_loss: 1.0291 - val_acc: 0.4852\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0571 - acc: 0.4445 - val_loss: 1.0270 - val_acc: 0.4922\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0523 - acc: 0.4513 - val_loss: 1.0258 - val_acc: 0.4961\n",
      "1265/1265 [==============================] - 1s 514us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.1156 - acc: 0.4034 - val_loss: 1.0353 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0713 - acc: 0.4351 - val_loss: 1.0324 - val_acc: 0.4766\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0622 - acc: 0.4421 - val_loss: 1.0279 - val_acc: 0.4813\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0545 - acc: 0.4525 - val_loss: 1.0286 - val_acc: 0.4938\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0502 - acc: 0.4534 - val_loss: 1.0261 - val_acc: 0.4868\n",
      "1265/1265 [==============================] - 1s 548us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.1186 - acc: 0.4036 - val_loss: 1.0405 - val_acc: 0.4572\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0704 - acc: 0.4384 - val_loss: 1.0316 - val_acc: 0.4891\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0624 - acc: 0.4392 - val_loss: 1.0281 - val_acc: 0.4907\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0548 - acc: 0.4533 - val_loss: 1.0272 - val_acc: 0.4860\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0495 - acc: 0.4558 - val_loss: 1.0237 - val_acc: 0.4969\n",
      "1265/1265 [==============================] - 1s 598us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.1070 - acc: 0.4123 - val_loss: 1.0238 - val_acc: 0.4868\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0721 - acc: 0.4379 - val_loss: 1.0235 - val_acc: 0.4899\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0595 - acc: 0.4443 - val_loss: 1.0181 - val_acc: 0.4922\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0555 - acc: 0.4495 - val_loss: 1.0193 - val_acc: 0.4899\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0532 - acc: 0.4568 - val_loss: 1.0289 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 1s 625us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.1087 - acc: 0.4096 - val_loss: 1.0274 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0680 - acc: 0.4362 - val_loss: 1.0317 - val_acc: 0.4992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0625 - acc: 0.4471 - val_loss: 1.0280 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0551 - acc: 0.4475 - val_loss: 1.0202 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0533 - acc: 0.4467 - val_loss: 1.0230 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 1s 644us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0975 - acc: 0.4228 - val_loss: 1.0316 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0672 - acc: 0.4352 - val_loss: 1.0269 - val_acc: 0.4953\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0612 - acc: 0.4452 - val_loss: 1.0370 - val_acc: 0.4836\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0536 - acc: 0.4552 - val_loss: 1.0266 - val_acc: 0.4961\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0499 - acc: 0.4529 - val_loss: 1.0199 - val_acc: 0.4992\n",
      "1265/1265 [==============================] - 1s 670us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.1122 - acc: 0.4004 - val_loss: 1.0300 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0690 - acc: 0.4342 - val_loss: 1.0297 - val_acc: 0.4961\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0594 - acc: 0.4428 - val_loss: 1.0298 - val_acc: 0.4961\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0556 - acc: 0.4524 - val_loss: 1.0257 - val_acc: 0.4977\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 2ms/step - loss: 1.0522 - acc: 0.4503 - val_loss: 1.0261 - val_acc: 0.4977\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.1010 - acc: 0.4116 - val_loss: 1.0319 - val_acc: 0.4836\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0704 - acc: 0.4414 - val_loss: 1.0210 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0579 - acc: 0.4449 - val_loss: 1.0262 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0508 - acc: 0.4540 - val_loss: 1.0218 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0495 - acc: 0.4535 - val_loss: 1.0211 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.1027 - acc: 0.4107 - val_loss: 1.0292 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0714 - acc: 0.4298 - val_loss: 1.0333 - val_acc: 0.4883\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0640 - acc: 0.4409 - val_loss: 1.0358 - val_acc: 0.4914\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0577 - acc: 0.4428 - val_loss: 1.0249 - val_acc: 0.4829\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0519 - acc: 0.4542 - val_loss: 1.0297 - val_acc: 0.4891\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.1039 - acc: 0.4086 - val_loss: 1.0385 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0686 - acc: 0.4356 - val_loss: 1.0281 - val_acc: 0.4961\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0598 - acc: 0.4459 - val_loss: 1.0252 - val_acc: 0.4969\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0558 - acc: 0.4501 - val_loss: 1.0208 - val_acc: 0.4930\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0502 - acc: 0.4540 - val_loss: 1.0236 - val_acc: 0.5008\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0982 - acc: 0.4169 - val_loss: 1.0469 - val_acc: 0.4743\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0687 - acc: 0.4366 - val_loss: 1.0287 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0593 - acc: 0.4435 - val_loss: 1.0372 - val_acc: 0.4782\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0533 - acc: 0.4510 - val_loss: 1.0256 - val_acc: 0.4868\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0487 - acc: 0.4585 - val_loss: 1.0197 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.1008 - acc: 0.4085 - val_loss: 1.0389 - val_acc: 0.4743\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0701 - acc: 0.4327 - val_loss: 1.0349 - val_acc: 0.4852\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0593 - acc: 0.4464 - val_loss: 1.0302 - val_acc: 0.4860\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0550 - acc: 0.4461 - val_loss: 1.0282 - val_acc: 0.4883\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0526 - acc: 0.4552 - val_loss: 1.0265 - val_acc: 0.5000\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.0998 - acc: 0.4115 - val_loss: 1.0340 - val_acc: 0.4727\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0697 - acc: 0.4336 - val_loss: 1.0244 - val_acc: 0.4922\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0600 - acc: 0.4407 - val_loss: 1.0255 - val_acc: 0.4969\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0600 - acc: 0.4427 - val_loss: 1.0271 - val_acc: 0.4914\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0505 - acc: 0.4586 - val_loss: 1.0226 - val_acc: 0.4977\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 1.0927 - acc: 0.4181 - val_loss: 1.0395 - val_acc: 0.4657\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0656 - acc: 0.4412 - val_loss: 1.0306 - val_acc: 0.4868\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0598 - acc: 0.4433 - val_loss: 1.0224 - val_acc: 0.4907\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0555 - acc: 0.4423 - val_loss: 1.0285 - val_acc: 0.4875\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0515 - acc: 0.4537 - val_loss: 1.0242 - val_acc: 0.4930\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.0946 - acc: 0.4096 - val_loss: 1.0366 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0679 - acc: 0.4288 - val_loss: 1.0434 - val_acc: 0.4470\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0600 - acc: 0.4380 - val_loss: 1.0254 - val_acc: 0.4844\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0514 - acc: 0.4522 - val_loss: 1.0247 - val_acc: 0.4930\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0519 - acc: 0.4522 - val_loss: 1.0268 - val_acc: 0.5055\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.0969 - acc: 0.4119 - val_loss: 1.0385 - val_acc: 0.4727\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0663 - acc: 0.4358 - val_loss: 1.0331 - val_acc: 0.4813\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0567 - acc: 0.4447 - val_loss: 1.0340 - val_acc: 0.4868\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0527 - acc: 0.4453 - val_loss: 1.0324 - val_acc: 0.4891\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0509 - acc: 0.4553 - val_loss: 1.0311 - val_acc: 0.4930\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.0974 - acc: 0.4032 - val_loss: 1.0349 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0631 - acc: 0.4366 - val_loss: 1.0361 - val_acc: 0.4852\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0607 - acc: 0.4407 - val_loss: 1.0362 - val_acc: 0.4759\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0560 - acc: 0.4408 - val_loss: 1.0394 - val_acc: 0.4704\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0543 - acc: 0.4458 - val_loss: 1.0298 - val_acc: 0.4860\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 81s 8ms/step - loss: 1.0911 - acc: 0.4171 - val_loss: 1.0318 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0650 - acc: 0.4312 - val_loss: 1.0259 - val_acc: 0.4914\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0560 - acc: 0.4508 - val_loss: 1.0231 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0509 - acc: 0.4538 - val_loss: 1.0260 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0507 - acc: 0.4534 - val_loss: 1.0202 - val_acc: 0.4969\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 83s 8ms/step - loss: 1.0906 - acc: 0.4210 - val_loss: 1.0315 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0673 - acc: 0.4309 - val_loss: 1.0311 - val_acc: 0.4844\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0613 - acc: 0.4360 - val_loss: 1.0279 - val_acc: 0.4829\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0586 - acc: 0.4396 - val_loss: 1.0303 - val_acc: 0.4852\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0537 - acc: 0.4526 - val_loss: 1.0388 - val_acc: 0.4805\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 89s 9ms/step - loss: 1.0916 - acc: 0.4054 - val_loss: 1.0323 - val_acc: 0.4868\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0685 - acc: 0.4249 - val_loss: 1.0300 - val_acc: 0.4813\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0614 - acc: 0.4317 - val_loss: 1.0332 - val_acc: 0.4766\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0569 - acc: 0.4443 - val_loss: 1.0317 - val_acc: 0.4813\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0519 - acc: 0.4489 - val_loss: 1.0351 - val_acc: 0.4883\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 84s 8ms/step - loss: 1.0867 - acc: 0.4125 - val_loss: 1.0322 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0670 - acc: 0.4268 - val_loss: 1.0371 - val_acc: 0.4829\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0593 - acc: 0.4386 - val_loss: 1.0285 - val_acc: 0.4782\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0540 - acc: 0.4443 - val_loss: 1.0301 - val_acc: 0.4930\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0505 - acc: 0.4544 - val_loss: 1.0257 - val_acc: 0.4938\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 93s 9ms/step - loss: 1.0893 - acc: 0.4156 - val_loss: 1.0340 - val_acc: 0.4696\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0626 - acc: 0.4321 - val_loss: 1.0333 - val_acc: 0.4790\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0570 - acc: 0.4412 - val_loss: 1.0333 - val_acc: 0.4813\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0553 - acc: 0.4460 - val_loss: 1.0267 - val_acc: 0.4875\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0522 - acc: 0.4458 - val_loss: 1.0263 - val_acc: 0.4945\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 94s 9ms/step - loss: 1.0879 - acc: 0.4039 - val_loss: 1.0385 - val_acc: 0.4766\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0644 - acc: 0.4323 - val_loss: 1.0313 - val_acc: 0.4829\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0588 - acc: 0.4360 - val_loss: 1.0304 - val_acc: 0.4860\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0527 - acc: 0.4496 - val_loss: 1.0313 - val_acc: 0.4977\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0527 - acc: 0.4508 - val_loss: 1.0257 - val_acc: 0.4891\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 95s 9ms/step - loss: 1.0907 - acc: 0.4110 - val_loss: 1.0354 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0623 - acc: 0.4268 - val_loss: 1.0346 - val_acc: 0.4821\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0577 - acc: 0.4431 - val_loss: 1.0306 - val_acc: 0.4844\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0531 - acc: 0.4481 - val_loss: 1.0264 - val_acc: 0.4922\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0521 - acc: 0.4517 - val_loss: 1.0229 - val_acc: 0.4930\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 94s 9ms/step - loss: 1.0908 - acc: 0.4104 - val_loss: 1.0334 - val_acc: 0.4704\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0691 - acc: 0.4280 - val_loss: 1.0327 - val_acc: 0.4688\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0614 - acc: 0.4369 - val_loss: 1.0367 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0571 - acc: 0.4415 - val_loss: 1.0283 - val_acc: 0.4727\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0534 - acc: 0.4474 - val_loss: 1.0353 - val_acc: 0.4782\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.1089 - acc: 0.4056 - val_loss: 1.0392 - val_acc: 0.4798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 8s 824us/step - loss: 1.0726 - acc: 0.4211 - val_loss: 1.0380 - val_acc: 0.4727\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 8s 829us/step - loss: 1.0675 - acc: 0.4277 - val_loss: 1.0458 - val_acc: 0.4626\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 8s 830us/step - loss: 1.0635 - acc: 0.4319 - val_loss: 1.0320 - val_acc: 0.4829\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 8s 827us/step - loss: 1.0623 - acc: 0.4319 - val_loss: 1.0313 - val_acc: 0.4829\n",
      "1265/1265 [==============================] - 0s 363us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.1211 - acc: 0.3879 - val_loss: 1.0425 - val_acc: 0.4603\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 9s 887us/step - loss: 1.0813 - acc: 0.4159 - val_loss: 1.0401 - val_acc: 0.4665\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 9s 880us/step - loss: 1.0716 - acc: 0.4210 - val_loss: 1.0371 - val_acc: 0.4953\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 9s 906us/step - loss: 1.0641 - acc: 0.4310 - val_loss: 1.0365 - val_acc: 0.4805\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 9s 889us/step - loss: 1.0606 - acc: 0.4352 - val_loss: 1.0308 - val_acc: 0.4953\n",
      "1265/1265 [==============================] - 1s 398us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.1105 - acc: 0.4022 - val_loss: 1.0442 - val_acc: 0.4766\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 9s 905us/step - loss: 1.0766 - acc: 0.4302 - val_loss: 1.0361 - val_acc: 0.4720\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 10s 953us/step - loss: 1.0662 - acc: 0.4347 - val_loss: 1.0329 - val_acc: 0.4759\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 10s 950us/step - loss: 1.0614 - acc: 0.4449 - val_loss: 1.0364 - val_acc: 0.4712\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 10s 956us/step - loss: 1.0588 - acc: 0.4400 - val_loss: 1.0316 - val_acc: 0.4844\n",
      "1265/1265 [==============================] - 1s 431us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.1121 - acc: 0.4015 - val_loss: 1.0342 - val_acc: 0.4759\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 10s 1ms/step - loss: 1.0724 - acc: 0.4315 - val_loss: 1.0405 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 10s 1ms/step - loss: 1.0632 - acc: 0.4395 - val_loss: 1.0295 - val_acc: 0.4751\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 10s 1ms/step - loss: 1.0596 - acc: 0.4497 - val_loss: 1.0317 - val_acc: 0.4696\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 10s 1ms/step - loss: 1.0554 - acc: 0.4397 - val_loss: 1.0337 - val_acc: 0.4805\n",
      "1265/1265 [==============================] - 1s 461us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.1052 - acc: 0.4091 - val_loss: 1.0439 - val_acc: 0.4455\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0739 - acc: 0.4301 - val_loss: 1.0321 - val_acc: 0.4945\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0652 - acc: 0.4346 - val_loss: 1.0426 - val_acc: 0.4470\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0585 - acc: 0.4416 - val_loss: 1.0244 - val_acc: 0.4883\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0571 - acc: 0.4450 - val_loss: 1.0280 - val_acc: 0.4922\n",
      "1265/1265 [==============================] - 1s 503us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.1089 - acc: 0.4066 - val_loss: 1.0329 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0707 - acc: 0.4328 - val_loss: 1.0278 - val_acc: 0.4914\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0643 - acc: 0.4426 - val_loss: 1.0289 - val_acc: 0.4883\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0573 - acc: 0.4476 - val_loss: 1.0262 - val_acc: 0.4922\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0523 - acc: 0.4547 - val_loss: 1.0272 - val_acc: 0.4914\n",
      "1265/1265 [==============================] - 1s 544us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.1098 - acc: 0.4075 - val_loss: 1.0279 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0725 - acc: 0.4313 - val_loss: 1.0256 - val_acc: 0.4977\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0628 - acc: 0.4444 - val_loss: 1.0344 - val_acc: 0.4743\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0559 - acc: 0.4430 - val_loss: 1.0276 - val_acc: 0.4907\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0502 - acc: 0.4547 - val_loss: 1.0202 - val_acc: 0.4891\n",
      "1265/1265 [==============================] - 1s 539us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.1124 - acc: 0.4058 - val_loss: 1.0358 - val_acc: 0.4657\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0715 - acc: 0.4361 - val_loss: 1.0347 - val_acc: 0.4673\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0613 - acc: 0.4494 - val_loss: 1.0307 - val_acc: 0.4790\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0566 - acc: 0.4484 - val_loss: 1.0302 - val_acc: 0.4751\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0559 - acc: 0.4536 - val_loss: 1.0342 - val_acc: 0.4774\n",
      "1265/1265 [==============================] - 1s 606us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.1092 - acc: 0.4110 - val_loss: 1.0303 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0696 - acc: 0.4359 - val_loss: 1.0286 - val_acc: 0.4907\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0579 - acc: 0.4530 - val_loss: 1.0289 - val_acc: 0.4836\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0564 - acc: 0.4491 - val_loss: 1.0264 - val_acc: 0.4930\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0516 - acc: 0.4493 - val_loss: 1.0233 - val_acc: 0.4977\n",
      "1265/1265 [==============================] - 1s 628us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.1065 - acc: 0.4118 - val_loss: 1.0303 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0666 - acc: 0.4405 - val_loss: 1.0213 - val_acc: 0.4875\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0601 - acc: 0.4433 - val_loss: 1.0257 - val_acc: 0.4922\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0548 - acc: 0.4502 - val_loss: 1.0223 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0545 - acc: 0.4490 - val_loss: 1.0248 - val_acc: 0.5055\n",
      "1265/1265 [==============================] - 1s 672us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.1098 - acc: 0.4139 - val_loss: 1.0252 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0755 - acc: 0.4335 - val_loss: 1.0288 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0596 - acc: 0.4461 - val_loss: 1.0282 - val_acc: 0.4992\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0563 - acc: 0.4489 - val_loss: 1.0266 - val_acc: 0.4977\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0537 - acc: 0.4503 - val_loss: 1.0226 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 1s 664us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.1040 - acc: 0.4163 - val_loss: 1.0331 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0748 - acc: 0.4312 - val_loss: 1.0293 - val_acc: 0.4977\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0630 - acc: 0.4433 - val_loss: 1.0261 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0554 - acc: 0.4463 - val_loss: 1.0309 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0524 - acc: 0.4536 - val_loss: 1.0249 - val_acc: 0.4961\n",
      "1265/1265 [==============================] - 1s 730us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0952 - acc: 0.4221 - val_loss: 1.0257 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0694 - acc: 0.4413 - val_loss: 1.0289 - val_acc: 0.4961\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0597 - acc: 0.4428 - val_loss: 1.0307 - val_acc: 0.4945\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0566 - acc: 0.4447 - val_loss: 1.0278 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0507 - acc: 0.4545 - val_loss: 1.0236 - val_acc: 0.4953\n",
      "1265/1265 [==============================] - 1s 769us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.1099 - acc: 0.4068 - val_loss: 1.0319 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0690 - acc: 0.4373 - val_loss: 1.0250 - val_acc: 0.4914\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0619 - acc: 0.4473 - val_loss: 1.0317 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0568 - acc: 0.4475 - val_loss: 1.0232 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0529 - acc: 0.4498 - val_loss: 1.0208 - val_acc: 0.4945\n",
      "1265/1265 [==============================] - 1s 824us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.1007 - acc: 0.4174 - val_loss: 1.0385 - val_acc: 0.4665\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0680 - acc: 0.4347 - val_loss: 1.0274 - val_acc: 0.4883\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0579 - acc: 0.4471 - val_loss: 1.0272 - val_acc: 0.4953\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0560 - acc: 0.4523 - val_loss: 1.0233 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0517 - acc: 0.4511 - val_loss: 1.0247 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 1s 815us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.1002 - acc: 0.4129 - val_loss: 1.0291 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0663 - acc: 0.4397 - val_loss: 1.0297 - val_acc: 0.4860\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0592 - acc: 0.4439 - val_loss: 1.0256 - val_acc: 0.4938\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0535 - acc: 0.4512 - val_loss: 1.0238 - val_acc: 0.4945\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0489 - acc: 0.4552 - val_loss: 1.0223 - val_acc: 0.4984\n",
      "1265/1265 [==============================] - 1s 939us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 83s 8ms/step - loss: 1.1000 - acc: 0.4195 - val_loss: 1.0301 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0694 - acc: 0.4324 - val_loss: 1.0411 - val_acc: 0.4579\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0631 - acc: 0.4413 - val_loss: 1.0373 - val_acc: 0.4681\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0560 - acc: 0.4457 - val_loss: 1.0325 - val_acc: 0.4735\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0530 - acc: 0.4525 - val_loss: 1.0240 - val_acc: 0.4938\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 85s 8ms/step - loss: 1.0944 - acc: 0.4146 - val_loss: 1.0393 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0678 - acc: 0.4325 - val_loss: 1.0377 - val_acc: 0.4891\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0603 - acc: 0.4426 - val_loss: 1.0321 - val_acc: 0.4782\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0548 - acc: 0.4501 - val_loss: 1.0274 - val_acc: 0.5000\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0510 - acc: 0.4553 - val_loss: 1.0236 - val_acc: 0.4875\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 86s 8ms/step - loss: 1.0969 - acc: 0.4117 - val_loss: 1.0361 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0675 - acc: 0.4322 - val_loss: 1.0271 - val_acc: 0.4868\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0599 - acc: 0.4398 - val_loss: 1.0249 - val_acc: 0.4860\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0557 - acc: 0.4471 - val_loss: 1.0250 - val_acc: 0.4891\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0505 - acc: 0.4525 - val_loss: 1.0244 - val_acc: 0.4883\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 89s 9ms/step - loss: 1.0988 - acc: 0.4150 - val_loss: 1.0332 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0671 - acc: 0.4307 - val_loss: 1.0307 - val_acc: 0.4914\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0613 - acc: 0.4437 - val_loss: 1.0211 - val_acc: 0.4860\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0576 - acc: 0.4409 - val_loss: 1.0225 - val_acc: 0.4977\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0537 - acc: 0.4474 - val_loss: 1.0216 - val_acc: 0.5000\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 88s 9ms/step - loss: 1.0967 - acc: 0.4081 - val_loss: 1.0259 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0658 - acc: 0.4343 - val_loss: 1.0357 - val_acc: 0.4829\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0602 - acc: 0.4366 - val_loss: 1.0253 - val_acc: 0.4914\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0559 - acc: 0.4516 - val_loss: 1.0249 - val_acc: 0.4899\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0514 - acc: 0.4549 - val_loss: 1.0239 - val_acc: 0.4992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 94s 9ms/step - loss: 1.0960 - acc: 0.4077 - val_loss: 1.0392 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0718 - acc: 0.4317 - val_loss: 1.0313 - val_acc: 0.4860\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0599 - acc: 0.4425 - val_loss: 1.0305 - val_acc: 0.4844\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0579 - acc: 0.4416 - val_loss: 1.0273 - val_acc: 0.4899\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0534 - acc: 0.4481 - val_loss: 1.0268 - val_acc: 0.4907\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 92s 9ms/step - loss: 1.0873 - acc: 0.4198 - val_loss: 1.0338 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0695 - acc: 0.4308 - val_loss: 1.0426 - val_acc: 0.4868\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0599 - acc: 0.4434 - val_loss: 1.0310 - val_acc: 0.4813\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0546 - acc: 0.4514 - val_loss: 1.0306 - val_acc: 0.4836\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0526 - acc: 0.4499 - val_loss: 1.0281 - val_acc: 0.4829\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 101s 10ms/step - loss: 1.0925 - acc: 0.4104 - val_loss: 1.0376 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0673 - acc: 0.4335 - val_loss: 1.0321 - val_acc: 0.4875\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0593 - acc: 0.4446 - val_loss: 1.0287 - val_acc: 0.4938\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0523 - acc: 0.4503 - val_loss: 1.0260 - val_acc: 0.4945\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0484 - acc: 0.4576 - val_loss: 1.0241 - val_acc: 0.4977\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 101s 10ms/step - loss: 1.0890 - acc: 0.4101 - val_loss: 1.0305 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0688 - acc: 0.4289 - val_loss: 1.0357 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0607 - acc: 0.4357 - val_loss: 1.0299 - val_acc: 0.4891\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0574 - acc: 0.4406 - val_loss: 1.0397 - val_acc: 0.4696\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0510 - acc: 0.4485 - val_loss: 1.0281 - val_acc: 0.4891\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 108s 11ms/step - loss: 1.0812 - acc: 0.4180 - val_loss: 1.0332 - val_acc: 0.4696\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0655 - acc: 0.4319 - val_loss: 1.0318 - val_acc: 0.4735\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0605 - acc: 0.4387 - val_loss: 1.0309 - val_acc: 0.4712\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0559 - acc: 0.4489 - val_loss: 1.0293 - val_acc: 0.4782\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0482 - acc: 0.4497 - val_loss: 1.0262 - val_acc: 0.4891\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 103s 10ms/step - loss: 1.0911 - acc: 0.4099 - val_loss: 1.0303 - val_acc: 0.4782\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0699 - acc: 0.4257 - val_loss: 1.0321 - val_acc: 0.4759\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0597 - acc: 0.4374 - val_loss: 1.0299 - val_acc: 0.4790\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0594 - acc: 0.4375 - val_loss: 1.0297 - val_acc: 0.4821\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0533 - acc: 0.4416 - val_loss: 1.0305 - val_acc: 0.4860\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 99s 10ms/step - loss: 1.0941 - acc: 0.4084 - val_loss: 1.0425 - val_acc: 0.4696\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0676 - acc: 0.4277 - val_loss: 1.0367 - val_acc: 0.4774\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0585 - acc: 0.4418 - val_loss: 1.0320 - val_acc: 0.4813\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0561 - acc: 0.4463 - val_loss: 1.0323 - val_acc: 0.4821\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0516 - acc: 0.4491 - val_loss: 1.0264 - val_acc: 0.4930\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 107s 10ms/step - loss: 1.0899 - acc: 0.4094 - val_loss: 1.0345 - val_acc: 0.4774\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0645 - acc: 0.4307 - val_loss: 1.0335 - val_acc: 0.4759\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0608 - acc: 0.4391 - val_loss: 1.0312 - val_acc: 0.4875\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0551 - acc: 0.4437 - val_loss: 1.0248 - val_acc: 0.4868\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0536 - acc: 0.4489 - val_loss: 1.0286 - val_acc: 0.4930\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 102s 10ms/step - loss: 1.0816 - acc: 0.4200 - val_loss: 1.0383 - val_acc: 0.4774\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0696 - acc: 0.4300 - val_loss: 1.0303 - val_acc: 0.4766\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0601 - acc: 0.4429 - val_loss: 1.0287 - val_acc: 0.4813\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0550 - acc: 0.4463 - val_loss: 1.0305 - val_acc: 0.4922\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0535 - acc: 0.4481 - val_loss: 1.0226 - val_acc: 0.4852\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 101s 10ms/step - loss: 1.0864 - acc: 0.4177 - val_loss: 1.0305 - val_acc: 0.4743\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0652 - acc: 0.4270 - val_loss: 1.0311 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0614 - acc: 0.4326 - val_loss: 1.0399 - val_acc: 0.4727\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0582 - acc: 0.4328 - val_loss: 1.0391 - val_acc: 0.4743\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0548 - acc: 0.4418 - val_loss: 1.0261 - val_acc: 0.4860\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.1044 - acc: 0.4004 - val_loss: 1.0342 - val_acc: 0.4759\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 9s 888us/step - loss: 1.0785 - acc: 0.4145 - val_loss: 1.0347 - val_acc: 0.4774\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 9s 887us/step - loss: 1.0697 - acc: 0.4243 - val_loss: 1.0371 - val_acc: 0.4688\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 9s 887us/step - loss: 1.0637 - acc: 0.4272 - val_loss: 1.0299 - val_acc: 0.4759\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 9s 893us/step - loss: 1.0619 - acc: 0.4270 - val_loss: 1.0355 - val_acc: 0.4766\n",
      "1265/1265 [==============================] - 1s 397us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.1134 - acc: 0.4013 - val_loss: 1.0505 - val_acc: 0.4120\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 10s 944us/step - loss: 1.0743 - acc: 0.4228 - val_loss: 1.0359 - val_acc: 0.4922\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 10s 952us/step - loss: 1.0698 - acc: 0.4215 - val_loss: 1.0347 - val_acc: 0.4914\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 10s 950us/step - loss: 1.0604 - acc: 0.4358 - val_loss: 1.0321 - val_acc: 0.4961\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 10s 955us/step - loss: 1.0615 - acc: 0.4340 - val_loss: 1.0295 - val_acc: 0.4907\n",
      "1265/1265 [==============================] - 1s 437us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.1080 - acc: 0.4033 - val_loss: 1.0354 - val_acc: 0.4782\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 10s 976us/step - loss: 1.0771 - acc: 0.4239 - val_loss: 1.0380 - val_acc: 0.4774\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 10s 977us/step - loss: 1.0664 - acc: 0.4361 - val_loss: 1.0451 - val_acc: 0.4579\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 10s 977us/step - loss: 1.0627 - acc: 0.4472 - val_loss: 1.0355 - val_acc: 0.4759\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 10s 973us/step - loss: 1.0561 - acc: 0.4393 - val_loss: 1.0301 - val_acc: 0.4759\n",
      "1265/1265 [==============================] - 1s 464us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.1038 - acc: 0.4120 - val_loss: 1.0377 - val_acc: 0.4681\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0740 - acc: 0.4263 - val_loss: 1.0369 - val_acc: 0.4720\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0649 - acc: 0.4378 - val_loss: 1.0358 - val_acc: 0.4790\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0599 - acc: 0.4391 - val_loss: 1.0372 - val_acc: 0.4813\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0587 - acc: 0.4441 - val_loss: 1.0331 - val_acc: 0.4759\n",
      "1265/1265 [==============================] - 1s 452us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.1089 - acc: 0.4100 - val_loss: 1.0349 - val_acc: 0.4782\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0732 - acc: 0.4313 - val_loss: 1.0314 - val_acc: 0.4938\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0619 - acc: 0.4399 - val_loss: 1.0309 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0592 - acc: 0.4417 - val_loss: 1.0262 - val_acc: 0.4868\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0530 - acc: 0.4459 - val_loss: 1.0317 - val_acc: 0.4868\n",
      "1265/1265 [==============================] - 1s 550us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.1109 - acc: 0.4022 - val_loss: 1.0282 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0731 - acc: 0.4301 - val_loss: 1.0308 - val_acc: 0.4774\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0610 - acc: 0.4459 - val_loss: 1.0265 - val_acc: 0.4868\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0549 - acc: 0.4432 - val_loss: 1.0248 - val_acc: 0.4938\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0548 - acc: 0.4478 - val_loss: 1.0268 - val_acc: 0.4938\n",
      "1265/1265 [==============================] - 1s 582us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.1099 - acc: 0.4106 - val_loss: 1.0374 - val_acc: 0.4517\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0688 - acc: 0.4376 - val_loss: 1.0245 - val_acc: 0.4930\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0627 - acc: 0.4439 - val_loss: 1.0307 - val_acc: 0.4782\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0528 - acc: 0.4527 - val_loss: 1.0279 - val_acc: 0.4836\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0545 - acc: 0.4533 - val_loss: 1.0345 - val_acc: 0.4642\n",
      "1265/1265 [==============================] - 1s 607us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.1165 - acc: 0.4064 - val_loss: 1.0465 - val_acc: 0.4704\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0724 - acc: 0.4345 - val_loss: 1.0307 - val_acc: 0.4821\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0586 - acc: 0.4468 - val_loss: 1.0349 - val_acc: 0.4774\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0575 - acc: 0.4533 - val_loss: 1.0326 - val_acc: 0.4790\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0534 - acc: 0.4537 - val_loss: 1.0277 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 1s 631us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.1056 - acc: 0.4141 - val_loss: 1.0262 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0714 - acc: 0.4319 - val_loss: 1.0262 - val_acc: 0.4938\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0591 - acc: 0.4424 - val_loss: 1.0272 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0561 - acc: 0.4508 - val_loss: 1.0288 - val_acc: 0.4805\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0493 - acc: 0.4598 - val_loss: 1.0236 - val_acc: 0.4977\n",
      "1265/1265 [==============================] - 1s 684us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.1100 - acc: 0.4150 - val_loss: 1.0287 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0713 - acc: 0.4367 - val_loss: 1.0221 - val_acc: 0.4891\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0611 - acc: 0.4479 - val_loss: 1.0211 - val_acc: 0.4891\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0583 - acc: 0.4478 - val_loss: 1.0267 - val_acc: 0.4938\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 2ms/step - loss: 1.0528 - acc: 0.4516 - val_loss: 1.0197 - val_acc: 0.4852\n",
      "1265/1265 [==============================] - 1s 719us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0935 - acc: 0.4150 - val_loss: 1.0333 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0710 - acc: 0.4333 - val_loss: 1.0307 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0576 - acc: 0.4506 - val_loss: 1.0265 - val_acc: 0.5055\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0541 - acc: 0.4537 - val_loss: 1.0224 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0492 - acc: 0.4556 - val_loss: 1.0215 - val_acc: 0.4961\n",
      "1265/1265 [==============================] - 1s 738us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.0999 - acc: 0.4157 - val_loss: 1.0376 - val_acc: 0.4782\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0732 - acc: 0.4295 - val_loss: 1.0327 - val_acc: 0.4875\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0616 - acc: 0.4444 - val_loss: 1.0262 - val_acc: 0.4891\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0550 - acc: 0.4495 - val_loss: 1.0233 - val_acc: 0.4938\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0548 - acc: 0.4471 - val_loss: 1.0227 - val_acc: 0.4961\n",
      "1265/1265 [==============================] - 1s 766us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 1.1020 - acc: 0.4118 - val_loss: 1.0318 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0711 - acc: 0.4329 - val_loss: 1.0227 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0633 - acc: 0.4388 - val_loss: 1.0308 - val_acc: 0.4914\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0565 - acc: 0.4526 - val_loss: 1.0248 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0503 - acc: 0.4542 - val_loss: 1.0360 - val_acc: 0.4782\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 93s 9ms/step - loss: 1.1101 - acc: 0.4073 - val_loss: 1.0509 - val_acc: 0.4377\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0714 - acc: 0.4350 - val_loss: 1.0279 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0631 - acc: 0.4438 - val_loss: 1.0223 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0586 - acc: 0.4458 - val_loss: 1.0266 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0502 - acc: 0.4545 - val_loss: 1.0208 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 83s 8ms/step - loss: 1.0983 - acc: 0.4143 - val_loss: 1.0452 - val_acc: 0.4517\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0701 - acc: 0.4359 - val_loss: 1.0249 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0588 - acc: 0.4475 - val_loss: 1.0210 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0533 - acc: 0.4523 - val_loss: 1.0206 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0491 - acc: 0.4593 - val_loss: 1.0200 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 86s 8ms/step - loss: 1.0955 - acc: 0.4186 - val_loss: 1.0307 - val_acc: 0.4930\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0638 - acc: 0.4457 - val_loss: 1.0522 - val_acc: 0.4229\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0621 - acc: 0.4431 - val_loss: 1.0327 - val_acc: 0.4743\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0550 - acc: 0.4529 - val_loss: 1.0272 - val_acc: 0.4961\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0492 - acc: 0.4624 - val_loss: 1.0230 - val_acc: 0.4922\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 96s 9ms/step - loss: 1.0958 - acc: 0.4130 - val_loss: 1.0499 - val_acc: 0.4276\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0720 - acc: 0.4355 - val_loss: 1.0275 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0645 - acc: 0.4402 - val_loss: 1.0261 - val_acc: 0.4984\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0550 - acc: 0.4478 - val_loss: 1.0272 - val_acc: 0.4961\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0497 - acc: 0.4533 - val_loss: 1.0217 - val_acc: 0.5016\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 91s 9ms/step - loss: 1.1056 - acc: 0.4008 - val_loss: 1.0325 - val_acc: 0.4743\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0721 - acc: 0.4268 - val_loss: 1.0320 - val_acc: 0.4790\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0628 - acc: 0.4391 - val_loss: 1.0316 - val_acc: 0.4875\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0592 - acc: 0.4396 - val_loss: 1.0260 - val_acc: 0.4875\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0545 - acc: 0.4468 - val_loss: 1.0265 - val_acc: 0.4852\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 94s 9ms/step - loss: 1.0991 - acc: 0.4090 - val_loss: 1.0311 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0698 - acc: 0.4308 - val_loss: 1.0371 - val_acc: 0.4696\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0576 - acc: 0.4499 - val_loss: 1.0287 - val_acc: 0.4969\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0570 - acc: 0.4485 - val_loss: 1.0237 - val_acc: 0.4914\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0523 - acc: 0.4469 - val_loss: 1.0273 - val_acc: 0.4984\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 100s 10ms/step - loss: 1.0969 - acc: 0.4124 - val_loss: 1.0325 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0649 - acc: 0.4429 - val_loss: 1.0353 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0614 - acc: 0.4374 - val_loss: 1.0277 - val_acc: 0.4914\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0564 - acc: 0.4464 - val_loss: 1.0277 - val_acc: 0.4868\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0535 - acc: 0.4524 - val_loss: 1.0295 - val_acc: 0.4969\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 102s 10ms/step - loss: 1.0959 - acc: 0.4120 - val_loss: 1.0363 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0657 - acc: 0.4312 - val_loss: 1.0324 - val_acc: 0.4922\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0595 - acc: 0.4434 - val_loss: 1.0273 - val_acc: 0.4961\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0555 - acc: 0.4488 - val_loss: 1.0243 - val_acc: 0.4899\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0517 - acc: 0.4509 - val_loss: 1.0221 - val_acc: 0.4969\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 99s 10ms/step - loss: 1.0964 - acc: 0.4087 - val_loss: 1.0370 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0724 - acc: 0.4318 - val_loss: 1.0298 - val_acc: 0.4821\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0588 - acc: 0.4414 - val_loss: 1.0266 - val_acc: 0.4875\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0560 - acc: 0.4476 - val_loss: 1.0226 - val_acc: 0.4883\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0551 - acc: 0.4456 - val_loss: 1.0225 - val_acc: 0.4899\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 104s 10ms/step - loss: 1.0954 - acc: 0.4190 - val_loss: 1.0303 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0671 - acc: 0.4319 - val_loss: 1.0346 - val_acc: 0.4930\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0605 - acc: 0.4380 - val_loss: 1.0259 - val_acc: 0.4875\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0549 - acc: 0.4463 - val_loss: 1.0250 - val_acc: 0.4961\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0523 - acc: 0.4475 - val_loss: 1.0310 - val_acc: 0.4836\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0895 - acc: 0.4150 - val_loss: 1.0331 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0673 - acc: 0.4279 - val_loss: 1.0314 - val_acc: 0.4813\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 37s 4ms/step - loss: 1.0599 - acc: 0.4415 - val_loss: 1.0323 - val_acc: 0.4813\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0570 - acc: 0.4431 - val_loss: 1.0274 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0545 - acc: 0.4426 - val_loss: 1.0253 - val_acc: 0.4829\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 108s 11ms/step - loss: 1.0930 - acc: 0.4127 - val_loss: 1.0443 - val_acc: 0.4626\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0657 - acc: 0.4346 - val_loss: 1.0323 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0592 - acc: 0.4423 - val_loss: 1.0366 - val_acc: 0.4883\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0543 - acc: 0.4480 - val_loss: 1.0287 - val_acc: 0.4860\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0504 - acc: 0.4479 - val_loss: 1.0262 - val_acc: 0.4914\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 108s 11ms/step - loss: 1.0938 - acc: 0.4134 - val_loss: 1.0533 - val_acc: 0.4136\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0652 - acc: 0.4318 - val_loss: 1.0311 - val_acc: 0.4782\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0578 - acc: 0.4413 - val_loss: 1.0318 - val_acc: 0.4875\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0577 - acc: 0.4391 - val_loss: 1.0343 - val_acc: 0.4836\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0532 - acc: 0.4469 - val_loss: 1.0257 - val_acc: 0.4899\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 106s 10ms/step - loss: 1.0969 - acc: 0.4035 - val_loss: 1.0343 - val_acc: 0.4704\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0652 - acc: 0.4357 - val_loss: 1.0270 - val_acc: 0.4860\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0587 - acc: 0.4417 - val_loss: 1.0313 - val_acc: 0.4922\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0540 - acc: 0.4510 - val_loss: 1.0353 - val_acc: 0.4759\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0531 - acc: 0.4512 - val_loss: 1.0284 - val_acc: 0.4992\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0915 - acc: 0.4158 - val_loss: 1.0302 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0675 - acc: 0.4362 - val_loss: 1.0309 - val_acc: 0.4821\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0634 - acc: 0.4423 - val_loss: 1.0298 - val_acc: 0.4899\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0583 - acc: 0.4457 - val_loss: 1.0288 - val_acc: 0.4852\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0524 - acc: 0.4501 - val_loss: 1.0277 - val_acc: 0.4984\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 111s 11ms/step - loss: 1.0890 - acc: 0.4100 - val_loss: 1.0361 - val_acc: 0.4836\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0636 - acc: 0.4306 - val_loss: 1.0362 - val_acc: 0.4883\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0625 - acc: 0.4428 - val_loss: 1.0350 - val_acc: 0.4907\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0566 - acc: 0.4441 - val_loss: 1.0346 - val_acc: 0.4914\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0523 - acc: 0.4474 - val_loss: 1.0249 - val_acc: 0.4860\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0913 - acc: 0.4079 - val_loss: 1.0371 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0653 - acc: 0.4326 - val_loss: 1.0293 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0571 - acc: 0.4437 - val_loss: 1.0317 - val_acc: 0.4891\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0563 - acc: 0.4456 - val_loss: 1.0296 - val_acc: 0.4875\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0537 - acc: 0.4418 - val_loss: 1.0295 - val_acc: 0.4891\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0878 - acc: 0.4087 - val_loss: 1.0318 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0649 - acc: 0.4369 - val_loss: 1.0389 - val_acc: 0.4875\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0615 - acc: 0.4365 - val_loss: 1.0352 - val_acc: 0.4821\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0568 - acc: 0.4446 - val_loss: 1.0311 - val_acc: 0.4883\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0546 - acc: 0.4447 - val_loss: 1.0291 - val_acc: 0.4891\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.1134 - acc: 0.3992 - val_loss: 1.0373 - val_acc: 0.4751\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 9s 917us/step - loss: 1.0774 - acc: 0.4212 - val_loss: 1.0336 - val_acc: 0.4852\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 9s 909us/step - loss: 1.0669 - acc: 0.4294 - val_loss: 1.0422 - val_acc: 0.4751\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 9s 879us/step - loss: 1.0651 - acc: 0.4328 - val_loss: 1.0374 - val_acc: 0.4852\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 9s 917us/step - loss: 1.0598 - acc: 0.4364 - val_loss: 1.0330 - val_acc: 0.4813\n",
      "1265/1265 [==============================] - 1s 421us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.1088 - acc: 0.4062 - val_loss: 1.0397 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 10s 955us/step - loss: 1.0774 - acc: 0.4212 - val_loss: 1.0329 - val_acc: 0.4938\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 10s 950us/step - loss: 1.0667 - acc: 0.4282 - val_loss: 1.0350 - val_acc: 0.4883\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 10s 954us/step - loss: 1.0620 - acc: 0.4302 - val_loss: 1.0356 - val_acc: 0.4883\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 10s 951us/step - loss: 1.0585 - acc: 0.4378 - val_loss: 1.0325 - val_acc: 0.4938\n",
      "1265/1265 [==============================] - 1s 449us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.1075 - acc: 0.4035 - val_loss: 1.0342 - val_acc: 0.4774\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0779 - acc: 0.4202 - val_loss: 1.0327 - val_acc: 0.4766\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0656 - acc: 0.4339 - val_loss: 1.0360 - val_acc: 0.4673\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0617 - acc: 0.4410 - val_loss: 1.0404 - val_acc: 0.4579\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0579 - acc: 0.4429 - val_loss: 1.0333 - val_acc: 0.4836\n",
      "1265/1265 [==============================] - 1s 496us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.1082 - acc: 0.4107 - val_loss: 1.0327 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0749 - acc: 0.4265 - val_loss: 1.0319 - val_acc: 0.4868\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0652 - acc: 0.4323 - val_loss: 1.0301 - val_acc: 0.4813\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0599 - acc: 0.4400 - val_loss: 1.0301 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0582 - acc: 0.4455 - val_loss: 1.0295 - val_acc: 0.4829\n",
      "1265/1265 [==============================] - 1s 548us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.1015 - acc: 0.4162 - val_loss: 1.0280 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0762 - acc: 0.4255 - val_loss: 1.0271 - val_acc: 0.4821\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0608 - acc: 0.4385 - val_loss: 1.0274 - val_acc: 0.4899\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0595 - acc: 0.4408 - val_loss: 1.0256 - val_acc: 0.4868\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0574 - acc: 0.4419 - val_loss: 1.0321 - val_acc: 0.4984\n",
      "1265/1265 [==============================] - 1s 590us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.1165 - acc: 0.4039 - val_loss: 1.0421 - val_acc: 0.4548\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0739 - acc: 0.4203 - val_loss: 1.0347 - val_acc: 0.4712\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0669 - acc: 0.4348 - val_loss: 1.0326 - val_acc: 0.4829\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0627 - acc: 0.4398 - val_loss: 1.0298 - val_acc: 0.4868\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0554 - acc: 0.4474 - val_loss: 1.0364 - val_acc: 0.4587\n",
      "1265/1265 [==============================] - 1s 624us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.1114 - acc: 0.4102 - val_loss: 1.0351 - val_acc: 0.4735\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0741 - acc: 0.4369 - val_loss: 1.0286 - val_acc: 0.4774\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0645 - acc: 0.4381 - val_loss: 1.0240 - val_acc: 0.4899\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0575 - acc: 0.4465 - val_loss: 1.0315 - val_acc: 0.4891\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0523 - acc: 0.4515 - val_loss: 1.0264 - val_acc: 0.4891\n",
      "1265/1265 [==============================] - 1s 642us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.1090 - acc: 0.4128 - val_loss: 1.0326 - val_acc: 0.4735\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0690 - acc: 0.4425 - val_loss: 1.0294 - val_acc: 0.4829\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0620 - acc: 0.4489 - val_loss: 1.0286 - val_acc: 0.4782\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0544 - acc: 0.4463 - val_loss: 1.0298 - val_acc: 0.4805\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0534 - acc: 0.4561 - val_loss: 1.0337 - val_acc: 0.4735\n",
      "1265/1265 [==============================] - 1s 690us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.1085 - acc: 0.4133 - val_loss: 1.0261 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0701 - acc: 0.4382 - val_loss: 1.0267 - val_acc: 0.4883\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0633 - acc: 0.4449 - val_loss: 1.0272 - val_acc: 0.4922\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0555 - acc: 0.4521 - val_loss: 1.0288 - val_acc: 0.4953\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0513 - acc: 0.4600 - val_loss: 1.0339 - val_acc: 0.4774\n",
      "1265/1265 [==============================] - 1s 695us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 1.1144 - acc: 0.4062 - val_loss: 1.0290 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0738 - acc: 0.4359 - val_loss: 1.0200 - val_acc: 0.4930\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 15s 2ms/step - loss: 1.0650 - acc: 0.4473 - val_loss: 1.0261 - val_acc: 0.5016\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 15s 2ms/step - loss: 1.0569 - acc: 0.4559 - val_loss: 1.0213 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0539 - acc: 0.4502 - val_loss: 1.0185 - val_acc: 0.5031\n",
      "1265/1265 [==============================] - 1s 742us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 81s 8ms/step - loss: 1.1074 - acc: 0.4076 - val_loss: 1.0269 - val_acc: 0.4945\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0715 - acc: 0.4371 - val_loss: 1.0223 - val_acc: 0.4992\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0680 - acc: 0.4397 - val_loss: 1.0222 - val_acc: 0.4899\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0575 - acc: 0.4501 - val_loss: 1.0324 - val_acc: 0.4868\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0511 - acc: 0.4593 - val_loss: 1.0287 - val_acc: 0.4938\n",
      "1265/1265 [==============================] - 1s 790us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 85s 8ms/step - loss: 1.1138 - acc: 0.4164 - val_loss: 1.0342 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0725 - acc: 0.4407 - val_loss: 1.0334 - val_acc: 0.4891\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0601 - acc: 0.4431 - val_loss: 1.0246 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0565 - acc: 0.4538 - val_loss: 1.0299 - val_acc: 0.4891\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0534 - acc: 0.4537 - val_loss: 1.0258 - val_acc: 0.4945\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 95s 9ms/step - loss: 1.1084 - acc: 0.4075 - val_loss: 1.0284 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0705 - acc: 0.4331 - val_loss: 1.0243 - val_acc: 0.4821\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0597 - acc: 0.4477 - val_loss: 1.0236 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0543 - acc: 0.4539 - val_loss: 1.0302 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0530 - acc: 0.4552 - val_loss: 1.0255 - val_acc: 0.5039\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 98s 10ms/step - loss: 1.1040 - acc: 0.4186 - val_loss: 1.0321 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0724 - acc: 0.4344 - val_loss: 1.0265 - val_acc: 0.4914\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0601 - acc: 0.4448 - val_loss: 1.0243 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0554 - acc: 0.4522 - val_loss: 1.0206 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0493 - acc: 0.4618 - val_loss: 1.0251 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 100s 10ms/step - loss: 1.1025 - acc: 0.4134 - val_loss: 1.0352 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0680 - acc: 0.4385 - val_loss: 1.0272 - val_acc: 0.4907\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0576 - acc: 0.4475 - val_loss: 1.0307 - val_acc: 0.4860\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0555 - acc: 0.4476 - val_loss: 1.0261 - val_acc: 0.4984\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0528 - acc: 0.4575 - val_loss: 1.0254 - val_acc: 0.5070\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 103s 10ms/step - loss: 1.1076 - acc: 0.4016 - val_loss: 1.0390 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0732 - acc: 0.4312 - val_loss: 1.0360 - val_acc: 0.4759\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0611 - acc: 0.4435 - val_loss: 1.0331 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0558 - acc: 0.4491 - val_loss: 1.0261 - val_acc: 0.4922\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0501 - acc: 0.4533 - val_loss: 1.0265 - val_acc: 0.4977\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 106s 10ms/step - loss: 1.1040 - acc: 0.4123 - val_loss: 1.0413 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0768 - acc: 0.4240 - val_loss: 1.0314 - val_acc: 0.4907\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0669 - acc: 0.4364 - val_loss: 1.0275 - val_acc: 0.4922\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0564 - acc: 0.4518 - val_loss: 1.0259 - val_acc: 0.4922\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0534 - acc: 0.4548 - val_loss: 1.0279 - val_acc: 0.4868\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 108s 11ms/step - loss: 1.0996 - acc: 0.4131 - val_loss: 1.0296 - val_acc: 0.4844\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0694 - acc: 0.4320 - val_loss: 1.0282 - val_acc: 0.4891\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0599 - acc: 0.4379 - val_loss: 1.0253 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0573 - acc: 0.4434 - val_loss: 1.0235 - val_acc: 0.4953\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0545 - acc: 0.4487 - val_loss: 1.0218 - val_acc: 0.4992\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 103s 10ms/step - loss: 1.0980 - acc: 0.4106 - val_loss: 1.0307 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0734 - acc: 0.4333 - val_loss: 1.0281 - val_acc: 0.4860\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0650 - acc: 0.4360 - val_loss: 1.0289 - val_acc: 0.4883\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0559 - acc: 0.4401 - val_loss: 1.0274 - val_acc: 0.4977\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0518 - acc: 0.4515 - val_loss: 1.0222 - val_acc: 0.4914\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 109s 11ms/step - loss: 1.0949 - acc: 0.4120 - val_loss: 1.0350 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0664 - acc: 0.4323 - val_loss: 1.0272 - val_acc: 0.4860\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 22s 2ms/step - loss: 1.0625 - acc: 0.4358 - val_loss: 1.0322 - val_acc: 0.4836\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0548 - acc: 0.4478 - val_loss: 1.0281 - val_acc: 0.4875\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0502 - acc: 0.4540 - val_loss: 1.0262 - val_acc: 0.4852\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 108s 11ms/step - loss: 1.0936 - acc: 0.4166 - val_loss: 1.0347 - val_acc: 0.4727\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0698 - acc: 0.4346 - val_loss: 1.0324 - val_acc: 0.4836\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0602 - acc: 0.4375 - val_loss: 1.0341 - val_acc: 0.4821\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0545 - acc: 0.4494 - val_loss: 1.0311 - val_acc: 0.4860\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0502 - acc: 0.4561 - val_loss: 1.0239 - val_acc: 0.4899s: 1.0497 - acc: 0 - ETA: 0s - loss: 1.0501 - acc: 0.456\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0921 - acc: 0.4076 - val_loss: 1.0345 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0716 - acc: 0.4251 - val_loss: 1.0375 - val_acc: 0.4899\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0598 - acc: 0.4396 - val_loss: 1.0381 - val_acc: 0.4673\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0568 - acc: 0.4451 - val_loss: 1.0263 - val_acc: 0.4844\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0509 - acc: 0.4550 - val_loss: 1.0311 - val_acc: 0.4961\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0928 - acc: 0.4115 - val_loss: 1.0349 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0681 - acc: 0.4318 - val_loss: 1.0297 - val_acc: 0.4852\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0630 - acc: 0.4373 - val_loss: 1.0277 - val_acc: 0.4836\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0574 - acc: 0.4472 - val_loss: 1.0268 - val_acc: 0.4735\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0549 - acc: 0.4502 - val_loss: 1.0285 - val_acc: 0.4914\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0927 - acc: 0.4071 - val_loss: 1.0345 - val_acc: 0.4759\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0614 - acc: 0.4332 - val_loss: 1.0311 - val_acc: 0.4821\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0589 - acc: 0.4450 - val_loss: 1.0268 - val_acc: 0.4782\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0537 - acc: 0.4467 - val_loss: 1.0299 - val_acc: 0.4899\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0508 - acc: 0.4555 - val_loss: 1.0316 - val_acc: 0.4743\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0894 - acc: 0.4053 - val_loss: 1.0347 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0666 - acc: 0.4303 - val_loss: 1.0342 - val_acc: 0.4852\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0552 - acc: 0.4467 - val_loss: 1.0298 - val_acc: 0.4875\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0559 - acc: 0.4495 - val_loss: 1.0290 - val_acc: 0.4930\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0513 - acc: 0.4539 - val_loss: 1.0261 - val_acc: 0.4945\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0873 - acc: 0.4175 - val_loss: 1.0316 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0651 - acc: 0.4298 - val_loss: 1.0342 - val_acc: 0.4759\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0605 - acc: 0.4382 - val_loss: 1.0311 - val_acc: 0.4782\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0531 - acc: 0.4481 - val_loss: 1.0268 - val_acc: 0.4821\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0510 - acc: 0.4539 - val_loss: 1.0268 - val_acc: 0.4907\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0886 - acc: 0.4076 - val_loss: 1.0370 - val_acc: 0.4603\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0632 - acc: 0.4348 - val_loss: 1.0335 - val_acc: 0.4790\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0623 - acc: 0.4333 - val_loss: 1.0372 - val_acc: 0.4774\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0575 - acc: 0.4361 - val_loss: 1.0300 - val_acc: 0.4844\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0538 - acc: 0.4420 - val_loss: 1.0263 - val_acc: 0.4821\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0910 - acc: 0.4067 - val_loss: 1.0342 - val_acc: 0.4720\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0656 - acc: 0.4344 - val_loss: 1.0298 - val_acc: 0.4836\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0609 - acc: 0.4358 - val_loss: 1.0291 - val_acc: 0.4751\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0562 - acc: 0.4423 - val_loss: 1.0357 - val_acc: 0.4790\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0533 - acc: 0.4434 - val_loss: 1.0277 - val_acc: 0.4829\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0942 - acc: 0.4088 - val_loss: 1.0434 - val_acc: 0.4673\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0695 - acc: 0.4216 - val_loss: 1.0382 - val_acc: 0.4743\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0614 - acc: 0.4345 - val_loss: 1.0395 - val_acc: 0.4766\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0588 - acc: 0.4349 - val_loss: 1.0271 - val_acc: 0.4774\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0558 - acc: 0.4488 - val_loss: 1.0326 - val_acc: 0.4821\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0930 - acc: 0.4074 - val_loss: 1.0313 - val_acc: 0.4891\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0682 - acc: 0.4247 - val_loss: 1.0293 - val_acc: 0.4883\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0578 - acc: 0.4366 - val_loss: 1.0354 - val_acc: 0.4899\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0593 - acc: 0.4393 - val_loss: 1.0305 - val_acc: 0.4899\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0543 - acc: 0.4477 - val_loss: 1.0407 - val_acc: 0.4759\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0892 - acc: 0.4043 - val_loss: 1.0392 - val_acc: 0.4891\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0652 - acc: 0.4300 - val_loss: 1.0294 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0607 - acc: 0.4403 - val_loss: 1.0323 - val_acc: 0.4844\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0569 - acc: 0.4414 - val_loss: 1.0307 - val_acc: 0.4883\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0529 - acc: 0.4508 - val_loss: 1.0259 - val_acc: 0.4860\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.1159 - acc: 0.4069 - val_loss: 1.0348 - val_acc: 0.4727\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 10s 931us/step - loss: 1.0806 - acc: 0.4160 - val_loss: 1.0355 - val_acc: 0.4712\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 9s 924us/step - loss: 1.0713 - acc: 0.4249 - val_loss: 1.0328 - val_acc: 0.4914\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 9s 927us/step - loss: 1.0651 - acc: 0.4289 - val_loss: 1.0331 - val_acc: 0.4875\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 9s 870us/step - loss: 1.0583 - acc: 0.4328 - val_loss: 1.0301 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 1s 428us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 79s 8ms/step - loss: 1.1144 - acc: 0.4007 - val_loss: 1.0396 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0753 - acc: 0.4270 - val_loss: 1.0316 - val_acc: 0.4945\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0671 - acc: 0.4292 - val_loss: 1.0368 - val_acc: 0.4914\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0615 - acc: 0.4368 - val_loss: 1.0321 - val_acc: 0.4891\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0605 - acc: 0.4372 - val_loss: 1.0307 - val_acc: 0.4977\n",
      "1265/1265 [==============================] - 1s 486us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.1126 - acc: 0.3985 - val_loss: 1.0366 - val_acc: 0.4766\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0762 - acc: 0.4200 - val_loss: 1.0338 - val_acc: 0.4829\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0636 - acc: 0.4340 - val_loss: 1.0286 - val_acc: 0.4829\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0600 - acc: 0.4380 - val_loss: 1.0328 - val_acc: 0.4829\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0571 - acc: 0.4436 - val_loss: 1.0339 - val_acc: 0.4836\n",
      "1265/1265 [==============================] - 1s 540us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 79s 8ms/step - loss: 1.1142 - acc: 0.4041 - val_loss: 1.0322 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0729 - acc: 0.4299 - val_loss: 1.0284 - val_acc: 0.4790\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 11s 1ms/step - loss: 1.0634 - acc: 0.4403 - val_loss: 1.0339 - val_acc: 0.4720\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0606 - acc: 0.4385 - val_loss: 1.0323 - val_acc: 0.4805\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0584 - acc: 0.4436 - val_loss: 1.0304 - val_acc: 0.4844\n",
      "1265/1265 [==============================] - 1s 567us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.1055 - acc: 0.4042 - val_loss: 1.0351 - val_acc: 0.4836\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0695 - acc: 0.4289 - val_loss: 1.0255 - val_acc: 0.4852\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 12s 1ms/step - loss: 1.0635 - acc: 0.4416 - val_loss: 1.0295 - val_acc: 0.4961\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0586 - acc: 0.4445 - val_loss: 1.0301 - val_acc: 0.5023\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 13s 1ms/step - loss: 1.0547 - acc: 0.4447 - val_loss: 1.0349 - val_acc: 0.4930\n",
      "1265/1265 [==============================] - 1s 623us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 80s 8ms/step - loss: 1.1056 - acc: 0.4091 - val_loss: 1.0461 - val_acc: 0.4455\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0735 - acc: 0.4365 - val_loss: 1.0330 - val_acc: 0.4844\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0637 - acc: 0.4390 - val_loss: 1.0315 - val_acc: 0.4821\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0578 - acc: 0.4476 - val_loss: 1.0309 - val_acc: 0.4977\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0533 - acc: 0.4471 - val_loss: 1.0343 - val_acc: 0.4735\n",
      "1265/1265 [==============================] - 1s 654us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 82s 8ms/step - loss: 1.1074 - acc: 0.4133 - val_loss: 1.0267 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0716 - acc: 0.4277 - val_loss: 1.0331 - val_acc: 0.4720\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0633 - acc: 0.4457 - val_loss: 1.0258 - val_acc: 0.4875\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0548 - acc: 0.4470 - val_loss: 1.0257 - val_acc: 0.4938\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0531 - acc: 0.4522 - val_loss: 1.0257 - val_acc: 0.4899\n",
      "1265/1265 [==============================] - 1s 585us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 108s 11ms/step - loss: 1.1080 - acc: 0.4133 - val_loss: 1.0318 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0720 - acc: 0.4297 - val_loss: 1.0328 - val_acc: 0.4696\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0624 - acc: 0.4468 - val_loss: 1.0312 - val_acc: 0.4844\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 14s 1ms/step - loss: 1.0551 - acc: 0.4508 - val_loss: 1.0336 - val_acc: 0.4774\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 1.0548 - acc: 0.4495 - val_loss: 1.0310 - val_acc: 0.4844\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 90s 9ms/step - loss: 1.1009 - acc: 0.4127 - val_loss: 1.0313 - val_acc: 0.4774\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0674 - acc: 0.4362 - val_loss: 1.0276 - val_acc: 0.4891\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0605 - acc: 0.4504 - val_loss: 1.0222 - val_acc: 0.4922\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0564 - acc: 0.4513 - val_loss: 1.0238 - val_acc: 0.4899\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0518 - acc: 0.4559 - val_loss: 1.0262 - val_acc: 0.4836\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 96s 9ms/step - loss: 1.1122 - acc: 0.4096 - val_loss: 1.0334 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0715 - acc: 0.4361 - val_loss: 1.0381 - val_acc: 0.4914\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0600 - acc: 0.4488 - val_loss: 1.0354 - val_acc: 0.4696\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0548 - acc: 0.4522 - val_loss: 1.0281 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 16s 2ms/step - loss: 1.0526 - acc: 0.4521 - val_loss: 1.0244 - val_acc: 0.5039\n",
      "1265/1265 [==============================] - 1s 965us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 91s 9ms/step - loss: 1.1143 - acc: 0.4050 - val_loss: 1.0295 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0733 - acc: 0.4306 - val_loss: 1.0350 - val_acc: 0.4953\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0615 - acc: 0.4481 - val_loss: 1.0234 - val_acc: 0.4969\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0577 - acc: 0.4457 - val_loss: 1.0216 - val_acc: 0.4977\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0565 - acc: 0.4525 - val_loss: 1.0205 - val_acc: 0.5031\n",
      "1265/1265 [==============================] - 1s 821us/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 101s 10ms/step - loss: 1.1011 - acc: 0.4171 - val_loss: 1.0394 - val_acc: 0.4720\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 17s 2ms/step - loss: 1.0685 - acc: 0.4362 - val_loss: 1.0349 - val_acc: 0.4868\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0606 - acc: 0.4413 - val_loss: 1.0262 - val_acc: 0.4945\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0537 - acc: 0.4457 - val_loss: 1.0236 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 1.0491 - acc: 0.4539 - val_loss: 1.0218 - val_acc: 0.4969\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 101s 10ms/step - loss: 1.1054 - acc: 0.4072 - val_loss: 1.0323 - val_acc: 0.4782\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0703 - acc: 0.4359 - val_loss: 1.0291 - val_acc: 0.4992\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0614 - acc: 0.4422 - val_loss: 1.0299 - val_acc: 0.4953\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0547 - acc: 0.4489 - val_loss: 1.0212 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0512 - acc: 0.4622 - val_loss: 1.0247 - val_acc: 0.5000\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 104s 10ms/step - loss: 1.1063 - acc: 0.4183 - val_loss: 1.0267 - val_acc: 0.4844\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0708 - acc: 0.4399 - val_loss: 1.0258 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0589 - acc: 0.4469 - val_loss: 1.0237 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0543 - acc: 0.4498 - val_loss: 1.0286 - val_acc: 0.5000\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 19s 2ms/step - loss: 1.0506 - acc: 0.4609 - val_loss: 1.0270 - val_acc: 0.4969\n",
      "1265/1265 [==============================] - 1s 1ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0984 - acc: 0.4070 - val_loss: 1.0485 - val_acc: 0.4595\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0690 - acc: 0.4368 - val_loss: 1.0307 - val_acc: 0.4899\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0652 - acc: 0.4410 - val_loss: 1.0385 - val_acc: 0.4611\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0552 - acc: 0.4498 - val_loss: 1.0280 - val_acc: 0.4969\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 20s 2ms/step - loss: 1.0540 - acc: 0.4510 - val_loss: 1.0285 - val_acc: 0.5008\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.1012 - acc: 0.4152 - val_loss: 1.0363 - val_acc: 0.4634\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0677 - acc: 0.4404 - val_loss: 1.0301 - val_acc: 0.4868\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0614 - acc: 0.4431 - val_loss: 1.0260 - val_acc: 0.4899\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0553 - acc: 0.4486 - val_loss: 1.0281 - val_acc: 0.4992\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0521 - acc: 0.4529 - val_loss: 1.0330 - val_acc: 0.4743\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0997 - acc: 0.4067 - val_loss: 1.0371 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0694 - acc: 0.4335 - val_loss: 1.0352 - val_acc: 0.4727\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0583 - acc: 0.4489 - val_loss: 1.0292 - val_acc: 0.4836\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0580 - acc: 0.4457 - val_loss: 1.0262 - val_acc: 0.4969\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0517 - acc: 0.4486 - val_loss: 1.0246 - val_acc: 0.4984\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.1042 - acc: 0.4011 - val_loss: 1.0374 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0707 - acc: 0.4252 - val_loss: 1.0320 - val_acc: 0.4844\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0609 - acc: 0.4432 - val_loss: 1.0305 - val_acc: 0.4945\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0589 - acc: 0.4468 - val_loss: 1.0291 - val_acc: 0.4930\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 21s 2ms/step - loss: 1.0517 - acc: 0.4497 - val_loss: 1.0290 - val_acc: 0.4945\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.1032 - acc: 0.4078 - val_loss: 1.0275 - val_acc: 0.4844\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0671 - acc: 0.4381 - val_loss: 1.0226 - val_acc: 0.4891\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0627 - acc: 0.4436 - val_loss: 1.0226 - val_acc: 0.49610s - loss: 1.0626 - acc: 0.4\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0550 - acc: 0.4464 - val_loss: 1.0228 - val_acc: 0.4914\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0538 - acc: 0.4465 - val_loss: 1.0229 - val_acc: 0.4922\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 138s 13ms/step - loss: 1.1043 - acc: 0.4102 - val_loss: 1.0306 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0709 - acc: 0.4360 - val_loss: 1.0278 - val_acc: 0.4868\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0616 - acc: 0.4393 - val_loss: 1.0314 - val_acc: 0.4883: 9s - loss: 1.0622 - acc - ETA: 8s - loss: 1.0608 - acc: 0.439 - ETA: 8s - loss: 1.0608 - acc: 0 - ETA: 8s - loss: 1.0621 - acc:\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0558 - acc: 0.4453 - val_loss: 1.0270 - val_acc: 0.4953 loss: 1.0559\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 23s 2ms/step - loss: 1.0528 - acc: 0.4473 - val_loss: 1.0254 - val_acc: 0.4883oss: 1.0527 - acc: - ETA: 1s - loss:  - ETA: 0s - loss: 1.0527 - acc\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0945 - acc: 0.4120 - val_loss: 1.0416 - val_acc: 0.4688\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0684 - acc: 0.4358 - val_loss: 1.0276 - val_acc: 0.4891\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0609 - acc: 0.4394 - val_loss: 1.0290 - val_acc: 0.4945\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0567 - acc: 0.4427 - val_loss: 1.0302 - val_acc: 0.4860\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 24s 2ms/step - loss: 1.0531 - acc: 0.4495 - val_loss: 1.0262 - val_acc: 0.4829\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0888 - acc: 0.4174 - val_loss: 1.0360 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0662 - acc: 0.4337 - val_loss: 1.0339 - val_acc: 0.4883\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0607 - acc: 0.4330 - val_loss: 1.0309 - val_acc: 0.4945\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0557 - acc: 0.4450 - val_loss: 1.0256 - val_acc: 0.4836\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 25s 2ms/step - loss: 1.0517 - acc: 0.4492 - val_loss: 1.0291 - val_acc: 0.4899\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0915 - acc: 0.4164 - val_loss: 1.0378 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0686 - acc: 0.4286 - val_loss: 1.0374 - val_acc: 0.4743\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0618 - acc: 0.4407 - val_loss: 1.0379 - val_acc: 0.4844\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0562 - acc: 0.4462 - val_loss: 1.0447 - val_acc: 0.4821\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0560 - acc: 0.4450 - val_loss: 1.0380 - val_acc: 0.4875\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0977 - acc: 0.4097 - val_loss: 1.0433 - val_acc: 0.4634\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0708 - acc: 0.4244 - val_loss: 1.0275 - val_acc: 0.4836\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0605 - acc: 0.4351 - val_loss: 1.0355 - val_acc: 0.4875\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0548 - acc: 0.4444 - val_loss: 1.0268 - val_acc: 0.4907\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 26s 3ms/step - loss: 1.0540 - acc: 0.4510 - val_loss: 1.0257 - val_acc: 0.4914\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0931 - acc: 0.4106 - val_loss: 1.0437 - val_acc: 0.4720\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0656 - acc: 0.4332 - val_loss: 1.0387 - val_acc: 0.4743\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0586 - acc: 0.4392 - val_loss: 1.0307 - val_acc: 0.4790\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0556 - acc: 0.4489 - val_loss: 1.0304 - val_acc: 0.4860\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 27s 3ms/step - loss: 1.0519 - acc: 0.4537 - val_loss: 1.0260 - val_acc: 0.4727\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 1.0863 - acc: 0.4173 - val_loss: 1.0380 - val_acc: 0.4696\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0625 - acc: 0.4347 - val_loss: 1.0403 - val_acc: 0.4759\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0581 - acc: 0.4483 - val_loss: 1.0317 - val_acc: 0.4759\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0550 - acc: 0.4481 - val_loss: 1.0344 - val_acc: 0.4860\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 28s 3ms/step - loss: 1.0511 - acc: 0.4517 - val_loss: 1.0256 - val_acc: 0.4891\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 1.0912 - acc: 0.4139 - val_loss: 1.0430 - val_acc: 0.4712\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0657 - acc: 0.4301 - val_loss: 1.0293 - val_acc: 0.4836\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0629 - acc: 0.4424 - val_loss: 1.0288 - val_acc: 0.4907\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0557 - acc: 0.4458 - val_loss: 1.0275 - val_acc: 0.4883\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0505 - acc: 0.4503 - val_loss: 1.0215 - val_acc: 0.5000\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0882 - acc: 0.4113 - val_loss: 1.0337 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0688 - acc: 0.4279 - val_loss: 1.0351 - val_acc: 0.4829\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0629 - acc: 0.4336 - val_loss: 1.0316 - val_acc: 0.4766\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0549 - acc: 0.4432 - val_loss: 1.0287 - val_acc: 0.4860\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0540 - acc: 0.4482 - val_loss: 1.0313 - val_acc: 0.4938\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 135s 13ms/step - loss: 1.0824 - acc: 0.4173 - val_loss: 1.0293 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0668 - acc: 0.4283 - val_loss: 1.0273 - val_acc: 0.4914\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0597 - acc: 0.4448 - val_loss: 1.0325 - val_acc: 0.4751\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0516 - acc: 0.4534 - val_loss: 1.0229 - val_acc: 0.4914\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 29s 3ms/step - loss: 1.0521 - acc: 0.4529 - val_loss: 1.0240 - val_acc: 0.5008\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 138s 14ms/step - loss: 1.0861 - acc: 0.4149 - val_loss: 1.0326 - val_acc: 0.4844\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0646 - acc: 0.4312 - val_loss: 1.0301 - val_acc: 0.4766\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0592 - acc: 0.4320 - val_loss: 1.0276 - val_acc: 0.4829\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 31s 3ms/step - loss: 1.0563 - acc: 0.4418 - val_loss: 1.0280 - val_acc: 0.4914\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 30s 3ms/step - loss: 1.0512 - acc: 0.4485 - val_loss: 1.0305 - val_acc: 0.4984\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0895 - acc: 0.4108 - val_loss: 1.0420 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0631 - acc: 0.4368 - val_loss: 1.0363 - val_acc: 0.4836\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0598 - acc: 0.4405 - val_loss: 1.0310 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0562 - acc: 0.4472 - val_loss: 1.0332 - val_acc: 0.4899\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 32s 3ms/step - loss: 1.0532 - acc: 0.4435 - val_loss: 1.0298 - val_acc: 0.4922\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "CPU times: user 21h 57min 15s, sys: 5h 10min 41s, total: 1d 3h 7min 57s\n",
      "Wall time: 7h 56min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "concatenated_gpt = {\n",
    "    fold: [np.concatenate(np.array(statement)) for statement in gpt[fold]['statement']]\n",
    "    for fold in gpt.keys()\n",
    "}\n",
    "\n",
    "gpt_rounds = [calculate_round(concatenated_gpt) for round in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{5: 0.4395256920294328,\n",
       "  6: 0.445849802607133,\n",
       "  7: 0.47114624534199834,\n",
       "  8: 0.44743083036935377,\n",
       "  9: 0.46245059321520354,\n",
       "  10: 0.46166007938121145,\n",
       "  11: 0.46640316238516405,\n",
       "  12: 0.46245059321520354,\n",
       "  13: 0.44664031658248,\n",
       "  14: 0.4545454549224009,\n",
       "  15: 0.4640316208831878,\n",
       "  16: 0.47826086984792715,\n",
       "  17: 0.4750988145590771,\n",
       "  18: 0.474308300725085,\n",
       "  19: 0.46640316243228236,\n",
       "  20: 0.4727272730099825,\n",
       "  21: 0.4727272730099825,\n",
       "  22: 0.47984189751591133,\n",
       "  23: 0.4703557315551245,\n",
       "  24: 0.47351778658482396,\n",
       "  25: 0.47905138377615586,\n",
       "  26: 0.4750988145119588,\n",
       "  27: 0.46798419005314823,\n",
       "  28: 0.46877470393425863,\n",
       "  29: 0.474308300725085,\n",
       "  30: 0.48142292518389557,\n",
       "  31: 0.4561264825432668,\n",
       "  32: 0.47826086984792715,\n",
       "  33: 0.46640316238516405,\n",
       "  34: 0.4632411070020774,\n",
       "  35: 0.474308300725085},\n",
       " {5: 0.43636363669346445,\n",
       "  6: 0.44901185799021964,\n",
       "  7: 0.4584980239981248,\n",
       "  8: 0.44664031648824337,\n",
       "  9: 0.46245059321520354,\n",
       "  10: 0.46245059321520354,\n",
       "  11: 0.46482213471717987,\n",
       "  12: 0.45138339958643253,\n",
       "  13: 0.4529644272072984,\n",
       "  14: 0.45533596870927473,\n",
       "  15: 0.46561264855117196,\n",
       "  16: 0.47747035606105337,\n",
       "  17: 0.4687747038871403,\n",
       "  18: 0.4521739133733063,\n",
       "  19: 0.4743083006779667,\n",
       "  20: 0.4577075101641327,\n",
       "  21: 0.47114624534199834,\n",
       "  22: 0.46007905166610896,\n",
       "  23: 0.47905138368191924,\n",
       "  24: 0.46166007907494255,\n",
       "  25: 0.48221343906500597,\n",
       "  26: 0.46640316238516405,\n",
       "  27: 0.45375494104129055,\n",
       "  28: 0.4687747038871403,\n",
       "  29: 0.455335968756393,\n",
       "  30: 0.4632411070020774,\n",
       "  31: 0.4632411070020774,\n",
       "  32: 0.4640316208831878,\n",
       "  33: 0.47667984217994297,\n",
       "  34: 0.44347826119939326,\n",
       "  35: 0.45296442716018015},\n",
       " {5: 0.46245059321520354,\n",
       "  6: 0.4553359686150381,\n",
       "  7: 0.45375494099417224,\n",
       "  8: 0.4584980239981248,\n",
       "  9: 0.4592885378792352,\n",
       "  10: 0.45375494104129055,\n",
       "  11: 0.46007905171322727,\n",
       "  12: 0.45533596870927473,\n",
       "  13: 0.44980237187133004,\n",
       "  14: 0.4569169963301406,\n",
       "  15: 0.44980237182421173,\n",
       "  16: 0.47667984217994297,\n",
       "  17: 0.46719367626627445,\n",
       "  18: 0.4703557315551245,\n",
       "  19: 0.4727272730099825,\n",
       "  20: 0.46719367617203783,\n",
       "  21: 0.46561264850405365,\n",
       "  22: 0.4679841900060299,\n",
       "  23: 0.4735177868910929,\n",
       "  24: 0.46640316243228236,\n",
       "  25: 0.46798419010026654,\n",
       "  26: 0.44822134425046417,\n",
       "  27: 0.4814229252310138,\n",
       "  28: 0.45454545482816433,\n",
       "  29: 0.4640316208831878,\n",
       "  30: 0.47667984217994297,\n",
       "  31: 0.46798419005314823,\n",
       "  32: 0.4735177868439746,\n",
       "  33: 0.44980237182421173,\n",
       "  34: 0.45296442716018015,\n",
       "  35: 0.474308300725085},\n",
       " {5: 0.44743083036935377,\n",
       "  6: 0.45217391332618806,\n",
       "  7: 0.4561264824490302,\n",
       "  8: 0.4513833995393142,\n",
       "  9: 0.4584980240452431,\n",
       "  10: 0.46640316243228236,\n",
       "  11: 0.4719367592231087,\n",
       "  12: 0.46482213471717987,\n",
       "  13: 0.4529644269010295,\n",
       "  14: 0.45296442716018015,\n",
       "  15: 0.4703557312488556,\n",
       "  16: 0.48221343906500597,\n",
       "  17: 0.47905138372903755,\n",
       "  18: 0.46719367621915614,\n",
       "  19: 0.4806324113499035,\n",
       "  20: 0.46640316233804574,\n",
       "  21: 0.4814229252310138,\n",
       "  22: 0.4679841900060299,\n",
       "  23: 0.46561264859829027,\n",
       "  24: 0.47351778693821117,\n",
       "  25: 0.46877470393425863,\n",
       "  26: 0.4687747038871403,\n",
       "  27: 0.4766798422270613,\n",
       "  28: 0.47747035575478447,\n",
       "  29: 0.4750988146061954,\n",
       "  30: 0.47905138372903755,\n",
       "  31: 0.4466403165353617,\n",
       "  32: 0.4529644272072984,\n",
       "  33: 0.44743083032223546,\n",
       "  34: 0.45770750995210036,\n",
       "  35: 0.4584980240452431},\n",
       " {5: 0.4387351781954407,\n",
       "  6: 0.45217391332618806,\n",
       "  7: 0.47114624534199834,\n",
       "  8: 0.4529644272072984,\n",
       "  9: 0.46640316238516405,\n",
       "  10: 0.46166007938121145,\n",
       "  11: 0.4569169963772589,\n",
       "  12: 0.46166007938121145,\n",
       "  13: 0.46561264855117196,\n",
       "  14: 0.4521739133733063,\n",
       "  15: 0.46640316238516405,\n",
       "  16: 0.47667984217994297,\n",
       "  17: 0.46877470393425863,\n",
       "  18: 0.46324110709631394,\n",
       "  19: 0.4727272730571008,\n",
       "  20: 0.4600790517603456,\n",
       "  21: 0.4727272730571008,\n",
       "  22: 0.4703557315551245,\n",
       "  23: 0.46482213471717987,\n",
       "  24: 0.4648221344109109,\n",
       "  25: 0.47984189756302964,\n",
       "  26: 0.47747035606105337,\n",
       "  27: 0.47747035606105337,\n",
       "  28: 0.468774703840022,\n",
       "  29: 0.4403162058634249,\n",
       "  30: 0.4569169963301406,\n",
       "  31: 0.46719367621915614,\n",
       "  32: 0.4861660081878481,\n",
       "  33: 0.4703557315551245,\n",
       "  34: 0.4703557315551245,\n",
       "  35: 0.46166007938121145}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Round 0",
         "type": "scatter",
         "uid": "90431a2e-f0f0-4283-8d4d-67fb42dc3125",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.4395256920294328,
          0.445849802607133,
          0.47114624534199834,
          0.44743083036935377,
          0.46245059321520354,
          0.46166007938121145,
          0.46640316238516405,
          0.46245059321520354,
          0.44664031658248,
          0.4545454549224009,
          0.4640316208831878,
          0.47826086984792715,
          0.4750988145590771,
          0.474308300725085,
          0.46640316243228236,
          0.4727272730099825,
          0.4727272730099825,
          0.47984189751591133,
          0.4703557315551245,
          0.47351778658482396,
          0.47905138377615586,
          0.4750988145119588,
          0.46798419005314823,
          0.46877470393425863,
          0.474308300725085,
          0.48142292518389557,
          0.4561264825432668,
          0.47826086984792715,
          0.46640316238516405,
          0.4632411070020774,
          0.474308300725085
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 1",
         "type": "scatter",
         "uid": "9fce59f5-8b0a-4cd3-b5eb-ada1667906c0",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.43636363669346445,
          0.44901185799021964,
          0.4584980239981248,
          0.44664031648824337,
          0.46245059321520354,
          0.46245059321520354,
          0.46482213471717987,
          0.45138339958643253,
          0.4529644272072984,
          0.45533596870927473,
          0.46561264855117196,
          0.47747035606105337,
          0.4687747038871403,
          0.4521739133733063,
          0.4743083006779667,
          0.4577075101641327,
          0.47114624534199834,
          0.46007905166610896,
          0.47905138368191924,
          0.46166007907494255,
          0.48221343906500597,
          0.46640316238516405,
          0.45375494104129055,
          0.4687747038871403,
          0.455335968756393,
          0.4632411070020774,
          0.4632411070020774,
          0.4640316208831878,
          0.47667984217994297,
          0.44347826119939326,
          0.45296442716018015
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 2",
         "type": "scatter",
         "uid": "02ccb3be-804f-4448-a899-89b8aa1541c7",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.46245059321520354,
          0.4553359686150381,
          0.45375494099417224,
          0.4584980239981248,
          0.4592885378792352,
          0.45375494104129055,
          0.46007905171322727,
          0.45533596870927473,
          0.44980237187133004,
          0.4569169963301406,
          0.44980237182421173,
          0.47667984217994297,
          0.46719367626627445,
          0.4703557315551245,
          0.4727272730099825,
          0.46719367617203783,
          0.46561264850405365,
          0.4679841900060299,
          0.4735177868910929,
          0.46640316243228236,
          0.46798419010026654,
          0.44822134425046417,
          0.4814229252310138,
          0.45454545482816433,
          0.4640316208831878,
          0.47667984217994297,
          0.46798419005314823,
          0.4735177868439746,
          0.44980237182421173,
          0.45296442716018015,
          0.474308300725085
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 3",
         "type": "scatter",
         "uid": "6161ec66-a770-47d8-b0f1-e94cdea3c0cc",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.44743083036935377,
          0.45217391332618806,
          0.4561264824490302,
          0.4513833995393142,
          0.4584980240452431,
          0.46640316243228236,
          0.4719367592231087,
          0.46482213471717987,
          0.4529644269010295,
          0.45296442716018015,
          0.4703557312488556,
          0.48221343906500597,
          0.47905138372903755,
          0.46719367621915614,
          0.4806324113499035,
          0.46640316233804574,
          0.4814229252310138,
          0.4679841900060299,
          0.46561264859829027,
          0.47351778693821117,
          0.46877470393425863,
          0.4687747038871403,
          0.4766798422270613,
          0.47747035575478447,
          0.4750988146061954,
          0.47905138372903755,
          0.4466403165353617,
          0.4529644272072984,
          0.44743083032223546,
          0.45770750995210036,
          0.4584980240452431
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 4",
         "type": "scatter",
         "uid": "200feabd-2f01-4144-9ec8-838f6efcfc17",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.4387351781954407,
          0.45217391332618806,
          0.47114624534199834,
          0.4529644272072984,
          0.46640316238516405,
          0.46166007938121145,
          0.4569169963772589,
          0.46166007938121145,
          0.46561264855117196,
          0.4521739133733063,
          0.46640316238516405,
          0.47667984217994297,
          0.46877470393425863,
          0.46324110709631394,
          0.4727272730571008,
          0.4600790517603456,
          0.4727272730571008,
          0.4703557315551245,
          0.46482213471717987,
          0.4648221344109109,
          0.47984189756302964,
          0.47747035606105337,
          0.47747035606105337,
          0.468774703840022,
          0.4403162058634249,
          0.4569169963301406,
          0.46719367621915614,
          0.4861660081878481,
          0.4703557315551245,
          0.4703557315551245,
          0.46166007938121145
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "Test set accuracy of padded GPT dataset with variable maximum lengths"
        }
       }
      },
      "text/html": [
       "<div id=\"511383ca-9ebb-4cd0-aa95-5e21efdfd702\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"511383ca-9ebb-4cd0-aa95-5e21efdfd702\")) {\n",
       "    Plotly.newPlot(\"511383ca-9ebb-4cd0-aa95-5e21efdfd702\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4395256920294328, 0.445849802607133, 0.47114624534199834, 0.44743083036935377, 0.46245059321520354, 0.46166007938121145, 0.46640316238516405, 0.46245059321520354, 0.44664031658248, 0.4545454549224009, 0.4640316208831878, 0.47826086984792715, 0.4750988145590771, 0.474308300725085, 0.46640316243228236, 0.4727272730099825, 0.4727272730099825, 0.47984189751591133, 0.4703557315551245, 0.47351778658482396, 0.47905138377615586, 0.4750988145119588, 0.46798419005314823, 0.46877470393425863, 0.474308300725085, 0.48142292518389557, 0.4561264825432668, 0.47826086984792715, 0.46640316238516405, 0.4632411070020774, 0.474308300725085], \"type\": \"scatter\", \"uid\": \"90431a2e-f0f0-4283-8d4d-67fb42dc3125\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.43636363669346445, 0.44901185799021964, 0.4584980239981248, 0.44664031648824337, 0.46245059321520354, 0.46245059321520354, 0.46482213471717987, 0.45138339958643253, 0.4529644272072984, 0.45533596870927473, 0.46561264855117196, 0.47747035606105337, 0.4687747038871403, 0.4521739133733063, 0.4743083006779667, 0.4577075101641327, 0.47114624534199834, 0.46007905166610896, 0.47905138368191924, 0.46166007907494255, 0.48221343906500597, 0.46640316238516405, 0.45375494104129055, 0.4687747038871403, 0.455335968756393, 0.4632411070020774, 0.4632411070020774, 0.4640316208831878, 0.47667984217994297, 0.44347826119939326, 0.45296442716018015], \"type\": \"scatter\", \"uid\": \"9fce59f5-8b0a-4cd3-b5eb-ada1667906c0\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.46245059321520354, 0.4553359686150381, 0.45375494099417224, 0.4584980239981248, 0.4592885378792352, 0.45375494104129055, 0.46007905171322727, 0.45533596870927473, 0.44980237187133004, 0.4569169963301406, 0.44980237182421173, 0.47667984217994297, 0.46719367626627445, 0.4703557315551245, 0.4727272730099825, 0.46719367617203783, 0.46561264850405365, 0.4679841900060299, 0.4735177868910929, 0.46640316243228236, 0.46798419010026654, 0.44822134425046417, 0.4814229252310138, 0.45454545482816433, 0.4640316208831878, 0.47667984217994297, 0.46798419005314823, 0.4735177868439746, 0.44980237182421173, 0.45296442716018015, 0.474308300725085], \"type\": \"scatter\", \"uid\": \"02ccb3be-804f-4448-a899-89b8aa1541c7\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.44743083036935377, 0.45217391332618806, 0.4561264824490302, 0.4513833995393142, 0.4584980240452431, 0.46640316243228236, 0.4719367592231087, 0.46482213471717987, 0.4529644269010295, 0.45296442716018015, 0.4703557312488556, 0.48221343906500597, 0.47905138372903755, 0.46719367621915614, 0.4806324113499035, 0.46640316233804574, 0.4814229252310138, 0.4679841900060299, 0.46561264859829027, 0.47351778693821117, 0.46877470393425863, 0.4687747038871403, 0.4766798422270613, 0.47747035575478447, 0.4750988146061954, 0.47905138372903755, 0.4466403165353617, 0.4529644272072984, 0.44743083032223546, 0.45770750995210036, 0.4584980240452431], \"type\": \"scatter\", \"uid\": \"6161ec66-a770-47d8-b0f1-e94cdea3c0cc\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4387351781954407, 0.45217391332618806, 0.47114624534199834, 0.4529644272072984, 0.46640316238516405, 0.46166007938121145, 0.4569169963772589, 0.46166007938121145, 0.46561264855117196, 0.4521739133733063, 0.46640316238516405, 0.47667984217994297, 0.46877470393425863, 0.46324110709631394, 0.4727272730571008, 0.4600790517603456, 0.4727272730571008, 0.4703557315551245, 0.46482213471717987, 0.4648221344109109, 0.47984189756302964, 0.47747035606105337, 0.47747035606105337, 0.468774703840022, 0.4403162058634249, 0.4569169963301406, 0.46719367621915614, 0.4861660081878481, 0.4703557315551245, 0.4703557315551245, 0.46166007938121145], \"type\": \"scatter\", \"uid\": \"200feabd-2f01-4144-9ec8-838f6efcfc17\"}], {\"title\": {\"text\": \"Test set accuracy of padded GPT dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"511383ca-9ebb-4cd0-aa95-5e21efdfd702\")) {window._Plotly.Plots.resize(document.getElementById(\"511383ca-9ebb-4cd0-aa95-5e21efdfd702\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"511383ca-9ebb-4cd0-aa95-5e21efdfd702\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"511383ca-9ebb-4cd0-aa95-5e21efdfd702\")) {\n",
       "    Plotly.newPlot(\"511383ca-9ebb-4cd0-aa95-5e21efdfd702\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4395256920294328, 0.445849802607133, 0.47114624534199834, 0.44743083036935377, 0.46245059321520354, 0.46166007938121145, 0.46640316238516405, 0.46245059321520354, 0.44664031658248, 0.4545454549224009, 0.4640316208831878, 0.47826086984792715, 0.4750988145590771, 0.474308300725085, 0.46640316243228236, 0.4727272730099825, 0.4727272730099825, 0.47984189751591133, 0.4703557315551245, 0.47351778658482396, 0.47905138377615586, 0.4750988145119588, 0.46798419005314823, 0.46877470393425863, 0.474308300725085, 0.48142292518389557, 0.4561264825432668, 0.47826086984792715, 0.46640316238516405, 0.4632411070020774, 0.474308300725085], \"type\": \"scatter\", \"uid\": \"90431a2e-f0f0-4283-8d4d-67fb42dc3125\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.43636363669346445, 0.44901185799021964, 0.4584980239981248, 0.44664031648824337, 0.46245059321520354, 0.46245059321520354, 0.46482213471717987, 0.45138339958643253, 0.4529644272072984, 0.45533596870927473, 0.46561264855117196, 0.47747035606105337, 0.4687747038871403, 0.4521739133733063, 0.4743083006779667, 0.4577075101641327, 0.47114624534199834, 0.46007905166610896, 0.47905138368191924, 0.46166007907494255, 0.48221343906500597, 0.46640316238516405, 0.45375494104129055, 0.4687747038871403, 0.455335968756393, 0.4632411070020774, 0.4632411070020774, 0.4640316208831878, 0.47667984217994297, 0.44347826119939326, 0.45296442716018015], \"type\": \"scatter\", \"uid\": \"9fce59f5-8b0a-4cd3-b5eb-ada1667906c0\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.46245059321520354, 0.4553359686150381, 0.45375494099417224, 0.4584980239981248, 0.4592885378792352, 0.45375494104129055, 0.46007905171322727, 0.45533596870927473, 0.44980237187133004, 0.4569169963301406, 0.44980237182421173, 0.47667984217994297, 0.46719367626627445, 0.4703557315551245, 0.4727272730099825, 0.46719367617203783, 0.46561264850405365, 0.4679841900060299, 0.4735177868910929, 0.46640316243228236, 0.46798419010026654, 0.44822134425046417, 0.4814229252310138, 0.45454545482816433, 0.4640316208831878, 0.47667984217994297, 0.46798419005314823, 0.4735177868439746, 0.44980237182421173, 0.45296442716018015, 0.474308300725085], \"type\": \"scatter\", \"uid\": \"02ccb3be-804f-4448-a899-89b8aa1541c7\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.44743083036935377, 0.45217391332618806, 0.4561264824490302, 0.4513833995393142, 0.4584980240452431, 0.46640316243228236, 0.4719367592231087, 0.46482213471717987, 0.4529644269010295, 0.45296442716018015, 0.4703557312488556, 0.48221343906500597, 0.47905138372903755, 0.46719367621915614, 0.4806324113499035, 0.46640316233804574, 0.4814229252310138, 0.4679841900060299, 0.46561264859829027, 0.47351778693821117, 0.46877470393425863, 0.4687747038871403, 0.4766798422270613, 0.47747035575478447, 0.4750988146061954, 0.47905138372903755, 0.4466403165353617, 0.4529644272072984, 0.44743083032223546, 0.45770750995210036, 0.4584980240452431], \"type\": \"scatter\", \"uid\": \"6161ec66-a770-47d8-b0f1-e94cdea3c0cc\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4387351781954407, 0.45217391332618806, 0.47114624534199834, 0.4529644272072984, 0.46640316238516405, 0.46166007938121145, 0.4569169963772589, 0.46166007938121145, 0.46561264855117196, 0.4521739133733063, 0.46640316238516405, 0.47667984217994297, 0.46877470393425863, 0.46324110709631394, 0.4727272730571008, 0.4600790517603456, 0.4727272730571008, 0.4703557315551245, 0.46482213471717987, 0.4648221344109109, 0.47984189756302964, 0.47747035606105337, 0.47747035606105337, 0.468774703840022, 0.4403162058634249, 0.4569169963301406, 0.46719367621915614, 0.4861660081878481, 0.4703557315551245, 0.4703557315551245, 0.46166007938121145], \"type\": \"scatter\", \"uid\": \"200feabd-2f01-4144-9ec8-838f6efcfc17\"}], {\"title\": {\"text\": \"Test set accuracy of padded GPT dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"511383ca-9ebb-4cd0-aa95-5e21efdfd702\")) {window._Plotly.Plots.resize(document.getElementById(\"511383ca-9ebb-4cd0-aa95-5e21efdfd702\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traces = gpt_rounds\n",
    "\n",
    "# Create traces\n",
    "def create_scatter(counter):\n",
    "    acc_dict = traces[counter]\n",
    "    \n",
    "    return go.Scatter(\n",
    "        x = list(acc_dict.keys()),\n",
    "        y = list(acc_dict.values()),\n",
    "        mode = 'lines+markers',\n",
    "        name = 'Round ' + str(counter)\n",
    "    )\n",
    "\n",
    "trace_data = [create_scatter(trace) for trace in range(len(traces))]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'Test set accuracy of padded GPT dataset with variable maximum lengths',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = trace_data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "fill": "tozerox",
         "fillcolor": "rgba(0,100,80,0.2)",
         "line": {
          "color": "rgba(255,255,255,0)"
         },
         "name": "BERT",
         "showlegend": false,
         "type": "scatter",
         "uid": "1c8871ab-5e2c-459e-a40b-c22d2287ec1d",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          35,
          34,
          33,
          32,
          31,
          30,
          29,
          28,
          27,
          26,
          25,
          24,
          23,
          22,
          21,
          20,
          19,
          18,
          17,
          16,
          15,
          14,
          13,
          12,
          11,
          10,
          9,
          8,
          7,
          6,
          5
         ],
         "y": [
          0.513043478637816,
          0.5122529648038239,
          0.5090909094207372,
          0.5090909091144682,
          0.5098814233018476,
          0.5122529647567056,
          0.5075098817998712,
          0.5146245063058001,
          0.505928854131887,
          0.5027667987488004,
          0.5106719368295707,
          0.5106719367824524,
          0.513043478637816,
          0.5122529648038239,
          0.5106719371358397,
          0.5177865616417685,
          0.5193675893097527,
          0.5256916999816894,
          0.5233201584797131,
          0.522529644645721,
          0.5169960478077764,
          0.521739130811729,
          0.5201581031437448,
          0.5154150201397922,
          0.5209486169777369,
          0.5169960478077764,
          0.5162055336203971,
          0.5154150201397922,
          0.5130434783315471,
          0.5154150201397922,
          0.5185770754286423,
          0.5067193679658791,
          0.49723320195797405,
          0.5043478264639029,
          0.5043478264639029,
          0.5011857711279345,
          0.5027667987959187,
          0.4996047434599503,
          0.505928854131887,
          0.5114624509698318,
          0.5106719367824524,
          0.505138340297895,
          0.5059288540847687,
          0.5106719368295707,
          0.5019762849619266,
          0.5106719371358397,
          0.5043478264167846,
          0.5051383402507766,
          0.5019762846085394,
          0.4956521739366026,
          0.49802371543857893,
          0.4996047431065631,
          0.488537549784061,
          0.4940711466220057,
          0.4996047434599503,
          0.49723320195797405,
          0.4996047434599503,
          0.49407114631573673,
          0.49090909128603727,
          0.4948616604559978,
          0.5027667984425315,
          0.49249011895402145
         ]
        },
        {
         "line": {
          "color": "rgb(0,100,80)"
         },
         "mode": "lines+markers",
         "name": "BERT",
         "type": "scatter",
         "uid": "23f45320-2984-4306-b778-690a059ffcd1",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5048221346936207,
          0.5081422926550326,
          0.5021343876392003,
          0.5038735179703225,
          0.5027667985226326,
          0.5067193679564556,
          0.502924901482616,
          0.5065612651378271,
          0.5000790517414982,
          0.4932806327785899,
          0.5057707512237337,
          0.504822134410911,
          0.5037154153071844,
          0.5076679844158911,
          0.5081422927869638,
          0.5119367591807024,
          0.5141501979347274,
          0.5130434785671384,
          0.5168379449184705,
          0.51478260892182,
          0.5120948619757717,
          0.5162055339031069,
          0.5171541505038973,
          0.5092490121734,
          0.5098814233018476,
          0.5097233205350491,
          0.5098814230898153,
          0.5095652176975732,
          0.5089328065691258,
          0.5076679844959922,
          0.5109881425893354
         ]
        },
        {
         "fill": "tozerox",
         "fillcolor": "rgba(0,176,246,0.2)",
         "line": {
          "color": "rgba(255,255,255,0)"
         },
         "name": "ELMo",
         "showlegend": false,
         "type": "scatter",
         "uid": "e6e423fd-eb9a-429d-a241-564091ec7df1",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          35,
          34,
          33,
          32,
          31,
          30,
          29,
          28,
          27,
          26,
          25,
          24,
          23,
          22,
          21,
          20,
          19,
          18,
          17,
          16,
          15,
          14,
          13,
          12,
          11,
          10,
          9,
          8,
          7,
          6,
          5
         ],
         "y": [
          0.5177865616417685,
          0.5264822137685633,
          0.5146245062586818,
          0.5233201584325948,
          0.5154150201397922,
          0.5075098817998712,
          0.5027667987488004,
          0.5146245063058001,
          0.5027667987488004,
          0.5122529648038239,
          0.5090909094207372,
          0.524901186100579,
          0.5169960477606581,
          0.5177865616417685,
          0.516205533926666,
          0.5185770754286423,
          0.5304347829385238,
          0.5312252967725158,
          0.5217391307646106,
          0.5193675892626344,
          0.5209486169306186,
          0.5233201584325948,
          0.5169960478077764,
          0.5201581030966265,
          0.5241106723137052,
          0.5209486169306186,
          0.5233201584325948,
          0.5177865615946502,
          0.5185770754286423,
          0.513833992471808,
          0.5225296445986027,
          0.5035573125827925,
          0.499604743412832,
          0.5090909094207372,
          0.5090909094678555,
          0.5035573126299108,
          0.5098814233018476,
          0.4988142295788399,
          0.5122529648038239,
          0.505138340297895,
          0.5122529648038239,
          0.5138339924246897,
          0.5106719370887214,
          0.5011857710808162,
          0.5106719370887214,
          0.5067193679187609,
          0.49802371574484783,
          0.5059288540847687,
          0.5090909094207372,
          0.507509881752753,
          0.5067193679187609,
          0.4932806327408953,
          0.49723320191085574,
          0.49169960507291105,
          0.49090909123891896,
          0.49090909123891896,
          0.4996047434599503,
          0.4996047431065631,
          0.5067193676124919,
          0.5003952572468241,
          0.5130434785906977,
          0.505928854131887
         ]
        },
        {
         "line": {
          "color": "rgb(0,176,246)"
         },
         "mode": "lines+markers",
         "name": "ELMo",
         "type": "scatter",
         "uid": "d5441fb1-3fb8-4f78-9293-bdb0d7dfe19e",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5127272730853718,
          0.5209486169494658,
          0.5086166011297656,
          0.5135177868392627,
          0.5083003955443386,
          0.5037154153684382,
          0.4977075101499972,
          0.4997628461984777,
          0.49802371574484783,
          0.5048221347077562,
          0.5051383402507768,
          0.5144664035107308,
          0.5128853758144756,
          0.514466403501307,
          0.5106719370792977,
          0.5090909094301608,
          0.5177865615946502,
          0.5209486169494658,
          0.5144664034306297,
          0.5133596841337182,
          0.5168379450032834,
          0.5176284588561227,
          0.5113043481747624,
          0.5158893284213402,
          0.5157312256356944,
          0.5165217394696866,
          0.511462450932137,
          0.5128853758427466,
          0.513675889667315,
          0.5090909094301608,
          0.513517786891093
         ]
        },
        {
         "fill": "tozerox",
         "fillcolor": "rgba(231,107,243,0.2)",
         "line": {
          "color": "rgba(255,255,255,0)"
         },
         "name": "Transformer-XL",
         "showlegend": false,
         "type": "scatter",
         "uid": "ca01c846-84a5-4c8f-a82c-8986d28a2b2c",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          35,
          34,
          33,
          32,
          31,
          30,
          29,
          28,
          27,
          26,
          25,
          24,
          23,
          22,
          21,
          20,
          19,
          18,
          17,
          16,
          15,
          14,
          13,
          12,
          11,
          10,
          9,
          8,
          7,
          6,
          5
         ],
         "y": [
          0.5003952572468241,
          0.4940711466220057,
          0.4996047434599503,
          0.5106719370416031,
          0.49802371574484783,
          0.49169960512002936,
          0.5019762849619266,
          0.49723320191085574,
          0.49565217428998987,
          0.4988142296259582,
          0.49723320191085574,
          0.4988142296259582,
          0.49090909093265006,
          0.4956521739366026,
          0.5027667987488004,
          0.5090909094678555,
          0.5059288540847687,
          0.499604743412832,
          0.4948616604559978,
          0.4988142296259582,
          0.49723320191085574,
          0.4988142295788399,
          0.5011857710808162,
          0.499604743412832,
          0.49802371579196614,
          0.4948616604559978,
          0.5043478264167846,
          0.5011857710336979,
          0.5051383399445077,
          0.4948616604559978,
          0.5027667987488004,
          0.49169960512002936,
          0.4853754944009743,
          0.48458498061410055,
          0.4822134391121242,
          0.4735177868910929,
          0.4869565221160768,
          0.4885375497369427,
          0.4924901189069032,
          0.4877470359500689,
          0.48063241144414004,
          0.47984189751591133,
          0.4766798422270613,
          0.488537549784061,
          0.4861660082820847,
          0.4893280636180531,
          0.48063241144414004,
          0.48142292527813213,
          0.4837944664267212,
          0.47826086994216377,
          0.4885375497369427,
          0.47905138377615586,
          0.488537549784061,
          0.4869565217626896,
          0.47984189730387905,
          0.47905138372903755,
          0.4853754944009743,
          0.4766798422270613,
          0.4869565221160768,
          0.48616600792869746,
          0.474308300725085,
          0.49090909123891896
         ]
        },
        {
         "line": {
          "color": "rgb(231,107,243)"
         },
         "mode": "lines+markers",
         "name": "Transformer-XL",
         "type": "scatter",
         "uid": "b093c7ee-a18c-4a8c-b5c5-a4fb4e36b755",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.49501976318510155,
          0.48616600819255995,
          0.49438735206607776,
          0.49660079080125563,
          0.48932806358035846,
          0.4898023718807537,
          0.4890118580467616,
          0.4896442690715488,
          0.49059288561108555,
          0.4926482217113962,
          0.4890118580750326,
          0.49312252999294415,
          0.48711462478392675,
          0.489644269010295,
          0.4935968383027631,
          0.49501976315211876,
          0.4962845851969814,
          0.4924901189163268,
          0.49106719404341204,
          0.4882213441703631,
          0.48837944696072066,
          0.49169960508233473,
          0.4932806327597426,
          0.4953359687092747,
          0.4939130437468352,
          0.4910671940245647,
          0.4858498027107932,
          0.49375494098946043,
          0.49075098842029047,
          0.4894861663471569,
          0.4962845853289125
         ]
        },
        {
         "fill": "tozerox",
         "fillcolor": "rgba(205,12,24,0.2)",
         "line": {
          "color": "rgba(205,12,24,0)"
         },
         "name": "Flair",
         "showlegend": false,
         "type": "scatter",
         "uid": "f59b6fb1-c839-4665-8d69-76837eac2c6c",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          35,
          34,
          33,
          32,
          31,
          30,
          29,
          28,
          27,
          26,
          25,
          24,
          23,
          22,
          21,
          20,
          19,
          18,
          17,
          16,
          15,
          14,
          13,
          12,
          11,
          10,
          9,
          8,
          7,
          6,
          5
         ],
         "y": [
          0.4901185771457763,
          0.5011857711279345,
          0.5035573122765236,
          0.504347826157634,
          0.49644268777059475,
          0.4996047434599503,
          0.49802371543857893,
          0.49644268812398196,
          0.4940711465748874,
          0.499604743412832,
          0.5090909094207372,
          0.5114624509227135,
          0.5122529647567056,
          0.5003952572939424,
          0.5035573122765236,
          0.5083003955867451,
          0.5051383402507766,
          0.507509881752753,
          0.5090909094678555,
          0.5035573126299108,
          0.5075098817998712,
          0.5011857710808162,
          0.4996047434599503,
          0.5035573125827925,
          0.5059288538256181,
          0.5067193676596102,
          0.5035573126299108,
          0.49802371543857893,
          0.507509881446484,
          0.5019762847498943,
          0.5098814232547293,
          0.4869565221160768,
          0.49644268812398196,
          0.4814229249247449,
          0.49169960512002936,
          0.48774703569091826,
          0.49169960486087877,
          0.4940711466220057,
          0.49249011864775255,
          0.49090909123891896,
          0.48932806335890244,
          0.49249011860063424,
          0.4940711462686184,
          0.5019762849619266,
          0.49565217424287156,
          0.49802371574484783,
          0.499604743412832,
          0.4885375497369427,
          0.49011857740492687,
          0.4940711465748874,
          0.4996047431065631,
          0.49486166040887947,
          0.48458498061410055,
          0.4877470359029506,
          0.49328063243462633,
          0.49169960512002936,
          0.4885375494306738,
          0.4822134391121242,
          0.48458498061410055,
          0.4940711462686184,
          0.4885375494306738,
          0.48616600792869746
         ]
        },
        {
         "line": {
          "color": "rgb(205,12,24)"
         },
         "mode": "lines+markers",
         "name": "Flair",
         "type": "scatter",
         "uid": "f607d209-2c3e-45c5-a8ba-e86796cb2a9e",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.4880632412716334,
          0.49201581044159387,
          0.49691699607099,
          0.4959683795503006,
          0.49059288560166187,
          0.49517786577756223,
          0.4948616602439654,
          0.4945454547009449,
          0.4918577078491332,
          0.49343873546528716,
          0.5011857709677323,
          0.5064031623333338,
          0.5052964430081514,
          0.4954940714430903,
          0.49486166035704926,
          0.5038735181258129,
          0.5015019766238367,
          0.5037154152553543,
          0.5052964429233385,
          0.5002371543857891,
          0.4994466404905432,
          0.49660079064576523,
          0.4956521740779575,
          0.4983399211842081,
          0.5010276681773747,
          0.4983399211418016,
          0.49802371560820474,
          0.4961264823783528,
          0.49407114640054967,
          0.49865612674607596,
          0.49802371570244136
         ]
        },
        {
         "fill": "tozerox",
         "fillcolor": "rgba(81, 45, 168, 0.2)",
         "line": {
          "color": "rgba(81, 45, 168, 0)"
         },
         "name": "GPT",
         "showlegend": false,
         "type": "scatter",
         "uid": "252a4b7a-9c8a-45fd-a7be-18f1017e0958",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          35,
          34,
          33,
          32,
          31,
          30,
          29,
          28,
          27,
          26,
          25,
          24,
          23,
          22,
          21,
          20,
          19,
          18,
          17,
          16,
          15,
          14,
          13,
          12,
          11,
          10,
          9,
          8,
          7,
          6,
          5
         ],
         "y": [
          0.46245059321520354,
          0.4553359686150381,
          0.47114624534199834,
          0.4584980239981248,
          0.46640316238516405,
          0.46640316243228236,
          0.4719367592231087,
          0.46482213471717987,
          0.46561264855117196,
          0.4569169963301406,
          0.4703557312488556,
          0.48221343906500597,
          0.47905138372903755,
          0.474308300725085,
          0.4806324113499035,
          0.4727272730099825,
          0.4814229252310138,
          0.47984189751591133,
          0.47905138368191924,
          0.47351778693821117,
          0.48221343906500597,
          0.47747035606105337,
          0.4814229252310138,
          0.47747035575478447,
          0.4750988146061954,
          0.48142292518389557,
          0.46798419005314823,
          0.4861660081878481,
          0.47667984217994297,
          0.4703557315551245,
          0.474308300725085,
          0.45296442716018015,
          0.44347826119939326,
          0.44743083032223546,
          0.4529644272072984,
          0.4466403165353617,
          0.4569169963301406,
          0.4403162058634249,
          0.45454545482816433,
          0.45375494104129055,
          0.44822134425046417,
          0.46798419010026654,
          0.46166007907494255,
          0.46482213471717987,
          0.46007905166610896,
          0.46561264850405365,
          0.4577075101641327,
          0.46640316243228236,
          0.4521739133733063,
          0.46719367626627445,
          0.47667984217994297,
          0.44980237182421173,
          0.4521739133733063,
          0.44664031658248,
          0.45138339958643253,
          0.4569169963772589,
          0.45375494104129055,
          0.4584980240452431,
          0.44664031648824337,
          0.45375494099417224,
          0.445849802607133,
          0.43636363669346445
         ]
        },
        {
         "line": {
          "color": "rgb(81, 45, 168)"
         },
         "mode": "lines+markers",
         "name": "GPT",
         "type": "scatter",
         "uid": "d151f824-c071-4801-841b-be0b8780e089",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.44490118610057905,
          0.4509090911729534,
          0.46213438762506476,
          0.4513833995204669,
          0.4618181821480099,
          0.4611857710902399,
          0.4640316208831877,
          0.45913043512186047,
          0.45359683822266195,
          0.45438735209906056,
          0.4632411069785182,
          0.47826086986677446,
          0.47177865647515765,
          0.4654545457937972,
          0.47335968410544715,
          0.4648221346889089,
          0.47272727302882983,
          0.4692490121498409,
          0.4706719370887214,
          0.46798418988823426,
          0.47557312288774334,
          0.4671936762191562,
          0.4714624509227134,
          0.4676679844488739,
          0.4618181821668572,
          0.4714624508850188,
          0.46023715447060204,
          0.47098814259404725,
          0.46213438765333575,
          0.4575494073737751,
          0.464347826407361
         ]
        },
        {
         "line": {
          "color": "rgb(0,0,0)"
         },
         "mode": "lines",
         "name": "Majority vote",
         "type": "scatter",
         "uid": "6250b400-661f-4f53-aa40-c0f69d5643f3",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428
         ]
        },
        {
         "mode": "lines",
         "name": "Khurana (2017)",
         "type": "scatter",
         "uid": "294266b7-fc65-440a-aafe-c50a7b975a95",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903
         ]
        }
       ],
       "layout": {
        "paper_bgcolor": "rgb(255,255,255)",
        "plot_bgcolor": "rgb(229,229,229)",
        "title": {
         "text": "Bi-LSTM test set accuracy of padded datasets with variable maximum lengths"
        },
        "xaxis": {
         "gridcolor": "rgb(255,255,255)",
         "range": [
          5,
          35
         ],
         "showgrid": true,
         "showline": false,
         "showticklabels": true,
         "tickcolor": "rgb(127,127,127)",
         "ticks": "outside",
         "zeroline": false
        },
        "yaxis": {
         "gridcolor": "rgb(255,255,255)",
         "showgrid": true,
         "showline": false,
         "showticklabels": true,
         "tickcolor": "rgb(127,127,127)",
         "ticks": "outside",
         "zeroline": false
        }
       }
      },
      "text/html": [
       "<div id=\"b4d5a432-3a0b-4a92-84e5-118138df3a23\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"b4d5a432-3a0b-4a92-84e5-118138df3a23\")) {\n",
       "    Plotly.newPlot(\"b4d5a432-3a0b-4a92-84e5-118138df3a23\", [{\"fill\": \"tozerox\", \"fillcolor\": \"rgba(0,100,80,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"BERT\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.513043478637816, 0.5122529648038239, 0.5090909094207372, 0.5090909091144682, 0.5098814233018476, 0.5122529647567056, 0.5075098817998712, 0.5146245063058001, 0.505928854131887, 0.5027667987488004, 0.5106719368295707, 0.5106719367824524, 0.513043478637816, 0.5122529648038239, 0.5106719371358397, 0.5177865616417685, 0.5193675893097527, 0.5256916999816894, 0.5233201584797131, 0.522529644645721, 0.5169960478077764, 0.521739130811729, 0.5201581031437448, 0.5154150201397922, 0.5209486169777369, 0.5169960478077764, 0.5162055336203971, 0.5154150201397922, 0.5130434783315471, 0.5154150201397922, 0.5185770754286423, 0.5067193679658791, 0.49723320195797405, 0.5043478264639029, 0.5043478264639029, 0.5011857711279345, 0.5027667987959187, 0.4996047434599503, 0.505928854131887, 0.5114624509698318, 0.5106719367824524, 0.505138340297895, 0.5059288540847687, 0.5106719368295707, 0.5019762849619266, 0.5106719371358397, 0.5043478264167846, 0.5051383402507766, 0.5019762846085394, 0.4956521739366026, 0.49802371543857893, 0.4996047431065631, 0.488537549784061, 0.4940711466220057, 0.4996047434599503, 0.49723320195797405, 0.4996047434599503, 0.49407114631573673, 0.49090909128603727, 0.4948616604559978, 0.5027667984425315, 0.49249011895402145], \"type\": \"scatter\", \"uid\": \"1c8871ab-5e2c-459e-a40b-c22d2287ec1d\"}, {\"line\": {\"color\": \"rgb(0,100,80)\"}, \"mode\": \"lines+markers\", \"name\": \"BERT\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5048221346936207, 0.5081422926550326, 0.5021343876392003, 0.5038735179703225, 0.5027667985226326, 0.5067193679564556, 0.502924901482616, 0.5065612651378271, 0.5000790517414982, 0.4932806327785899, 0.5057707512237337, 0.504822134410911, 0.5037154153071844, 0.5076679844158911, 0.5081422927869638, 0.5119367591807024, 0.5141501979347274, 0.5130434785671384, 0.5168379449184705, 0.51478260892182, 0.5120948619757717, 0.5162055339031069, 0.5171541505038973, 0.5092490121734, 0.5098814233018476, 0.5097233205350491, 0.5098814230898153, 0.5095652176975732, 0.5089328065691258, 0.5076679844959922, 0.5109881425893354], \"type\": \"scatter\", \"uid\": \"23f45320-2984-4306-b778-690a059ffcd1\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(0,176,246,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"ELMo\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.5177865616417685, 0.5264822137685633, 0.5146245062586818, 0.5233201584325948, 0.5154150201397922, 0.5075098817998712, 0.5027667987488004, 0.5146245063058001, 0.5027667987488004, 0.5122529648038239, 0.5090909094207372, 0.524901186100579, 0.5169960477606581, 0.5177865616417685, 0.516205533926666, 0.5185770754286423, 0.5304347829385238, 0.5312252967725158, 0.5217391307646106, 0.5193675892626344, 0.5209486169306186, 0.5233201584325948, 0.5169960478077764, 0.5201581030966265, 0.5241106723137052, 0.5209486169306186, 0.5233201584325948, 0.5177865615946502, 0.5185770754286423, 0.513833992471808, 0.5225296445986027, 0.5035573125827925, 0.499604743412832, 0.5090909094207372, 0.5090909094678555, 0.5035573126299108, 0.5098814233018476, 0.4988142295788399, 0.5122529648038239, 0.505138340297895, 0.5122529648038239, 0.5138339924246897, 0.5106719370887214, 0.5011857710808162, 0.5106719370887214, 0.5067193679187609, 0.49802371574484783, 0.5059288540847687, 0.5090909094207372, 0.507509881752753, 0.5067193679187609, 0.4932806327408953, 0.49723320191085574, 0.49169960507291105, 0.49090909123891896, 0.49090909123891896, 0.4996047434599503, 0.4996047431065631, 0.5067193676124919, 0.5003952572468241, 0.5130434785906977, 0.505928854131887], \"type\": \"scatter\", \"uid\": \"e6e423fd-eb9a-429d-a241-564091ec7df1\"}, {\"line\": {\"color\": \"rgb(0,176,246)\"}, \"mode\": \"lines+markers\", \"name\": \"ELMo\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5127272730853718, 0.5209486169494658, 0.5086166011297656, 0.5135177868392627, 0.5083003955443386, 0.5037154153684382, 0.4977075101499972, 0.4997628461984777, 0.49802371574484783, 0.5048221347077562, 0.5051383402507768, 0.5144664035107308, 0.5128853758144756, 0.514466403501307, 0.5106719370792977, 0.5090909094301608, 0.5177865615946502, 0.5209486169494658, 0.5144664034306297, 0.5133596841337182, 0.5168379450032834, 0.5176284588561227, 0.5113043481747624, 0.5158893284213402, 0.5157312256356944, 0.5165217394696866, 0.511462450932137, 0.5128853758427466, 0.513675889667315, 0.5090909094301608, 0.513517786891093], \"type\": \"scatter\", \"uid\": \"d5441fb1-3fb8-4f78-9293-bdb0d7dfe19e\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(231,107,243,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"Transformer-XL\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.5003952572468241, 0.4940711466220057, 0.4996047434599503, 0.5106719370416031, 0.49802371574484783, 0.49169960512002936, 0.5019762849619266, 0.49723320191085574, 0.49565217428998987, 0.4988142296259582, 0.49723320191085574, 0.4988142296259582, 0.49090909093265006, 0.4956521739366026, 0.5027667987488004, 0.5090909094678555, 0.5059288540847687, 0.499604743412832, 0.4948616604559978, 0.4988142296259582, 0.49723320191085574, 0.4988142295788399, 0.5011857710808162, 0.499604743412832, 0.49802371579196614, 0.4948616604559978, 0.5043478264167846, 0.5011857710336979, 0.5051383399445077, 0.4948616604559978, 0.5027667987488004, 0.49169960512002936, 0.4853754944009743, 0.48458498061410055, 0.4822134391121242, 0.4735177868910929, 0.4869565221160768, 0.4885375497369427, 0.4924901189069032, 0.4877470359500689, 0.48063241144414004, 0.47984189751591133, 0.4766798422270613, 0.488537549784061, 0.4861660082820847, 0.4893280636180531, 0.48063241144414004, 0.48142292527813213, 0.4837944664267212, 0.47826086994216377, 0.4885375497369427, 0.47905138377615586, 0.488537549784061, 0.4869565217626896, 0.47984189730387905, 0.47905138372903755, 0.4853754944009743, 0.4766798422270613, 0.4869565221160768, 0.48616600792869746, 0.474308300725085, 0.49090909123891896], \"type\": \"scatter\", \"uid\": \"ca01c846-84a5-4c8f-a82c-8986d28a2b2c\"}, {\"line\": {\"color\": \"rgb(231,107,243)\"}, \"mode\": \"lines+markers\", \"name\": \"Transformer-XL\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49501976318510155, 0.48616600819255995, 0.49438735206607776, 0.49660079080125563, 0.48932806358035846, 0.4898023718807537, 0.4890118580467616, 0.4896442690715488, 0.49059288561108555, 0.4926482217113962, 0.4890118580750326, 0.49312252999294415, 0.48711462478392675, 0.489644269010295, 0.4935968383027631, 0.49501976315211876, 0.4962845851969814, 0.4924901189163268, 0.49106719404341204, 0.4882213441703631, 0.48837944696072066, 0.49169960508233473, 0.4932806327597426, 0.4953359687092747, 0.4939130437468352, 0.4910671940245647, 0.4858498027107932, 0.49375494098946043, 0.49075098842029047, 0.4894861663471569, 0.4962845853289125], \"type\": \"scatter\", \"uid\": \"b093c7ee-a18c-4a8c-b5c5-a4fb4e36b755\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(205,12,24,0.2)\", \"line\": {\"color\": \"rgba(205,12,24,0)\"}, \"name\": \"Flair\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.4901185771457763, 0.5011857711279345, 0.5035573122765236, 0.504347826157634, 0.49644268777059475, 0.4996047434599503, 0.49802371543857893, 0.49644268812398196, 0.4940711465748874, 0.499604743412832, 0.5090909094207372, 0.5114624509227135, 0.5122529647567056, 0.5003952572939424, 0.5035573122765236, 0.5083003955867451, 0.5051383402507766, 0.507509881752753, 0.5090909094678555, 0.5035573126299108, 0.5075098817998712, 0.5011857710808162, 0.4996047434599503, 0.5035573125827925, 0.5059288538256181, 0.5067193676596102, 0.5035573126299108, 0.49802371543857893, 0.507509881446484, 0.5019762847498943, 0.5098814232547293, 0.4869565221160768, 0.49644268812398196, 0.4814229249247449, 0.49169960512002936, 0.48774703569091826, 0.49169960486087877, 0.4940711466220057, 0.49249011864775255, 0.49090909123891896, 0.48932806335890244, 0.49249011860063424, 0.4940711462686184, 0.5019762849619266, 0.49565217424287156, 0.49802371574484783, 0.499604743412832, 0.4885375497369427, 0.49011857740492687, 0.4940711465748874, 0.4996047431065631, 0.49486166040887947, 0.48458498061410055, 0.4877470359029506, 0.49328063243462633, 0.49169960512002936, 0.4885375494306738, 0.4822134391121242, 0.48458498061410055, 0.4940711462686184, 0.4885375494306738, 0.48616600792869746], \"type\": \"scatter\", \"uid\": \"f59b6fb1-c839-4665-8d69-76837eac2c6c\"}, {\"line\": {\"color\": \"rgb(205,12,24)\"}, \"mode\": \"lines+markers\", \"name\": \"Flair\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4880632412716334, 0.49201581044159387, 0.49691699607099, 0.4959683795503006, 0.49059288560166187, 0.49517786577756223, 0.4948616602439654, 0.4945454547009449, 0.4918577078491332, 0.49343873546528716, 0.5011857709677323, 0.5064031623333338, 0.5052964430081514, 0.4954940714430903, 0.49486166035704926, 0.5038735181258129, 0.5015019766238367, 0.5037154152553543, 0.5052964429233385, 0.5002371543857891, 0.4994466404905432, 0.49660079064576523, 0.4956521740779575, 0.4983399211842081, 0.5010276681773747, 0.4983399211418016, 0.49802371560820474, 0.4961264823783528, 0.49407114640054967, 0.49865612674607596, 0.49802371570244136], \"type\": \"scatter\", \"uid\": \"f607d209-2c3e-45c5-a8ba-e86796cb2a9e\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(81, 45, 168, 0.2)\", \"line\": {\"color\": \"rgba(81, 45, 168, 0)\"}, \"name\": \"GPT\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.46245059321520354, 0.4553359686150381, 0.47114624534199834, 0.4584980239981248, 0.46640316238516405, 0.46640316243228236, 0.4719367592231087, 0.46482213471717987, 0.46561264855117196, 0.4569169963301406, 0.4703557312488556, 0.48221343906500597, 0.47905138372903755, 0.474308300725085, 0.4806324113499035, 0.4727272730099825, 0.4814229252310138, 0.47984189751591133, 0.47905138368191924, 0.47351778693821117, 0.48221343906500597, 0.47747035606105337, 0.4814229252310138, 0.47747035575478447, 0.4750988146061954, 0.48142292518389557, 0.46798419005314823, 0.4861660081878481, 0.47667984217994297, 0.4703557315551245, 0.474308300725085, 0.45296442716018015, 0.44347826119939326, 0.44743083032223546, 0.4529644272072984, 0.4466403165353617, 0.4569169963301406, 0.4403162058634249, 0.45454545482816433, 0.45375494104129055, 0.44822134425046417, 0.46798419010026654, 0.46166007907494255, 0.46482213471717987, 0.46007905166610896, 0.46561264850405365, 0.4577075101641327, 0.46640316243228236, 0.4521739133733063, 0.46719367626627445, 0.47667984217994297, 0.44980237182421173, 0.4521739133733063, 0.44664031658248, 0.45138339958643253, 0.4569169963772589, 0.45375494104129055, 0.4584980240452431, 0.44664031648824337, 0.45375494099417224, 0.445849802607133, 0.43636363669346445], \"type\": \"scatter\", \"uid\": \"252a4b7a-9c8a-45fd-a7be-18f1017e0958\"}, {\"line\": {\"color\": \"rgb(81, 45, 168)\"}, \"mode\": \"lines+markers\", \"name\": \"GPT\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.44490118610057905, 0.4509090911729534, 0.46213438762506476, 0.4513833995204669, 0.4618181821480099, 0.4611857710902399, 0.4640316208831877, 0.45913043512186047, 0.45359683822266195, 0.45438735209906056, 0.4632411069785182, 0.47826086986677446, 0.47177865647515765, 0.4654545457937972, 0.47335968410544715, 0.4648221346889089, 0.47272727302882983, 0.4692490121498409, 0.4706719370887214, 0.46798418988823426, 0.47557312288774334, 0.4671936762191562, 0.4714624509227134, 0.4676679844488739, 0.4618181821668572, 0.4714624508850188, 0.46023715447060204, 0.47098814259404725, 0.46213438765333575, 0.4575494073737751, 0.464347826407361], \"type\": \"scatter\", \"uid\": \"d151f824-c071-4801-841b-be0b8780e089\"}, {\"line\": {\"color\": \"rgb(0,0,0)\"}, \"mode\": \"lines\", \"name\": \"Majority vote\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428], \"type\": \"scatter\", \"uid\": \"6250b400-661f-4f53-aa40-c0f69d5643f3\"}, {\"mode\": \"lines\", \"name\": \"Khurana (2017)\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903], \"type\": \"scatter\", \"uid\": \"294266b7-fc65-440a-aafe-c50a7b975a95\"}], {\"paper_bgcolor\": \"rgb(255,255,255)\", \"plot_bgcolor\": \"rgb(229,229,229)\", \"title\": {\"text\": \"Bi-LSTM test set accuracy of padded datasets with variable maximum lengths\"}, \"xaxis\": {\"gridcolor\": \"rgb(255,255,255)\", \"range\": [5, 35], \"showgrid\": true, \"showline\": false, \"showticklabels\": true, \"tickcolor\": \"rgb(127,127,127)\", \"ticks\": \"outside\", \"zeroline\": false}, \"yaxis\": {\"gridcolor\": \"rgb(255,255,255)\", \"showgrid\": true, \"showline\": false, \"showticklabels\": true, \"tickcolor\": \"rgb(127,127,127)\", \"ticks\": \"outside\", \"zeroline\": false}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"b4d5a432-3a0b-4a92-84e5-118138df3a23\")) {window._Plotly.Plots.resize(document.getElementById(\"b4d5a432-3a0b-4a92-84e5-118138df3a23\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"b4d5a432-3a0b-4a92-84e5-118138df3a23\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"b4d5a432-3a0b-4a92-84e5-118138df3a23\")) {\n",
       "    Plotly.newPlot(\"b4d5a432-3a0b-4a92-84e5-118138df3a23\", [{\"fill\": \"tozerox\", \"fillcolor\": \"rgba(0,100,80,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"BERT\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.513043478637816, 0.5122529648038239, 0.5090909094207372, 0.5090909091144682, 0.5098814233018476, 0.5122529647567056, 0.5075098817998712, 0.5146245063058001, 0.505928854131887, 0.5027667987488004, 0.5106719368295707, 0.5106719367824524, 0.513043478637816, 0.5122529648038239, 0.5106719371358397, 0.5177865616417685, 0.5193675893097527, 0.5256916999816894, 0.5233201584797131, 0.522529644645721, 0.5169960478077764, 0.521739130811729, 0.5201581031437448, 0.5154150201397922, 0.5209486169777369, 0.5169960478077764, 0.5162055336203971, 0.5154150201397922, 0.5130434783315471, 0.5154150201397922, 0.5185770754286423, 0.5067193679658791, 0.49723320195797405, 0.5043478264639029, 0.5043478264639029, 0.5011857711279345, 0.5027667987959187, 0.4996047434599503, 0.505928854131887, 0.5114624509698318, 0.5106719367824524, 0.505138340297895, 0.5059288540847687, 0.5106719368295707, 0.5019762849619266, 0.5106719371358397, 0.5043478264167846, 0.5051383402507766, 0.5019762846085394, 0.4956521739366026, 0.49802371543857893, 0.4996047431065631, 0.488537549784061, 0.4940711466220057, 0.4996047434599503, 0.49723320195797405, 0.4996047434599503, 0.49407114631573673, 0.49090909128603727, 0.4948616604559978, 0.5027667984425315, 0.49249011895402145], \"type\": \"scatter\", \"uid\": \"1c8871ab-5e2c-459e-a40b-c22d2287ec1d\"}, {\"line\": {\"color\": \"rgb(0,100,80)\"}, \"mode\": \"lines+markers\", \"name\": \"BERT\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5048221346936207, 0.5081422926550326, 0.5021343876392003, 0.5038735179703225, 0.5027667985226326, 0.5067193679564556, 0.502924901482616, 0.5065612651378271, 0.5000790517414982, 0.4932806327785899, 0.5057707512237337, 0.504822134410911, 0.5037154153071844, 0.5076679844158911, 0.5081422927869638, 0.5119367591807024, 0.5141501979347274, 0.5130434785671384, 0.5168379449184705, 0.51478260892182, 0.5120948619757717, 0.5162055339031069, 0.5171541505038973, 0.5092490121734, 0.5098814233018476, 0.5097233205350491, 0.5098814230898153, 0.5095652176975732, 0.5089328065691258, 0.5076679844959922, 0.5109881425893354], \"type\": \"scatter\", \"uid\": \"23f45320-2984-4306-b778-690a059ffcd1\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(0,176,246,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"ELMo\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.5177865616417685, 0.5264822137685633, 0.5146245062586818, 0.5233201584325948, 0.5154150201397922, 0.5075098817998712, 0.5027667987488004, 0.5146245063058001, 0.5027667987488004, 0.5122529648038239, 0.5090909094207372, 0.524901186100579, 0.5169960477606581, 0.5177865616417685, 0.516205533926666, 0.5185770754286423, 0.5304347829385238, 0.5312252967725158, 0.5217391307646106, 0.5193675892626344, 0.5209486169306186, 0.5233201584325948, 0.5169960478077764, 0.5201581030966265, 0.5241106723137052, 0.5209486169306186, 0.5233201584325948, 0.5177865615946502, 0.5185770754286423, 0.513833992471808, 0.5225296445986027, 0.5035573125827925, 0.499604743412832, 0.5090909094207372, 0.5090909094678555, 0.5035573126299108, 0.5098814233018476, 0.4988142295788399, 0.5122529648038239, 0.505138340297895, 0.5122529648038239, 0.5138339924246897, 0.5106719370887214, 0.5011857710808162, 0.5106719370887214, 0.5067193679187609, 0.49802371574484783, 0.5059288540847687, 0.5090909094207372, 0.507509881752753, 0.5067193679187609, 0.4932806327408953, 0.49723320191085574, 0.49169960507291105, 0.49090909123891896, 0.49090909123891896, 0.4996047434599503, 0.4996047431065631, 0.5067193676124919, 0.5003952572468241, 0.5130434785906977, 0.505928854131887], \"type\": \"scatter\", \"uid\": \"e6e423fd-eb9a-429d-a241-564091ec7df1\"}, {\"line\": {\"color\": \"rgb(0,176,246)\"}, \"mode\": \"lines+markers\", \"name\": \"ELMo\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5127272730853718, 0.5209486169494658, 0.5086166011297656, 0.5135177868392627, 0.5083003955443386, 0.5037154153684382, 0.4977075101499972, 0.4997628461984777, 0.49802371574484783, 0.5048221347077562, 0.5051383402507768, 0.5144664035107308, 0.5128853758144756, 0.514466403501307, 0.5106719370792977, 0.5090909094301608, 0.5177865615946502, 0.5209486169494658, 0.5144664034306297, 0.5133596841337182, 0.5168379450032834, 0.5176284588561227, 0.5113043481747624, 0.5158893284213402, 0.5157312256356944, 0.5165217394696866, 0.511462450932137, 0.5128853758427466, 0.513675889667315, 0.5090909094301608, 0.513517786891093], \"type\": \"scatter\", \"uid\": \"d5441fb1-3fb8-4f78-9293-bdb0d7dfe19e\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(231,107,243,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"Transformer-XL\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.5003952572468241, 0.4940711466220057, 0.4996047434599503, 0.5106719370416031, 0.49802371574484783, 0.49169960512002936, 0.5019762849619266, 0.49723320191085574, 0.49565217428998987, 0.4988142296259582, 0.49723320191085574, 0.4988142296259582, 0.49090909093265006, 0.4956521739366026, 0.5027667987488004, 0.5090909094678555, 0.5059288540847687, 0.499604743412832, 0.4948616604559978, 0.4988142296259582, 0.49723320191085574, 0.4988142295788399, 0.5011857710808162, 0.499604743412832, 0.49802371579196614, 0.4948616604559978, 0.5043478264167846, 0.5011857710336979, 0.5051383399445077, 0.4948616604559978, 0.5027667987488004, 0.49169960512002936, 0.4853754944009743, 0.48458498061410055, 0.4822134391121242, 0.4735177868910929, 0.4869565221160768, 0.4885375497369427, 0.4924901189069032, 0.4877470359500689, 0.48063241144414004, 0.47984189751591133, 0.4766798422270613, 0.488537549784061, 0.4861660082820847, 0.4893280636180531, 0.48063241144414004, 0.48142292527813213, 0.4837944664267212, 0.47826086994216377, 0.4885375497369427, 0.47905138377615586, 0.488537549784061, 0.4869565217626896, 0.47984189730387905, 0.47905138372903755, 0.4853754944009743, 0.4766798422270613, 0.4869565221160768, 0.48616600792869746, 0.474308300725085, 0.49090909123891896], \"type\": \"scatter\", \"uid\": \"ca01c846-84a5-4c8f-a82c-8986d28a2b2c\"}, {\"line\": {\"color\": \"rgb(231,107,243)\"}, \"mode\": \"lines+markers\", \"name\": \"Transformer-XL\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49501976318510155, 0.48616600819255995, 0.49438735206607776, 0.49660079080125563, 0.48932806358035846, 0.4898023718807537, 0.4890118580467616, 0.4896442690715488, 0.49059288561108555, 0.4926482217113962, 0.4890118580750326, 0.49312252999294415, 0.48711462478392675, 0.489644269010295, 0.4935968383027631, 0.49501976315211876, 0.4962845851969814, 0.4924901189163268, 0.49106719404341204, 0.4882213441703631, 0.48837944696072066, 0.49169960508233473, 0.4932806327597426, 0.4953359687092747, 0.4939130437468352, 0.4910671940245647, 0.4858498027107932, 0.49375494098946043, 0.49075098842029047, 0.4894861663471569, 0.4962845853289125], \"type\": \"scatter\", \"uid\": \"b093c7ee-a18c-4a8c-b5c5-a4fb4e36b755\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(205,12,24,0.2)\", \"line\": {\"color\": \"rgba(205,12,24,0)\"}, \"name\": \"Flair\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.4901185771457763, 0.5011857711279345, 0.5035573122765236, 0.504347826157634, 0.49644268777059475, 0.4996047434599503, 0.49802371543857893, 0.49644268812398196, 0.4940711465748874, 0.499604743412832, 0.5090909094207372, 0.5114624509227135, 0.5122529647567056, 0.5003952572939424, 0.5035573122765236, 0.5083003955867451, 0.5051383402507766, 0.507509881752753, 0.5090909094678555, 0.5035573126299108, 0.5075098817998712, 0.5011857710808162, 0.4996047434599503, 0.5035573125827925, 0.5059288538256181, 0.5067193676596102, 0.5035573126299108, 0.49802371543857893, 0.507509881446484, 0.5019762847498943, 0.5098814232547293, 0.4869565221160768, 0.49644268812398196, 0.4814229249247449, 0.49169960512002936, 0.48774703569091826, 0.49169960486087877, 0.4940711466220057, 0.49249011864775255, 0.49090909123891896, 0.48932806335890244, 0.49249011860063424, 0.4940711462686184, 0.5019762849619266, 0.49565217424287156, 0.49802371574484783, 0.499604743412832, 0.4885375497369427, 0.49011857740492687, 0.4940711465748874, 0.4996047431065631, 0.49486166040887947, 0.48458498061410055, 0.4877470359029506, 0.49328063243462633, 0.49169960512002936, 0.4885375494306738, 0.4822134391121242, 0.48458498061410055, 0.4940711462686184, 0.4885375494306738, 0.48616600792869746], \"type\": \"scatter\", \"uid\": \"f59b6fb1-c839-4665-8d69-76837eac2c6c\"}, {\"line\": {\"color\": \"rgb(205,12,24)\"}, \"mode\": \"lines+markers\", \"name\": \"Flair\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4880632412716334, 0.49201581044159387, 0.49691699607099, 0.4959683795503006, 0.49059288560166187, 0.49517786577756223, 0.4948616602439654, 0.4945454547009449, 0.4918577078491332, 0.49343873546528716, 0.5011857709677323, 0.5064031623333338, 0.5052964430081514, 0.4954940714430903, 0.49486166035704926, 0.5038735181258129, 0.5015019766238367, 0.5037154152553543, 0.5052964429233385, 0.5002371543857891, 0.4994466404905432, 0.49660079064576523, 0.4956521740779575, 0.4983399211842081, 0.5010276681773747, 0.4983399211418016, 0.49802371560820474, 0.4961264823783528, 0.49407114640054967, 0.49865612674607596, 0.49802371570244136], \"type\": \"scatter\", \"uid\": \"f607d209-2c3e-45c5-a8ba-e86796cb2a9e\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(81, 45, 168, 0.2)\", \"line\": {\"color\": \"rgba(81, 45, 168, 0)\"}, \"name\": \"GPT\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.46245059321520354, 0.4553359686150381, 0.47114624534199834, 0.4584980239981248, 0.46640316238516405, 0.46640316243228236, 0.4719367592231087, 0.46482213471717987, 0.46561264855117196, 0.4569169963301406, 0.4703557312488556, 0.48221343906500597, 0.47905138372903755, 0.474308300725085, 0.4806324113499035, 0.4727272730099825, 0.4814229252310138, 0.47984189751591133, 0.47905138368191924, 0.47351778693821117, 0.48221343906500597, 0.47747035606105337, 0.4814229252310138, 0.47747035575478447, 0.4750988146061954, 0.48142292518389557, 0.46798419005314823, 0.4861660081878481, 0.47667984217994297, 0.4703557315551245, 0.474308300725085, 0.45296442716018015, 0.44347826119939326, 0.44743083032223546, 0.4529644272072984, 0.4466403165353617, 0.4569169963301406, 0.4403162058634249, 0.45454545482816433, 0.45375494104129055, 0.44822134425046417, 0.46798419010026654, 0.46166007907494255, 0.46482213471717987, 0.46007905166610896, 0.46561264850405365, 0.4577075101641327, 0.46640316243228236, 0.4521739133733063, 0.46719367626627445, 0.47667984217994297, 0.44980237182421173, 0.4521739133733063, 0.44664031658248, 0.45138339958643253, 0.4569169963772589, 0.45375494104129055, 0.4584980240452431, 0.44664031648824337, 0.45375494099417224, 0.445849802607133, 0.43636363669346445], \"type\": \"scatter\", \"uid\": \"252a4b7a-9c8a-45fd-a7be-18f1017e0958\"}, {\"line\": {\"color\": \"rgb(81, 45, 168)\"}, \"mode\": \"lines+markers\", \"name\": \"GPT\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.44490118610057905, 0.4509090911729534, 0.46213438762506476, 0.4513833995204669, 0.4618181821480099, 0.4611857710902399, 0.4640316208831877, 0.45913043512186047, 0.45359683822266195, 0.45438735209906056, 0.4632411069785182, 0.47826086986677446, 0.47177865647515765, 0.4654545457937972, 0.47335968410544715, 0.4648221346889089, 0.47272727302882983, 0.4692490121498409, 0.4706719370887214, 0.46798418988823426, 0.47557312288774334, 0.4671936762191562, 0.4714624509227134, 0.4676679844488739, 0.4618181821668572, 0.4714624508850188, 0.46023715447060204, 0.47098814259404725, 0.46213438765333575, 0.4575494073737751, 0.464347826407361], \"type\": \"scatter\", \"uid\": \"d151f824-c071-4801-841b-be0b8780e089\"}, {\"line\": {\"color\": \"rgb(0,0,0)\"}, \"mode\": \"lines\", \"name\": \"Majority vote\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428], \"type\": \"scatter\", \"uid\": \"6250b400-661f-4f53-aa40-c0f69d5643f3\"}, {\"mode\": \"lines\", \"name\": \"Khurana (2017)\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903], \"type\": \"scatter\", \"uid\": \"294266b7-fc65-440a-aafe-c50a7b975a95\"}], {\"paper_bgcolor\": \"rgb(255,255,255)\", \"plot_bgcolor\": \"rgb(229,229,229)\", \"title\": {\"text\": \"Bi-LSTM test set accuracy of padded datasets with variable maximum lengths\"}, \"xaxis\": {\"gridcolor\": \"rgb(255,255,255)\", \"range\": [5, 35], \"showgrid\": true, \"showline\": false, \"showticklabels\": true, \"tickcolor\": \"rgb(127,127,127)\", \"ticks\": \"outside\", \"zeroline\": false}, \"yaxis\": {\"gridcolor\": \"rgb(255,255,255)\", \"showgrid\": true, \"showline\": false, \"showticklabels\": true, \"tickcolor\": \"rgb(127,127,127)\", \"ticks\": \"outside\", \"zeroline\": false}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"b4d5a432-3a0b-4a92-84e5-118138df3a23\")) {window._Plotly.Plots.resize(document.getElementById(\"b4d5a432-3a0b-4a92-84e5-118138df3a23\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(5, 36))\n",
    "x_rev = x[::-1]\n",
    "\n",
    "# BERT\n",
    "bert_matrix = np.transpose(np.array([np.array(list(acc_round.values())) for acc_round in bert_rounds]))\n",
    "bert_y = [np.average(row) for row in bert_matrix]\n",
    "bert_y_upper = [row.max() for row in bert_matrix]\n",
    "bert_y_lower = [row.min() for row in bert_matrix]\n",
    "bert_y_lower = bert_y_lower[::-1]\n",
    "\n",
    "bert1 = go.Scatter(\n",
    "    x = x + x_rev,\n",
    "    y = bert_y_upper + bert_y_lower,\n",
    "    fill = 'tozerox',\n",
    "    fillcolor = 'rgba(0,100,80,0.2)',\n",
    "    line = dict(color = 'rgba(255,255,255,0)'),\n",
    "    showlegend = False,\n",
    "    name = 'BERT',\n",
    ")\n",
    "bert2 = go.Scatter(\n",
    "    x = x,\n",
    "    y = bert_y,\n",
    "    line = dict(color='rgb(0,100,80)'),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'BERT',\n",
    ")\n",
    "\n",
    "# ELMo\n",
    "elmo_matrix = np.transpose(np.array([np.array(list(acc_round.values())) for acc_round in elmo_rounds]))\n",
    "elmo_y = [np.average(row) for row in elmo_matrix]\n",
    "elmo_y_upper = [row.max() for row in elmo_matrix]\n",
    "elmo_y_lower = [row.min() for row in elmo_matrix]\n",
    "elmo_y_lower = elmo_y_lower[::-1]\n",
    "\n",
    "elmo1 = go.Scatter(\n",
    "    x = x + x_rev,\n",
    "    y = elmo_y_upper + elmo_y_lower,\n",
    "    fill = 'tozerox',\n",
    "    fillcolor = 'rgba(0,176,246,0.2)',\n",
    "    line = dict(color = 'rgba(255,255,255,0)'),\n",
    "    showlegend = False,\n",
    "    name = 'ELMo',\n",
    ")\n",
    "elmo2 = go.Scatter(\n",
    "    x = x,\n",
    "    y = elmo_y,\n",
    "    line = dict(color='rgb(0,176,246)'),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'ELMo',\n",
    ")\n",
    "\n",
    "# Transformer-XL\n",
    "transformerxl_matrix = np.transpose(np.array([np.array(list(acc_round.values())) for acc_round in transformerxl_rounds]))\n",
    "transformerxl_y = [np.average(row) for row in transformerxl_matrix]\n",
    "transformerxl_y_upper = [row.max() for row in transformerxl_matrix]\n",
    "transformerxl_y_lower = [row.min() for row in transformerxl_matrix]\n",
    "transformerxl_y_lower = transformerxl_y_lower[::-1]\n",
    "\n",
    "transformerxl1 = go.Scatter(\n",
    "    x = x + x_rev,\n",
    "    y = transformerxl_y_upper + transformerxl_y_lower,\n",
    "    fill = 'tozerox',\n",
    "    fillcolor = 'rgba(231,107,243,0.2)',\n",
    "    line = dict(color = 'rgba(255,255,255,0)'),\n",
    "    showlegend = False,\n",
    "    name = 'Transformer-XL',\n",
    ")\n",
    "transformerxl2 = go.Scatter(\n",
    "    x = x,\n",
    "    y = transformerxl_y,\n",
    "    line = dict(color='rgb(231,107,243)'),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Transformer-XL',\n",
    ")\n",
    "\n",
    "# Flair\n",
    "flair_matrix = np.transpose(np.array([np.array(list(acc_round.values())) for acc_round in flair_rounds]))\n",
    "flair_y = [np.average(row) for row in flair_matrix]\n",
    "flair_y_upper = [row.max() for row in flair_matrix]\n",
    "flair_y_lower = [row.min() for row in flair_matrix]\n",
    "flair_y_lower = flair_y_lower[::-1]\n",
    "\n",
    "flair1 = go.Scatter(\n",
    "    x = x + x_rev,\n",
    "    y = flair_y_upper + flair_y_lower,\n",
    "    fill = 'tozerox',\n",
    "    fillcolor = 'rgba(205,12,24,0.2)',\n",
    "    line = dict(color = 'rgba(205,12,24,0)'),\n",
    "    showlegend = False,\n",
    "    name = 'Flair',\n",
    ")\n",
    "flair2 = go.Scatter(\n",
    "    x = x,\n",
    "    y = flair_y,\n",
    "    line = dict(color='rgb(205,12,24)'),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Flair',\n",
    ")\n",
    "\n",
    "# GPT\n",
    "gpt_matrix = np.transpose(np.array([np.array(list(acc_round.values())) for acc_round in gpt_rounds]))\n",
    "gpt_y = [np.average(row) for row in gpt_matrix]\n",
    "gpt_y_upper = [row.max() for row in gpt_matrix]\n",
    "gpt_y_lower = [row.min() for row in gpt_matrix]\n",
    "gpt_y_lower = gpt_y_lower[::-1]\n",
    "\n",
    "gpt1 = go.Scatter(\n",
    "    x = x + x_rev,\n",
    "    y = gpt_y_upper + gpt_y_lower,\n",
    "    fill = 'tozerox',\n",
    "    fillcolor = 'rgba(81, 45, 168, 0.2)',\n",
    "    line = dict(color = 'rgba(81, 45, 168, 0)'),\n",
    "    showlegend = False,\n",
    "    name = 'GPT',\n",
    ")\n",
    "gpt2 = go.Scatter(\n",
    "    x = x,\n",
    "    y = gpt_y,\n",
    "    line = dict(color='rgb(81, 45, 168)'),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'GPT',\n",
    ")\n",
    "\n",
    "# Baseline and highest performance\n",
    "baseline = go.Scatter(\n",
    "    x = x,\n",
    "    y = [0.4428 for i in x],\n",
    "    line = dict(color='rgb(0,0,0)'),\n",
    "    mode = 'lines',\n",
    "    name = 'Majority vote',\n",
    ")\n",
    "best_past_performance = go.Scatter(\n",
    "    x = x,\n",
    "    y = [0.4903 for i in x],\n",
    "    mode = 'lines',\n",
    "    name = 'Khurana (2017)',\n",
    ")\n",
    "\n",
    "\n",
    "data = [bert1, bert2, elmo1, elmo2, transformerxl1, transformerxl2, flair1, flair2, gpt1, gpt2, baseline, best_past_performance]\n",
    "layout = go.Layout(\n",
    "    title = 'Bi-LSTM test set accuracy of padded datasets with variable maximum lengths',\n",
    "    paper_bgcolor = 'rgb(255,255,255)',\n",
    "    plot_bgcolor = 'rgb(229,229,229)',\n",
    "    xaxis = dict(\n",
    "        gridcolor = 'rgb(255,255,255)',\n",
    "        range = [5,35],\n",
    "        showgrid = True,\n",
    "        showline = False,\n",
    "        showticklabels = True,\n",
    "        tickcolor = 'rgb(127,127,127)',\n",
    "        ticks = 'outside',\n",
    "        zeroline = False\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        gridcolor='rgb(255,255,255)',\n",
    "        showgrid = True,\n",
    "        showline = False,\n",
    "        showticklabels = True,\n",
    "        tickcolor = 'rgb(127,127,127)',\n",
    "        ticks = 'outside',\n",
    "        zeroline = False\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Convolutional neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_score(X_train, X_test, X_validation, y_train = general['train']['label'], y_test = general['test']['label'], y_validation = general['validation']['label'], reshape = True):\n",
    "    # Rearrange data types    \n",
    "    params = locals().copy()    \n",
    "    inputs = {\n",
    "        dataset: np.array(params[dataset])\n",
    "        for dataset in params.keys()\n",
    "    }\n",
    "    \n",
    "    # Reshape datasets\n",
    "    for dataset in inputs.keys():\n",
    "        if dataset[0:1] == 'X':\n",
    "            if reshape:\n",
    "                inputs[dataset] = np.reshape(inputs[dataset], (inputs[dataset].shape[0], inputs[dataset].shape[1], 1))\n",
    "            \n",
    "        elif dataset[0:1] == 'y':\n",
    "            inputs[dataset] = np_utils.to_categorical(np.array(inputs[dataset]), 3)\n",
    "            \n",
    "    # Set model parameters\n",
    "    epochs = 5\n",
    "    batch_size = 32\n",
    "    input_shape =  inputs['X_train'].shape\n",
    "    \n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(128, kernel_size = 2, activation='relu', input_shape = (input_shape[1], input_shape[2]), data_format = 'channels_first'))\n",
    "    model.add(Conv1D(128, kernel_size = 3, activation='relu'))\n",
    "    model.add(Conv1D(128, kernel_size = 4, activation='relu'))\n",
    "    model.add(Dropout(0.8))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "    model.compile('sgd', 'categorical_crossentropy', metrics = ['accuracy']) \n",
    "    \n",
    "    # Fit the training set over the model and correct on the validation set\n",
    "    model.fit(inputs['X_train'], inputs['y_train'],\n",
    "            batch_size = batch_size,\n",
    "            epochs = epochs,\n",
    "            validation_data = (inputs['X_validation'], inputs['y_validation']))\n",
    "    \n",
    "    # Get score over the test set\n",
    "    score, acc = model.evaluate(inputs['X_test'], inputs['y_test'])\n",
    "    print(acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_bert = {\n",
    "    dataset: [np.concatenate(np.array(statement)) for statement in bert[dataset].statement]\n",
    "    for dataset in bert.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_round(dataset):\n",
    "    # Store accuracies\n",
    "    accuracies = {\n",
    "        padding_len: 0.0 for padding_len in list(range(5,36))\n",
    "    }\n",
    "\n",
    "    for max_len in accuracies.keys():\n",
    "        padded_dataset = {\n",
    "            fold: sequence.pad_sequences(dataset[fold], maxlen = max_len, dtype = float)\n",
    "            for fold in dataset.keys()\n",
    "        }\n",
    "\n",
    "        accuracies[max_len] = get_cnn_score(padded_dataset['train'], padded_dataset['test'], padded_dataset['validation'], reshape = False)\n",
    "        \n",
    "    print(accuracies)\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0628 - acc: 0.4374 - val_loss: 1.0260 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0405 - acc: 0.4690 - val_loss: 1.0127 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0328 - acc: 0.4813 - val_loss: 1.0137 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0260 - acc: 0.4858 - val_loss: 1.0025 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0140 - acc: 0.4943 - val_loss: 1.0029 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5067193679658791\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0658 - acc: 0.4321 - val_loss: 1.0336 - val_acc: 0.4766\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0416 - acc: 0.4671 - val_loss: 1.0154 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0289 - acc: 0.4830 - val_loss: 1.0081 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0194 - acc: 0.4926 - val_loss: 1.0208 - val_acc: 0.4930\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0048 - acc: 0.5065 - val_loss: 1.0021 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5162055339737843\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0632 - acc: 0.4349 - val_loss: 1.0215 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0413 - acc: 0.4681 - val_loss: 1.0099 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0275 - acc: 0.4859 - val_loss: 1.0068 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0160 - acc: 0.4997 - val_loss: 1.0038 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0061 - acc: 0.5033 - val_loss: 0.9960 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5130434782844288\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0670 - acc: 0.4259 - val_loss: 1.0216 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0397 - acc: 0.4730 - val_loss: 1.0382 - val_acc: 0.4650\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0284 - acc: 0.4890 - val_loss: 1.0091 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0198 - acc: 0.4973 - val_loss: 1.0041 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0090 - acc: 0.5069 - val_loss: 1.0099 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5225296443865705\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0653 - acc: 0.4367 - val_loss: 1.0289 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0410 - acc: 0.4748 - val_loss: 1.0105 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0271 - acc: 0.4912 - val_loss: 1.0199 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0189 - acc: 0.4967 - val_loss: 1.0011 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0075 - acc: 0.5097 - val_loss: 1.0015 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5098814232547293\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0637 - acc: 0.4327 - val_loss: 1.0179 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0389 - acc: 0.4709 - val_loss: 1.0111 - val_acc: 0.4953\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0269 - acc: 0.4914 - val_loss: 1.0065 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0162 - acc: 0.4955 - val_loss: 0.9997 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0043 - acc: 0.5111 - val_loss: 0.9950 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.505928854131887\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0671 - acc: 0.4272 - val_loss: 1.0208 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0398 - acc: 0.4758 - val_loss: 1.0183 - val_acc: 0.4961\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0254 - acc: 0.4912 - val_loss: 1.0059 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0133 - acc: 0.4989 - val_loss: 0.9972 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0073 - acc: 0.4975 - val_loss: 0.9935 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5027667987016821\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0684 - acc: 0.4432 - val_loss: 1.0118 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0410 - acc: 0.4807 - val_loss: 1.0126 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0252 - acc: 0.4902 - val_loss: 1.0027 - val_acc: 0.4953\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0116 - acc: 0.5079 - val_loss: 1.0014 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0031 - acc: 0.5113 - val_loss: 0.9927 - val_acc: 0.5031\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.507509881752753\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0714 - acc: 0.4350 - val_loss: 1.0247 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0422 - acc: 0.4730 - val_loss: 1.0116 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0277 - acc: 0.4885 - val_loss: 1.0053 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0151 - acc: 0.5030 - val_loss: 1.0255 - val_acc: 0.4953\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0024 - acc: 0.5114 - val_loss: 0.9995 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.49644268807686365\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0685 - acc: 0.4322 - val_loss: 1.0167 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0411 - acc: 0.4752 - val_loss: 1.0061 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0291 - acc: 0.4887 - val_loss: 0.9969 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0176 - acc: 0.5065 - val_loss: 0.9997 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0048 - acc: 0.5093 - val_loss: 1.0077 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.48458498056698224\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 133s 13ms/step - loss: 1.0735 - acc: 0.4336 - val_loss: 1.0215 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0370 - acc: 0.4797 - val_loss: 1.0095 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0269 - acc: 0.4952 - val_loss: 1.0007 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0114 - acc: 0.5042 - val_loss: 1.0095 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0017 - acc: 0.5116 - val_loss: 0.9926 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.507509881752753\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 138s 13ms/step - loss: 1.0652 - acc: 0.4345 - val_loss: 1.0294 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0373 - acc: 0.4734 - val_loss: 1.0120 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0221 - acc: 0.4930 - val_loss: 1.0020 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0122 - acc: 0.5059 - val_loss: 1.0004 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9943 - acc: 0.5177 - val_loss: 1.0105 - val_acc: 0.5031\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.4996047431536814\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0692 - acc: 0.4346 - val_loss: 1.0283 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0395 - acc: 0.4787 - val_loss: 1.0061 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0227 - acc: 0.4938 - val_loss: 1.0029 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0099 - acc: 0.5093 - val_loss: 1.0006 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0026 - acc: 0.5135 - val_loss: 0.9952 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5114624508755952\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0712 - acc: 0.4348 - val_loss: 1.0157 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0393 - acc: 0.4726 - val_loss: 1.0095 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0231 - acc: 0.4949 - val_loss: 1.0019 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0126 - acc: 0.4999 - val_loss: 0.9997 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0013 - acc: 0.5187 - val_loss: 0.9994 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.49802371574484783\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 1.0700 - acc: 0.4366 - val_loss: 1.0161 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0386 - acc: 0.4795 - val_loss: 1.0018 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0221 - acc: 0.4935 - val_loss: 0.9988 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0129 - acc: 0.5052 - val_loss: 1.0102 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9962 - acc: 0.5131 - val_loss: 0.9936 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5011857710808162\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 146s 14ms/step - loss: 1.0722 - acc: 0.4313 - val_loss: 1.0233 - val_acc: 0.4930\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0389 - acc: 0.4715 - val_loss: 1.0050 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0235 - acc: 0.4961 - val_loss: 1.0160 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0128 - acc: 0.5032 - val_loss: 1.0066 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9954 - acc: 0.5190 - val_loss: 1.0112 - val_acc: 0.5047\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5043478261105157\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 147s 14ms/step - loss: 1.0746 - acc: 0.4254 - val_loss: 1.0233 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0411 - acc: 0.4716 - val_loss: 1.0105 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0318 - acc: 0.4872 - val_loss: 1.0100 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0117 - acc: 0.5030 - val_loss: 1.0010 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0012 - acc: 0.5142 - val_loss: 1.0128 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5209486169777369\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 148s 14ms/step - loss: 1.0755 - acc: 0.4265 - val_loss: 1.0262 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0415 - acc: 0.4736 - val_loss: 1.0382 - val_acc: 0.4657\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0235 - acc: 0.4939 - val_loss: 1.0078 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0154 - acc: 0.5008 - val_loss: 1.0035 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0020 - acc: 0.5193 - val_loss: 0.9975 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.516205533926666\n",
      "{5: 0.5067193679658791, 6: 0.5162055339737843, 7: 0.5130434782844288, 8: 0.5225296443865705, 9: 0.5098814232547293, 10: 0.505928854131887, 11: 0.5027667987016821, 12: 0.507509881752753, 13: 0.49644268807686365, 14: 0.48458498056698224, 15: 0.507509881752753, 16: 0.4996047431536814, 17: 0.5114624508755952, 18: 0.49802371574484783, 19: 0.5011857710808162, 20: 0.5043478261105157, 21: 0.5209486169777369, 22: 0.516205533926666}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0607 - acc: 0.4305 - val_loss: 1.0217 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0419 - acc: 0.4672 - val_loss: 1.0184 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0309 - acc: 0.4818 - val_loss: 1.0142 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0197 - acc: 0.4963 - val_loss: 1.0098 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0080 - acc: 0.5020 - val_loss: 1.0047 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5122529644975549\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0594 - acc: 0.4399 - val_loss: 1.0197 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0418 - acc: 0.4730 - val_loss: 1.0229 - val_acc: 0.4961\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0281 - acc: 0.4889 - val_loss: 1.0044 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0179 - acc: 0.5020 - val_loss: 1.0074 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 111s 11ms/step - loss: 1.0096 - acc: 0.5063 - val_loss: 1.0190 - val_acc: 0.4945\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5027667984896498\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0642 - acc: 0.4396 - val_loss: 1.0196 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0419 - acc: 0.4727 - val_loss: 1.0234 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0288 - acc: 0.4856 - val_loss: 1.0077 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 11ms/step - loss: 1.0198 - acc: 0.4969 - val_loss: 1.0109 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0057 - acc: 0.5076 - val_loss: 1.0220 - val_acc: 0.4992\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.485375494188942\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0620 - acc: 0.4427 - val_loss: 1.0201 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0410 - acc: 0.4729 - val_loss: 1.0105 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0286 - acc: 0.4837 - val_loss: 1.0057 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0182 - acc: 0.5002 - val_loss: 1.0250 - val_acc: 0.4907\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0060 - acc: 0.5116 - val_loss: 0.9968 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5177865616417685\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0677 - acc: 0.4322 - val_loss: 1.0190 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0376 - acc: 0.4775 - val_loss: 1.0228 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0284 - acc: 0.4944 - val_loss: 1.0146 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0191 - acc: 0.5005 - val_loss: 0.9988 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0082 - acc: 0.5097 - val_loss: 0.9959 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5106719371358397\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0655 - acc: 0.4343 - val_loss: 1.0224 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0368 - acc: 0.4737 - val_loss: 1.0108 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0271 - acc: 0.4874 - val_loss: 1.0100 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0174 - acc: 0.4959 - val_loss: 1.0074 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0036 - acc: 0.5134 - val_loss: 0.9965 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5138339924246897\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0627 - acc: 0.4400 - val_loss: 1.0273 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0410 - acc: 0.4689 - val_loss: 1.0164 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0241 - acc: 0.4938 - val_loss: 1.0050 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0150 - acc: 0.4992 - val_loss: 1.0070 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0023 - acc: 0.5083 - val_loss: 1.0331 - val_acc: 0.4860\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.48695652180980786\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0676 - acc: 0.4341 - val_loss: 1.0165 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0399 - acc: 0.4739 - val_loss: 1.0085 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0276 - acc: 0.4907 - val_loss: 1.0050 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0145 - acc: 0.5005 - val_loss: 1.0021 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0030 - acc: 0.5172 - val_loss: 1.0270 - val_acc: 0.4945\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.4861660080229341\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0717 - acc: 0.4341 - val_loss: 1.0182 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0401 - acc: 0.4740 - val_loss: 1.0169 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0251 - acc: 0.4924 - val_loss: 1.0031 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0126 - acc: 0.5067 - val_loss: 1.0416 - val_acc: 0.4727\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0018 - acc: 0.5151 - val_loss: 1.0194 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.507509881752753\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0757 - acc: 0.4299 - val_loss: 1.0125 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0387 - acc: 0.4811 - val_loss: 1.0262 - val_acc: 0.4891\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0272 - acc: 0.4945 - val_loss: 1.0151 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0103 - acc: 0.5022 - val_loss: 1.0029 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0007 - acc: 0.5130 - val_loss: 0.9971 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5027667987488004\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 1.0699 - acc: 0.4389 - val_loss: 1.0229 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0392 - acc: 0.4792 - val_loss: 1.0086 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0248 - acc: 0.5009 - val_loss: 1.0237 - val_acc: 0.4938\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0140 - acc: 0.5027 - val_loss: 1.0068 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9988 - acc: 0.5130 - val_loss: 1.0078 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.5146245063058001\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0679 - acc: 0.4389 - val_loss: 1.0203 - val_acc: 0.5132\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0359 - acc: 0.4745 - val_loss: 1.0065 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0172 - acc: 0.5013 - val_loss: 0.9999 - val_acc: 0.5187\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0096 - acc: 0.5057 - val_loss: 0.9964 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 0.9983 - acc: 0.5156 - val_loss: 0.9963 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 6s 4ms/step\n",
      "0.4980237156977295\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0707 - acc: 0.4367 - val_loss: 1.0412 - val_acc: 0.4548\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0401 - acc: 0.4781 - val_loss: 1.0096 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0221 - acc: 0.4986 - val_loss: 1.0191 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0128 - acc: 0.5006 - val_loss: 0.9964 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0043 - acc: 0.5158 - val_loss: 1.0084 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.4814229252310138\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0716 - acc: 0.4404 - val_loss: 1.0156 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0400 - acc: 0.4751 - val_loss: 1.0044 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0226 - acc: 0.4982 - val_loss: 1.0068 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0149 - acc: 0.4994 - val_loss: 0.9970 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9997 - acc: 0.5149 - val_loss: 0.9958 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5098814233018476\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 1.0701 - acc: 0.4302 - val_loss: 1.0159 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0382 - acc: 0.4808 - val_loss: 1.0080 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0260 - acc: 0.4866 - val_loss: 1.0163 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0128 - acc: 0.5007 - val_loss: 1.0121 - val_acc: 0.4945\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9965 - acc: 0.5173 - val_loss: 0.9959 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.513833992471808\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 145s 14ms/step - loss: 1.0785 - acc: 0.4338 - val_loss: 1.0226 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0396 - acc: 0.4752 - val_loss: 1.0080 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0246 - acc: 0.4922 - val_loss: 0.9982 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0089 - acc: 0.5043 - val_loss: 0.9968 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9975 - acc: 0.5161 - val_loss: 0.9986 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5019762849619266\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 1.0715 - acc: 0.4313 - val_loss: 1.0176 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0400 - acc: 0.4687 - val_loss: 1.0070 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0248 - acc: 0.4958 - val_loss: 0.9996 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0145 - acc: 0.5044 - val_loss: 1.0104 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0025 - acc: 0.5120 - val_loss: 0.9952 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.49723320191085574\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 150s 15ms/step - loss: 1.0754 - acc: 0.4249 - val_loss: 1.0149 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0399 - acc: 0.4780 - val_loss: 1.0078 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0292 - acc: 0.4940 - val_loss: 1.0051 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0115 - acc: 0.5025 - val_loss: 1.0066 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0004 - acc: 0.5078 - val_loss: 0.9939 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5106719370887214\n",
      "{5: 0.5122529644975549, 6: 0.5027667984896498, 7: 0.485375494188942, 8: 0.5177865616417685, 9: 0.5106719371358397, 10: 0.5138339924246897, 11: 0.48695652180980786, 12: 0.4861660080229341, 13: 0.507509881752753, 14: 0.5027667987488004, 15: 0.5146245063058001, 16: 0.4980237156977295, 17: 0.4814229252310138, 18: 0.5098814233018476, 19: 0.513833992471808, 20: 0.5019762849619266, 21: 0.49723320191085574, 22: 0.5106719370887214}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0608 - acc: 0.4430 - val_loss: 1.0204 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0423 - acc: 0.4717 - val_loss: 1.0211 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 111s 11ms/step - loss: 1.0301 - acc: 0.4813 - val_loss: 1.0149 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 111s 11ms/step - loss: 1.0202 - acc: 0.4890 - val_loss: 1.0097 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0114 - acc: 0.5017 - val_loss: 1.0062 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.49328063278801354\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0639 - acc: 0.4344 - val_loss: 1.0250 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0409 - acc: 0.4747 - val_loss: 1.0105 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0262 - acc: 0.4927 - val_loss: 1.0202 - val_acc: 0.4938\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0205 - acc: 0.4924 - val_loss: 1.0011 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0107 - acc: 0.5040 - val_loss: 0.9997 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5122529644975549\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0664 - acc: 0.4371 - val_loss: 1.0219 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0408 - acc: 0.4731 - val_loss: 1.0106 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0267 - acc: 0.4875 - val_loss: 1.0075 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0195 - acc: 0.4985 - val_loss: 1.0142 - val_acc: 0.4977\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0092 - acc: 0.5058 - val_loss: 1.0022 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.4893280636180531\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0657 - acc: 0.4360 - val_loss: 1.0219 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0412 - acc: 0.4734 - val_loss: 1.0091 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0287 - acc: 0.4863 - val_loss: 1.0158 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0151 - acc: 0.5009 - val_loss: 0.9974 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0085 - acc: 0.5084 - val_loss: 0.9939 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.509881423207611\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0634 - acc: 0.4439 - val_loss: 1.0297 - val_acc: 0.4868\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0407 - acc: 0.4763 - val_loss: 1.0132 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0270 - acc: 0.4862 - val_loss: 1.0043 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0136 - acc: 0.4999 - val_loss: 1.0265 - val_acc: 0.4992\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0026 - acc: 0.5103 - val_loss: 0.9999 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5043478264167846\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0651 - acc: 0.4319 - val_loss: 1.0257 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0398 - acc: 0.4744 - val_loss: 1.0208 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0249 - acc: 0.4908 - val_loss: 1.0144 - val_acc: 0.4969\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0158 - acc: 0.5011 - val_loss: 1.0120 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0033 - acc: 0.5067 - val_loss: 1.0097 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5090909094678555\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0672 - acc: 0.4363 - val_loss: 1.0192 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0389 - acc: 0.4772 - val_loss: 1.0136 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0245 - acc: 0.4915 - val_loss: 1.0163 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0167 - acc: 0.5022 - val_loss: 1.0020 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0030 - acc: 0.5050 - val_loss: 0.9953 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5201581031437448\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0715 - acc: 0.4332 - val_loss: 1.0210 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0427 - acc: 0.4724 - val_loss: 1.0174 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0286 - acc: 0.4909 - val_loss: 1.0024 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0182 - acc: 0.4985 - val_loss: 0.9991 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0066 - acc: 0.5093 - val_loss: 1.0027 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5027667987959187\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0674 - acc: 0.4362 - val_loss: 1.0122 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0423 - acc: 0.4723 - val_loss: 1.0092 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0276 - acc: 0.4939 - val_loss: 1.0050 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0134 - acc: 0.5033 - val_loss: 0.9959 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0058 - acc: 0.5035 - val_loss: 0.9999 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.4948616601026105\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0692 - acc: 0.4362 - val_loss: 1.0412 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0416 - acc: 0.4820 - val_loss: 1.0151 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0250 - acc: 0.4911 - val_loss: 1.0157 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0145 - acc: 0.5013 - val_loss: 0.9980 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0032 - acc: 0.5090 - val_loss: 1.0134 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5043478264639029\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0734 - acc: 0.4319 - val_loss: 1.0163 - val_acc: 0.4899\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0424 - acc: 0.4731 - val_loss: 1.0235 - val_acc: 0.4922\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 118s 11ms/step - loss: 1.0235 - acc: 0.4924 - val_loss: 1.0012 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0113 - acc: 0.5026 - val_loss: 0.9973 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0039 - acc: 0.5118 - val_loss: 0.9937 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5067193679187609\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0689 - acc: 0.4260 - val_loss: 1.0366 - val_acc: 0.4743\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0422 - acc: 0.4748 - val_loss: 1.0066 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0288 - acc: 0.4878 - val_loss: 1.0012 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0127 - acc: 0.5034 - val_loss: 0.9946 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0049 - acc: 0.5089 - val_loss: 1.0083 - val_acc: 0.5070\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5154150200926739\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0664 - acc: 0.4362 - val_loss: 1.0201 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 1.0393 - acc: 0.4771 - val_loss: 1.0083 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0189 - acc: 0.4987 - val_loss: 0.9985 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0067 - acc: 0.5100 - val_loss: 0.9987 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9970 - acc: 0.5153 - val_loss: 0.9942 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.499604743412832\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0711 - acc: 0.4366 - val_loss: 1.0196 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0364 - acc: 0.4766 - val_loss: 1.0049 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0213 - acc: 0.4958 - val_loss: 1.0057 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0111 - acc: 0.5043 - val_loss: 0.9927 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0004 - acc: 0.5134 - val_loss: 0.9935 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.49723320191085574\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 147s 14ms/step - loss: 1.0712 - acc: 0.4286 - val_loss: 1.0225 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0429 - acc: 0.4731 - val_loss: 1.0137 - val_acc: 0.4992\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0261 - acc: 0.4952 - val_loss: 1.0061 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0150 - acc: 0.5020 - val_loss: 1.0013 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9978 - acc: 0.5105 - val_loss: 0.9983 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5043478264167846\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 149s 15ms/step - loss: 1.0742 - acc: 0.4294 - val_loss: 1.0198 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0396 - acc: 0.4748 - val_loss: 1.0180 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0280 - acc: 0.4864 - val_loss: 1.0091 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0101 - acc: 0.5093 - val_loss: 1.0005 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0007 - acc: 0.5099 - val_loss: 1.0055 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.507509881752753\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 150s 15ms/step - loss: 1.0744 - acc: 0.4289 - val_loss: 1.0279 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0400 - acc: 0.4751 - val_loss: 1.0155 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0241 - acc: 0.4933 - val_loss: 1.0058 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 157s 15ms/step - loss: 1.0108 - acc: 0.5050 - val_loss: 1.0033 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9997 - acc: 0.5122 - val_loss: 1.0019 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5154150200926739\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 151s 15ms/step - loss: 1.0713 - acc: 0.4380 - val_loss: 1.0149 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0422 - acc: 0.4747 - val_loss: 1.0101 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0253 - acc: 0.4943 - val_loss: 1.0031 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0108 - acc: 0.5042 - val_loss: 1.0034 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9974 - acc: 0.5132 - val_loss: 0.9993 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5067193679658791\n",
      "{5: 0.49328063278801354, 6: 0.5122529644975549, 7: 0.4893280636180531, 8: 0.509881423207611, 9: 0.5043478264167846, 10: 0.5090909094678555, 11: 0.5201581031437448, 12: 0.5027667987959187, 13: 0.4948616601026105, 14: 0.5043478264639029, 15: 0.5067193679187609, 16: 0.5154150200926739, 17: 0.499604743412832, 18: 0.49723320191085574, 19: 0.5043478264167846, 20: 0.507509881752753, 21: 0.5154150200926739, 22: 0.5067193679658791}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0652 - acc: 0.4383 - val_loss: 1.0169 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0441 - acc: 0.4652 - val_loss: 1.0133 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0327 - acc: 0.4826 - val_loss: 1.0104 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0219 - acc: 0.4890 - val_loss: 1.0037 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0130 - acc: 0.4996 - val_loss: 1.0065 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5169960477606581\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0615 - acc: 0.4355 - val_loss: 1.0312 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0413 - acc: 0.4695 - val_loss: 1.0231 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0291 - acc: 0.4854 - val_loss: 1.0059 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0201 - acc: 0.4898 - val_loss: 1.0018 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0076 - acc: 0.5077 - val_loss: 1.0007 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5067193679658791\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0622 - acc: 0.4404 - val_loss: 1.0222 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0426 - acc: 0.4706 - val_loss: 1.0368 - val_acc: 0.4657\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0296 - acc: 0.4867 - val_loss: 1.0186 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0189 - acc: 0.4952 - val_loss: 1.0054 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0091 - acc: 0.5078 - val_loss: 1.0024 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5193675889563655\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0613 - acc: 0.4440 - val_loss: 1.0303 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0414 - acc: 0.4695 - val_loss: 1.0210 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0298 - acc: 0.4862 - val_loss: 1.0186 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0184 - acc: 0.4984 - val_loss: 1.0050 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0070 - acc: 0.5064 - val_loss: 1.0072 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5130434783315471\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0626 - acc: 0.4414 - val_loss: 1.0245 - val_acc: 0.4945\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0405 - acc: 0.4732 - val_loss: 1.0084 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0259 - acc: 0.4881 - val_loss: 1.0054 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0167 - acc: 0.4962 - val_loss: 1.0020 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0077 - acc: 0.5051 - val_loss: 1.0180 - val_acc: 0.5070\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.507509881446484\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0602 - acc: 0.4458 - val_loss: 1.0198 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0374 - acc: 0.4778 - val_loss: 1.0328 - val_acc: 0.4844\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0268 - acc: 0.4897 - val_loss: 1.0074 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0150 - acc: 0.4960 - val_loss: 1.0049 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0034 - acc: 0.5101 - val_loss: 1.0072 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5035573122765236\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0640 - acc: 0.4378 - val_loss: 1.0185 - val_acc: 0.4930\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0363 - acc: 0.4805 - val_loss: 1.0081 - val_acc: 0.5202\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0250 - acc: 0.4903 - val_loss: 1.0053 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0130 - acc: 0.5029 - val_loss: 1.0207 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0050 - acc: 0.5075 - val_loss: 0.9941 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5090909094207372\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0670 - acc: 0.4370 - val_loss: 1.0310 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0409 - acc: 0.4738 - val_loss: 1.0132 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0268 - acc: 0.4915 - val_loss: 1.0017 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0160 - acc: 0.5012 - val_loss: 1.0033 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0060 - acc: 0.5081 - val_loss: 0.9995 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5067193679658791\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0713 - acc: 0.4338 - val_loss: 1.0198 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0380 - acc: 0.4810 - val_loss: 1.0154 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0258 - acc: 0.4937 - val_loss: 1.0039 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0124 - acc: 0.5073 - val_loss: 0.9989 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0020 - acc: 0.5112 - val_loss: 0.9986 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.49644268807686365\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0702 - acc: 0.4356 - val_loss: 1.0126 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0379 - acc: 0.4807 - val_loss: 1.0301 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 118s 11ms/step - loss: 1.0274 - acc: 0.4916 - val_loss: 1.0039 - val_acc: 0.5241\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0107 - acc: 0.5040 - val_loss: 1.0032 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 118s 11ms/step - loss: 1.0045 - acc: 0.5095 - val_loss: 0.9936 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5090909093736189\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 138s 13ms/step - loss: 1.0695 - acc: 0.4379 - val_loss: 1.0140 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0367 - acc: 0.4835 - val_loss: 1.0100 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0254 - acc: 0.4915 - val_loss: 1.0055 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0134 - acc: 0.5062 - val_loss: 1.0000 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0028 - acc: 0.5166 - val_loss: 0.9937 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.5035573126299108\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 1.0707 - acc: 0.4351 - val_loss: 1.0290 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0360 - acc: 0.4828 - val_loss: 1.0082 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0190 - acc: 0.4979 - val_loss: 1.0028 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0109 - acc: 0.5043 - val_loss: 0.9961 - val_acc: 0.5280\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 0.9974 - acc: 0.5128 - val_loss: 0.9933 - val_acc: 0.5280\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5043478263696664\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 145s 14ms/step - loss: 1.0696 - acc: 0.4350 - val_loss: 1.0298 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0394 - acc: 0.4765 - val_loss: 1.0232 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0241 - acc: 0.4953 - val_loss: 1.0038 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0126 - acc: 0.5085 - val_loss: 1.0017 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0028 - acc: 0.5124 - val_loss: 0.9971 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.49486166040887947\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 1.0702 - acc: 0.4296 - val_loss: 1.0177 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0411 - acc: 0.4754 - val_loss: 1.0151 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0256 - acc: 0.4969 - val_loss: 1.0012 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0166 - acc: 0.5029 - val_loss: 1.0116 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0066 - acc: 0.5099 - val_loss: 0.9998 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.507509881446484\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 196s 19ms/step - loss: 1.0701 - acc: 0.4346 - val_loss: 1.0262 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0387 - acc: 0.4777 - val_loss: 1.0209 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0250 - acc: 0.4938 - val_loss: 1.0042 - val_acc: 0.5101\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0120 - acc: 0.5033 - val_loss: 0.9987 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9978 - acc: 0.5173 - val_loss: 0.9945 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5106719370416031\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 152s 15ms/step - loss: 1.0685 - acc: 0.4330 - val_loss: 1.0268 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0380 - acc: 0.4801 - val_loss: 1.0182 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0225 - acc: 0.4940 - val_loss: 1.0065 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0065 - acc: 0.5085 - val_loss: 1.0135 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9974 - acc: 0.5183 - val_loss: 0.9954 - val_acc: 0.5273\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5059288540847687\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 151s 15ms/step - loss: 1.0752 - acc: 0.4339 - val_loss: 1.0139 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0433 - acc: 0.4662 - val_loss: 1.0036 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0244 - acc: 0.4873 - val_loss: 1.0069 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0138 - acc: 0.5043 - val_loss: 1.0050 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0057 - acc: 0.5112 - val_loss: 0.9980 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.516205533926666\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 151s 15ms/step - loss: 1.0702 - acc: 0.4346 - val_loss: 1.0392 - val_acc: 0.4502\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0395 - acc: 0.4741 - val_loss: 1.0091 - val_acc: 0.5202\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0239 - acc: 0.4948 - val_loss: 1.0004 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0111 - acc: 0.5039 - val_loss: 0.9974 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9953 - acc: 0.5190 - val_loss: 0.9995 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5090909094207372\n",
      "{5: 0.5169960477606581, 6: 0.5067193679658791, 7: 0.5193675889563655, 8: 0.5130434783315471, 9: 0.507509881446484, 10: 0.5035573122765236, 11: 0.5090909094207372, 12: 0.5067193679658791, 13: 0.49644268807686365, 14: 0.5090909093736189, 15: 0.5035573126299108, 16: 0.5043478263696664, 17: 0.49486166040887947, 18: 0.507509881446484, 19: 0.5106719370416031, 20: 0.5059288540847687, 21: 0.516205533926666, 22: 0.5090909094207372}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0627 - acc: 0.4388 - val_loss: 1.0254 - val_acc: 0.5109\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0431 - acc: 0.4660 - val_loss: 1.0182 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0311 - acc: 0.4810 - val_loss: 1.0139 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0230 - acc: 0.4911 - val_loss: 1.0067 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0111 - acc: 0.5027 - val_loss: 1.0218 - val_acc: 0.4852\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.49249011860063424\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0655 - acc: 0.4345 - val_loss: 1.0229 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0423 - acc: 0.4722 - val_loss: 1.0172 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0308 - acc: 0.4807 - val_loss: 1.0083 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0211 - acc: 0.4954 - val_loss: 1.0026 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0111 - acc: 0.5023 - val_loss: 1.0081 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5241106723137052\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0664 - acc: 0.4257 - val_loss: 1.0214 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0411 - acc: 0.4680 - val_loss: 1.0254 - val_acc: 0.4930\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0299 - acc: 0.4864 - val_loss: 1.0075 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0223 - acc: 0.4928 - val_loss: 1.0401 - val_acc: 0.4790\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0114 - acc: 0.5041 - val_loss: 1.0026 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.513833992471808\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0645 - acc: 0.4326 - val_loss: 1.0180 - val_acc: 0.5171\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0373 - acc: 0.4772 - val_loss: 1.0189 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0276 - acc: 0.4900 - val_loss: 1.0018 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0169 - acc: 0.5030 - val_loss: 1.0055 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0075 - acc: 0.5125 - val_loss: 1.0014 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5122529644975549\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 138s 13ms/step - loss: 1.0674 - acc: 0.4336 - val_loss: 1.0277 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0433 - acc: 0.4696 - val_loss: 1.0123 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0326 - acc: 0.4821 - val_loss: 1.0166 - val_acc: 0.5016\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0215 - acc: 0.4969 - val_loss: 1.0039 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0089 - acc: 0.5055 - val_loss: 0.9985 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5098814229484603\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 135s 13ms/step - loss: 1.0635 - acc: 0.4444 - val_loss: 1.0207 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0390 - acc: 0.4776 - val_loss: 1.0150 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0283 - acc: 0.4847 - val_loss: 1.0062 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0153 - acc: 0.5033 - val_loss: 0.9990 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0071 - acc: 0.5075 - val_loss: 0.9984 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 6s 4ms/step\n",
      "0.5083003956338633\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 301s 29ms/step - loss: 1.0638 - acc: 0.4336 - val_loss: 1.0194 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 287s 28ms/step - loss: 1.0384 - acc: 0.4781 - val_loss: 1.0085 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 224s 22ms/step - loss: 1.0247 - acc: 0.4932 - val_loss: 1.0013 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0123 - acc: 0.5024 - val_loss: 1.0020 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0040 - acc: 0.5055 - val_loss: 0.9993 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.4972332016517051\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0653 - acc: 0.4365 - val_loss: 1.0252 - val_acc: 0.5187\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 138s 13ms/step - loss: 1.0384 - acc: 0.4785 - val_loss: 1.0072 - val_acc: 0.5179\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0262 - acc: 0.4914 - val_loss: 1.0273 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 223s 22ms/step - loss: 1.0151 - acc: 0.5032 - val_loss: 1.0039 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 194s 19ms/step - loss: 1.0008 - acc: 0.5134 - val_loss: 0.9966 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.5051383402507766\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0663 - acc: 0.4366 - val_loss: 1.0269 - val_acc: 0.4945\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 153s 15ms/step - loss: 1.0394 - acc: 0.4680 - val_loss: 1.0171 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0217 - acc: 0.4979 - val_loss: 1.0147 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0157 - acc: 0.4978 - val_loss: 1.0054 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 145s 14ms/step - loss: 1.0016 - acc: 0.5160 - val_loss: 1.0039 - val_acc: 0.5070\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5059288540847687\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 1.0621 - acc: 0.4468 - val_loss: 1.0189 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 135s 13ms/step - loss: 1.0385 - acc: 0.4799 - val_loss: 1.0062 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 1.0222 - acc: 0.4918 - val_loss: 1.0070 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0104 - acc: 0.5072 - val_loss: 1.0203 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 137s 13ms/step - loss: 1.0010 - acc: 0.5136 - val_loss: 1.0050 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 6s 4ms/step\n",
      "0.49960474331859545\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0681 - acc: 0.4329 - val_loss: 1.0277 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 315s 31ms/step - loss: 1.0384 - acc: 0.4757 - val_loss: 1.0256 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 1.0234 - acc: 0.4956 - val_loss: 1.0045 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 135s 13ms/step - loss: 1.0136 - acc: 0.5032 - val_loss: 1.0194 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 133s 13ms/step - loss: 0.9989 - acc: 0.5128 - val_loss: 0.9961 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.5035573122765236\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 150s 15ms/step - loss: 1.0682 - acc: 0.4349 - val_loss: 1.0332 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 133s 13ms/step - loss: 1.0415 - acc: 0.4713 - val_loss: 1.0211 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0238 - acc: 0.4985 - val_loss: 1.0064 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0078 - acc: 0.5099 - val_loss: 1.0010 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 0.9976 - acc: 0.5160 - val_loss: 0.9959 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.5059288537784998\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 155s 15ms/step - loss: 1.0657 - acc: 0.4433 - val_loss: 1.0201 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0376 - acc: 0.4779 - val_loss: 1.0048 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0203 - acc: 0.4988 - val_loss: 0.9988 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 147s 14ms/step - loss: 1.0101 - acc: 0.5098 - val_loss: 0.9994 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 138s 14ms/step - loss: 0.9942 - acc: 0.5199 - val_loss: 1.0289 - val_acc: 0.5016\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.47905138342276865\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 153s 15ms/step - loss: 1.0683 - acc: 0.4322 - val_loss: 1.0215 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0385 - acc: 0.4759 - val_loss: 1.0075 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0216 - acc: 0.5027 - val_loss: 1.0038 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0103 - acc: 0.4992 - val_loss: 1.0095 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 135s 13ms/step - loss: 0.9967 - acc: 0.5125 - val_loss: 1.0043 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5003952572468241\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 157s 15ms/step - loss: 1.0688 - acc: 0.4362 - val_loss: 1.0152 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0384 - acc: 0.4784 - val_loss: 1.0152 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0256 - acc: 0.4947 - val_loss: 0.9993 - val_acc: 0.5249\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0126 - acc: 0.4985 - val_loss: 0.9991 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 0.9989 - acc: 0.5183 - val_loss: 0.9939 - val_acc: 0.5265\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5090909093265006\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 215s 21ms/step - loss: 1.0712 - acc: 0.4373 - val_loss: 1.0186 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0373 - acc: 0.4760 - val_loss: 1.0051 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0232 - acc: 0.4921 - val_loss: 1.0132 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 137s 13ms/step - loss: 1.0077 - acc: 0.5043 - val_loss: 0.9936 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 0.9928 - acc: 0.5170 - val_loss: 0.9943 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.4988142295788399\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0770 - acc: 0.4317 - val_loss: 1.0303 - val_acc: 0.4961\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0416 - acc: 0.4709 - val_loss: 1.0174 - val_acc: 0.4945\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 137s 13ms/step - loss: 1.0239 - acc: 0.4954 - val_loss: 1.0038 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0123 - acc: 0.5000 - val_loss: 1.0056 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9980 - acc: 0.5153 - val_loss: 0.9975 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5122529648038239\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0735 - acc: 0.4315 - val_loss: 1.0252 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0392 - acc: 0.4754 - val_loss: 1.0253 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0254 - acc: 0.4951 - val_loss: 1.0008 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0117 - acc: 0.5029 - val_loss: 1.0064 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0003 - acc: 0.5164 - val_loss: 1.0020 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.518577075381524\n",
      "{5: 0.49249011860063424, 6: 0.5241106723137052, 7: 0.513833992471808, 8: 0.5122529644975549, 9: 0.5098814229484603, 10: 0.5083003956338633, 11: 0.4972332016517051, 12: 0.5051383402507766, 13: 0.5059288540847687, 14: 0.49960474331859545, 15: 0.5035573122765236, 16: 0.5059288537784998, 17: 0.47905138342276865, 18: 0.5003952572468241, 19: 0.5090909093265006, 20: 0.4988142295788399, 21: 0.5122529648038239, 22: 0.518577075381524}\n",
      "CPU times: user 3d 17h 48min 25s, sys: 7h 20min 15s, total: 4d 1h 8min 41s\n",
      "Wall time: 16h 29min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cnn_bert_rounds = [calculate_round(concatenated_bert) for round in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{5: 0.5067193679658791,\n",
       "  6: 0.5162055339737843,\n",
       "  7: 0.5130434782844288,\n",
       "  8: 0.5225296443865705,\n",
       "  9: 0.5098814232547293,\n",
       "  10: 0.505928854131887,\n",
       "  11: 0.5027667987016821,\n",
       "  12: 0.507509881752753,\n",
       "  13: 0.49644268807686365,\n",
       "  14: 0.48458498056698224,\n",
       "  15: 0.507509881752753,\n",
       "  16: 0.4996047431536814,\n",
       "  17: 0.5114624508755952,\n",
       "  18: 0.49802371574484783,\n",
       "  19: 0.5011857710808162,\n",
       "  20: 0.5043478261105157,\n",
       "  21: 0.5209486169777369,\n",
       "  22: 0.516205533926666,\n",
       "  23: 0.49723320191085574,\n",
       "  24: 0.4758893284401875,\n",
       "  25: 0.5098814233018476,\n",
       "  26: 0.5043478264639029,\n",
       "  27: 0.5035573126299108,\n",
       "  28: 0.4877470355966817,\n",
       "  29: 0.5114624509698318,\n",
       "  30: 0.5083003953275944,\n",
       "  31: 0.5209486169306186,\n",
       "  32: 0.5090909094678555,\n",
       "  33: 0.5035573125827925,\n",
       "  34: 0.5169960478077764,\n",
       "  35: 0.5035573125827925},\n",
       " {5: 0.5122529644975549,\n",
       "  6: 0.5027667984896498,\n",
       "  7: 0.485375494188942,\n",
       "  8: 0.5177865616417685,\n",
       "  9: 0.5106719371358397,\n",
       "  10: 0.5138339924246897,\n",
       "  11: 0.48695652180980786,\n",
       "  12: 0.4861660080229341,\n",
       "  13: 0.507509881752753,\n",
       "  14: 0.5027667987488004,\n",
       "  15: 0.5146245063058001,\n",
       "  16: 0.4980237156977295,\n",
       "  17: 0.4814229252310138,\n",
       "  18: 0.5098814233018476,\n",
       "  19: 0.513833992471808,\n",
       "  20: 0.5019762849619266,\n",
       "  21: 0.49723320191085574,\n",
       "  22: 0.5106719370887214,\n",
       "  23: 0.5114624509698318,\n",
       "  24: 0.507509881446484,\n",
       "  25: 0.5122529644504367,\n",
       "  26: 0.5114624509227135,\n",
       "  27: 0.5035573125356742,\n",
       "  28: 0.5177865615475319,\n",
       "  29: 0.5114624509698318,\n",
       "  30: 0.505928854131887,\n",
       "  31: 0.5083003952804761,\n",
       "  32: 0.5090909093736189,\n",
       "  33: 0.5098814233018476,\n",
       "  34: 0.5067193679187609,\n",
       "  35: 0.507509881752753},\n",
       " {5: 0.49328063278801354,\n",
       "  6: 0.5122529644975549,\n",
       "  7: 0.4893280636180531,\n",
       "  8: 0.509881423207611,\n",
       "  9: 0.5043478264167846,\n",
       "  10: 0.5090909094678555,\n",
       "  11: 0.5201581031437448,\n",
       "  12: 0.5027667987959187,\n",
       "  13: 0.4948616601026105,\n",
       "  14: 0.5043478264639029,\n",
       "  15: 0.5067193679187609,\n",
       "  16: 0.5154150200926739,\n",
       "  17: 0.499604743412832,\n",
       "  18: 0.49723320191085574,\n",
       "  19: 0.5043478264167846,\n",
       "  20: 0.507509881752753,\n",
       "  21: 0.5154150200926739,\n",
       "  22: 0.5067193679658791,\n",
       "  23: 0.4980237156977295,\n",
       "  24: 0.505138340297895,\n",
       "  25: 0.4877470359029506,\n",
       "  26: 0.49644268807686365,\n",
       "  27: 0.5019762846085394,\n",
       "  28: 0.5003952572468241,\n",
       "  29: 0.49249011895402145,\n",
       "  30: 0.5043478264639029,\n",
       "  31: 0.4988142296259582,\n",
       "  32: 0.5067193679187609,\n",
       "  33: 0.5098814232547293,\n",
       "  34: 0.5090909091144682,\n",
       "  35: 0.5106719370416031},\n",
       " {5: 0.5169960477606581,\n",
       "  6: 0.5067193679658791,\n",
       "  7: 0.5193675889563655,\n",
       "  8: 0.5130434783315471,\n",
       "  9: 0.507509881446484,\n",
       "  10: 0.5035573122765236,\n",
       "  11: 0.5090909094207372,\n",
       "  12: 0.5067193679658791,\n",
       "  13: 0.49644268807686365,\n",
       "  14: 0.5090909093736189,\n",
       "  15: 0.5035573126299108,\n",
       "  16: 0.5043478263696664,\n",
       "  17: 0.49486166040887947,\n",
       "  18: 0.507509881446484,\n",
       "  19: 0.5106719370416031,\n",
       "  20: 0.5059288540847687,\n",
       "  21: 0.516205533926666,\n",
       "  22: 0.5090909094207372,\n",
       "  23: 0.5019762846085394,\n",
       "  24: 0.49723320160458684,\n",
       "  25: 0.50197628486769,\n",
       "  26: 0.5059288540847687,\n",
       "  27: 0.5083003956338633,\n",
       "  28: 0.49644268807686365,\n",
       "  29: 0.5011857711279345,\n",
       "  30: 0.5138339923775714,\n",
       "  31: 0.513833992471808,\n",
       "  32: 0.5051383399445077,\n",
       "  33: 0.49169960481376046,\n",
       "  34: 0.505928854131887,\n",
       "  35: 0.5083003955867451},\n",
       " {5: 0.49249011860063424,\n",
       "  6: 0.5241106723137052,\n",
       "  7: 0.513833992471808,\n",
       "  8: 0.5122529644975549,\n",
       "  9: 0.5098814229484603,\n",
       "  10: 0.5083003956338633,\n",
       "  11: 0.4972332016517051,\n",
       "  12: 0.5051383402507766,\n",
       "  13: 0.5059288540847687,\n",
       "  14: 0.49960474331859545,\n",
       "  15: 0.5035573122765236,\n",
       "  16: 0.5059288537784998,\n",
       "  17: 0.47905138342276865,\n",
       "  18: 0.5003952572468241,\n",
       "  19: 0.5090909093265006,\n",
       "  20: 0.4988142295788399,\n",
       "  21: 0.5122529648038239,\n",
       "  22: 0.518577075381524,\n",
       "  23: 0.5011857710808162,\n",
       "  24: 0.496442687817713,\n",
       "  25: 0.5059288540847687,\n",
       "  26: 0.5177865612883813,\n",
       "  27: 0.5130434785906977,\n",
       "  28: 0.49802371574484783,\n",
       "  29: 0.5019762849148083,\n",
       "  30: 0.5098814233018476,\n",
       "  31: 0.5154150201397922,\n",
       "  32: 0.513043478496461,\n",
       "  33: 0.507509881446484,\n",
       "  34: 0.5059288537784998,\n",
       "  35: 0.5106719370416031}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_bert_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Round 0",
         "type": "scatter",
         "uid": "2effce92-43eb-4801-88cd-6aefaec50746",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5067193679658791,
          0.5162055339737843,
          0.5130434782844288,
          0.5225296443865705,
          0.5098814232547293,
          0.505928854131887,
          0.5027667987016821,
          0.507509881752753,
          0.49644268807686365,
          0.48458498056698224,
          0.507509881752753,
          0.4996047431536814,
          0.5114624508755952,
          0.49802371574484783,
          0.5011857710808162,
          0.5043478261105157,
          0.5209486169777369,
          0.516205533926666,
          0.49723320191085574,
          0.4758893284401875,
          0.5098814233018476,
          0.5043478264639029,
          0.5035573126299108,
          0.4877470355966817,
          0.5114624509698318,
          0.5083003953275944,
          0.5209486169306186,
          0.5090909094678555,
          0.5035573125827925,
          0.5169960478077764,
          0.5035573125827925
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 1",
         "type": "scatter",
         "uid": "5eaafa57-d4a1-437d-8362-90f888e6fe1c",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5122529644975549,
          0.5027667984896498,
          0.485375494188942,
          0.5177865616417685,
          0.5106719371358397,
          0.5138339924246897,
          0.48695652180980786,
          0.4861660080229341,
          0.507509881752753,
          0.5027667987488004,
          0.5146245063058001,
          0.4980237156977295,
          0.4814229252310138,
          0.5098814233018476,
          0.513833992471808,
          0.5019762849619266,
          0.49723320191085574,
          0.5106719370887214,
          0.5114624509698318,
          0.507509881446484,
          0.5122529644504367,
          0.5114624509227135,
          0.5035573125356742,
          0.5177865615475319,
          0.5114624509698318,
          0.505928854131887,
          0.5083003952804761,
          0.5090909093736189,
          0.5098814233018476,
          0.5067193679187609,
          0.507509881752753
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 2",
         "type": "scatter",
         "uid": "a249a111-582e-4ba0-afbd-c2f50accfcad",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.49328063278801354,
          0.5122529644975549,
          0.4893280636180531,
          0.509881423207611,
          0.5043478264167846,
          0.5090909094678555,
          0.5201581031437448,
          0.5027667987959187,
          0.4948616601026105,
          0.5043478264639029,
          0.5067193679187609,
          0.5154150200926739,
          0.499604743412832,
          0.49723320191085574,
          0.5043478264167846,
          0.507509881752753,
          0.5154150200926739,
          0.5067193679658791,
          0.4980237156977295,
          0.505138340297895,
          0.4877470359029506,
          0.49644268807686365,
          0.5019762846085394,
          0.5003952572468241,
          0.49249011895402145,
          0.5043478264639029,
          0.4988142296259582,
          0.5067193679187609,
          0.5098814232547293,
          0.5090909091144682,
          0.5106719370416031
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 3",
         "type": "scatter",
         "uid": "e914b6ab-8908-42b0-bd36-e737e33414dc",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5169960477606581,
          0.5067193679658791,
          0.5193675889563655,
          0.5130434783315471,
          0.507509881446484,
          0.5035573122765236,
          0.5090909094207372,
          0.5067193679658791,
          0.49644268807686365,
          0.5090909093736189,
          0.5035573126299108,
          0.5043478263696664,
          0.49486166040887947,
          0.507509881446484,
          0.5106719370416031,
          0.5059288540847687,
          0.516205533926666,
          0.5090909094207372,
          0.5019762846085394,
          0.49723320160458684,
          0.50197628486769,
          0.5059288540847687,
          0.5083003956338633,
          0.49644268807686365,
          0.5011857711279345,
          0.5138339923775714,
          0.513833992471808,
          0.5051383399445077,
          0.49169960481376046,
          0.505928854131887,
          0.5083003955867451
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 4",
         "type": "scatter",
         "uid": "49e5acb4-4c1d-4c65-8cf0-e3ba1dc94aa7",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.49249011860063424,
          0.5241106723137052,
          0.513833992471808,
          0.5122529644975549,
          0.5098814229484603,
          0.5083003956338633,
          0.4972332016517051,
          0.5051383402507766,
          0.5059288540847687,
          0.49960474331859545,
          0.5035573122765236,
          0.5059288537784998,
          0.47905138342276865,
          0.5003952572468241,
          0.5090909093265006,
          0.4988142295788399,
          0.5122529648038239,
          0.518577075381524,
          0.5011857710808162,
          0.496442687817713,
          0.5059288540847687,
          0.5177865612883813,
          0.5130434785906977,
          0.49802371574484783,
          0.5019762849148083,
          0.5098814233018476,
          0.5154150201397922,
          0.513043478496461,
          0.507509881446484,
          0.5059288537784998,
          0.5106719370416031
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "CNN test set accuracy of padded BERT dataset with variable maximum lengths"
        }
       }
      },
      "text/html": [
       "<div id=\"35ca2786-cb18-4987-a3df-9973288a05cc\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"35ca2786-cb18-4987-a3df-9973288a05cc\")) {\n",
       "    Plotly.newPlot(\"35ca2786-cb18-4987-a3df-9973288a05cc\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5067193679658791, 0.5162055339737843, 0.5130434782844288, 0.5225296443865705, 0.5098814232547293, 0.505928854131887, 0.5027667987016821, 0.507509881752753, 0.49644268807686365, 0.48458498056698224, 0.507509881752753, 0.4996047431536814, 0.5114624508755952, 0.49802371574484783, 0.5011857710808162, 0.5043478261105157, 0.5209486169777369, 0.516205533926666, 0.49723320191085574, 0.4758893284401875, 0.5098814233018476, 0.5043478264639029, 0.5035573126299108, 0.4877470355966817, 0.5114624509698318, 0.5083003953275944, 0.5209486169306186, 0.5090909094678555, 0.5035573125827925, 0.5169960478077764, 0.5035573125827925], \"type\": \"scatter\", \"uid\": \"2effce92-43eb-4801-88cd-6aefaec50746\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5122529644975549, 0.5027667984896498, 0.485375494188942, 0.5177865616417685, 0.5106719371358397, 0.5138339924246897, 0.48695652180980786, 0.4861660080229341, 0.507509881752753, 0.5027667987488004, 0.5146245063058001, 0.4980237156977295, 0.4814229252310138, 0.5098814233018476, 0.513833992471808, 0.5019762849619266, 0.49723320191085574, 0.5106719370887214, 0.5114624509698318, 0.507509881446484, 0.5122529644504367, 0.5114624509227135, 0.5035573125356742, 0.5177865615475319, 0.5114624509698318, 0.505928854131887, 0.5083003952804761, 0.5090909093736189, 0.5098814233018476, 0.5067193679187609, 0.507509881752753], \"type\": \"scatter\", \"uid\": \"5eaafa57-d4a1-437d-8362-90f888e6fe1c\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49328063278801354, 0.5122529644975549, 0.4893280636180531, 0.509881423207611, 0.5043478264167846, 0.5090909094678555, 0.5201581031437448, 0.5027667987959187, 0.4948616601026105, 0.5043478264639029, 0.5067193679187609, 0.5154150200926739, 0.499604743412832, 0.49723320191085574, 0.5043478264167846, 0.507509881752753, 0.5154150200926739, 0.5067193679658791, 0.4980237156977295, 0.505138340297895, 0.4877470359029506, 0.49644268807686365, 0.5019762846085394, 0.5003952572468241, 0.49249011895402145, 0.5043478264639029, 0.4988142296259582, 0.5067193679187609, 0.5098814232547293, 0.5090909091144682, 0.5106719370416031], \"type\": \"scatter\", \"uid\": \"a249a111-582e-4ba0-afbd-c2f50accfcad\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5169960477606581, 0.5067193679658791, 0.5193675889563655, 0.5130434783315471, 0.507509881446484, 0.5035573122765236, 0.5090909094207372, 0.5067193679658791, 0.49644268807686365, 0.5090909093736189, 0.5035573126299108, 0.5043478263696664, 0.49486166040887947, 0.507509881446484, 0.5106719370416031, 0.5059288540847687, 0.516205533926666, 0.5090909094207372, 0.5019762846085394, 0.49723320160458684, 0.50197628486769, 0.5059288540847687, 0.5083003956338633, 0.49644268807686365, 0.5011857711279345, 0.5138339923775714, 0.513833992471808, 0.5051383399445077, 0.49169960481376046, 0.505928854131887, 0.5083003955867451], \"type\": \"scatter\", \"uid\": \"e914b6ab-8908-42b0-bd36-e737e33414dc\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49249011860063424, 0.5241106723137052, 0.513833992471808, 0.5122529644975549, 0.5098814229484603, 0.5083003956338633, 0.4972332016517051, 0.5051383402507766, 0.5059288540847687, 0.49960474331859545, 0.5035573122765236, 0.5059288537784998, 0.47905138342276865, 0.5003952572468241, 0.5090909093265006, 0.4988142295788399, 0.5122529648038239, 0.518577075381524, 0.5011857710808162, 0.496442687817713, 0.5059288540847687, 0.5177865612883813, 0.5130434785906977, 0.49802371574484783, 0.5019762849148083, 0.5098814233018476, 0.5154150201397922, 0.513043478496461, 0.507509881446484, 0.5059288537784998, 0.5106719370416031], \"type\": \"scatter\", \"uid\": \"49e5acb4-4c1d-4c65-8cf0-e3ba1dc94aa7\"}], {\"title\": {\"text\": \"CNN test set accuracy of padded BERT dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"35ca2786-cb18-4987-a3df-9973288a05cc\")) {window._Plotly.Plots.resize(document.getElementById(\"35ca2786-cb18-4987-a3df-9973288a05cc\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"35ca2786-cb18-4987-a3df-9973288a05cc\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"35ca2786-cb18-4987-a3df-9973288a05cc\")) {\n",
       "    Plotly.newPlot(\"35ca2786-cb18-4987-a3df-9973288a05cc\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5067193679658791, 0.5162055339737843, 0.5130434782844288, 0.5225296443865705, 0.5098814232547293, 0.505928854131887, 0.5027667987016821, 0.507509881752753, 0.49644268807686365, 0.48458498056698224, 0.507509881752753, 0.4996047431536814, 0.5114624508755952, 0.49802371574484783, 0.5011857710808162, 0.5043478261105157, 0.5209486169777369, 0.516205533926666, 0.49723320191085574, 0.4758893284401875, 0.5098814233018476, 0.5043478264639029, 0.5035573126299108, 0.4877470355966817, 0.5114624509698318, 0.5083003953275944, 0.5209486169306186, 0.5090909094678555, 0.5035573125827925, 0.5169960478077764, 0.5035573125827925], \"type\": \"scatter\", \"uid\": \"2effce92-43eb-4801-88cd-6aefaec50746\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5122529644975549, 0.5027667984896498, 0.485375494188942, 0.5177865616417685, 0.5106719371358397, 0.5138339924246897, 0.48695652180980786, 0.4861660080229341, 0.507509881752753, 0.5027667987488004, 0.5146245063058001, 0.4980237156977295, 0.4814229252310138, 0.5098814233018476, 0.513833992471808, 0.5019762849619266, 0.49723320191085574, 0.5106719370887214, 0.5114624509698318, 0.507509881446484, 0.5122529644504367, 0.5114624509227135, 0.5035573125356742, 0.5177865615475319, 0.5114624509698318, 0.505928854131887, 0.5083003952804761, 0.5090909093736189, 0.5098814233018476, 0.5067193679187609, 0.507509881752753], \"type\": \"scatter\", \"uid\": \"5eaafa57-d4a1-437d-8362-90f888e6fe1c\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49328063278801354, 0.5122529644975549, 0.4893280636180531, 0.509881423207611, 0.5043478264167846, 0.5090909094678555, 0.5201581031437448, 0.5027667987959187, 0.4948616601026105, 0.5043478264639029, 0.5067193679187609, 0.5154150200926739, 0.499604743412832, 0.49723320191085574, 0.5043478264167846, 0.507509881752753, 0.5154150200926739, 0.5067193679658791, 0.4980237156977295, 0.505138340297895, 0.4877470359029506, 0.49644268807686365, 0.5019762846085394, 0.5003952572468241, 0.49249011895402145, 0.5043478264639029, 0.4988142296259582, 0.5067193679187609, 0.5098814232547293, 0.5090909091144682, 0.5106719370416031], \"type\": \"scatter\", \"uid\": \"a249a111-582e-4ba0-afbd-c2f50accfcad\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5169960477606581, 0.5067193679658791, 0.5193675889563655, 0.5130434783315471, 0.507509881446484, 0.5035573122765236, 0.5090909094207372, 0.5067193679658791, 0.49644268807686365, 0.5090909093736189, 0.5035573126299108, 0.5043478263696664, 0.49486166040887947, 0.507509881446484, 0.5106719370416031, 0.5059288540847687, 0.516205533926666, 0.5090909094207372, 0.5019762846085394, 0.49723320160458684, 0.50197628486769, 0.5059288540847687, 0.5083003956338633, 0.49644268807686365, 0.5011857711279345, 0.5138339923775714, 0.513833992471808, 0.5051383399445077, 0.49169960481376046, 0.505928854131887, 0.5083003955867451], \"type\": \"scatter\", \"uid\": \"e914b6ab-8908-42b0-bd36-e737e33414dc\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49249011860063424, 0.5241106723137052, 0.513833992471808, 0.5122529644975549, 0.5098814229484603, 0.5083003956338633, 0.4972332016517051, 0.5051383402507766, 0.5059288540847687, 0.49960474331859545, 0.5035573122765236, 0.5059288537784998, 0.47905138342276865, 0.5003952572468241, 0.5090909093265006, 0.4988142295788399, 0.5122529648038239, 0.518577075381524, 0.5011857710808162, 0.496442687817713, 0.5059288540847687, 0.5177865612883813, 0.5130434785906977, 0.49802371574484783, 0.5019762849148083, 0.5098814233018476, 0.5154150201397922, 0.513043478496461, 0.507509881446484, 0.5059288537784998, 0.5106719370416031], \"type\": \"scatter\", \"uid\": \"49e5acb4-4c1d-4c65-8cf0-e3ba1dc94aa7\"}], {\"title\": {\"text\": \"CNN test set accuracy of padded BERT dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"35ca2786-cb18-4987-a3df-9973288a05cc\")) {window._Plotly.Plots.resize(document.getElementById(\"35ca2786-cb18-4987-a3df-9973288a05cc\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traces = cnn_bert_rounds\n",
    "\n",
    "# Create traces\n",
    "def create_scatter(counter):\n",
    "    acc_dict = traces[counter]\n",
    "    \n",
    "    return go.Scatter(\n",
    "        x = list(acc_dict.keys()),\n",
    "        y = list(acc_dict.values()),\n",
    "        mode = 'lines+markers',\n",
    "        name = 'Round ' + str(counter)\n",
    "    )\n",
    "\n",
    "trace_data = [create_scatter(trace) for trace in range(len(traces))]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'CNN test set accuracy of padded BERT dataset with variable maximum lengths',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = trace_data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_elmo = {\n",
    "    dataset: [np.concatenate(np.array(statement)) for statement in elmo[dataset].statement]\n",
    "    for dataset in elmo.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "2019-06-06 17:27:55,650 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "2019-06-06 17:27:55,736 From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-06-06 17:27:55,831 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 148s 14ms/step - loss: 1.0634 - acc: 0.4377 - val_loss: 1.0281 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0388 - acc: 0.4691 - val_loss: 1.0276 - val_acc: 0.4766\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0272 - acc: 0.4848 - val_loss: 1.0075 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0152 - acc: 0.5015 - val_loss: 1.0009 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0032 - acc: 0.5063 - val_loss: 1.0124 - val_acc: 0.4961\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.49565217424287156\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0649 - acc: 0.4289 - val_loss: 1.0213 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0392 - acc: 0.4790 - val_loss: 1.0129 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0273 - acc: 0.4854 - val_loss: 1.0126 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0121 - acc: 0.5003 - val_loss: 1.0068 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 0.9998 - acc: 0.5116 - val_loss: 1.0072 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5114624506164446\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0691 - acc: 0.4293 - val_loss: 1.0227 - val_acc: 0.5210\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0430 - acc: 0.4644 - val_loss: 1.0466 - val_acc: 0.4673\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0256 - acc: 0.4885 - val_loss: 1.0194 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0100 - acc: 0.5038 - val_loss: 1.0070 - val_acc: 0.5257\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0023 - acc: 0.5161 - val_loss: 1.0035 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5169960477606581\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0693 - acc: 0.4295 - val_loss: 1.0191 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0400 - acc: 0.4763 - val_loss: 1.0068 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0247 - acc: 0.4957 - val_loss: 1.0099 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0047 - acc: 0.5105 - val_loss: 0.9980 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 0.9918 - acc: 0.5213 - val_loss: 0.9987 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5122529644504367\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0699 - acc: 0.4253 - val_loss: 1.0297 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0379 - acc: 0.4741 - val_loss: 1.0096 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0247 - acc: 0.4946 - val_loss: 1.0000 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0071 - acc: 0.5088 - val_loss: 1.0081 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 0.9919 - acc: 0.5207 - val_loss: 1.0052 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.49249011860063424\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0633 - acc: 0.4319 - val_loss: 1.0190 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0321 - acc: 0.4764 - val_loss: 1.0080 - val_acc: 0.5202\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0162 - acc: 0.5043 - val_loss: 1.0043 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0044 - acc: 0.5118 - val_loss: 1.0133 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 0.9891 - acc: 0.5230 - val_loss: 1.0007 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.49644268807686365\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0667 - acc: 0.4401 - val_loss: 1.0161 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0364 - acc: 0.4795 - val_loss: 1.0064 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0181 - acc: 0.4974 - val_loss: 0.9965 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0017 - acc: 0.5122 - val_loss: 1.0060 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 0.9865 - acc: 0.5237 - val_loss: 0.9975 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5075098817998712\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0692 - acc: 0.4333 - val_loss: 1.0229 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0358 - acc: 0.4839 - val_loss: 1.0106 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0200 - acc: 0.4957 - val_loss: 1.0067 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0007 - acc: 0.5116 - val_loss: 1.0024 - val_acc: 0.5148\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 115s 11ms/step - loss: 0.9866 - acc: 0.5274 - val_loss: 1.0097 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - ETA:  - 5s 4ms/step\n",
      "0.5185770751223734\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0726 - acc: 0.4361 - val_loss: 1.0170 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0353 - acc: 0.4823 - val_loss: 1.0163 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0192 - acc: 0.4979 - val_loss: 1.0048 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 0.9997 - acc: 0.5165 - val_loss: 1.0008 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 0.9799 - acc: 0.5297 - val_loss: 0.9985 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.507509881752753\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0737 - acc: 0.4322 - val_loss: 1.0191 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0338 - acc: 0.4839 - val_loss: 1.0006 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0174 - acc: 0.5008 - val_loss: 1.0058 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0020 - acc: 0.5121 - val_loss: 0.9981 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 0.9854 - acc: 0.5206 - val_loss: 0.9955 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.4988142295788399\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0713 - acc: 0.4351 - val_loss: 1.0136 - val_acc: 0.5164\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0351 - acc: 0.4792 - val_loss: 1.0058 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0138 - acc: 0.5092 - val_loss: 1.0164 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0035 - acc: 0.5143 - val_loss: 0.9958 - val_acc: 0.5249\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 0.9823 - acc: 0.5255 - val_loss: 0.9933 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5043478264639029\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0705 - acc: 0.4286 - val_loss: 1.0229 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0362 - acc: 0.4822 - val_loss: 1.0044 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0190 - acc: 0.5028 - val_loss: 1.0182 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9985 - acc: 0.5097 - val_loss: 0.9977 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 0.9819 - acc: 0.5269 - val_loss: 0.9979 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5122529647567056\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0759 - acc: 0.4280 - val_loss: 1.0478 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0390 - acc: 0.4815 - val_loss: 1.0072 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0172 - acc: 0.4978 - val_loss: 1.0114 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9981 - acc: 0.5174 - val_loss: 1.0015 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9751 - acc: 0.5318 - val_loss: 0.9941 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5067193679187609\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 137s 13ms/step - loss: 1.0764 - acc: 0.4326 - val_loss: 1.0181 - val_acc: 0.5210\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0421 - acc: 0.4716 - val_loss: 0.9973 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0167 - acc: 0.5044 - val_loss: 1.0025 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0000 - acc: 0.5133 - val_loss: 1.0205 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9775 - acc: 0.5358 - val_loss: 0.9899 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5090909094207372\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0719 - acc: 0.4394 - val_loss: 1.0286 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0357 - acc: 0.4830 - val_loss: 1.0083 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0140 - acc: 0.5021 - val_loss: 1.0049 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9983 - acc: 0.5160 - val_loss: 0.9886 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9788 - acc: 0.5319 - val_loss: 0.9833 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.513833992471808\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 145s 14ms/step - loss: 1.0751 - acc: 0.4305 - val_loss: 1.0125 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0341 - acc: 0.4829 - val_loss: 1.0054 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0157 - acc: 0.5039 - val_loss: 0.9951 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9945 - acc: 0.5168 - val_loss: 0.9904 - val_acc: 0.5312\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9767 - acc: 0.5336 - val_loss: 0.9957 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5059288540847687\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 1.0773 - acc: 0.4323 - val_loss: 1.0292 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0385 - acc: 0.4779 - val_loss: 1.0074 - val_acc: 0.5210\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0181 - acc: 0.5004 - val_loss: 1.0045 - val_acc: 0.5257\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9999 - acc: 0.5172 - val_loss: 0.9987 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9798 - acc: 0.5330 - val_loss: 0.9935 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5209486166243497\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 147s 14ms/step - loss: 1.0693 - acc: 0.4384 - val_loss: 1.0239 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0366 - acc: 0.4782 - val_loss: 1.0084 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0168 - acc: 0.5005 - val_loss: 0.9947 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0007 - acc: 0.5132 - val_loss: 0.9920 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9763 - acc: 0.5312 - val_loss: 0.9994 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.49565217424287156\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 149s 15ms/step - loss: 1.0796 - acc: 0.4376 - val_loss: 1.0234 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0417 - acc: 0.4739 - val_loss: 1.0178 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0220 - acc: 0.5025 - val_loss: 1.0053 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9976 - acc: 0.5126 - val_loss: 0.9961 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9822 - acc: 0.5294 - val_loss: 0.9972 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5122529647567056\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 146s 14ms/step - loss: 1.0904 - acc: 0.4325 - val_loss: 1.0260 - val_acc: 0.5132\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0354 - acc: 0.4827 - val_loss: 1.0065 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0173 - acc: 0.5014 - val_loss: 0.9982 - val_acc: 0.5257\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9964 - acc: 0.5214 - val_loss: 1.0102 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9773 - acc: 0.5305 - val_loss: 1.0002 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5098814229484603\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 152s 15ms/step - loss: 1.0768 - acc: 0.4339 - val_loss: 1.0173 - val_acc: 0.5109\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0356 - acc: 0.4830 - val_loss: 0.9995 - val_acc: 0.5226\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0125 - acc: 0.5016 - val_loss: 1.0006 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9924 - acc: 0.5212 - val_loss: 0.9966 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9746 - acc: 0.5324 - val_loss: 1.0022 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5272727276025554\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 152s 15ms/step - loss: 1.0714 - acc: 0.4396 - val_loss: 1.0252 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0393 - acc: 0.4805 - val_loss: 1.0013 - val_acc: 0.5241\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0177 - acc: 0.4995 - val_loss: 0.9898 - val_acc: 0.5273\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9897 - acc: 0.5266 - val_loss: 0.9968 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9744 - acc: 0.5363 - val_loss: 0.9841 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5225296445514844\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 154s 15ms/step - loss: 1.0737 - acc: 0.4404 - val_loss: 1.0173 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0328 - acc: 0.4850 - val_loss: 1.0014 - val_acc: 0.5265\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0164 - acc: 0.4996 - val_loss: 1.0090 - val_acc: 0.5249\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 0.9990 - acc: 0.5191 - val_loss: 0.9915 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9777 - acc: 0.5314 - val_loss: 0.9893 - val_acc: 0.5382\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5146245063058001\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 154s 15ms/step - loss: 1.0775 - acc: 0.4237 - val_loss: 1.0271 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0421 - acc: 0.4735 - val_loss: 1.0012 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0187 - acc: 0.4957 - val_loss: 1.0042 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9968 - acc: 0.5178 - val_loss: 0.9944 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9790 - acc: 0.5324 - val_loss: 0.9993 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5051383402507766\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0771 - acc: 0.4273 - val_loss: 1.0254 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0375 - acc: 0.4830 - val_loss: 1.0050 - val_acc: 0.5296\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0183 - acc: 0.5065 - val_loss: 1.0021 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9975 - acc: 0.5188 - val_loss: 0.9946 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9721 - acc: 0.5379 - val_loss: 1.0126 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "0.5059288537784998\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 159s 16ms/step - loss: 1.0779 - acc: 0.4306 - val_loss: 1.0238 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0388 - acc: 0.4743 - val_loss: 1.0226 - val_acc: 0.4953\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0193 - acc: 0.5016 - val_loss: 1.0085 - val_acc: 0.5241\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9984 - acc: 0.5187 - val_loss: 0.9919 - val_acc: 0.5265\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9754 - acc: 0.5271 - val_loss: 1.0068 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "0.5169960477606581\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0717 - acc: 0.4354 - val_loss: 1.0235 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 1.0424 - acc: 0.4739 - val_loss: 1.0097 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 1.0114 - acc: 0.5058 - val_loss: 1.0094 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 0.9940 - acc: 0.5231 - val_loss: 0.9956 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 154s 15ms/step - loss: 0.9715 - acc: 0.5389 - val_loss: 0.9930 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5051383402507766\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0714 - acc: 0.4329 - val_loss: 1.0288 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0375 - acc: 0.4790 - val_loss: 1.0199 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0167 - acc: 0.4984 - val_loss: 1.0185 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 0.9985 - acc: 0.5245 - val_loss: 0.9955 - val_acc: 0.5226\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 140s 14ms/step - loss: 0.9791 - acc: 0.5320 - val_loss: 0.9960 - val_acc: 0.5280\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.513833992471808\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0716 - acc: 0.4357 - val_loss: 1.0388 - val_acc: 0.4899\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0345 - acc: 0.4812 - val_loss: 1.0053 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0126 - acc: 0.5059 - val_loss: 1.0064 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 0.9965 - acc: 0.5195 - val_loss: 0.9943 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 0.9783 - acc: 0.5332 - val_loss: 1.0063 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "0.5003952569405552\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 173s 17ms/step - loss: 1.0696 - acc: 0.4360 - val_loss: 1.0258 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0358 - acc: 0.4804 - val_loss: 1.0143 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0112 - acc: 0.5046 - val_loss: 1.0122 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 0.9951 - acc: 0.5194 - val_loss: 1.0022 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 0.9736 - acc: 0.5328 - val_loss: 0.9950 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5106719370887214\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0742 - acc: 0.4359 - val_loss: 1.0247 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0327 - acc: 0.4831 - val_loss: 1.0083 - val_acc: 0.5296\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0171 - acc: 0.5011 - val_loss: 0.9974 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 0.9942 - acc: 0.5208 - val_loss: 0.9937 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 0.9775 - acc: 0.5310 - val_loss: 0.9954 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5019762849148083\n",
      "{5: 0.49565217424287156, 6: 0.5114624506164446, 7: 0.5169960477606581, 8: 0.5122529644504367, 9: 0.49249011860063424, 10: 0.49644268807686365, 11: 0.5075098817998712, 12: 0.5185770751223734, 13: 0.507509881752753, 14: 0.4988142295788399, 15: 0.5043478264639029, 16: 0.5122529647567056, 17: 0.5067193679187609, 18: 0.5090909094207372, 19: 0.513833992471808, 20: 0.5059288540847687, 21: 0.5209486166243497, 22: 0.49565217424287156, 23: 0.5122529647567056, 24: 0.5098814229484603, 25: 0.5272727276025554, 26: 0.5225296445514844, 27: 0.5146245063058001, 28: 0.5051383402507766, 29: 0.5059288537784998, 30: 0.5169960477606581, 31: 0.5051383402507766, 32: 0.513833992471808, 33: 0.5003952569405552, 34: 0.5106719370887214, 35: 0.5019762849148083}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0652 - acc: 0.4305 - val_loss: 1.0246 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0423 - acc: 0.4665 - val_loss: 1.0098 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0296 - acc: 0.4885 - val_loss: 1.0081 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0157 - acc: 0.4983 - val_loss: 1.0033 - val_acc: 0.5249\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0041 - acc: 0.5085 - val_loss: 1.0059 - val_acc: 0.5047\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5043478261105157\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0655 - acc: 0.4277 - val_loss: 1.0398 - val_acc: 0.4502\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0408 - acc: 0.4743 - val_loss: 1.0090 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0253 - acc: 0.4961 - val_loss: 1.0273 - val_acc: 0.4782\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0118 - acc: 0.5019 - val_loss: 0.9998 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 0.9971 - acc: 0.5139 - val_loss: 1.0112 - val_acc: 0.5039\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5114624509698318\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0646 - acc: 0.4406 - val_loss: 1.0206 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0402 - acc: 0.4755 - val_loss: 1.0130 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0265 - acc: 0.4854 - val_loss: 1.0050 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0069 - acc: 0.5093 - val_loss: 1.0009 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 0.9925 - acc: 0.5179 - val_loss: 0.9992 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5304347829385238\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0649 - acc: 0.4339 - val_loss: 1.0195 - val_acc: 0.4899\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0391 - acc: 0.4745 - val_loss: 1.0174 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0234 - acc: 0.4953 - val_loss: 1.0076 - val_acc: 0.5241\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0050 - acc: 0.5123 - val_loss: 1.0105 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 0.9932 - acc: 0.5158 - val_loss: 1.0020 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5067193679658791\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0673 - acc: 0.4343 - val_loss: 1.0137 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0349 - acc: 0.4821 - val_loss: 1.0045 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0192 - acc: 0.4983 - val_loss: 1.0224 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0055 - acc: 0.5086 - val_loss: 1.0017 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 0.9897 - acc: 0.5224 - val_loss: 0.9950 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.499604743412832\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0667 - acc: 0.4310 - val_loss: 1.0251 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0321 - acc: 0.4839 - val_loss: 1.0083 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0170 - acc: 0.5003 - val_loss: 1.0081 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0046 - acc: 0.5121 - val_loss: 1.0029 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 0.9892 - acc: 0.5237 - val_loss: 1.0007 - val_acc: 0.5343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5130434782844288\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0715 - acc: 0.4344 - val_loss: 1.0310 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0360 - acc: 0.4796 - val_loss: 1.0304 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0128 - acc: 0.4983 - val_loss: 1.0062 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0000 - acc: 0.5127 - val_loss: 1.0049 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9813 - acc: 0.5326 - val_loss: 1.0011 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5098814233018476\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0702 - acc: 0.4283 - val_loss: 1.0165 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0367 - acc: 0.4789 - val_loss: 1.0094 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0218 - acc: 0.4922 - val_loss: 1.0019 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0045 - acc: 0.5096 - val_loss: 0.9989 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 0.9896 - acc: 0.5234 - val_loss: 0.9971 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.505138340297895\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0701 - acc: 0.4296 - val_loss: 1.0183 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0371 - acc: 0.4780 - val_loss: 1.0030 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0182 - acc: 0.5046 - val_loss: 1.0007 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0066 - acc: 0.5128 - val_loss: 0.9939 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 0.9836 - acc: 0.5305 - val_loss: 1.0001 - val_acc: 0.5280\n",
      "1265/1265 [==============================] - 6s 4ms/step\n",
      "0.5011857707745473\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0733 - acc: 0.4265 - val_loss: 1.0283 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0395 - acc: 0.4773 - val_loss: 1.0056 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0173 - acc: 0.4943 - val_loss: 0.9981 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0022 - acc: 0.5153 - val_loss: 0.9943 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9804 - acc: 0.5315 - val_loss: 1.0018 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5019762849148083\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 133s 13ms/step - loss: 1.0737 - acc: 0.4277 - val_loss: 1.0278 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0367 - acc: 0.4802 - val_loss: 1.0051 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0172 - acc: 0.4986 - val_loss: 0.9933 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 0.9982 - acc: 0.5118 - val_loss: 1.0088 - val_acc: 0.5257\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9814 - acc: 0.5241 - val_loss: 0.9951 - val_acc: 0.5288\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5177865615946502\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 135s 13ms/step - loss: 1.0695 - acc: 0.4354 - val_loss: 1.0219 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0325 - acc: 0.4828 - val_loss: 1.0372 - val_acc: 0.4977\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0162 - acc: 0.4967 - val_loss: 1.0079 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9966 - acc: 0.5160 - val_loss: 1.0001 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9773 - acc: 0.5316 - val_loss: 1.0239 - val_acc: 0.5023\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5011857707745473\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0716 - acc: 0.4315 - val_loss: 1.0233 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0334 - acc: 0.4850 - val_loss: 1.0071 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0126 - acc: 0.5082 - val_loss: 0.9996 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9997 - acc: 0.5186 - val_loss: 0.9976 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9792 - acc: 0.5313 - val_loss: 0.9885 - val_acc: 0.5280\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5201581030966265\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0752 - acc: 0.4352 - val_loss: 1.0196 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0340 - acc: 0.4803 - val_loss: 1.0085 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0147 - acc: 0.5041 - val_loss: 1.0071 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9974 - acc: 0.5189 - val_loss: 1.0050 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9814 - acc: 0.5279 - val_loss: 1.0114 - val_acc: 0.5031\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.505138340297895\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0873 - acc: 0.4337 - val_loss: 1.0157 - val_acc: 0.5171\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0371 - acc: 0.4802 - val_loss: 1.0361 - val_acc: 0.4883\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0165 - acc: 0.4978 - val_loss: 1.0018 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9942 - acc: 0.5162 - val_loss: 0.9908 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9796 - acc: 0.5263 - val_loss: 0.9895 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5280632414365475\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 146s 14ms/step - loss: 1.0741 - acc: 0.4331 - val_loss: 1.0256 - val_acc: 0.4899\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0379 - acc: 0.4699 - val_loss: 1.0109 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0179 - acc: 0.4987 - val_loss: 1.0012 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9962 - acc: 0.5188 - val_loss: 0.9955 - val_acc: 0.5257\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9808 - acc: 0.5311 - val_loss: 0.9912 - val_acc: 0.5312\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5359683797293501\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 1.0796 - acc: 0.4255 - val_loss: 1.0254 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 1.0388 - acc: 0.4805 - val_loss: 1.0205 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0168 - acc: 0.4996 - val_loss: 1.0099 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9987 - acc: 0.5131 - val_loss: 0.9988 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9797 - acc: 0.5264 - val_loss: 1.0140 - val_acc: 0.4977\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5264822138156815\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 149s 15ms/step - loss: 1.0860 - acc: 0.4310 - val_loss: 1.0129 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0361 - acc: 0.4803 - val_loss: 1.0169 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0158 - acc: 0.5022 - val_loss: 1.0009 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9955 - acc: 0.5166 - val_loss: 0.9906 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9765 - acc: 0.5306 - val_loss: 1.0030 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5185770751223734\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 151s 15ms/step - loss: 1.0786 - acc: 0.4292 - val_loss: 1.0279 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0377 - acc: 0.4746 - val_loss: 1.0025 - val_acc: 0.5234\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0197 - acc: 0.4975 - val_loss: 1.0169 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9988 - acc: 0.5204 - val_loss: 0.9869 - val_acc: 0.5265\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9778 - acc: 0.5364 - val_loss: 0.9914 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5114624509698318\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 150s 15ms/step - loss: 1.0789 - acc: 0.4265 - val_loss: 1.0153 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0385 - acc: 0.4787 - val_loss: 1.0147 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0143 - acc: 0.5007 - val_loss: 0.9982 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9941 - acc: 0.5160 - val_loss: 0.9906 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9762 - acc: 0.5320 - val_loss: 1.0090 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.4988142295788399\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 154s 15ms/step - loss: 1.0834 - acc: 0.4346 - val_loss: 1.0243 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0348 - acc: 0.4847 - val_loss: 0.9952 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0116 - acc: 0.5042 - val_loss: 0.9939 - val_acc: 0.5265\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9939 - acc: 0.5234 - val_loss: 0.9943 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9757 - acc: 0.5339 - val_loss: 0.9882 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5288537553176579\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 154s 15ms/step - loss: 1.0781 - acc: 0.4349 - val_loss: 1.0225 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0335 - acc: 0.4877 - val_loss: 1.0090 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0184 - acc: 0.4988 - val_loss: 1.0423 - val_acc: 0.4813\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9953 - acc: 0.5220 - val_loss: 0.9961 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9745 - acc: 0.5301 - val_loss: 0.9968 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5225296445986027\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0732 - acc: 0.4366 - val_loss: 1.0230 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0413 - acc: 0.4677 - val_loss: 1.0191 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0174 - acc: 0.5036 - val_loss: 0.9985 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9994 - acc: 0.5177 - val_loss: 1.0105 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9797 - acc: 0.5313 - val_loss: 0.9927 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "0.5201581030966265\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 159s 16ms/step - loss: 1.0736 - acc: 0.4310 - val_loss: 1.0223 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0414 - acc: 0.4830 - val_loss: 1.0027 - val_acc: 0.5234\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0179 - acc: 0.5027 - val_loss: 0.9963 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9941 - acc: 0.5195 - val_loss: 1.0047 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9806 - acc: 0.5297 - val_loss: 0.9914 - val_acc: 0.5265\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5154150200455556\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 157s 15ms/step - loss: 1.0844 - acc: 0.4213 - val_loss: 1.0286 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0476 - acc: 0.4691 - val_loss: 1.0124 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0171 - acc: 0.4999 - val_loss: 1.0115 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9954 - acc: 0.5165 - val_loss: 0.9967 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9792 - acc: 0.5280 - val_loss: 1.0188 - val_acc: 0.4945\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5027667987488004\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0748 - acc: 0.4325 - val_loss: 1.0279 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0427 - acc: 0.4744 - val_loss: 1.0019 - val_acc: 0.5226\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0183 - acc: 0.4999 - val_loss: 0.9959 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 0.9961 - acc: 0.5178 - val_loss: 1.0036 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9823 - acc: 0.5275 - val_loss: 0.9874 - val_acc: 0.5187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5217391307646106\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0778 - acc: 0.4259 - val_loss: 1.0365 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 1.0353 - acc: 0.4819 - val_loss: 1.0078 - val_acc: 0.5335\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 1.0154 - acc: 0.5017 - val_loss: 0.9910 - val_acc: 0.5249\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 133s 13ms/step - loss: 0.9936 - acc: 0.5215 - val_loss: 0.9856 - val_acc: 0.5304\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 0.9709 - acc: 0.5379 - val_loss: 0.9889 - val_acc: 0.5265\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5083003955867451\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0694 - acc: 0.4379 - val_loss: 1.0315 - val_acc: 0.4961\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 138s 14ms/step - loss: 1.0329 - acc: 0.4873 - val_loss: 1.0019 - val_acc: 0.5202\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0103 - acc: 0.5083 - val_loss: 0.9971 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 138s 14ms/step - loss: 0.9943 - acc: 0.5207 - val_loss: 0.9905 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 0.9766 - acc: 0.5339 - val_loss: 0.9982 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.505138339991626\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0729 - acc: 0.4310 - val_loss: 1.0224 - val_acc: 0.4961\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0308 - acc: 0.4896 - val_loss: 1.0073 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0116 - acc: 0.5071 - val_loss: 1.0161 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 0.9950 - acc: 0.5156 - val_loss: 0.9963 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 0.9723 - acc: 0.5340 - val_loss: 0.9942 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.513043478637816\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 173s 17ms/step - loss: 1.0706 - acc: 0.4281 - val_loss: 1.0262 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 1.0368 - acc: 0.4755 - val_loss: 1.0069 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 1.0128 - acc: 0.5053 - val_loss: 1.0082 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 0.9888 - acc: 0.5264 - val_loss: 0.9944 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 0.9721 - acc: 0.5344 - val_loss: 0.9884 - val_acc: 0.5312\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5201581031437448\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 175s 17ms/step - loss: 1.0684 - acc: 0.4382 - val_loss: 1.0237 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0360 - acc: 0.4774 - val_loss: 1.0100 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 1.0136 - acc: 0.5034 - val_loss: 1.0048 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 0.9915 - acc: 0.5244 - val_loss: 0.9953 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 145s 14ms/step - loss: 0.9696 - acc: 0.5326 - val_loss: 1.0130 - val_acc: 0.5016\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5051383402507766\n",
      "{5: 0.5043478261105157, 6: 0.5114624509698318, 7: 0.5304347829385238, 8: 0.5067193679658791, 9: 0.499604743412832, 10: 0.5130434782844288, 11: 0.5098814233018476, 12: 0.505138340297895, 13: 0.5011857707745473, 14: 0.5019762849148083, 15: 0.5177865615946502, 16: 0.5011857707745473, 17: 0.5201581030966265, 18: 0.505138340297895, 19: 0.5280632414365475, 20: 0.5359683797293501, 21: 0.5264822138156815, 22: 0.5185770751223734, 23: 0.5114624509698318, 24: 0.4988142295788399, 25: 0.5288537553176579, 26: 0.5225296445986027, 27: 0.5201581030966265, 28: 0.5154150200455556, 29: 0.5027667987488004, 30: 0.5217391307646106, 31: 0.5083003955867451, 32: 0.505138339991626, 33: 0.513043478637816, 34: 0.5201581031437448, 35: 0.5051383402507766}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0678 - acc: 0.4340 - val_loss: 1.0200 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 11ms/step - loss: 1.0400 - acc: 0.4672 - val_loss: 1.0166 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0276 - acc: 0.4880 - val_loss: 1.0361 - val_acc: 0.4650\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0154 - acc: 0.4983 - val_loss: 1.0041 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0015 - acc: 0.5106 - val_loss: 0.9976 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5067193679658791\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0645 - acc: 0.4312 - val_loss: 1.0239 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0413 - acc: 0.4690 - val_loss: 1.0115 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0270 - acc: 0.4848 - val_loss: 1.0058 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0153 - acc: 0.4997 - val_loss: 1.0228 - val_acc: 0.4821\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 0.9993 - acc: 0.5105 - val_loss: 1.0004 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5019762849148083\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0702 - acc: 0.4270 - val_loss: 1.0205 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0438 - acc: 0.4717 - val_loss: 1.0067 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0252 - acc: 0.4913 - val_loss: 1.0096 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0099 - acc: 0.5081 - val_loss: 1.0301 - val_acc: 0.4868\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 0.9996 - acc: 0.5096 - val_loss: 0.9979 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5090909094678555\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0664 - acc: 0.4345 - val_loss: 1.0246 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0389 - acc: 0.4739 - val_loss: 1.0096 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0207 - acc: 0.4931 - val_loss: 1.0207 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0071 - acc: 0.5008 - val_loss: 1.0028 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 0.9924 - acc: 0.5169 - val_loss: 1.0164 - val_acc: 0.5023\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.507509881446484\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0698 - acc: 0.4312 - val_loss: 1.0189 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0402 - acc: 0.4736 - val_loss: 1.0097 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0240 - acc: 0.4912 - val_loss: 1.0100 - val_acc: 0.5241\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0037 - acc: 0.5102 - val_loss: 1.0027 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 0.9899 - acc: 0.5189 - val_loss: 0.9985 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5177865616417685\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0680 - acc: 0.4350 - val_loss: 1.0286 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0381 - acc: 0.4765 - val_loss: 1.0203 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0237 - acc: 0.4938 - val_loss: 1.0061 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0078 - acc: 0.5045 - val_loss: 1.0086 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 0.9905 - acc: 0.5159 - val_loss: 1.0046 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5003952569405552\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0629 - acc: 0.4398 - val_loss: 1.0153 - val_acc: 0.5171\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0349 - acc: 0.4867 - val_loss: 1.0084 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0169 - acc: 0.5020 - val_loss: 1.0043 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0032 - acc: 0.5153 - val_loss: 0.9984 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9838 - acc: 0.5199 - val_loss: 1.0043 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.507509881752753\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0694 - acc: 0.4308 - val_loss: 1.0191 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0397 - acc: 0.4705 - val_loss: 1.0165 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0196 - acc: 0.4964 - val_loss: 0.9976 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0028 - acc: 0.5110 - val_loss: 0.9911 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9910 - acc: 0.5191 - val_loss: 0.9909 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5011857710336979\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 135s 13ms/step - loss: 1.0683 - acc: 0.4361 - val_loss: 1.0294 - val_acc: 0.4945\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0376 - acc: 0.4784 - val_loss: 1.0094 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 118s 11ms/step - loss: 1.0183 - acc: 0.5010 - val_loss: 1.0075 - val_acc: 0.5226\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0048 - acc: 0.5080 - val_loss: 1.0084 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 0.9854 - acc: 0.5283 - val_loss: 0.9923 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5090909093736189\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0735 - acc: 0.4329 - val_loss: 1.0142 - val_acc: 0.5195\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0364 - acc: 0.4797 - val_loss: 0.9998 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0191 - acc: 0.4962 - val_loss: 1.0049 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 0.9980 - acc: 0.5165 - val_loss: 1.0052 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 0.9810 - acc: 0.5296 - val_loss: 1.0012 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5027667987959187\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0757 - acc: 0.4325 - val_loss: 1.0187 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0371 - acc: 0.4793 - val_loss: 1.0076 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0183 - acc: 0.5030 - val_loss: 0.9952 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0022 - acc: 0.5171 - val_loss: 1.0179 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9824 - acc: 0.5267 - val_loss: 0.9928 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.507509881752753\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 1.0692 - acc: 0.4402 - val_loss: 1.0194 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0351 - acc: 0.4836 - val_loss: 1.0239 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0146 - acc: 0.4995 - val_loss: 1.0107 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0010 - acc: 0.5177 - val_loss: 0.9926 - val_acc: 0.5249\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9797 - acc: 0.5271 - val_loss: 0.9865 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.5185770754286423\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0750 - acc: 0.4319 - val_loss: 1.0284 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0326 - acc: 0.4886 - val_loss: 1.0112 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0138 - acc: 0.4992 - val_loss: 1.0154 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9952 - acc: 0.5159 - val_loss: 0.9973 - val_acc: 0.5257\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9768 - acc: 0.5329 - val_loss: 0.9999 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5122529648038239\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0722 - acc: 0.4338 - val_loss: 1.0504 - val_acc: 0.4595\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0431 - acc: 0.4727 - val_loss: 1.0016 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0129 - acc: 0.5036 - val_loss: 1.0032 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 0.9974 - acc: 0.5120 - val_loss: 0.9982 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9790 - acc: 0.5305 - val_loss: 0.9972 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5169960477606581\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 145s 14ms/step - loss: 1.0779 - acc: 0.4319 - val_loss: 1.0233 - val_acc: 0.5109\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0344 - acc: 0.4863 - val_loss: 1.0031 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0134 - acc: 0.5027 - val_loss: 1.0040 - val_acc: 0.5257\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0003 - acc: 0.5110 - val_loss: 0.9931 - val_acc: 0.5241\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9789 - acc: 0.5323 - val_loss: 0.9885 - val_acc: 0.5366\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.516205533926666\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 150s 15ms/step - loss: 1.0774 - acc: 0.4278 - val_loss: 1.0172 - val_acc: 0.4961\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0355 - acc: 0.4829 - val_loss: 1.0008 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0155 - acc: 0.5041 - val_loss: 0.9969 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9964 - acc: 0.5170 - val_loss: 1.0042 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9801 - acc: 0.5288 - val_loss: 0.9998 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5035573125827925\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 151s 15ms/step - loss: 1.0777 - acc: 0.4285 - val_loss: 1.0236 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0401 - acc: 0.4780 - val_loss: 1.0283 - val_acc: 0.4782\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0202 - acc: 0.5017 - val_loss: 1.0109 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9973 - acc: 0.5206 - val_loss: 0.9946 - val_acc: 0.5288\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9781 - acc: 0.5264 - val_loss: 0.9935 - val_acc: 0.5312\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5051383402507766\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 148s 14ms/step - loss: 1.0733 - acc: 0.4295 - val_loss: 1.0170 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0373 - acc: 0.4768 - val_loss: 0.9971 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0165 - acc: 0.4970 - val_loss: 0.9970 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9925 - acc: 0.5206 - val_loss: 0.9958 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9821 - acc: 0.5279 - val_loss: 1.0080 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5201581031437448\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 153s 15ms/step - loss: 1.0757 - acc: 0.4307 - val_loss: 1.0227 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0410 - acc: 0.4799 - val_loss: 1.0059 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0186 - acc: 0.4998 - val_loss: 0.9968 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9988 - acc: 0.5145 - val_loss: 0.9925 - val_acc: 0.5288\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9771 - acc: 0.5240 - val_loss: 0.9939 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5154150200926739\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 152s 15ms/step - loss: 1.0797 - acc: 0.4391 - val_loss: 1.0159 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0368 - acc: 0.4810 - val_loss: 1.0009 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0170 - acc: 0.5010 - val_loss: 0.9998 - val_acc: 0.5234\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 0.9950 - acc: 0.5177 - val_loss: 1.0024 - val_acc: 0.5257\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9746 - acc: 0.5359 - val_loss: 0.9905 - val_acc: 0.5312\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5122529647567056\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 159s 15ms/step - loss: 1.0730 - acc: 0.4275 - val_loss: 1.0250 - val_acc: 0.5195\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0358 - acc: 0.4816 - val_loss: 1.0235 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0193 - acc: 0.4980 - val_loss: 1.0056 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0000 - acc: 0.5132 - val_loss: 1.0043 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9812 - acc: 0.5290 - val_loss: 1.0128 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.4940711462686184\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 157s 15ms/step - loss: 1.0728 - acc: 0.4360 - val_loss: 1.0226 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0392 - acc: 0.4735 - val_loss: 0.9988 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0196 - acc: 0.4923 - val_loss: 1.0011 - val_acc: 0.5226\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9971 - acc: 0.5162 - val_loss: 0.9940 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9768 - acc: 0.5311 - val_loss: 0.9865 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.516205533926666\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0755 - acc: 0.4316 - val_loss: 1.0305 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0408 - acc: 0.4754 - val_loss: 1.0063 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0164 - acc: 0.4992 - val_loss: 1.0000 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9978 - acc: 0.5214 - val_loss: 1.0198 - val_acc: 0.5008\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9754 - acc: 0.5360 - val_loss: 0.9963 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5193675889563655\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0743 - acc: 0.4270 - val_loss: 1.0404 - val_acc: 0.4650\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0407 - acc: 0.4774 - val_loss: 1.0228 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0173 - acc: 0.5006 - val_loss: 1.0058 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9949 - acc: 0.5212 - val_loss: 1.0146 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9763 - acc: 0.5323 - val_loss: 1.0418 - val_acc: 0.4759\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.4830039526398474\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0771 - acc: 0.4252 - val_loss: 1.0210 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0373 - acc: 0.4782 - val_loss: 1.0115 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0168 - acc: 0.5003 - val_loss: 0.9971 - val_acc: 0.5312\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0000 - acc: 0.5188 - val_loss: 0.9941 - val_acc: 0.5241\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9764 - acc: 0.5331 - val_loss: 1.0009 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5114624509227135\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0850 - acc: 0.4295 - val_loss: 1.0199 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0409 - acc: 0.4747 - val_loss: 1.0107 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0112 - acc: 0.5056 - val_loss: 0.9975 - val_acc: 0.5234\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9952 - acc: 0.5201 - val_loss: 1.0069 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 0.9766 - acc: 0.5332 - val_loss: 0.9895 - val_acc: 0.5265\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5035573125827925\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0743 - acc: 0.4315 - val_loss: 1.0204 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 1.0358 - acc: 0.4819 - val_loss: 1.0081 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0122 - acc: 0.5012 - val_loss: 0.9908 - val_acc: 0.5288\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 138s 13ms/step - loss: 0.9919 - acc: 0.5184 - val_loss: 0.9894 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 137s 13ms/step - loss: 0.9699 - acc: 0.5349 - val_loss: 1.0170 - val_acc: 0.5055\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5098814229484603\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0714 - acc: 0.4284 - val_loss: 1.0387 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 137s 13ms/step - loss: 1.0360 - acc: 0.4759 - val_loss: 1.0142 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 138s 14ms/step - loss: 1.0198 - acc: 0.4980 - val_loss: 1.0093 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 138s 13ms/step - loss: 0.9957 - acc: 0.5163 - val_loss: 1.0058 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 138s 14ms/step - loss: 0.9796 - acc: 0.5308 - val_loss: 0.9960 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5043478264167846\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0702 - acc: 0.4350 - val_loss: 1.0337 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0358 - acc: 0.4762 - val_loss: 1.0168 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0141 - acc: 0.5052 - val_loss: 0.9962 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 0.9960 - acc: 0.5134 - val_loss: 0.9971 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 0.9768 - acc: 0.5321 - val_loss: 0.9926 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5090909094207372\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0727 - acc: 0.4287 - val_loss: 1.0224 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0352 - acc: 0.4806 - val_loss: 1.0105 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0135 - acc: 0.5011 - val_loss: 0.9959 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 0.9937 - acc: 0.5201 - val_loss: 0.9888 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 0.9765 - acc: 0.5314 - val_loss: 0.9876 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5090909094207372\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0682 - acc: 0.4339 - val_loss: 1.0258 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0358 - acc: 0.4786 - val_loss: 1.0010 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 150s 15ms/step - loss: 1.0155 - acc: 0.5012 - val_loss: 0.9934 - val_acc: 0.5241\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 148s 14ms/step - loss: 0.9942 - acc: 0.5211 - val_loss: 0.9946 - val_acc: 0.5304\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 0.9767 - acc: 0.5286 - val_loss: 0.9898 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5090909094678555\n",
      "{5: 0.5067193679658791, 6: 0.5019762849148083, 7: 0.5090909094678555, 8: 0.507509881446484, 9: 0.5177865616417685, 10: 0.5003952569405552, 11: 0.507509881752753, 12: 0.5011857710336979, 13: 0.5090909093736189, 14: 0.5027667987959187, 15: 0.507509881752753, 16: 0.5185770754286423, 17: 0.5122529648038239, 18: 0.5169960477606581, 19: 0.516205533926666, 20: 0.5035573125827925, 21: 0.5051383402507766, 22: 0.5201581031437448, 23: 0.5154150200926739, 24: 0.5122529647567056, 25: 0.4940711462686184, 26: 0.516205533926666, 27: 0.5193675889563655, 28: 0.4830039526398474, 29: 0.5114624509227135, 30: 0.5035573125827925, 31: 0.5098814229484603, 32: 0.5043478264167846, 33: 0.5090909094207372, 34: 0.5090909094207372, 35: 0.5090909094678555}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0654 - acc: 0.4361 - val_loss: 1.0202 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 113s 11ms/step - loss: 1.0423 - acc: 0.4671 - val_loss: 1.0103 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 112s 11ms/step - loss: 1.0280 - acc: 0.4844 - val_loss: 1.0103 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0155 - acc: 0.4998 - val_loss: 1.0477 - val_acc: 0.4603\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 114s 11ms/step - loss: 1.0039 - acc: 0.5078 - val_loss: 0.9986 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5098814232547293\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0667 - acc: 0.4394 - val_loss: 1.0172 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0417 - acc: 0.4696 - val_loss: 1.0068 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0262 - acc: 0.4911 - val_loss: 1.0109 - val_acc: 0.5265\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0144 - acc: 0.5052 - val_loss: 1.0102 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0009 - acc: 0.5098 - val_loss: 0.9970 - val_acc: 0.5312\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5114624509698318\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0671 - acc: 0.4354 - val_loss: 1.0286 - val_acc: 0.4992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0411 - acc: 0.4689 - val_loss: 1.0262 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0268 - acc: 0.4912 - val_loss: 1.0182 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0152 - acc: 0.5000 - val_loss: 1.0038 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 0.9988 - acc: 0.5113 - val_loss: 0.9992 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5098814232547293\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0691 - acc: 0.4301 - val_loss: 1.0168 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0389 - acc: 0.4739 - val_loss: 1.0149 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 1.0221 - acc: 0.4914 - val_loss: 0.9999 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0050 - acc: 0.5096 - val_loss: 1.0007 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 0.9958 - acc: 0.5166 - val_loss: 1.0067 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5114624506635628\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0735 - acc: 0.4324 - val_loss: 1.0268 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0352 - acc: 0.4798 - val_loss: 1.0117 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0218 - acc: 0.4997 - val_loss: 1.0045 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0045 - acc: 0.5109 - val_loss: 0.9985 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 0.9921 - acc: 0.5199 - val_loss: 1.0044 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.49802371543857893\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0633 - acc: 0.4378 - val_loss: 1.0206 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0362 - acc: 0.4761 - val_loss: 1.0206 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0211 - acc: 0.4997 - val_loss: 1.0126 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0033 - acc: 0.5098 - val_loss: 1.0134 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9860 - acc: 0.5232 - val_loss: 1.0191 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.4980237155328155\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0673 - acc: 0.4292 - val_loss: 1.0222 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0317 - acc: 0.4863 - val_loss: 1.0076 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0181 - acc: 0.4976 - val_loss: 1.0106 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0006 - acc: 0.5127 - val_loss: 1.0219 - val_acc: 0.4992\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 0.9858 - acc: 0.5210 - val_loss: 1.0005 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.505928854131887\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0711 - acc: 0.4381 - val_loss: 1.0262 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0364 - acc: 0.4777 - val_loss: 1.0192 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0182 - acc: 0.5019 - val_loss: 1.0053 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0022 - acc: 0.5116 - val_loss: 1.0059 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 0.9819 - acc: 0.5227 - val_loss: 0.9992 - val_acc: 0.5280\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5122529647567056\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0685 - acc: 0.4365 - val_loss: 1.0262 - val_acc: 0.5086\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0377 - acc: 0.4754 - val_loss: 1.0121 - val_acc: 0.5195\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0158 - acc: 0.5023 - val_loss: 1.0010 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0004 - acc: 0.5124 - val_loss: 1.0022 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9841 - acc: 0.5285 - val_loss: 1.0056 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5146245062586818\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 137s 13ms/step - loss: 1.0708 - acc: 0.4400 - val_loss: 1.0132 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0362 - acc: 0.4799 - val_loss: 1.0092 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 117s 11ms/step - loss: 1.0136 - acc: 0.5036 - val_loss: 1.0169 - val_acc: 0.4984\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0009 - acc: 0.5139 - val_loss: 1.0125 - val_acc: 0.4984\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 0.9856 - acc: 0.5258 - val_loss: 0.9929 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.505138340297895\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 185s 18ms/step - loss: 1.0695 - acc: 0.4292 - val_loss: 1.0262 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0381 - acc: 0.4790 - val_loss: 1.0247 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0152 - acc: 0.5036 - val_loss: 0.9971 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0001 - acc: 0.5145 - val_loss: 0.9946 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9818 - acc: 0.5285 - val_loss: 0.9907 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5146245062586818\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 145s 14ms/step - loss: 1.0754 - acc: 0.4335 - val_loss: 1.0156 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0379 - acc: 0.4760 - val_loss: 1.0047 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0198 - acc: 0.5026 - val_loss: 1.0004 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0020 - acc: 0.5150 - val_loss: 1.0002 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9824 - acc: 0.5271 - val_loss: 0.9913 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5154150200926739\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 145s 14ms/step - loss: 1.0729 - acc: 0.4377 - val_loss: 1.0090 - val_acc: 0.5109\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0334 - acc: 0.4878 - val_loss: 0.9961 - val_acc: 0.5218\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0139 - acc: 0.5066 - val_loss: 0.9927 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9940 - acc: 0.5202 - val_loss: 0.9938 - val_acc: 0.5249\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9771 - acc: 0.5288 - val_loss: 1.0009 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.5193675893097527\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 1.0753 - acc: 0.4353 - val_loss: 1.0169 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0341 - acc: 0.4889 - val_loss: 1.0019 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0087 - acc: 0.5127 - val_loss: 0.9983 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9956 - acc: 0.5200 - val_loss: 0.9920 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9733 - acc: 0.5325 - val_loss: 0.9964 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5130434785906977\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 148s 15ms/step - loss: 1.0721 - acc: 0.4382 - val_loss: 1.0232 - val_acc: 0.5140\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0384 - acc: 0.4754 - val_loss: 1.0015 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0203 - acc: 0.4994 - val_loss: 0.9980 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0008 - acc: 0.5192 - val_loss: 1.0012 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 151s 15ms/step - loss: 0.9817 - acc: 0.5226 - val_loss: 0.9903 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5169960477135398\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 148s 14ms/step - loss: 1.0737 - acc: 0.4362 - val_loss: 1.0199 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0309 - acc: 0.4882 - val_loss: 1.0067 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 1.0102 - acc: 0.5082 - val_loss: 0.9994 - val_acc: 0.5218\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 0.9950 - acc: 0.5206 - val_loss: 0.9986 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9747 - acc: 0.5329 - val_loss: 0.9905 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5146245062115635\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 152s 15ms/step - loss: 1.0744 - acc: 0.4311 - val_loss: 1.0232 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 133s 13ms/step - loss: 1.0369 - acc: 0.4763 - val_loss: 1.0051 - val_acc: 0.5179\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 137s 13ms/step - loss: 1.0120 - acc: 0.5070 - val_loss: 1.0055 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 0.9934 - acc: 0.5191 - val_loss: 0.9910 - val_acc: 0.5241\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9706 - acc: 0.5340 - val_loss: 1.0046 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.49565217424287156\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 155s 15ms/step - loss: 1.0860 - acc: 0.4297 - val_loss: 1.0179 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0338 - acc: 0.4869 - val_loss: 1.0233 - val_acc: 0.4875\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0144 - acc: 0.5043 - val_loss: 1.0033 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9907 - acc: 0.5230 - val_loss: 1.0114 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9698 - acc: 0.5324 - val_loss: 0.9827 - val_acc: 0.5327\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5375494074444526\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 155s 15ms/step - loss: 1.0774 - acc: 0.4332 - val_loss: 1.0220 - val_acc: 0.5179\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0389 - acc: 0.4758 - val_loss: 1.0111 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0140 - acc: 0.5040 - val_loss: 1.0063 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9999 - acc: 0.5138 - val_loss: 0.9967 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9806 - acc: 0.5273 - val_loss: 0.9934 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5185770754757606\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 155s 15ms/step - loss: 1.0740 - acc: 0.4270 - val_loss: 1.0286 - val_acc: 0.5016\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0395 - acc: 0.4783 - val_loss: 1.0143 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0177 - acc: 0.5018 - val_loss: 0.9966 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9998 - acc: 0.5168 - val_loss: 0.9940 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9799 - acc: 0.5308 - val_loss: 0.9960 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5106719370887214\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 159s 16ms/step - loss: 1.0712 - acc: 0.4321 - val_loss: 1.0369 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0371 - acc: 0.4781 - val_loss: 1.0052 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0152 - acc: 0.5045 - val_loss: 1.0020 - val_acc: 0.5288\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9947 - acc: 0.5240 - val_loss: 1.0102 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9813 - acc: 0.5320 - val_loss: 1.0026 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5114624508755952\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0732 - acc: 0.4398 - val_loss: 1.0213 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0377 - acc: 0.4834 - val_loss: 1.0155 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0158 - acc: 0.5044 - val_loss: 1.0256 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9946 - acc: 0.5191 - val_loss: 1.0067 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9777 - acc: 0.5322 - val_loss: 0.9975 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5154150200926739\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 165s 16ms/step - loss: 1.0796 - acc: 0.4235 - val_loss: 1.0189 - val_acc: 0.4938\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0389 - acc: 0.4823 - val_loss: 1.0070 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0159 - acc: 0.5061 - val_loss: 0.9986 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9977 - acc: 0.5173 - val_loss: 0.9886 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9811 - acc: 0.5289 - val_loss: 0.9926 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "0.5083003956338633\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0747 - acc: 0.4308 - val_loss: 1.0229 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0381 - acc: 0.4708 - val_loss: 1.0035 - val_acc: 0.5234\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0169 - acc: 0.5021 - val_loss: 0.9934 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9955 - acc: 0.5210 - val_loss: 0.9979 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9741 - acc: 0.5358 - val_loss: 0.9919 - val_acc: 0.5273\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5241106722665869\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 196s 19ms/step - loss: 1.0753 - acc: 0.4281 - val_loss: 1.0229 - val_acc: 0.5171\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0385 - acc: 0.4783 - val_loss: 1.0087 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0183 - acc: 0.5027 - val_loss: 1.0022 - val_acc: 0.5280\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9966 - acc: 0.5160 - val_loss: 0.9927 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 0.9744 - acc: 0.5343 - val_loss: 0.9887 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "0.5090909093736189\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0759 - acc: 0.4331 - val_loss: 1.0534 - val_acc: 0.4237\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0387 - acc: 0.4722 - val_loss: 1.0075 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0165 - acc: 0.5031 - val_loss: 1.0005 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9988 - acc: 0.5173 - val_loss: 1.0099 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9764 - acc: 0.5351 - val_loss: 0.9929 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5169960477606581\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0759 - acc: 0.4273 - val_loss: 1.0337 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 1.0404 - acc: 0.4793 - val_loss: 1.0032 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 133s 13ms/step - loss: 1.0165 - acc: 0.5051 - val_loss: 0.9970 - val_acc: 0.5226\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 0.9900 - acc: 0.5268 - val_loss: 0.9905 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 0.9788 - acc: 0.5278 - val_loss: 0.9899 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5177865615475319\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 173s 17ms/step - loss: 1.0691 - acc: 0.4359 - val_loss: 1.0194 - val_acc: 0.4930\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 137s 13ms/step - loss: 1.0333 - acc: 0.4801 - val_loss: 1.0175 - val_acc: 0.4977\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 1.0143 - acc: 0.5032 - val_loss: 0.9986 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 0.9945 - acc: 0.5230 - val_loss: 0.9901 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 0.9766 - acc: 0.5314 - val_loss: 0.9912 - val_acc: 0.5265\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5264822135094126\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0709 - acc: 0.4284 - val_loss: 1.0261 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0390 - acc: 0.4775 - val_loss: 1.0049 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0145 - acc: 0.5007 - val_loss: 1.0131 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 0.9983 - acc: 0.5193 - val_loss: 1.0029 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 201s 20ms/step - loss: 0.9748 - acc: 0.5334 - val_loss: 0.9979 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5217391307646106\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 178s 17ms/step - loss: 1.0681 - acc: 0.4370 - val_loss: 1.0290 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0389 - acc: 0.4702 - val_loss: 1.0081 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0194 - acc: 0.4938 - val_loss: 1.0134 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 0.9941 - acc: 0.5182 - val_loss: 1.0030 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 0.9775 - acc: 0.5304 - val_loss: 0.9932 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5217391307646106\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 179s 18ms/step - loss: 1.0729 - acc: 0.4276 - val_loss: 1.0328 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 1.0341 - acc: 0.4858 - val_loss: 1.0009 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 142s 14ms/step - loss: 1.0145 - acc: 0.5063 - val_loss: 1.0051 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 0.9939 - acc: 0.5235 - val_loss: 0.9908 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 143s 14ms/step - loss: 0.9712 - acc: 0.5411 - val_loss: 0.9874 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5217391307174923\n",
      "{5: 0.5098814232547293, 6: 0.5114624509698318, 7: 0.5098814232547293, 8: 0.5114624506635628, 9: 0.49802371543857893, 10: 0.4980237155328155, 11: 0.505928854131887, 12: 0.5122529647567056, 13: 0.5146245062586818, 14: 0.505138340297895, 15: 0.5146245062586818, 16: 0.5154150200926739, 17: 0.5193675893097527, 18: 0.5130434785906977, 19: 0.5169960477135398, 20: 0.5146245062115635, 21: 0.49565217424287156, 22: 0.5375494074444526, 23: 0.5185770754757606, 24: 0.5106719370887214, 25: 0.5114624508755952, 26: 0.5154150200926739, 27: 0.5083003956338633, 28: 0.5241106722665869, 29: 0.5090909093736189, 30: 0.5169960477606581, 31: 0.5177865615475319, 32: 0.5264822135094126, 33: 0.5217391307646106, 34: 0.5217391307646106, 35: 0.5217391307174923}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0647 - acc: 0.4319 - val_loss: 1.0267 - val_acc: 0.5171\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0407 - acc: 0.4661 - val_loss: 1.0173 - val_acc: 0.5156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0300 - acc: 0.4841 - val_loss: 1.0144 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0172 - acc: 0.5008 - val_loss: 1.0105 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0076 - acc: 0.5005 - val_loss: 1.0121 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.507509881446484\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0650 - acc: 0.4336 - val_loss: 1.0224 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0398 - acc: 0.4695 - val_loss: 1.0067 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 118s 12ms/step - loss: 1.0227 - acc: 0.4938 - val_loss: 1.0032 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 118s 11ms/step - loss: 1.0130 - acc: 0.5009 - val_loss: 0.9992 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 0.9999 - acc: 0.5171 - val_loss: 1.0042 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5083003955867451\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0679 - acc: 0.4332 - val_loss: 1.0201 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 121s 12ms/step - loss: 1.0369 - acc: 0.4726 - val_loss: 1.0280 - val_acc: 0.4977\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 1.0261 - acc: 0.4917 - val_loss: 1.0075 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 115s 11ms/step - loss: 1.0098 - acc: 0.5085 - val_loss: 1.0044 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 116s 11ms/step - loss: 0.9985 - acc: 0.5162 - val_loss: 1.0035 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5233201581263259\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0669 - acc: 0.4302 - val_loss: 1.0134 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0355 - acc: 0.4781 - val_loss: 1.0036 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 119s 12ms/step - loss: 1.0172 - acc: 0.5007 - val_loss: 1.0271 - val_acc: 0.4914\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 1.0051 - acc: 0.5053 - val_loss: 0.9947 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 120s 12ms/step - loss: 0.9904 - acc: 0.5240 - val_loss: 0.9936 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.513043478637816\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0689 - acc: 0.4317 - val_loss: 1.0207 - val_acc: 0.5132\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0393 - acc: 0.4703 - val_loss: 1.0160 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0216 - acc: 0.4954 - val_loss: 1.0062 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0087 - acc: 0.5049 - val_loss: 1.0168 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9947 - acc: 0.5158 - val_loss: 1.0090 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5019762846085394\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0669 - acc: 0.4410 - val_loss: 1.0191 - val_acc: 0.5062\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0347 - acc: 0.4798 - val_loss: 1.0300 - val_acc: 0.4868\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0170 - acc: 0.5028 - val_loss: 1.0003 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0043 - acc: 0.5130 - val_loss: 1.0072 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9900 - acc: 0.5177 - val_loss: 1.0061 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 6s 4ms/step\n",
      "0.5122529644504367\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0649 - acc: 0.4408 - val_loss: 1.0190 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0349 - acc: 0.4782 - val_loss: 1.0070 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0176 - acc: 0.4992 - val_loss: 1.0090 - val_acc: 0.5265\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0036 - acc: 0.5139 - val_loss: 0.9998 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 0.9855 - acc: 0.5251 - val_loss: 0.9921 - val_acc: 0.5304\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5011857711279345\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 136s 13ms/step - loss: 1.0699 - acc: 0.4266 - val_loss: 1.0289 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 122s 12ms/step - loss: 1.0363 - acc: 0.4801 - val_loss: 1.0094 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0194 - acc: 0.5048 - val_loss: 1.0183 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0070 - acc: 0.5085 - val_loss: 1.0018 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9904 - acc: 0.5223 - val_loss: 1.0002 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5027667987488004\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0711 - acc: 0.4387 - val_loss: 1.0263 - val_acc: 0.5202\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 1.0361 - acc: 0.4779 - val_loss: 1.0049 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 124s 12ms/step - loss: 1.0159 - acc: 0.5003 - val_loss: 1.0102 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9984 - acc: 0.5207 - val_loss: 1.0042 - val_acc: 0.5241\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 123s 12ms/step - loss: 0.9814 - acc: 0.5273 - val_loss: 1.0036 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 5s 4ms/step\n",
      "0.5114624509698318\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0691 - acc: 0.4325 - val_loss: 1.0169 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 1.0354 - acc: 0.4812 - val_loss: 1.0204 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 1.0191 - acc: 0.5005 - val_loss: 0.9959 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 133s 13ms/step - loss: 1.0000 - acc: 0.5156 - val_loss: 1.0105 - val_acc: 0.5241\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 134s 13ms/step - loss: 0.9823 - acc: 0.5255 - val_loss: 0.9999 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.5011857711279345\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 150s 15ms/step - loss: 1.0708 - acc: 0.4258 - val_loss: 1.0245 - val_acc: 0.5148\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0353 - acc: 0.4818 - val_loss: 1.0220 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0166 - acc: 0.5057 - val_loss: 1.0084 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9986 - acc: 0.5137 - val_loss: 0.9945 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9814 - acc: 0.5318 - val_loss: 1.0236 - val_acc: 0.4883\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.49644268777059475\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0706 - acc: 0.4369 - val_loss: 1.0145 - val_acc: 0.5202\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0351 - acc: 0.4783 - val_loss: 1.0246 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0141 - acc: 0.5061 - val_loss: 0.9990 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9984 - acc: 0.5194 - val_loss: 0.9926 - val_acc: 0.5327\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9793 - acc: 0.5315 - val_loss: 0.9889 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.5122529647567056\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 147s 14ms/step - loss: 1.0658 - acc: 0.4402 - val_loss: 1.0286 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0335 - acc: 0.4843 - val_loss: 1.0174 - val_acc: 0.5234\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0106 - acc: 0.5089 - val_loss: 1.0075 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9950 - acc: 0.5190 - val_loss: 0.9943 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9816 - acc: 0.5269 - val_loss: 1.0053 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.505138340297895\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 151s 15ms/step - loss: 1.0711 - acc: 0.4313 - val_loss: 1.0171 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0379 - acc: 0.4776 - val_loss: 1.0078 - val_acc: 0.5210\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0160 - acc: 0.4971 - val_loss: 1.0036 - val_acc: 0.5312\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9956 - acc: 0.5187 - val_loss: 1.0031 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9812 - acc: 0.5325 - val_loss: 0.9902 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5098814232547293\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 151s 15ms/step - loss: 1.0746 - acc: 0.4310 - val_loss: 1.0270 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0356 - acc: 0.4850 - val_loss: 1.0067 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0156 - acc: 0.5014 - val_loss: 1.0004 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9976 - acc: 0.5191 - val_loss: 0.9961 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9800 - acc: 0.5329 - val_loss: 1.0077 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5185770754286423\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 154s 15ms/step - loss: 1.0708 - acc: 0.4355 - val_loss: 1.0190 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0340 - acc: 0.4836 - val_loss: 1.0014 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 1.0170 - acc: 0.5043 - val_loss: 1.0017 - val_acc: 0.5280\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9955 - acc: 0.5111 - val_loss: 0.9943 - val_acc: 0.5241\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9791 - acc: 0.5320 - val_loss: 0.9910 - val_acc: 0.5273\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.5209486169306186\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 158s 15ms/step - loss: 1.0713 - acc: 0.4340 - val_loss: 1.0186 - val_acc: 0.5070\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0369 - acc: 0.4847 - val_loss: 1.0105 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0158 - acc: 0.5047 - val_loss: 0.9980 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9949 - acc: 0.5190 - val_loss: 0.9919 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9795 - acc: 0.5292 - val_loss: 0.9916 - val_acc: 0.5296\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5256916999816894\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 158s 15ms/step - loss: 1.0789 - acc: 0.4265 - val_loss: 1.0158 - val_acc: 0.5210\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0315 - acc: 0.4893 - val_loss: 1.0033 - val_acc: 0.5226\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0094 - acc: 0.5079 - val_loss: 1.0031 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9920 - acc: 0.5216 - val_loss: 0.9941 - val_acc: 0.5249\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9725 - acc: 0.5370 - val_loss: 0.9929 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5201581030966265\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 153s 15ms/step - loss: 1.0711 - acc: 0.4304 - val_loss: 1.0264 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 1.0378 - acc: 0.4791 - val_loss: 1.0123 - val_acc: 0.5179\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 1.0146 - acc: 0.5020 - val_loss: 0.9989 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9955 - acc: 0.5242 - val_loss: 0.9945 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9743 - acc: 0.5394 - val_loss: 1.0041 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5177865615946502\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 157s 15ms/step - loss: 1.0751 - acc: 0.4240 - val_loss: 1.0271 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0370 - acc: 0.4796 - val_loss: 1.0054 - val_acc: 0.5179\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0166 - acc: 0.5014 - val_loss: 0.9995 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9972 - acc: 0.5173 - val_loss: 1.0044 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 0.9757 - acc: 0.5276 - val_loss: 0.9903 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5193675892155161\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0754 - acc: 0.4275 - val_loss: 1.0248 - val_acc: 0.4969\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 145s 14ms/step - loss: 1.0395 - acc: 0.4757 - val_loss: 1.0068 - val_acc: 0.5187\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0178 - acc: 0.5014 - val_loss: 0.9965 - val_acc: 0.5249\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0000 - acc: 0.5150 - val_loss: 0.9935 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9782 - acc: 0.5321 - val_loss: 0.9930 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "0.5138339924246897\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 165s 16ms/step - loss: 1.0795 - acc: 0.4297 - val_loss: 1.0287 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0383 - acc: 0.4809 - val_loss: 1.0086 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 125s 12ms/step - loss: 1.0171 - acc: 0.5005 - val_loss: 0.9995 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9985 - acc: 0.5206 - val_loss: 0.9990 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9822 - acc: 0.5305 - val_loss: 1.0025 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5185770754286423\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0737 - acc: 0.4288 - val_loss: 1.0243 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0372 - acc: 0.4862 - val_loss: 1.0178 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 1.0158 - acc: 0.5031 - val_loss: 1.0030 - val_acc: 0.5304\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 127s 12ms/step - loss: 0.9980 - acc: 0.5177 - val_loss: 0.9912 - val_acc: 0.5257\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 128s 12ms/step - loss: 0.9774 - acc: 0.5272 - val_loss: 0.9932 - val_acc: 0.5288\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.5335968382744921\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 165s 16ms/step - loss: 1.0728 - acc: 0.4370 - val_loss: 1.0200 - val_acc: 0.5101\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0311 - acc: 0.4868 - val_loss: 1.0074 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 129s 13ms/step - loss: 1.0160 - acc: 0.5034 - val_loss: 1.0008 - val_acc: 0.5234\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 128s 13ms/step - loss: 0.9930 - acc: 0.5243 - val_loss: 0.9982 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 126s 12ms/step - loss: 0.9767 - acc: 0.5309 - val_loss: 1.0011 - val_acc: 0.5343\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "0.5154150200926739\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0778 - acc: 0.4282 - val_loss: 1.0248 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0367 - acc: 0.4793 - val_loss: 1.0061 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 1.0145 - acc: 0.5042 - val_loss: 0.9965 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 131s 13ms/step - loss: 0.9959 - acc: 0.5162 - val_loss: 0.9914 - val_acc: 0.5304\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 0.9773 - acc: 0.5295 - val_loss: 0.9961 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5320158105593896\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0723 - acc: 0.4341 - val_loss: 1.0203 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 149s 15ms/step - loss: 1.0336 - acc: 0.4847 - val_loss: 0.9990 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 1.0133 - acc: 0.5044 - val_loss: 1.0010 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 130s 13ms/step - loss: 0.9912 - acc: 0.5234 - val_loss: 0.9964 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 0.9738 - acc: 0.5394 - val_loss: 0.9888 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5169960477606581\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0780 - acc: 0.4322 - val_loss: 1.0405 - val_acc: 0.4657\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 135s 13ms/step - loss: 1.0345 - acc: 0.4834 - val_loss: 1.0103 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 135s 13ms/step - loss: 1.0117 - acc: 0.5103 - val_loss: 1.0030 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 133s 13ms/step - loss: 0.9919 - acc: 0.5237 - val_loss: 0.9990 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 132s 13ms/step - loss: 0.9770 - acc: 0.5284 - val_loss: 0.9874 - val_acc: 0.5273\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5146245062586818\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 176s 17ms/step - loss: 1.0744 - acc: 0.4252 - val_loss: 1.0339 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 138s 13ms/step - loss: 1.0399 - acc: 0.4745 - val_loss: 1.0108 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0171 - acc: 0.5003 - val_loss: 1.0155 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 138s 13ms/step - loss: 1.0009 - acc: 0.5128 - val_loss: 1.0072 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 138s 13ms/step - loss: 0.9837 - acc: 0.5323 - val_loss: 0.9927 - val_acc: 0.5226\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5241106722194687\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0689 - acc: 0.4320 - val_loss: 1.0237 - val_acc: 0.4891\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0353 - acc: 0.4788 - val_loss: 1.0109 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 144s 14ms/step - loss: 1.0118 - acc: 0.5030 - val_loss: 0.9976 - val_acc: 0.5280\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 0.9930 - acc: 0.5203 - val_loss: 1.0125 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 0.9738 - acc: 0.5330 - val_loss: 0.9889 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5146245062586818\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0692 - acc: 0.4403 - val_loss: 1.0304 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 139s 14ms/step - loss: 1.0371 - acc: 0.4786 - val_loss: 1.0069 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 140s 14ms/step - loss: 1.0165 - acc: 0.5005 - val_loss: 1.0031 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 1.0013 - acc: 0.5218 - val_loss: 1.0067 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 141s 14ms/step - loss: 0.9809 - acc: 0.5304 - val_loss: 0.9910 - val_acc: 0.5312\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "0.5130434785906977\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0725 - acc: 0.4210 - val_loss: 1.0329 - val_acc: 0.4984\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 178s 17ms/step - loss: 1.0384 - acc: 0.4778 - val_loss: 1.0224 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0145 - acc: 0.5018 - val_loss: 0.9994 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 0.9951 - acc: 0.5175 - val_loss: 0.9965 - val_acc: 0.5296\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 158s 15ms/step - loss: 0.9750 - acc: 0.5325 - val_loss: 0.9902 - val_acc: 0.5280\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5146245062115635\n",
      "{5: 0.507509881446484, 6: 0.5083003955867451, 7: 0.5233201581263259, 8: 0.513043478637816, 9: 0.5019762846085394, 10: 0.5122529644504367, 11: 0.5011857711279345, 12: 0.5027667987488004, 13: 0.5114624509698318, 14: 0.5011857711279345, 15: 0.49644268777059475, 16: 0.5122529647567056, 17: 0.505138340297895, 18: 0.5098814232547293, 19: 0.5185770754286423, 20: 0.5209486169306186, 21: 0.5256916999816894, 22: 0.5201581030966265, 23: 0.5177865615946502, 24: 0.5193675892155161, 25: 0.5138339924246897, 26: 0.5185770754286423, 27: 0.5335968382744921, 28: 0.5154150200926739, 29: 0.5320158105593896, 30: 0.5169960477606581, 31: 0.5146245062586818, 32: 0.5241106722194687, 33: 0.5146245062586818, 34: 0.5130434785906977, 35: 0.5146245062115635}\n",
      "CPU times: user 6d 10h 38min 12s, sys: 14h 16min 32s, total: 7d 54min 45s\n",
      "Wall time: 1d 5h 54min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cnn_elmo_rounds = [calculate_round(concatenated_elmo) for round in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{5: 0.49565217424287156,\n",
       "  6: 0.5114624506164446,\n",
       "  7: 0.5169960477606581,\n",
       "  8: 0.5122529644504367,\n",
       "  9: 0.49249011860063424,\n",
       "  10: 0.49644268807686365,\n",
       "  11: 0.5075098817998712,\n",
       "  12: 0.5185770751223734,\n",
       "  13: 0.507509881752753,\n",
       "  14: 0.4988142295788399,\n",
       "  15: 0.5043478264639029,\n",
       "  16: 0.5122529647567056,\n",
       "  17: 0.5067193679187609,\n",
       "  18: 0.5090909094207372,\n",
       "  19: 0.513833992471808,\n",
       "  20: 0.5059288540847687,\n",
       "  21: 0.5209486166243497,\n",
       "  22: 0.49565217424287156,\n",
       "  23: 0.5122529647567056,\n",
       "  24: 0.5098814229484603,\n",
       "  25: 0.5272727276025554,\n",
       "  26: 0.5225296445514844,\n",
       "  27: 0.5146245063058001,\n",
       "  28: 0.5051383402507766,\n",
       "  29: 0.5059288537784998,\n",
       "  30: 0.5169960477606581,\n",
       "  31: 0.5051383402507766,\n",
       "  32: 0.513833992471808,\n",
       "  33: 0.5003952569405552,\n",
       "  34: 0.5106719370887214,\n",
       "  35: 0.5019762849148083},\n",
       " {5: 0.5043478261105157,\n",
       "  6: 0.5114624509698318,\n",
       "  7: 0.5304347829385238,\n",
       "  8: 0.5067193679658791,\n",
       "  9: 0.499604743412832,\n",
       "  10: 0.5130434782844288,\n",
       "  11: 0.5098814233018476,\n",
       "  12: 0.505138340297895,\n",
       "  13: 0.5011857707745473,\n",
       "  14: 0.5019762849148083,\n",
       "  15: 0.5177865615946502,\n",
       "  16: 0.5011857707745473,\n",
       "  17: 0.5201581030966265,\n",
       "  18: 0.505138340297895,\n",
       "  19: 0.5280632414365475,\n",
       "  20: 0.5359683797293501,\n",
       "  21: 0.5264822138156815,\n",
       "  22: 0.5185770751223734,\n",
       "  23: 0.5114624509698318,\n",
       "  24: 0.4988142295788399,\n",
       "  25: 0.5288537553176579,\n",
       "  26: 0.5225296445986027,\n",
       "  27: 0.5201581030966265,\n",
       "  28: 0.5154150200455556,\n",
       "  29: 0.5027667987488004,\n",
       "  30: 0.5217391307646106,\n",
       "  31: 0.5083003955867451,\n",
       "  32: 0.505138339991626,\n",
       "  33: 0.513043478637816,\n",
       "  34: 0.5201581031437448,\n",
       "  35: 0.5051383402507766},\n",
       " {5: 0.5067193679658791,\n",
       "  6: 0.5019762849148083,\n",
       "  7: 0.5090909094678555,\n",
       "  8: 0.507509881446484,\n",
       "  9: 0.5177865616417685,\n",
       "  10: 0.5003952569405552,\n",
       "  11: 0.507509881752753,\n",
       "  12: 0.5011857710336979,\n",
       "  13: 0.5090909093736189,\n",
       "  14: 0.5027667987959187,\n",
       "  15: 0.507509881752753,\n",
       "  16: 0.5185770754286423,\n",
       "  17: 0.5122529648038239,\n",
       "  18: 0.5169960477606581,\n",
       "  19: 0.516205533926666,\n",
       "  20: 0.5035573125827925,\n",
       "  21: 0.5051383402507766,\n",
       "  22: 0.5201581031437448,\n",
       "  23: 0.5154150200926739,\n",
       "  24: 0.5122529647567056,\n",
       "  25: 0.4940711462686184,\n",
       "  26: 0.516205533926666,\n",
       "  27: 0.5193675889563655,\n",
       "  28: 0.4830039526398474,\n",
       "  29: 0.5114624509227135,\n",
       "  30: 0.5035573125827925,\n",
       "  31: 0.5098814229484603,\n",
       "  32: 0.5043478264167846,\n",
       "  33: 0.5090909094207372,\n",
       "  34: 0.5090909094207372,\n",
       "  35: 0.5090909094678555},\n",
       " {5: 0.5098814232547293,\n",
       "  6: 0.5114624509698318,\n",
       "  7: 0.5098814232547293,\n",
       "  8: 0.5114624506635628,\n",
       "  9: 0.49802371543857893,\n",
       "  10: 0.4980237155328155,\n",
       "  11: 0.505928854131887,\n",
       "  12: 0.5122529647567056,\n",
       "  13: 0.5146245062586818,\n",
       "  14: 0.505138340297895,\n",
       "  15: 0.5146245062586818,\n",
       "  16: 0.5154150200926739,\n",
       "  17: 0.5193675893097527,\n",
       "  18: 0.5130434785906977,\n",
       "  19: 0.5169960477135398,\n",
       "  20: 0.5146245062115635,\n",
       "  21: 0.49565217424287156,\n",
       "  22: 0.5375494074444526,\n",
       "  23: 0.5185770754757606,\n",
       "  24: 0.5106719370887214,\n",
       "  25: 0.5114624508755952,\n",
       "  26: 0.5154150200926739,\n",
       "  27: 0.5083003956338633,\n",
       "  28: 0.5241106722665869,\n",
       "  29: 0.5090909093736189,\n",
       "  30: 0.5169960477606581,\n",
       "  31: 0.5177865615475319,\n",
       "  32: 0.5264822135094126,\n",
       "  33: 0.5217391307646106,\n",
       "  34: 0.5217391307646106,\n",
       "  35: 0.5217391307174923},\n",
       " {5: 0.507509881446484,\n",
       "  6: 0.5083003955867451,\n",
       "  7: 0.5233201581263259,\n",
       "  8: 0.513043478637816,\n",
       "  9: 0.5019762846085394,\n",
       "  10: 0.5122529644504367,\n",
       "  11: 0.5011857711279345,\n",
       "  12: 0.5027667987488004,\n",
       "  13: 0.5114624509698318,\n",
       "  14: 0.5011857711279345,\n",
       "  15: 0.49644268777059475,\n",
       "  16: 0.5122529647567056,\n",
       "  17: 0.505138340297895,\n",
       "  18: 0.5098814232547293,\n",
       "  19: 0.5185770754286423,\n",
       "  20: 0.5209486169306186,\n",
       "  21: 0.5256916999816894,\n",
       "  22: 0.5201581030966265,\n",
       "  23: 0.5177865615946502,\n",
       "  24: 0.5193675892155161,\n",
       "  25: 0.5138339924246897,\n",
       "  26: 0.5185770754286423,\n",
       "  27: 0.5335968382744921,\n",
       "  28: 0.5154150200926739,\n",
       "  29: 0.5320158105593896,\n",
       "  30: 0.5169960477606581,\n",
       "  31: 0.5146245062586818,\n",
       "  32: 0.5241106722194687,\n",
       "  33: 0.5146245062586818,\n",
       "  34: 0.5130434785906977,\n",
       "  35: 0.5146245062115635}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_elmo_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Round 0",
         "type": "scatter",
         "uid": "1aed7a30-cb5d-404d-b860-e1ae780ab8e3",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.49565217424287156,
          0.5114624506164446,
          0.5169960477606581,
          0.5122529644504367,
          0.49249011860063424,
          0.49644268807686365,
          0.5075098817998712,
          0.5185770751223734,
          0.507509881752753,
          0.4988142295788399,
          0.5043478264639029,
          0.5122529647567056,
          0.5067193679187609,
          0.5090909094207372,
          0.513833992471808,
          0.5059288540847687,
          0.5209486166243497,
          0.49565217424287156,
          0.5122529647567056,
          0.5098814229484603,
          0.5272727276025554,
          0.5225296445514844,
          0.5146245063058001,
          0.5051383402507766,
          0.5059288537784998,
          0.5169960477606581,
          0.5051383402507766,
          0.513833992471808,
          0.5003952569405552,
          0.5106719370887214,
          0.5019762849148083
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 1",
         "type": "scatter",
         "uid": "fb87cc8e-1753-4561-960d-ccb8e6b19ac8",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5043478261105157,
          0.5114624509698318,
          0.5304347829385238,
          0.5067193679658791,
          0.499604743412832,
          0.5130434782844288,
          0.5098814233018476,
          0.505138340297895,
          0.5011857707745473,
          0.5019762849148083,
          0.5177865615946502,
          0.5011857707745473,
          0.5201581030966265,
          0.505138340297895,
          0.5280632414365475,
          0.5359683797293501,
          0.5264822138156815,
          0.5185770751223734,
          0.5114624509698318,
          0.4988142295788399,
          0.5288537553176579,
          0.5225296445986027,
          0.5201581030966265,
          0.5154150200455556,
          0.5027667987488004,
          0.5217391307646106,
          0.5083003955867451,
          0.505138339991626,
          0.513043478637816,
          0.5201581031437448,
          0.5051383402507766
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 2",
         "type": "scatter",
         "uid": "5f681289-a508-4e60-8ec0-26eb472ba05c",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5067193679658791,
          0.5019762849148083,
          0.5090909094678555,
          0.507509881446484,
          0.5177865616417685,
          0.5003952569405552,
          0.507509881752753,
          0.5011857710336979,
          0.5090909093736189,
          0.5027667987959187,
          0.507509881752753,
          0.5185770754286423,
          0.5122529648038239,
          0.5169960477606581,
          0.516205533926666,
          0.5035573125827925,
          0.5051383402507766,
          0.5201581031437448,
          0.5154150200926739,
          0.5122529647567056,
          0.4940711462686184,
          0.516205533926666,
          0.5193675889563655,
          0.4830039526398474,
          0.5114624509227135,
          0.5035573125827925,
          0.5098814229484603,
          0.5043478264167846,
          0.5090909094207372,
          0.5090909094207372,
          0.5090909094678555
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 3",
         "type": "scatter",
         "uid": "c788682b-204b-4631-9ac6-55369bb5aa96",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5098814232547293,
          0.5114624509698318,
          0.5098814232547293,
          0.5114624506635628,
          0.49802371543857893,
          0.4980237155328155,
          0.505928854131887,
          0.5122529647567056,
          0.5146245062586818,
          0.505138340297895,
          0.5146245062586818,
          0.5154150200926739,
          0.5193675893097527,
          0.5130434785906977,
          0.5169960477135398,
          0.5146245062115635,
          0.49565217424287156,
          0.5375494074444526,
          0.5185770754757606,
          0.5106719370887214,
          0.5114624508755952,
          0.5154150200926739,
          0.5083003956338633,
          0.5241106722665869,
          0.5090909093736189,
          0.5169960477606581,
          0.5177865615475319,
          0.5264822135094126,
          0.5217391307646106,
          0.5217391307646106,
          0.5217391307174923
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 4",
         "type": "scatter",
         "uid": "30faf463-f81b-4a97-ac2d-1d737a08ce24",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.507509881446484,
          0.5083003955867451,
          0.5233201581263259,
          0.513043478637816,
          0.5019762846085394,
          0.5122529644504367,
          0.5011857711279345,
          0.5027667987488004,
          0.5114624509698318,
          0.5011857711279345,
          0.49644268777059475,
          0.5122529647567056,
          0.505138340297895,
          0.5098814232547293,
          0.5185770754286423,
          0.5209486169306186,
          0.5256916999816894,
          0.5201581030966265,
          0.5177865615946502,
          0.5193675892155161,
          0.5138339924246897,
          0.5185770754286423,
          0.5335968382744921,
          0.5154150200926739,
          0.5320158105593896,
          0.5169960477606581,
          0.5146245062586818,
          0.5241106722194687,
          0.5146245062586818,
          0.5130434785906977,
          0.5146245062115635
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "CNN test set accuracy of padded ELMo dataset with variable maximum lengths"
        }
       }
      },
      "text/html": [
       "<div id=\"d207fdd7-4664-45de-9568-7ec7dfbf273c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"d207fdd7-4664-45de-9568-7ec7dfbf273c\")) {\n",
       "    Plotly.newPlot(\"d207fdd7-4664-45de-9568-7ec7dfbf273c\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49565217424287156, 0.5114624506164446, 0.5169960477606581, 0.5122529644504367, 0.49249011860063424, 0.49644268807686365, 0.5075098817998712, 0.5185770751223734, 0.507509881752753, 0.4988142295788399, 0.5043478264639029, 0.5122529647567056, 0.5067193679187609, 0.5090909094207372, 0.513833992471808, 0.5059288540847687, 0.5209486166243497, 0.49565217424287156, 0.5122529647567056, 0.5098814229484603, 0.5272727276025554, 0.5225296445514844, 0.5146245063058001, 0.5051383402507766, 0.5059288537784998, 0.5169960477606581, 0.5051383402507766, 0.513833992471808, 0.5003952569405552, 0.5106719370887214, 0.5019762849148083], \"type\": \"scatter\", \"uid\": \"1aed7a30-cb5d-404d-b860-e1ae780ab8e3\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5043478261105157, 0.5114624509698318, 0.5304347829385238, 0.5067193679658791, 0.499604743412832, 0.5130434782844288, 0.5098814233018476, 0.505138340297895, 0.5011857707745473, 0.5019762849148083, 0.5177865615946502, 0.5011857707745473, 0.5201581030966265, 0.505138340297895, 0.5280632414365475, 0.5359683797293501, 0.5264822138156815, 0.5185770751223734, 0.5114624509698318, 0.4988142295788399, 0.5288537553176579, 0.5225296445986027, 0.5201581030966265, 0.5154150200455556, 0.5027667987488004, 0.5217391307646106, 0.5083003955867451, 0.505138339991626, 0.513043478637816, 0.5201581031437448, 0.5051383402507766], \"type\": \"scatter\", \"uid\": \"fb87cc8e-1753-4561-960d-ccb8e6b19ac8\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5067193679658791, 0.5019762849148083, 0.5090909094678555, 0.507509881446484, 0.5177865616417685, 0.5003952569405552, 0.507509881752753, 0.5011857710336979, 0.5090909093736189, 0.5027667987959187, 0.507509881752753, 0.5185770754286423, 0.5122529648038239, 0.5169960477606581, 0.516205533926666, 0.5035573125827925, 0.5051383402507766, 0.5201581031437448, 0.5154150200926739, 0.5122529647567056, 0.4940711462686184, 0.516205533926666, 0.5193675889563655, 0.4830039526398474, 0.5114624509227135, 0.5035573125827925, 0.5098814229484603, 0.5043478264167846, 0.5090909094207372, 0.5090909094207372, 0.5090909094678555], \"type\": \"scatter\", \"uid\": \"5f681289-a508-4e60-8ec0-26eb472ba05c\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5098814232547293, 0.5114624509698318, 0.5098814232547293, 0.5114624506635628, 0.49802371543857893, 0.4980237155328155, 0.505928854131887, 0.5122529647567056, 0.5146245062586818, 0.505138340297895, 0.5146245062586818, 0.5154150200926739, 0.5193675893097527, 0.5130434785906977, 0.5169960477135398, 0.5146245062115635, 0.49565217424287156, 0.5375494074444526, 0.5185770754757606, 0.5106719370887214, 0.5114624508755952, 0.5154150200926739, 0.5083003956338633, 0.5241106722665869, 0.5090909093736189, 0.5169960477606581, 0.5177865615475319, 0.5264822135094126, 0.5217391307646106, 0.5217391307646106, 0.5217391307174923], \"type\": \"scatter\", \"uid\": \"c788682b-204b-4631-9ac6-55369bb5aa96\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.507509881446484, 0.5083003955867451, 0.5233201581263259, 0.513043478637816, 0.5019762846085394, 0.5122529644504367, 0.5011857711279345, 0.5027667987488004, 0.5114624509698318, 0.5011857711279345, 0.49644268777059475, 0.5122529647567056, 0.505138340297895, 0.5098814232547293, 0.5185770754286423, 0.5209486169306186, 0.5256916999816894, 0.5201581030966265, 0.5177865615946502, 0.5193675892155161, 0.5138339924246897, 0.5185770754286423, 0.5335968382744921, 0.5154150200926739, 0.5320158105593896, 0.5169960477606581, 0.5146245062586818, 0.5241106722194687, 0.5146245062586818, 0.5130434785906977, 0.5146245062115635], \"type\": \"scatter\", \"uid\": \"30faf463-f81b-4a97-ac2d-1d737a08ce24\"}], {\"title\": {\"text\": \"CNN test set accuracy of padded ELMo dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"d207fdd7-4664-45de-9568-7ec7dfbf273c\")) {window._Plotly.Plots.resize(document.getElementById(\"d207fdd7-4664-45de-9568-7ec7dfbf273c\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"d207fdd7-4664-45de-9568-7ec7dfbf273c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"d207fdd7-4664-45de-9568-7ec7dfbf273c\")) {\n",
       "    Plotly.newPlot(\"d207fdd7-4664-45de-9568-7ec7dfbf273c\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.49565217424287156, 0.5114624506164446, 0.5169960477606581, 0.5122529644504367, 0.49249011860063424, 0.49644268807686365, 0.5075098817998712, 0.5185770751223734, 0.507509881752753, 0.4988142295788399, 0.5043478264639029, 0.5122529647567056, 0.5067193679187609, 0.5090909094207372, 0.513833992471808, 0.5059288540847687, 0.5209486166243497, 0.49565217424287156, 0.5122529647567056, 0.5098814229484603, 0.5272727276025554, 0.5225296445514844, 0.5146245063058001, 0.5051383402507766, 0.5059288537784998, 0.5169960477606581, 0.5051383402507766, 0.513833992471808, 0.5003952569405552, 0.5106719370887214, 0.5019762849148083], \"type\": \"scatter\", \"uid\": \"1aed7a30-cb5d-404d-b860-e1ae780ab8e3\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5043478261105157, 0.5114624509698318, 0.5304347829385238, 0.5067193679658791, 0.499604743412832, 0.5130434782844288, 0.5098814233018476, 0.505138340297895, 0.5011857707745473, 0.5019762849148083, 0.5177865615946502, 0.5011857707745473, 0.5201581030966265, 0.505138340297895, 0.5280632414365475, 0.5359683797293501, 0.5264822138156815, 0.5185770751223734, 0.5114624509698318, 0.4988142295788399, 0.5288537553176579, 0.5225296445986027, 0.5201581030966265, 0.5154150200455556, 0.5027667987488004, 0.5217391307646106, 0.5083003955867451, 0.505138339991626, 0.513043478637816, 0.5201581031437448, 0.5051383402507766], \"type\": \"scatter\", \"uid\": \"fb87cc8e-1753-4561-960d-ccb8e6b19ac8\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5067193679658791, 0.5019762849148083, 0.5090909094678555, 0.507509881446484, 0.5177865616417685, 0.5003952569405552, 0.507509881752753, 0.5011857710336979, 0.5090909093736189, 0.5027667987959187, 0.507509881752753, 0.5185770754286423, 0.5122529648038239, 0.5169960477606581, 0.516205533926666, 0.5035573125827925, 0.5051383402507766, 0.5201581031437448, 0.5154150200926739, 0.5122529647567056, 0.4940711462686184, 0.516205533926666, 0.5193675889563655, 0.4830039526398474, 0.5114624509227135, 0.5035573125827925, 0.5098814229484603, 0.5043478264167846, 0.5090909094207372, 0.5090909094207372, 0.5090909094678555], \"type\": \"scatter\", \"uid\": \"5f681289-a508-4e60-8ec0-26eb472ba05c\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5098814232547293, 0.5114624509698318, 0.5098814232547293, 0.5114624506635628, 0.49802371543857893, 0.4980237155328155, 0.505928854131887, 0.5122529647567056, 0.5146245062586818, 0.505138340297895, 0.5146245062586818, 0.5154150200926739, 0.5193675893097527, 0.5130434785906977, 0.5169960477135398, 0.5146245062115635, 0.49565217424287156, 0.5375494074444526, 0.5185770754757606, 0.5106719370887214, 0.5114624508755952, 0.5154150200926739, 0.5083003956338633, 0.5241106722665869, 0.5090909093736189, 0.5169960477606581, 0.5177865615475319, 0.5264822135094126, 0.5217391307646106, 0.5217391307646106, 0.5217391307174923], \"type\": \"scatter\", \"uid\": \"c788682b-204b-4631-9ac6-55369bb5aa96\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.507509881446484, 0.5083003955867451, 0.5233201581263259, 0.513043478637816, 0.5019762846085394, 0.5122529644504367, 0.5011857711279345, 0.5027667987488004, 0.5114624509698318, 0.5011857711279345, 0.49644268777059475, 0.5122529647567056, 0.505138340297895, 0.5098814232547293, 0.5185770754286423, 0.5209486169306186, 0.5256916999816894, 0.5201581030966265, 0.5177865615946502, 0.5193675892155161, 0.5138339924246897, 0.5185770754286423, 0.5335968382744921, 0.5154150200926739, 0.5320158105593896, 0.5169960477606581, 0.5146245062586818, 0.5241106722194687, 0.5146245062586818, 0.5130434785906977, 0.5146245062115635], \"type\": \"scatter\", \"uid\": \"30faf463-f81b-4a97-ac2d-1d737a08ce24\"}], {\"title\": {\"text\": \"CNN test set accuracy of padded ELMo dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"d207fdd7-4664-45de-9568-7ec7dfbf273c\")) {window._Plotly.Plots.resize(document.getElementById(\"d207fdd7-4664-45de-9568-7ec7dfbf273c\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traces = cnn_elmo_rounds\n",
    "\n",
    "# Create traces\n",
    "def create_scatter(counter):\n",
    "    acc_dict = traces[counter]\n",
    "    \n",
    "    return go.Scatter(\n",
    "        x = list(acc_dict.keys()),\n",
    "        y = list(acc_dict.values()),\n",
    "        mode = 'lines+markers',\n",
    "        name = 'Round ' + str(counter)\n",
    "    )\n",
    "\n",
    "trace_data = [create_scatter(trace) for trace in range(len(traces))]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'CNN test set accuracy of padded ELMo dataset with variable maximum lengths',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = trace_data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformerxl = data.get_transformerxl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_transformerxl = {\n",
    "    dataset: [np.concatenate(np.array(statement)) for statement in transformerxl[dataset].statement]\n",
    "    for dataset in transformerxl.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "2019-06-10 15:58:40,328 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "2019-06-10 15:58:40,431 From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-06-10 15:58:40,520 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0598 - acc: 0.4324 - val_loss: 1.0387 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0536 - acc: 0.4428 - val_loss: 1.0318 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0489 - acc: 0.4524 - val_loss: 1.0310 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0443 - acc: 0.4626 - val_loss: 1.0189 - val_acc: 0.4953\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0376 - acc: 0.4745 - val_loss: 1.0209 - val_acc: 0.5047\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4924901189069032\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0598 - acc: 0.4328 - val_loss: 1.0374 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0532 - acc: 0.4412 - val_loss: 1.0336 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0494 - acc: 0.4522 - val_loss: 1.0263 - val_acc: 0.4790\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0430 - acc: 0.4635 - val_loss: 1.0219 - val_acc: 0.5008\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0379 - acc: 0.4765 - val_loss: 1.0196 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49644268802974534\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0578 - acc: 0.4321 - val_loss: 1.0381 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0512 - acc: 0.4463 - val_loss: 1.0294 - val_acc: 0.4899\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0461 - acc: 0.4560 - val_loss: 1.0241 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0395 - acc: 0.4745 - val_loss: 1.0185 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0338 - acc: 0.4783 - val_loss: 1.0161 - val_acc: 0.5008\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49011857740492687\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0595 - acc: 0.4315 - val_loss: 1.0342 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0525 - acc: 0.4421 - val_loss: 1.0275 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0468 - acc: 0.4540 - val_loss: 1.0219 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0396 - acc: 0.4730 - val_loss: 1.0189 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0355 - acc: 0.4727 - val_loss: 1.0088 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49090909123891896\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0585 - acc: 0.4312 - val_loss: 1.0353 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 38s 4ms/step - loss: 1.0504 - acc: 0.4480 - val_loss: 1.0359 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0440 - acc: 0.4709 - val_loss: 1.0351 - val_acc: 0.4727\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0387 - acc: 0.4775 - val_loss: 1.0178 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0321 - acc: 0.4843 - val_loss: 1.0047 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4869565221160768\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0599 - acc: 0.4270 - val_loss: 1.0326 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0523 - acc: 0.4441 - val_loss: 1.0253 - val_acc: 0.4821\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0450 - acc: 0.4609 - val_loss: 1.0260 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0388 - acc: 0.4697 - val_loss: 1.0275 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0315 - acc: 0.4852 - val_loss: 1.0071 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4869565220689585\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0583 - acc: 0.4307 - val_loss: 1.0381 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0513 - acc: 0.4478 - val_loss: 1.0266 - val_acc: 0.4844\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0458 - acc: 0.4638 - val_loss: 1.0249 - val_acc: 0.4953\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0393 - acc: 0.4694 - val_loss: 1.0139 - val_acc: 0.5023\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0323 - acc: 0.4831 - val_loss: 1.0059 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4869565220689585\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0609 - acc: 0.4312 - val_loss: 1.0360 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0518 - acc: 0.4487 - val_loss: 1.0317 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0436 - acc: 0.4651 - val_loss: 1.0154 - val_acc: 0.4977\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0382 - acc: 0.4769 - val_loss: 1.0175 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0296 - acc: 0.4870 - val_loss: 1.0063 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4885375497369427\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0612 - acc: 0.4340 - val_loss: 1.0324 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0527 - acc: 0.4479 - val_loss: 1.0279 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0457 - acc: 0.4562 - val_loss: 1.0237 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0417 - acc: 0.4729 - val_loss: 1.0078 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0344 - acc: 0.4783 - val_loss: 1.0213 - val_acc: 0.4961\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.47826086989504546\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0605 - acc: 0.4287 - val_loss: 1.0352 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0527 - acc: 0.4484 - val_loss: 1.0271 - val_acc: 0.4813\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0532 - acc: 0.4471 - val_loss: 1.0194 - val_acc: 0.4883\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0492 - acc: 0.4575 - val_loss: 1.0178 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0398 - acc: 0.4800 - val_loss: 1.0057 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49644268812398196\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0601 - acc: 0.4256 - val_loss: 1.0354 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0507 - acc: 0.4504 - val_loss: 1.0248 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0465 - acc: 0.4626 - val_loss: 1.0265 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0436 - acc: 0.4729 - val_loss: 1.0063 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0302 - acc: 0.4904 - val_loss: 1.0103 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4893280636180531\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0605 - acc: 0.4215 - val_loss: 1.0417 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0521 - acc: 0.4517 - val_loss: 1.0275 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0436 - acc: 0.4651 - val_loss: 1.0174 - val_acc: 0.4969\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0349 - acc: 0.4795 - val_loss: 1.0108 - val_acc: 0.5218\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0302 - acc: 0.4878 - val_loss: 1.0076 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4924901189069032\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0597 - acc: 0.4333 - val_loss: 1.0378 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0530 - acc: 0.4430 - val_loss: 1.0284 - val_acc: 0.4836\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0461 - acc: 0.4587 - val_loss: 1.0217 - val_acc: 0.4875\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0408 - acc: 0.4714 - val_loss: 1.0196 - val_acc: 0.4945\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0325 - acc: 0.4825 - val_loss: 1.0077 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4940711465748874\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0625 - acc: 0.4321 - val_loss: 1.0370 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0505 - acc: 0.4539 - val_loss: 1.0241 - val_acc: 0.4899\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0439 - acc: 0.4666 - val_loss: 1.0142 - val_acc: 0.5195\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0350 - acc: 0.4760 - val_loss: 1.0029 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0297 - acc: 0.4883 - val_loss: 0.9985 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49169960507291105\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0611 - acc: 0.4330 - val_loss: 1.0337 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0535 - acc: 0.4466 - val_loss: 1.0331 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0499 - acc: 0.4577 - val_loss: 1.0210 - val_acc: 0.4969\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0473 - acc: 0.4696 - val_loss: 1.0039 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0610 - acc: 0.4655 - val_loss: 0.9999 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4932806327408953\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0630 - acc: 0.4270 - val_loss: 1.0352 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0566 - acc: 0.4412 - val_loss: 1.0317 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0516 - acc: 0.4577 - val_loss: 1.0133 - val_acc: 0.4992\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0581 - acc: 0.4612 - val_loss: 1.0317 - val_acc: 0.4735\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0544 - acc: 0.4599 - val_loss: 1.0072 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.48379446673299015\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0606 - acc: 0.4284 - val_loss: 1.0392 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0522 - acc: 0.4513 - val_loss: 1.0303 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0451 - acc: 0.4659 - val_loss: 1.0226 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0358 - acc: 0.4741 - val_loss: 1.0141 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0305 - acc: 0.4844 - val_loss: 1.0094 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49169960512002936\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0637 - acc: 0.4283 - val_loss: 1.0404 - val_acc: 0.4891\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0546 - acc: 0.4449 - val_loss: 1.0299 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0469 - acc: 0.4560 - val_loss: 1.0249 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0382 - acc: 0.4717 - val_loss: 1.0082 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0298 - acc: 0.4836 - val_loss: 1.0135 - val_acc: 0.5093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49090909093265006\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0632 - acc: 0.4306 - val_loss: 1.0548 - val_acc: 0.3863\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0544 - acc: 0.4400 - val_loss: 1.0278 - val_acc: 0.4836\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0473 - acc: 0.4571 - val_loss: 1.0240 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0413 - acc: 0.4692 - val_loss: 1.0219 - val_acc: 0.4953\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0312 - acc: 0.4829 - val_loss: 1.0048 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4885375497369427\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0595 - acc: 0.4378 - val_loss: 1.0378 - val_acc: 0.4992\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0539 - acc: 0.4449 - val_loss: 1.0279 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0443 - acc: 0.4619 - val_loss: 1.0137 - val_acc: 0.4961\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0413 - acc: 0.4670 - val_loss: 1.0023 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0562 - acc: 0.4687 - val_loss: 1.0148 - val_acc: 0.4992\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4735177868910929\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0629 - acc: 0.4283 - val_loss: 1.0351 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0570 - acc: 0.4399 - val_loss: 1.0214 - val_acc: 0.4860\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0515 - acc: 0.4626 - val_loss: 1.0150 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0522 - acc: 0.4651 - val_loss: 1.0023 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0615 - acc: 0.4653 - val_loss: 1.0066 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4924901189069032\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0633 - acc: 0.4286 - val_loss: 1.0373 - val_acc: 0.5031\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0592 - acc: 0.4391 - val_loss: 1.0221 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0499 - acc: 0.4537 - val_loss: 1.0297 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0440 - acc: 0.4652 - val_loss: 1.0122 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0286 - acc: 0.4834 - val_loss: 1.0067 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49802371574484783\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0653 - acc: 0.4231 - val_loss: 1.0457 - val_acc: 0.4712\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0585 - acc: 0.4405 - val_loss: 1.0346 - val_acc: 0.4883\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0470 - acc: 0.4592 - val_loss: 1.0160 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0375 - acc: 0.4777 - val_loss: 1.0149 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0289 - acc: 0.4880 - val_loss: 1.0087 - val_acc: 0.5031\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.49723320191085574\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0627 - acc: 0.4312 - val_loss: 1.0350 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0549 - acc: 0.4451 - val_loss: 1.0274 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0440 - acc: 0.4625 - val_loss: 1.0178 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0360 - acc: 0.4763 - val_loss: 1.0124 - val_acc: 0.4961\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0273 - acc: 0.4902 - val_loss: 1.0112 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4861660082349664\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0645 - acc: 0.4282 - val_loss: 1.0324 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0561 - acc: 0.4470 - val_loss: 1.0412 - val_acc: 0.4463\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0458 - acc: 0.4587 - val_loss: 1.0153 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0378 - acc: 0.4729 - val_loss: 1.0129 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0324 - acc: 0.4817 - val_loss: 1.0027 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.49249011895402145\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0637 - acc: 0.4288 - val_loss: 1.0374 - val_acc: 0.4774\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0532 - acc: 0.4458 - val_loss: 1.0280 - val_acc: 0.4961\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0490 - acc: 0.4559 - val_loss: 1.0170 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0372 - acc: 0.4689 - val_loss: 1.0127 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0269 - acc: 0.4845 - val_loss: 1.0022 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.48458498056698224\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0624 - acc: 0.4331 - val_loss: 1.0403 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0543 - acc: 0.4448 - val_loss: 1.0262 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0423 - acc: 0.4581 - val_loss: 1.0307 - val_acc: 0.4945\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0380 - acc: 0.4720 - val_loss: 1.0211 - val_acc: 0.5031\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0286 - acc: 0.4844 - val_loss: 1.0024 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4774703561081717\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0647 - acc: 0.4277 - val_loss: 1.0374 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0531 - acc: 0.4480 - val_loss: 1.0344 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0449 - acc: 0.4637 - val_loss: 1.0249 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0345 - acc: 0.4757 - val_loss: 1.0253 - val_acc: 0.4961\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0271 - acc: 0.4838 - val_loss: 1.0086 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.48379446673299015\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0617 - acc: 0.4306 - val_loss: 1.0468 - val_acc: 0.4891\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0520 - acc: 0.4479 - val_loss: 1.0320 - val_acc: 0.4844\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0450 - acc: 0.4627 - val_loss: 1.0249 - val_acc: 0.4922\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0397 - acc: 0.4697 - val_loss: 1.0205 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0299 - acc: 0.4891 - val_loss: 1.0200 - val_acc: 0.4961\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49090909123891896\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0611 - acc: 0.4352 - val_loss: 1.0391 - val_acc: 0.4844\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0522 - acc: 0.4500 - val_loss: 1.0339 - val_acc: 0.4907\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0447 - acc: 0.4579 - val_loss: 1.0238 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0392 - acc: 0.4741 - val_loss: 1.0242 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0310 - acc: 0.4847 - val_loss: 1.0112 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.48300395289899806\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0633 - acc: 0.4313 - val_loss: 1.0473 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0517 - acc: 0.4456 - val_loss: 1.0337 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0439 - acc: 0.4676 - val_loss: 1.0337 - val_acc: 0.4860\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0344 - acc: 0.4817 - val_loss: 1.0151 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0268 - acc: 0.4895 - val_loss: 1.0120 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4940711466220057\n",
      "{5: 0.4924901189069032, 6: 0.49644268802974534, 7: 0.49011857740492687, 8: 0.49090909123891896, 9: 0.4869565221160768, 10: 0.4869565220689585, 11: 0.4869565220689585, 12: 0.4885375497369427, 13: 0.47826086989504546, 14: 0.49644268812398196, 15: 0.4893280636180531, 16: 0.4924901189069032, 17: 0.4940711465748874, 18: 0.49169960507291105, 19: 0.4932806327408953, 20: 0.48379446673299015, 21: 0.49169960512002936, 22: 0.49090909093265006, 23: 0.4885375497369427, 24: 0.4735177868910929, 25: 0.4924901189069032, 26: 0.49802371574484783, 27: 0.49723320191085574, 28: 0.4861660082349664, 29: 0.49249011895402145, 30: 0.48458498056698224, 31: 0.4774703561081717, 32: 0.48379446673299015, 33: 0.49090909123891896, 34: 0.48300395289899806, 35: 0.4940711466220057}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0587 - acc: 0.4333 - val_loss: 1.0372 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0532 - acc: 0.4430 - val_loss: 1.0347 - val_acc: 0.4836\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0492 - acc: 0.4494 - val_loss: 1.0367 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0457 - acc: 0.4627 - val_loss: 1.0273 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0384 - acc: 0.4750 - val_loss: 1.0145 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.474308300725085\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0598 - acc: 0.4306 - val_loss: 1.0354 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0517 - acc: 0.4459 - val_loss: 1.0316 - val_acc: 0.4836\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0470 - acc: 0.4537 - val_loss: 1.0296 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0411 - acc: 0.4669 - val_loss: 1.0160 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0340 - acc: 0.4803 - val_loss: 1.0134 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49090909123891896\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0596 - acc: 0.4319 - val_loss: 1.0364 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0521 - acc: 0.4448 - val_loss: 1.0338 - val_acc: 0.4829\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0475 - acc: 0.4532 - val_loss: 1.0251 - val_acc: 0.4883\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0416 - acc: 0.4671 - val_loss: 1.0261 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0356 - acc: 0.4857 - val_loss: 1.0126 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49486166040887947\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0592 - acc: 0.4347 - val_loss: 1.0355 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0522 - acc: 0.4446 - val_loss: 1.0288 - val_acc: 0.4829\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0457 - acc: 0.4569 - val_loss: 1.0270 - val_acc: 0.4992\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0410 - acc: 0.4715 - val_loss: 1.0179 - val_acc: 0.5031\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0343 - acc: 0.4810 - val_loss: 1.0064 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.48063241139702173\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0593 - acc: 0.4351 - val_loss: 1.0448 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0518 - acc: 0.4483 - val_loss: 1.0329 - val_acc: 0.4977\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0467 - acc: 0.4588 - val_loss: 1.0222 - val_acc: 0.5016\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0419 - acc: 0.4680 - val_loss: 1.0193 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0365 - acc: 0.4764 - val_loss: 1.0189 - val_acc: 0.4969\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4830039529461163\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0593 - acc: 0.4335 - val_loss: 1.0440 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0524 - acc: 0.4489 - val_loss: 1.0303 - val_acc: 0.4844\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0471 - acc: 0.4572 - val_loss: 1.0229 - val_acc: 0.4907\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0393 - acc: 0.4710 - val_loss: 1.0187 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0338 - acc: 0.4835 - val_loss: 1.0184 - val_acc: 0.5023\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4869565221160768\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0585 - acc: 0.4304 - val_loss: 1.0340 - val_acc: 0.4798\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0504 - acc: 0.4485 - val_loss: 1.0276 - val_acc: 0.4821\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0426 - acc: 0.4667 - val_loss: 1.0220 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0370 - acc: 0.4779 - val_loss: 1.0207 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0329 - acc: 0.4836 - val_loss: 1.0127 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49169960507291105\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0590 - acc: 0.4353 - val_loss: 1.0377 - val_acc: 0.4945\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0528 - acc: 0.4404 - val_loss: 1.0294 - val_acc: 0.5078\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0441 - acc: 0.4609 - val_loss: 1.0186 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0372 - acc: 0.4702 - val_loss: 1.0125 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0335 - acc: 0.4810 - val_loss: 1.0083 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4885375497369427\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0594 - acc: 0.4344 - val_loss: 1.0386 - val_acc: 0.4945\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0535 - acc: 0.4409 - val_loss: 1.0296 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0503 - acc: 0.4578 - val_loss: 1.0140 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0477 - acc: 0.4596 - val_loss: 1.0077 - val_acc: 0.5280\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0443 - acc: 0.4767 - val_loss: 1.0164 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.48063241144414004\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0621 - acc: 0.4306 - val_loss: 1.0364 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0549 - acc: 0.4408 - val_loss: 1.0254 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0503 - acc: 0.4503 - val_loss: 1.0155 - val_acc: 0.4938\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0485 - acc: 0.4687 - val_loss: 1.0180 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0459 - acc: 0.4705 - val_loss: 1.0191 - val_acc: 0.5070\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.48537549444809264\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0604 - acc: 0.4304 - val_loss: 1.0343 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0567 - acc: 0.4352 - val_loss: 1.0338 - val_acc: 0.4914\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0487 - acc: 0.4507 - val_loss: 1.0207 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0430 - acc: 0.4639 - val_loss: 1.0188 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0354 - acc: 0.4835 - val_loss: 1.0054 - val_acc: 0.5055\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4885375497369427\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0598 - acc: 0.4299 - val_loss: 1.0371 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0519 - acc: 0.4472 - val_loss: 1.0271 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0445 - acc: 0.4631 - val_loss: 1.0254 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0359 - acc: 0.4805 - val_loss: 1.0101 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0293 - acc: 0.4849 - val_loss: 1.0049 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4901185774520452\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0601 - acc: 0.4280 - val_loss: 1.0401 - val_acc: 0.4891\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0522 - acc: 0.4450 - val_loss: 1.0274 - val_acc: 0.4868\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0454 - acc: 0.4585 - val_loss: 1.0264 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0387 - acc: 0.4749 - val_loss: 1.0113 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0301 - acc: 0.4805 - val_loss: 1.0088 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4869565220689585\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0604 - acc: 0.4331 - val_loss: 1.0415 - val_acc: 0.4891\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0556 - acc: 0.4404 - val_loss: 1.0249 - val_acc: 0.4875\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0493 - acc: 0.4550 - val_loss: 1.0205 - val_acc: 0.4852\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0397 - acc: 0.4690 - val_loss: 1.0062 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0297 - acc: 0.4838 - val_loss: 1.0044 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4948616604559978\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0613 - acc: 0.4298 - val_loss: 1.0326 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0530 - acc: 0.4477 - val_loss: 1.0309 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0468 - acc: 0.4586 - val_loss: 1.0173 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0349 - acc: 0.4805 - val_loss: 1.0048 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0310 - acc: 0.4822 - val_loss: 1.0067 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4869565221160768\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0620 - acc: 0.4248 - val_loss: 1.0344 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0524 - acc: 0.4417 - val_loss: 1.0393 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0448 - acc: 0.4603 - val_loss: 1.0233 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0364 - acc: 0.4743 - val_loss: 1.0238 - val_acc: 0.4891\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0304 - acc: 0.4787 - val_loss: 1.0049 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49723320195797405\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0630 - acc: 0.4275 - val_loss: 1.0348 - val_acc: 0.4836\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0527 - acc: 0.4439 - val_loss: 1.0216 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0464 - acc: 0.4647 - val_loss: 1.0109 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0350 - acc: 0.4787 - val_loss: 1.0110 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0300 - acc: 0.4829 - val_loss: 1.0070 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49723320191085574\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0642 - acc: 0.4259 - val_loss: 1.0436 - val_acc: 0.4759\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0544 - acc: 0.4443 - val_loss: 1.0318 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0448 - acc: 0.4641 - val_loss: 1.0210 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0362 - acc: 0.4753 - val_loss: 1.0156 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0318 - acc: 0.4819 - val_loss: 1.0108 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4885375497369427\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0646 - acc: 0.4299 - val_loss: 1.0320 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0544 - acc: 0.4485 - val_loss: 1.0233 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0464 - acc: 0.4590 - val_loss: 1.0150 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0352 - acc: 0.4757 - val_loss: 1.0084 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0273 - acc: 0.4880 - val_loss: 1.0090 - val_acc: 0.5008\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4893280632646659\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0622 - acc: 0.4306 - val_loss: 1.0417 - val_acc: 0.4891\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0555 - acc: 0.4439 - val_loss: 1.0322 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0530 - acc: 0.4536 - val_loss: 1.0304 - val_acc: 0.4829\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0642 - acc: 0.4539 - val_loss: 1.0052 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0463 - acc: 0.4725 - val_loss: 0.9989 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49723320195797405\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0630 - acc: 0.4314 - val_loss: 1.0419 - val_acc: 0.4961\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0553 - acc: 0.4460 - val_loss: 1.0351 - val_acc: 0.4891\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0519 - acc: 0.4578 - val_loss: 1.0127 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0544 - acc: 0.4581 - val_loss: 1.0035 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0458 - acc: 0.4697 - val_loss: 1.0150 - val_acc: 0.5047\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4932806327408953\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0630 - acc: 0.4352 - val_loss: 1.0310 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0586 - acc: 0.4409 - val_loss: 1.0299 - val_acc: 0.5164\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0513 - acc: 0.4566 - val_loss: 1.0190 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0372 - acc: 0.4731 - val_loss: 1.0031 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0337 - acc: 0.4829 - val_loss: 0.9990 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4956521739366026\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0638 - acc: 0.4299 - val_loss: 1.0342 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0546 - acc: 0.4467 - val_loss: 1.0346 - val_acc: 0.4852\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0468 - acc: 0.4591 - val_loss: 1.0126 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0367 - acc: 0.4770 - val_loss: 1.0054 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0283 - acc: 0.4831 - val_loss: 1.0034 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.48537549444809264\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0660 - acc: 0.4245 - val_loss: 1.0344 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0551 - acc: 0.4412 - val_loss: 1.0248 - val_acc: 0.4891\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0475 - acc: 0.4589 - val_loss: 1.0161 - val_acc: 0.4875\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0392 - acc: 0.4710 - val_loss: 1.0238 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0374 - acc: 0.4756 - val_loss: 1.0146 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.49090909093265006\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0630 - acc: 0.4317 - val_loss: 1.0395 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0529 - acc: 0.4505 - val_loss: 1.0220 - val_acc: 0.4914\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0412 - acc: 0.4663 - val_loss: 1.0146 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0344 - acc: 0.4765 - val_loss: 1.0140 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0296 - acc: 0.4840 - val_loss: 1.0003 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4893280635709348\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0635 - acc: 0.4248 - val_loss: 1.0411 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0527 - acc: 0.4474 - val_loss: 1.0221 - val_acc: 0.4899\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0499 - acc: 0.4568 - val_loss: 1.0191 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0376 - acc: 0.4784 - val_loss: 1.0059 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0299 - acc: 0.4868 - val_loss: 1.0036 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4869565220689585\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0633 - acc: 0.4277 - val_loss: 1.0419 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0569 - acc: 0.4443 - val_loss: 1.0418 - val_acc: 0.4953\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0480 - acc: 0.4547 - val_loss: 1.0267 - val_acc: 0.5117\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0404 - acc: 0.4687 - val_loss: 1.0067 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0398 - acc: 0.4747 - val_loss: 1.0054 - val_acc: 0.5016\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.47905138372903755\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0625 - acc: 0.4347 - val_loss: 1.0411 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0509 - acc: 0.4515 - val_loss: 1.0282 - val_acc: 0.4813\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0426 - acc: 0.4650 - val_loss: 1.0288 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0346 - acc: 0.4733 - val_loss: 1.0148 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0267 - acc: 0.4898 - val_loss: 1.0108 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.48063241139702173\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0635 - acc: 0.4344 - val_loss: 1.0461 - val_acc: 0.4899\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0504 - acc: 0.4502 - val_loss: 1.0276 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0425 - acc: 0.4653 - val_loss: 1.0198 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0347 - acc: 0.4766 - val_loss: 1.0130 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0257 - acc: 0.4903 - val_loss: 1.0082 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.49644268807686365\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0623 - acc: 0.4303 - val_loss: 1.0389 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0541 - acc: 0.4466 - val_loss: 1.0365 - val_acc: 0.4930\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0438 - acc: 0.4586 - val_loss: 1.0321 - val_acc: 0.4899\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0368 - acc: 0.4747 - val_loss: 1.0179 - val_acc: 0.4984\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0285 - acc: 0.4837 - val_loss: 1.0126 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.49169960507291105\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0642 - acc: 0.4331 - val_loss: 1.0465 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0531 - acc: 0.4476 - val_loss: 1.0355 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0438 - acc: 0.4617 - val_loss: 1.0210 - val_acc: 0.4977\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0358 - acc: 0.4770 - val_loss: 1.0161 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0275 - acc: 0.4853 - val_loss: 1.0055 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4869565220689585\n",
      "{5: 0.474308300725085, 6: 0.49090909123891896, 7: 0.49486166040887947, 8: 0.48063241139702173, 9: 0.4830039529461163, 10: 0.4869565221160768, 11: 0.49169960507291105, 12: 0.4885375497369427, 13: 0.48063241144414004, 14: 0.48537549444809264, 15: 0.4885375497369427, 16: 0.4901185774520452, 17: 0.4869565220689585, 18: 0.4948616604559978, 19: 0.4869565221160768, 20: 0.49723320195797405, 21: 0.49723320191085574, 22: 0.4885375497369427, 23: 0.4893280632646659, 24: 0.49723320195797405, 25: 0.4932806327408953, 26: 0.4956521739366026, 27: 0.48537549444809264, 28: 0.49090909093265006, 29: 0.4893280635709348, 30: 0.4869565220689585, 31: 0.47905138372903755, 32: 0.48063241139702173, 33: 0.49644268807686365, 34: 0.49169960507291105, 35: 0.4869565220689585}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0595 - acc: 0.4341 - val_loss: 1.0405 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0525 - acc: 0.4421 - val_loss: 1.0336 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0499 - acc: 0.4512 - val_loss: 1.0244 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0443 - acc: 0.4612 - val_loss: 1.0188 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0391 - acc: 0.4755 - val_loss: 1.0122 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4932806327408953\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0594 - acc: 0.4350 - val_loss: 1.0436 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0527 - acc: 0.4423 - val_loss: 1.0323 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0490 - acc: 0.4495 - val_loss: 1.0309 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0438 - acc: 0.4650 - val_loss: 1.0191 - val_acc: 0.4922\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0378 - acc: 0.4728 - val_loss: 1.0192 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49328063278801354\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0584 - acc: 0.4311 - val_loss: 1.0363 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0525 - acc: 0.4436 - val_loss: 1.0284 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0475 - acc: 0.4599 - val_loss: 1.0229 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0401 - acc: 0.4703 - val_loss: 1.0175 - val_acc: 0.5257\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0353 - acc: 0.4782 - val_loss: 1.0097 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49723320191085574\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0580 - acc: 0.4338 - val_loss: 1.0319 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0521 - acc: 0.4487 - val_loss: 1.0323 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0441 - acc: 0.4622 - val_loss: 1.0256 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0425 - acc: 0.4678 - val_loss: 1.0079 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0333 - acc: 0.4849 - val_loss: 1.0307 - val_acc: 0.4766\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.48379446668587184\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0583 - acc: 0.4356 - val_loss: 1.0314 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0521 - acc: 0.4410 - val_loss: 1.0267 - val_acc: 0.4883\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0445 - acc: 0.4648 - val_loss: 1.0265 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0393 - acc: 0.4709 - val_loss: 1.0169 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0354 - acc: 0.4821 - val_loss: 1.0057 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.499604743412832\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0593 - acc: 0.4338 - val_loss: 1.0349 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0517 - acc: 0.4473 - val_loss: 1.0295 - val_acc: 0.4868\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0456 - acc: 0.4623 - val_loss: 1.0230 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0421 - acc: 0.4710 - val_loss: 1.0139 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 39s 4ms/step - loss: 1.0413 - acc: 0.4725 - val_loss: 1.0196 - val_acc: 0.5000\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49169960507291105\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0598 - acc: 0.4297 - val_loss: 1.0381 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0495 - acc: 0.4521 - val_loss: 1.0339 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0444 - acc: 0.4668 - val_loss: 1.0217 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0395 - acc: 0.4748 - val_loss: 1.0221 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0324 - acc: 0.4887 - val_loss: 1.0146 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4901185774520452\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0609 - acc: 0.4310 - val_loss: 1.0382 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0528 - acc: 0.4450 - val_loss: 1.0328 - val_acc: 0.4868\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0468 - acc: 0.4568 - val_loss: 1.0208 - val_acc: 0.4868\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 82s 8ms/step - loss: 1.0405 - acc: 0.4657 - val_loss: 1.0212 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0340 - acc: 0.4803 - val_loss: 1.0093 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4861660082349664\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0601 - acc: 0.4277 - val_loss: 1.0359 - val_acc: 0.4844\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0526 - acc: 0.4466 - val_loss: 1.0269 - val_acc: 0.5148\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0485 - acc: 0.4555 - val_loss: 1.0122 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0536 - acc: 0.4590 - val_loss: 1.0081 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0486 - acc: 0.4684 - val_loss: 0.9994 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4837944667801084\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0617 - acc: 0.4295 - val_loss: 1.0354 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0527 - acc: 0.4407 - val_loss: 1.0326 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0507 - acc: 0.4513 - val_loss: 1.0269 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0403 - acc: 0.4721 - val_loss: 1.0147 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0381 - acc: 0.4718 - val_loss: 1.0178 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.5043478264167846\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0644 - acc: 0.4251 - val_loss: 1.0378 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0557 - acc: 0.4423 - val_loss: 1.0270 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0472 - acc: 0.4661 - val_loss: 1.0253 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0422 - acc: 0.4742 - val_loss: 1.0055 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0342 - acc: 0.4803 - val_loss: 1.0006 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49090909123891896\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0629 - acc: 0.4305 - val_loss: 1.0324 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0534 - acc: 0.4494 - val_loss: 1.0239 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0478 - acc: 0.4638 - val_loss: 1.0171 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0354 - acc: 0.4796 - val_loss: 1.0079 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0296 - acc: 0.4849 - val_loss: 1.0083 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4853754944009743\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0626 - acc: 0.4263 - val_loss: 1.0367 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0535 - acc: 0.4457 - val_loss: 1.0258 - val_acc: 0.4953\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0445 - acc: 0.4533 - val_loss: 1.0139 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0382 - acc: 0.4771 - val_loss: 1.0080 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0310 - acc: 0.4843 - val_loss: 1.0052 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4845849802607133\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0619 - acc: 0.4320 - val_loss: 1.0364 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0546 - acc: 0.4412 - val_loss: 1.0259 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0536 - acc: 0.4541 - val_loss: 1.0215 - val_acc: 0.4836\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0425 - acc: 0.4695 - val_loss: 1.0071 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0339 - acc: 0.4834 - val_loss: 1.0122 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49565217428998987\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0628 - acc: 0.4286 - val_loss: 1.0360 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0528 - acc: 0.4451 - val_loss: 1.0262 - val_acc: 0.4938\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0463 - acc: 0.4587 - val_loss: 1.0254 - val_acc: 0.4899\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0404 - acc: 0.4705 - val_loss: 1.0110 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0306 - acc: 0.4873 - val_loss: 1.0029 - val_acc: 0.5055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.48379446673299015\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0633 - acc: 0.4227 - val_loss: 1.0352 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0607 - acc: 0.4407 - val_loss: 1.0386 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0751 - acc: 0.4381 - val_loss: 1.0251 - val_acc: 0.4836\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 3.3349 - acc: 0.4073 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0616 - acc: 0.4305 - val_loss: 1.0331 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0544 - acc: 0.4426 - val_loss: 1.0244 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0453 - acc: 0.4616 - val_loss: 1.0146 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0357 - acc: 0.4754 - val_loss: 1.0052 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0334 - acc: 0.4817 - val_loss: 1.0042 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.48537549444809264\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0621 - acc: 0.4291 - val_loss: 1.0331 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0528 - acc: 0.4458 - val_loss: 1.0235 - val_acc: 0.4953\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0438 - acc: 0.4642 - val_loss: 1.0299 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0406 - acc: 0.4766 - val_loss: 1.0348 - val_acc: 0.4688\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0426 - acc: 0.4747 - val_loss: 1.0037 - val_acc: 0.5202\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4901185774520452\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0629 - acc: 0.4258 - val_loss: 1.0411 - val_acc: 0.4961\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0531 - acc: 0.4455 - val_loss: 1.0343 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0518 - acc: 0.4566 - val_loss: 1.0279 - val_acc: 0.5148\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0414 - acc: 0.4663 - val_loss: 1.0063 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0330 - acc: 0.4826 - val_loss: 1.0124 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.482213438758737\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0615 - acc: 0.4279 - val_loss: 1.0311 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0525 - acc: 0.4458 - val_loss: 1.0285 - val_acc: 0.5109\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0446 - acc: 0.4622 - val_loss: 1.0244 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0410 - acc: 0.4700 - val_loss: 1.0175 - val_acc: 0.4953\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0477 - acc: 0.4700 - val_loss: 1.0024 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.49565217424287156\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0626 - acc: 0.4304 - val_loss: 1.0342 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0566 - acc: 0.4447 - val_loss: 1.0341 - val_acc: 0.5140\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0503 - acc: 0.4548 - val_loss: 1.0184 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0539 - acc: 0.4638 - val_loss: 1.0183 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0570 - acc: 0.4659 - val_loss: 1.0049 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4940711465748874\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0654 - acc: 0.4247 - val_loss: 1.0306 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0616 - acc: 0.4348 - val_loss: 1.0211 - val_acc: 0.4992\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0628 - acc: 0.4448 - val_loss: 1.0157 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0580 - acc: 0.4562 - val_loss: 1.0141 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0346 - acc: 0.4789 - val_loss: 1.0071 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.4948616604559978\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0631 - acc: 0.4330 - val_loss: 1.0386 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0524 - acc: 0.4466 - val_loss: 1.0281 - val_acc: 0.4945\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0425 - acc: 0.4633 - val_loss: 1.0205 - val_acc: 0.5016\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0368 - acc: 0.4785 - val_loss: 1.0104 - val_acc: 0.4969\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0274 - acc: 0.4873 - val_loss: 1.0112 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4948616604559978\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0610 - acc: 0.4259 - val_loss: 1.0374 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0565 - acc: 0.4416 - val_loss: 1.0292 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0505 - acc: 0.4502 - val_loss: 1.0168 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0396 - acc: 0.4696 - val_loss: 1.0174 - val_acc: 0.4992\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0314 - acc: 0.4811 - val_loss: 1.0110 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4924901189069032\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0649 - acc: 0.4306 - val_loss: 1.0483 - val_acc: 0.4735\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0564 - acc: 0.4389 - val_loss: 1.0303 - val_acc: 0.4899\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0448 - acc: 0.4627 - val_loss: 1.0337 - val_acc: 0.4914\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0362 - acc: 0.4737 - val_loss: 1.0082 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0313 - acc: 0.4825 - val_loss: 1.0509 - val_acc: 0.4447\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.45928853762008454\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0649 - acc: 0.4255 - val_loss: 1.0337 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0525 - acc: 0.4445 - val_loss: 1.0250 - val_acc: 0.4953\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0447 - acc: 0.4634 - val_loss: 1.0149 - val_acc: 0.4969\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0402 - acc: 0.4745 - val_loss: 1.0073 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0298 - acc: 0.4821 - val_loss: 1.0075 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.49169960512002936\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0638 - acc: 0.4320 - val_loss: 1.0357 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0572 - acc: 0.4409 - val_loss: 1.0255 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0477 - acc: 0.4565 - val_loss: 1.0174 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0457 - acc: 0.4657 - val_loss: 1.0090 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0397 - acc: 0.4749 - val_loss: 1.0032 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4893280635709348\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0612 - acc: 0.4326 - val_loss: 1.0362 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0518 - acc: 0.4478 - val_loss: 1.0321 - val_acc: 0.4938\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0435 - acc: 0.4612 - val_loss: 1.0224 - val_acc: 0.5016\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0341 - acc: 0.4829 - val_loss: 1.0178 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0260 - acc: 0.4910 - val_loss: 1.0062 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4893280635709348\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0631 - acc: 0.4298 - val_loss: 1.0418 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0522 - acc: 0.4448 - val_loss: 1.0340 - val_acc: 0.4868\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0464 - acc: 0.4563 - val_loss: 1.0378 - val_acc: 0.4844\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0392 - acc: 0.4689 - val_loss: 1.0193 - val_acc: 0.4899\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0316 - acc: 0.4801 - val_loss: 1.0206 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.48537549444809264\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0623 - acc: 0.4338 - val_loss: 1.0397 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0524 - acc: 0.4463 - val_loss: 1.0331 - val_acc: 0.4938\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0421 - acc: 0.4682 - val_loss: 1.0269 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0342 - acc: 0.4812 - val_loss: 1.0218 - val_acc: 0.5023\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0278 - acc: 0.4901 - val_loss: 1.0092 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4940711466220057\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0628 - acc: 0.4300 - val_loss: 1.0407 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0534 - acc: 0.4393 - val_loss: 1.0324 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0468 - acc: 0.4555 - val_loss: 1.0274 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0386 - acc: 0.4689 - val_loss: 1.0310 - val_acc: 0.4914\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0299 - acc: 0.4836 - val_loss: 1.0115 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4814229252310138\n",
      "{5: 0.4932806327408953, 6: 0.49328063278801354, 7: 0.49723320191085574, 8: 0.48379446668587184, 9: 0.499604743412832, 10: 0.49169960507291105, 11: 0.4901185774520452, 12: 0.4861660082349664, 13: 0.4837944667801084, 14: 0.5043478264167846, 15: 0.49090909123891896, 16: 0.4853754944009743, 17: 0.4845849802607133, 18: 0.49565217428998987, 19: 0.48379446673299015, 20: 0.3541501976402381, 21: 0.48537549444809264, 22: 0.4901185774520452, 23: 0.482213438758737, 24: 0.49565217424287156, 25: 0.4940711465748874, 26: 0.4948616604559978, 27: 0.4948616604559978, 28: 0.4924901189069032, 29: 0.45928853762008454, 30: 0.49169960512002936, 31: 0.4893280635709348, 32: 0.4893280635709348, 33: 0.48537549444809264, 34: 0.4940711466220057, 35: 0.4814229252310138}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0582 - acc: 0.4294 - val_loss: 1.0335 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0535 - acc: 0.4400 - val_loss: 1.0298 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0497 - acc: 0.4500 - val_loss: 1.0309 - val_acc: 0.4961\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0445 - acc: 0.4613 - val_loss: 1.0183 - val_acc: 0.4907\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0395 - acc: 0.4752 - val_loss: 1.0171 - val_acc: 0.5218\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4806324113499035\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0598 - acc: 0.4274 - val_loss: 1.0344 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0522 - acc: 0.4412 - val_loss: 1.0343 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0482 - acc: 0.4544 - val_loss: 1.0240 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0431 - acc: 0.4648 - val_loss: 1.0251 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0365 - acc: 0.4808 - val_loss: 1.0137 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.49090909123891896\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0591 - acc: 0.4314 - val_loss: 1.0388 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0537 - acc: 0.4399 - val_loss: 1.0299 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0487 - acc: 0.4472 - val_loss: 1.0225 - val_acc: 0.4844\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0427 - acc: 0.4626 - val_loss: 1.0203 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0371 - acc: 0.4736 - val_loss: 1.0182 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4901185773578086\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0593 - acc: 0.4343 - val_loss: 1.0400 - val_acc: 0.4805\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0530 - acc: 0.4407 - val_loss: 1.0313 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0487 - acc: 0.4549 - val_loss: 1.0225 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0409 - acc: 0.4691 - val_loss: 1.0173 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0385 - acc: 0.4745 - val_loss: 1.0133 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.5043478264167846\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0585 - acc: 0.4338 - val_loss: 1.0325 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0514 - acc: 0.4467 - val_loss: 1.0257 - val_acc: 0.4883\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0437 - acc: 0.4620 - val_loss: 1.0314 - val_acc: 0.4891\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0359 - acc: 0.4789 - val_loss: 1.0187 - val_acc: 0.5031\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0386 - acc: 0.4747 - val_loss: 1.0134 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.49644268807686365\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0599 - acc: 0.4315 - val_loss: 1.0410 - val_acc: 0.5008\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0536 - acc: 0.4459 - val_loss: 1.0331 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0488 - acc: 0.4591 - val_loss: 1.0199 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0398 - acc: 0.4729 - val_loss: 1.0079 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0332 - acc: 0.4812 - val_loss: 1.0032 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.49011857740492687\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0583 - acc: 0.4341 - val_loss: 1.0317 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0506 - acc: 0.4431 - val_loss: 1.0302 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0467 - acc: 0.4577 - val_loss: 1.0216 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0377 - acc: 0.4751 - val_loss: 1.0199 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0318 - acc: 0.4850 - val_loss: 1.0069 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.4885375497369427\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0626 - acc: 0.4215 - val_loss: 1.0407 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0527 - acc: 0.4444 - val_loss: 1.0326 - val_acc: 0.4868\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0469 - acc: 0.4584 - val_loss: 1.0377 - val_acc: 0.4891\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0411 - acc: 0.4703 - val_loss: 1.0170 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0327 - acc: 0.4793 - val_loss: 1.0106 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.48537549444809264\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0607 - acc: 0.4353 - val_loss: 1.0317 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0536 - acc: 0.4449 - val_loss: 1.0311 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0493 - acc: 0.4574 - val_loss: 1.0187 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0511 - acc: 0.4602 - val_loss: 1.0065 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0514 - acc: 0.4697 - val_loss: 1.0012 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4996047434599503\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0588 - acc: 0.4377 - val_loss: 1.0344 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0527 - acc: 0.4500 - val_loss: 1.0257 - val_acc: 0.4899\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0483 - acc: 0.4624 - val_loss: 1.0190 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0400 - acc: 0.4724 - val_loss: 1.0070 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0311 - acc: 0.4842 - val_loss: 0.9993 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49090909123891896\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0605 - acc: 0.4310 - val_loss: 1.0312 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0516 - acc: 0.4455 - val_loss: 1.0278 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0449 - acc: 0.4645 - val_loss: 1.0141 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0381 - acc: 0.4743 - val_loss: 1.0121 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0303 - acc: 0.4922 - val_loss: 1.0062 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4885375497369427\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0641 - acc: 0.4297 - val_loss: 1.0351 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0606 - acc: 0.4369 - val_loss: 1.0428 - val_acc: 0.4657\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0673 - acc: 0.4495 - val_loss: 1.1637 - val_acc: 0.3341\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.1276 - acc: 0.4339 - val_loss: 1.1000 - val_acc: 0.4003\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 5.2008 - acc: 0.4236 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0627 - acc: 0.4319 - val_loss: 1.0319 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0559 - acc: 0.4392 - val_loss: 1.0269 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0558 - acc: 0.4518 - val_loss: 1.0088 - val_acc: 0.5226\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0575 - acc: 0.4547 - val_loss: 1.0227 - val_acc: 0.5000\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0346 - acc: 0.4815 - val_loss: 0.9977 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49802371574484783\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0626 - acc: 0.4206 - val_loss: 1.0368 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0568 - acc: 0.4451 - val_loss: 1.0276 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0479 - acc: 0.4596 - val_loss: 1.0145 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0352 - acc: 0.4799 - val_loss: 1.0101 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0299 - acc: 0.4819 - val_loss: 0.9991 - val_acc: 0.5249\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4885375497369427\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0637 - acc: 0.4275 - val_loss: 1.0364 - val_acc: 0.4836\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0540 - acc: 0.4518 - val_loss: 1.0218 - val_acc: 0.4899\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0411 - acc: 0.4690 - val_loss: 1.0231 - val_acc: 0.4953\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0359 - acc: 0.4792 - val_loss: 1.0061 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0272 - acc: 0.4853 - val_loss: 0.9982 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49802371574484783\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0614 - acc: 0.4256 - val_loss: 1.0369 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0489 - acc: 0.4495 - val_loss: 1.0215 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0417 - acc: 0.4632 - val_loss: 1.0242 - val_acc: 0.4984\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0322 - acc: 0.4799 - val_loss: 1.0054 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0263 - acc: 0.4838 - val_loss: 1.0067 - val_acc: 0.5265\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4853754944009743\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0641 - acc: 0.4291 - val_loss: 1.0365 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0558 - acc: 0.4405 - val_loss: 1.0340 - val_acc: 0.4977\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0492 - acc: 0.4532 - val_loss: 1.0191 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0367 - acc: 0.4774 - val_loss: 1.0082 - val_acc: 0.5234\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0307 - acc: 0.4790 - val_loss: 1.0027 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49011857740492687\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0635 - acc: 0.4275 - val_loss: 1.0352 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0530 - acc: 0.4449 - val_loss: 1.0269 - val_acc: 0.4852\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0457 - acc: 0.4575 - val_loss: 1.0182 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0370 - acc: 0.4678 - val_loss: 1.0181 - val_acc: 0.4945\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0307 - acc: 0.4815 - val_loss: 1.0008 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4901185774520452\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0660 - acc: 0.4276 - val_loss: 1.0323 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0614 - acc: 0.4402 - val_loss: 1.0337 - val_acc: 0.4860\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0607 - acc: 0.4487 - val_loss: 1.0121 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0810 - acc: 0.4456 - val_loss: 1.0134 - val_acc: 0.5008\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.1263 - acc: 0.4313 - val_loss: 1.0093 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49169960512002936\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0630 - acc: 0.4247 - val_loss: 1.0336 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0553 - acc: 0.4450 - val_loss: 1.0300 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0452 - acc: 0.4562 - val_loss: 1.0193 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0414 - acc: 0.4638 - val_loss: 1.0155 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0318 - acc: 0.4769 - val_loss: 1.0149 - val_acc: 0.5016\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4956521739837209\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0620 - acc: 0.4311 - val_loss: 1.0438 - val_acc: 0.4774\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0551 - acc: 0.4423 - val_loss: 1.0305 - val_acc: 0.4914\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0477 - acc: 0.4578 - val_loss: 1.0185 - val_acc: 0.4977\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0410 - acc: 0.4677 - val_loss: 1.0185 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0315 - acc: 0.4830 - val_loss: 1.0049 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4853754944009743\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0623 - acc: 0.4261 - val_loss: 1.0373 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0551 - acc: 0.4396 - val_loss: 1.0275 - val_acc: 0.4922\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0466 - acc: 0.4575 - val_loss: 1.0166 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0431 - acc: 0.4698 - val_loss: 1.0088 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0317 - acc: 0.4833 - val_loss: 1.0025 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4869565220689585\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0609 - acc: 0.4278 - val_loss: 1.0339 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0515 - acc: 0.4449 - val_loss: 1.0267 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0433 - acc: 0.4610 - val_loss: 1.0238 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0369 - acc: 0.4739 - val_loss: 1.0088 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0272 - acc: 0.4895 - val_loss: 1.0045 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4940711465748874\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 50s 5ms/step - loss: 1.0632 - acc: 0.4246 - val_loss: 1.0350 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0528 - acc: 0.4439 - val_loss: 1.0259 - val_acc: 0.4907\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0425 - acc: 0.4651 - val_loss: 1.0196 - val_acc: 0.5078\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0367 - acc: 0.4726 - val_loss: 1.0085 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0273 - acc: 0.4894 - val_loss: 1.0029 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4861660082349664\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0624 - acc: 0.4326 - val_loss: 1.0396 - val_acc: 0.4961\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0541 - acc: 0.4444 - val_loss: 1.0301 - val_acc: 0.5117\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0441 - acc: 0.4617 - val_loss: 1.0166 - val_acc: 0.4945\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0353 - acc: 0.4749 - val_loss: 1.0271 - val_acc: 0.5031\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0258 - acc: 0.4921 - val_loss: 1.0004 - val_acc: 0.5273\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49090909123891896\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0642 - acc: 0.4261 - val_loss: 1.0354 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0554 - acc: 0.4452 - val_loss: 1.0254 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0446 - acc: 0.4588 - val_loss: 1.0210 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0382 - acc: 0.4780 - val_loss: 1.0180 - val_acc: 0.4945\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0295 - acc: 0.4875 - val_loss: 1.0031 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4948616604559978\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0632 - acc: 0.4339 - val_loss: 1.0391 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0568 - acc: 0.4467 - val_loss: 1.0242 - val_acc: 0.4977\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0446 - acc: 0.4674 - val_loss: 1.0200 - val_acc: 0.5164\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0337 - acc: 0.4799 - val_loss: 1.0067 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0242 - acc: 0.4911 - val_loss: 1.0007 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.49011857740492687\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0624 - acc: 0.4257 - val_loss: 1.0358 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0518 - acc: 0.4456 - val_loss: 1.0264 - val_acc: 0.4813\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0443 - acc: 0.4643 - val_loss: 1.0227 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0351 - acc: 0.4787 - val_loss: 1.0134 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0286 - acc: 0.4840 - val_loss: 1.0170 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.48142292527813213\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0613 - acc: 0.4296 - val_loss: 1.0357 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0520 - acc: 0.4462 - val_loss: 1.0365 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0428 - acc: 0.4678 - val_loss: 1.0211 - val_acc: 0.4992\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0355 - acc: 0.4769 - val_loss: 1.0167 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0277 - acc: 0.4830 - val_loss: 1.0081 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4932806327408953\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0635 - acc: 0.4329 - val_loss: 1.0422 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0524 - acc: 0.4456 - val_loss: 1.0375 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0465 - acc: 0.4602 - val_loss: 1.0288 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0394 - acc: 0.4742 - val_loss: 1.0156 - val_acc: 0.4938\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0306 - acc: 0.4853 - val_loss: 1.0106 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.48537549444809264\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0628 - acc: 0.4319 - val_loss: 1.0424 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0529 - acc: 0.4447 - val_loss: 1.0354 - val_acc: 0.4899\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0461 - acc: 0.4601 - val_loss: 1.0233 - val_acc: 0.4868\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0384 - acc: 0.4770 - val_loss: 1.0165 - val_acc: 0.5031\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0302 - acc: 0.4824 - val_loss: 1.0135 - val_acc: 0.5070\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.48458498061410055\n",
      "{5: 0.4806324113499035, 6: 0.49090909123891896, 7: 0.4901185773578086, 8: 0.5043478264167846, 9: 0.49644268807686365, 10: 0.49011857740492687, 11: 0.4885375497369427, 12: 0.48537549444809264, 13: 0.4996047434599503, 14: 0.49090909123891896, 15: 0.4885375497369427, 16: 0.43715415052745654, 17: 0.49802371574484783, 18: 0.4885375497369427, 19: 0.49802371574484783, 20: 0.4853754944009743, 21: 0.49011857740492687, 22: 0.4901185774520452, 23: 0.49169960512002936, 24: 0.4956521739837209, 25: 0.4853754944009743, 26: 0.4869565220689585, 27: 0.4940711465748874, 28: 0.4861660082349664, 29: 0.49090909123891896, 30: 0.4948616604559978, 31: 0.49011857740492687, 32: 0.48142292527813213, 33: 0.4932806327408953, 34: 0.48537549444809264, 35: 0.48458498061410055}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0594 - acc: 0.4300 - val_loss: 1.0356 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0532 - acc: 0.4410 - val_loss: 1.0324 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0488 - acc: 0.4567 - val_loss: 1.0247 - val_acc: 0.4805\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0444 - acc: 0.4610 - val_loss: 1.0198 - val_acc: 0.4953\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0396 - acc: 0.4753 - val_loss: 1.0179 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4940711465748874\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0595 - acc: 0.4313 - val_loss: 1.0340 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0529 - acc: 0.4439 - val_loss: 1.0350 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0485 - acc: 0.4532 - val_loss: 1.0276 - val_acc: 0.4907\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0427 - acc: 0.4655 - val_loss: 1.0282 - val_acc: 0.4969\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0369 - acc: 0.4781 - val_loss: 1.0208 - val_acc: 0.5008\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4869565220689585\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0585 - acc: 0.4322 - val_loss: 1.0393 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0527 - acc: 0.4454 - val_loss: 1.0312 - val_acc: 0.4852\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0486 - acc: 0.4518 - val_loss: 1.0240 - val_acc: 0.4852\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0400 - acc: 0.4722 - val_loss: 1.0293 - val_acc: 0.4969\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0369 - acc: 0.4796 - val_loss: 1.0098 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4940711465748874\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0580 - acc: 0.4369 - val_loss: 1.0397 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0516 - acc: 0.4489 - val_loss: 1.0281 - val_acc: 0.4922\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0486 - acc: 0.4561 - val_loss: 1.0178 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0431 - acc: 0.4702 - val_loss: 1.0170 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0402 - acc: 0.4764 - val_loss: 1.0067 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4932806327408953\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0608 - acc: 0.4329 - val_loss: 1.0325 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0537 - acc: 0.4424 - val_loss: 1.0279 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0473 - acc: 0.4550 - val_loss: 1.0215 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0425 - acc: 0.4612 - val_loss: 1.0150 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0360 - acc: 0.4791 - val_loss: 1.0149 - val_acc: 0.5039\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49486166036176116\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0591 - acc: 0.4359 - val_loss: 1.0371 - val_acc: 0.4836\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0542 - acc: 0.4374 - val_loss: 1.0345 - val_acc: 0.5101\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0486 - acc: 0.4616 - val_loss: 1.0193 - val_acc: 0.4860\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0444 - acc: 0.4722 - val_loss: 1.0221 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 40s 4ms/step - loss: 1.0340 - acc: 0.4840 - val_loss: 1.0088 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49802371574484783\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0576 - acc: 0.4374 - val_loss: 1.0467 - val_acc: 0.4626\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0508 - acc: 0.4486 - val_loss: 1.0275 - val_acc: 0.4813\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0458 - acc: 0.4614 - val_loss: 1.0245 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0398 - acc: 0.4711 - val_loss: 1.0171 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0343 - acc: 0.4810 - val_loss: 1.0104 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4924901189069032\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0586 - acc: 0.4322 - val_loss: 1.0376 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0514 - acc: 0.4488 - val_loss: 1.0362 - val_acc: 0.4914\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0465 - acc: 0.4618 - val_loss: 1.0322 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0384 - acc: 0.4730 - val_loss: 1.0162 - val_acc: 0.4891\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0317 - acc: 0.4860 - val_loss: 1.0064 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4877470359029506\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0609 - acc: 0.4293 - val_loss: 1.0315 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0518 - acc: 0.4466 - val_loss: 1.0325 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0475 - acc: 0.4590 - val_loss: 1.0126 - val_acc: 0.4992\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0425 - acc: 0.4720 - val_loss: 1.0071 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0403 - acc: 0.4753 - val_loss: 1.0071 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.488537549784061\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0610 - acc: 0.4221 - val_loss: 1.0360 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0521 - acc: 0.4454 - val_loss: 1.0280 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0481 - acc: 0.4495 - val_loss: 1.0317 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0417 - acc: 0.4728 - val_loss: 1.0117 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0354 - acc: 0.4836 - val_loss: 1.0045 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49644268807686365\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0599 - acc: 0.4322 - val_loss: 1.0351 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0541 - acc: 0.4509 - val_loss: 1.0400 - val_acc: 0.4611\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0522 - acc: 0.4555 - val_loss: 1.0356 - val_acc: 0.4759\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0542 - acc: 0.4645 - val_loss: 1.0137 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0383 - acc: 0.4799 - val_loss: 1.0053 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49565217428998987\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 47s 5ms/step - loss: 1.0590 - acc: 0.4339 - val_loss: 1.0396 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0521 - acc: 0.4482 - val_loss: 1.0282 - val_acc: 0.4938\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0416 - acc: 0.4694 - val_loss: 1.0167 - val_acc: 0.4899\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0355 - acc: 0.4767 - val_loss: 1.0076 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 41s 4ms/step - loss: 1.0297 - acc: 0.4841 - val_loss: 1.0066 - val_acc: 0.5171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49723320191085574\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0619 - acc: 0.4305 - val_loss: 1.0346 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0536 - acc: 0.4462 - val_loss: 1.0285 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0444 - acc: 0.4645 - val_loss: 1.0185 - val_acc: 0.5156\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0376 - acc: 0.4745 - val_loss: 1.0077 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0299 - acc: 0.4780 - val_loss: 1.0010 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4893280635709348\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0621 - acc: 0.4296 - val_loss: 1.0381 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0540 - acc: 0.4418 - val_loss: 1.0260 - val_acc: 0.4813\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0486 - acc: 0.4583 - val_loss: 1.0315 - val_acc: 0.4953\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0392 - acc: 0.4751 - val_loss: 1.0327 - val_acc: 0.4720\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0300 - acc: 0.4898 - val_loss: 1.0231 - val_acc: 0.4891\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49090909093265006\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0604 - acc: 0.4322 - val_loss: 1.0368 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0525 - acc: 0.4488 - val_loss: 1.0285 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0449 - acc: 0.4621 - val_loss: 1.0297 - val_acc: 0.4984\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0348 - acc: 0.4799 - val_loss: 1.0104 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0281 - acc: 0.4872 - val_loss: 1.0082 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4893280636180531\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 48s 5ms/step - loss: 1.0618 - acc: 0.4283 - val_loss: 1.0316 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0555 - acc: 0.4442 - val_loss: 1.0218 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0430 - acc: 0.4666 - val_loss: 1.0208 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0327 - acc: 0.4818 - val_loss: 1.0149 - val_acc: 0.4992\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0282 - acc: 0.4912 - val_loss: 1.0011 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4877470359029506\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0636 - acc: 0.4296 - val_loss: 1.0368 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0592 - acc: 0.4402 - val_loss: 1.0402 - val_acc: 0.4618\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0560 - acc: 0.4462 - val_loss: 1.0354 - val_acc: 0.4681\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0411 - acc: 0.4702 - val_loss: 1.0116 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0315 - acc: 0.4857 - val_loss: 1.0016 - val_acc: 0.5257\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4877470359500689\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0629 - acc: 0.4353 - val_loss: 1.0394 - val_acc: 0.4930\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0533 - acc: 0.4496 - val_loss: 1.0250 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0459 - acc: 0.4625 - val_loss: 1.0229 - val_acc: 0.5210\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0347 - acc: 0.4768 - val_loss: 1.0128 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 42s 4ms/step - loss: 1.0294 - acc: 0.4863 - val_loss: 1.0129 - val_acc: 0.5055\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4869565221160768\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 49s 5ms/step - loss: 1.0610 - acc: 0.4318 - val_loss: 1.0348 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0541 - acc: 0.4480 - val_loss: 1.0277 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0463 - acc: 0.4631 - val_loss: 1.0229 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0360 - acc: 0.4795 - val_loss: 1.0075 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0303 - acc: 0.4839 - val_loss: 1.0108 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4901185774520452\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0654 - acc: 0.4263 - val_loss: 1.0417 - val_acc: 0.4953\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0619 - acc: 0.4351 - val_loss: 1.0246 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0619 - acc: 0.4449 - val_loss: 1.0215 - val_acc: 0.4977\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0780 - acc: 0.4475 - val_loss: 1.2428 - val_acc: 0.2313\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.1020 - acc: 0.4418 - val_loss: 1.0066 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.49565217428998987\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0662 - acc: 0.4263 - val_loss: 1.0404 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0618 - acc: 0.4397 - val_loss: 1.0305 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0718 - acc: 0.4420 - val_loss: 1.0445 - val_acc: 0.4276\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0866 - acc: 0.4414 - val_loss: 1.0079 - val_acc: 0.5023\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0526 - acc: 0.4613 - val_loss: 1.0101 - val_acc: 0.5008\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.46561264855117196\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0633 - acc: 0.4256 - val_loss: 1.0369 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0581 - acc: 0.4422 - val_loss: 1.0301 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0511 - acc: 0.4554 - val_loss: 1.0259 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0509 - acc: 0.4604 - val_loss: 1.0061 - val_acc: 0.5031\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0684 - acc: 0.4641 - val_loss: 1.0216 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 2s 1ms/step\n",
      "0.4885375497369427\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0633 - acc: 0.4297 - val_loss: 1.0413 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0532 - acc: 0.4475 - val_loss: 1.0293 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0429 - acc: 0.4647 - val_loss: 1.0132 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0398 - acc: 0.4751 - val_loss: 1.0089 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0292 - acc: 0.4873 - val_loss: 1.0041 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.49249011895402145\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0632 - acc: 0.4290 - val_loss: 1.0397 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0508 - acc: 0.4509 - val_loss: 1.0333 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0464 - acc: 0.4589 - val_loss: 1.0161 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0395 - acc: 0.4734 - val_loss: 1.0218 - val_acc: 0.5047\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0284 - acc: 0.4846 - val_loss: 1.0105 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.48458498061410055\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0630 - acc: 0.4313 - val_loss: 1.0377 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0537 - acc: 0.4423 - val_loss: 1.0236 - val_acc: 0.4821\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0453 - acc: 0.4611 - val_loss: 1.0310 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0326 - acc: 0.4828 - val_loss: 1.0103 - val_acc: 0.5023\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 44s 4ms/step - loss: 1.0292 - acc: 0.4850 - val_loss: 1.0224 - val_acc: 0.4922\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4869565221160768\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0666 - acc: 0.4189 - val_loss: 1.0470 - val_acc: 0.4439\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 43s 4ms/step - loss: 1.0568 - acc: 0.4426 - val_loss: 1.0233 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0435 - acc: 0.4611 - val_loss: 1.0271 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0339 - acc: 0.4779 - val_loss: 1.0105 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0279 - acc: 0.4866 - val_loss: 1.0035 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.4861660082349664\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0627 - acc: 0.4297 - val_loss: 1.0348 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0564 - acc: 0.4424 - val_loss: 1.0278 - val_acc: 0.4836\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0464 - acc: 0.4610 - val_loss: 1.0134 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0373 - acc: 0.4744 - val_loss: 1.0046 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0305 - acc: 0.4861 - val_loss: 1.0008 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.49486166040887947\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0628 - acc: 0.4363 - val_loss: 1.0504 - val_acc: 0.4494\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0514 - acc: 0.4473 - val_loss: 1.0289 - val_acc: 0.4938\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0444 - acc: 0.4620 - val_loss: 1.0194 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0344 - acc: 0.4784 - val_loss: 1.0197 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0260 - acc: 0.4912 - val_loss: 1.0081 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.48063241109075283\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0632 - acc: 0.4257 - val_loss: 1.0370 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0522 - acc: 0.4469 - val_loss: 1.0426 - val_acc: 0.4938\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0458 - acc: 0.4632 - val_loss: 1.0271 - val_acc: 0.4977\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0405 - acc: 0.4660 - val_loss: 1.0201 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0330 - acc: 0.4804 - val_loss: 1.0172 - val_acc: 0.4961\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.49328063278801354\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0647 - acc: 0.4293 - val_loss: 1.0405 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0516 - acc: 0.4520 - val_loss: 1.0329 - val_acc: 0.4953\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0448 - acc: 0.4601 - val_loss: 1.0287 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0382 - acc: 0.4724 - val_loss: 1.0186 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0294 - acc: 0.4832 - val_loss: 1.0198 - val_acc: 0.5055\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.49249011860063424\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0628 - acc: 0.4284 - val_loss: 1.0386 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0533 - acc: 0.4456 - val_loss: 1.0329 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 46s 5ms/step - loss: 1.0461 - acc: 0.4569 - val_loss: 1.0302 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 45s 4ms/step - loss: 1.0394 - acc: 0.4756 - val_loss: 1.0174 - val_acc: 0.4984\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 46s 4ms/step - loss: 1.0305 - acc: 0.4841 - val_loss: 1.0111 - val_acc: 0.5210\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.49090909123891896\n",
      "{5: 0.4940711465748874, 6: 0.4869565220689585, 7: 0.4940711465748874, 8: 0.4932806327408953, 9: 0.49486166036176116, 10: 0.49802371574484783, 11: 0.4924901189069032, 12: 0.4877470359029506, 13: 0.488537549784061, 14: 0.49644268807686365, 15: 0.49565217428998987, 16: 0.49723320191085574, 17: 0.4893280635709348, 18: 0.49090909093265006, 19: 0.4893280636180531, 20: 0.4877470359029506, 21: 0.4877470359500689, 22: 0.4869565221160768, 23: 0.4901185774520452, 24: 0.49565217428998987, 25: 0.46561264855117196, 26: 0.4885375497369427, 27: 0.49249011895402145, 28: 0.48458498061410055, 29: 0.4869565221160768, 30: 0.4861660082349664, 31: 0.49486166040887947, 32: 0.48063241109075283, 33: 0.49328063278801354, 34: 0.49249011860063424, 35: 0.49090909123891896}\n",
      "CPU times: user 2d 3h 38min 37s, sys: 3h 46min 41s, total: 2d 7h 25min 19s\n",
      "Wall time: 9h 38min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cnn_transformerxl_rounds = [calculate_round(concatenated_transformerxl) for round in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{5: 0.4924901189069032,\n",
       "  6: 0.49644268802974534,\n",
       "  7: 0.49011857740492687,\n",
       "  8: 0.49090909123891896,\n",
       "  9: 0.4869565221160768,\n",
       "  10: 0.4869565220689585,\n",
       "  11: 0.4869565220689585,\n",
       "  12: 0.4885375497369427,\n",
       "  13: 0.47826086989504546,\n",
       "  14: 0.49644268812398196,\n",
       "  15: 0.4893280636180531,\n",
       "  16: 0.4924901189069032,\n",
       "  17: 0.4940711465748874,\n",
       "  18: 0.49169960507291105,\n",
       "  19: 0.4932806327408953,\n",
       "  20: 0.48379446673299015,\n",
       "  21: 0.49169960512002936,\n",
       "  22: 0.49090909093265006,\n",
       "  23: 0.4885375497369427,\n",
       "  24: 0.4735177868910929,\n",
       "  25: 0.4924901189069032,\n",
       "  26: 0.49802371574484783,\n",
       "  27: 0.49723320191085574,\n",
       "  28: 0.4861660082349664,\n",
       "  29: 0.49249011895402145,\n",
       "  30: 0.48458498056698224,\n",
       "  31: 0.4774703561081717,\n",
       "  32: 0.48379446673299015,\n",
       "  33: 0.49090909123891896,\n",
       "  34: 0.48300395289899806,\n",
       "  35: 0.4940711466220057},\n",
       " {5: 0.474308300725085,\n",
       "  6: 0.49090909123891896,\n",
       "  7: 0.49486166040887947,\n",
       "  8: 0.48063241139702173,\n",
       "  9: 0.4830039529461163,\n",
       "  10: 0.4869565221160768,\n",
       "  11: 0.49169960507291105,\n",
       "  12: 0.4885375497369427,\n",
       "  13: 0.48063241144414004,\n",
       "  14: 0.48537549444809264,\n",
       "  15: 0.4885375497369427,\n",
       "  16: 0.4901185774520452,\n",
       "  17: 0.4869565220689585,\n",
       "  18: 0.4948616604559978,\n",
       "  19: 0.4869565221160768,\n",
       "  20: 0.49723320195797405,\n",
       "  21: 0.49723320191085574,\n",
       "  22: 0.4885375497369427,\n",
       "  23: 0.4893280632646659,\n",
       "  24: 0.49723320195797405,\n",
       "  25: 0.4932806327408953,\n",
       "  26: 0.4956521739366026,\n",
       "  27: 0.48537549444809264,\n",
       "  28: 0.49090909093265006,\n",
       "  29: 0.4893280635709348,\n",
       "  30: 0.4869565220689585,\n",
       "  31: 0.47905138372903755,\n",
       "  32: 0.48063241139702173,\n",
       "  33: 0.49644268807686365,\n",
       "  34: 0.49169960507291105,\n",
       "  35: 0.4869565220689585},\n",
       " {5: 0.4932806327408953,\n",
       "  6: 0.49328063278801354,\n",
       "  7: 0.49723320191085574,\n",
       "  8: 0.48379446668587184,\n",
       "  9: 0.499604743412832,\n",
       "  10: 0.49169960507291105,\n",
       "  11: 0.4901185774520452,\n",
       "  12: 0.4861660082349664,\n",
       "  13: 0.4837944667801084,\n",
       "  14: 0.5043478264167846,\n",
       "  15: 0.49090909123891896,\n",
       "  16: 0.4853754944009743,\n",
       "  17: 0.4845849802607133,\n",
       "  18: 0.49565217428998987,\n",
       "  19: 0.48379446673299015,\n",
       "  20: 0.3541501976402381,\n",
       "  21: 0.48537549444809264,\n",
       "  22: 0.4901185774520452,\n",
       "  23: 0.482213438758737,\n",
       "  24: 0.49565217424287156,\n",
       "  25: 0.4940711465748874,\n",
       "  26: 0.4948616604559978,\n",
       "  27: 0.4948616604559978,\n",
       "  28: 0.4924901189069032,\n",
       "  29: 0.45928853762008454,\n",
       "  30: 0.49169960512002936,\n",
       "  31: 0.4893280635709348,\n",
       "  32: 0.4893280635709348,\n",
       "  33: 0.48537549444809264,\n",
       "  34: 0.4940711466220057,\n",
       "  35: 0.4814229252310138},\n",
       " {5: 0.4806324113499035,\n",
       "  6: 0.49090909123891896,\n",
       "  7: 0.4901185773578086,\n",
       "  8: 0.5043478264167846,\n",
       "  9: 0.49644268807686365,\n",
       "  10: 0.49011857740492687,\n",
       "  11: 0.4885375497369427,\n",
       "  12: 0.48537549444809264,\n",
       "  13: 0.4996047434599503,\n",
       "  14: 0.49090909123891896,\n",
       "  15: 0.4885375497369427,\n",
       "  16: 0.43715415052745654,\n",
       "  17: 0.49802371574484783,\n",
       "  18: 0.4885375497369427,\n",
       "  19: 0.49802371574484783,\n",
       "  20: 0.4853754944009743,\n",
       "  21: 0.49011857740492687,\n",
       "  22: 0.4901185774520452,\n",
       "  23: 0.49169960512002936,\n",
       "  24: 0.4956521739837209,\n",
       "  25: 0.4853754944009743,\n",
       "  26: 0.4869565220689585,\n",
       "  27: 0.4940711465748874,\n",
       "  28: 0.4861660082349664,\n",
       "  29: 0.49090909123891896,\n",
       "  30: 0.4948616604559978,\n",
       "  31: 0.49011857740492687,\n",
       "  32: 0.48142292527813213,\n",
       "  33: 0.4932806327408953,\n",
       "  34: 0.48537549444809264,\n",
       "  35: 0.48458498061410055},\n",
       " {5: 0.4940711465748874,\n",
       "  6: 0.4869565220689585,\n",
       "  7: 0.4940711465748874,\n",
       "  8: 0.4932806327408953,\n",
       "  9: 0.49486166036176116,\n",
       "  10: 0.49802371574484783,\n",
       "  11: 0.4924901189069032,\n",
       "  12: 0.4877470359029506,\n",
       "  13: 0.488537549784061,\n",
       "  14: 0.49644268807686365,\n",
       "  15: 0.49565217428998987,\n",
       "  16: 0.49723320191085574,\n",
       "  17: 0.4893280635709348,\n",
       "  18: 0.49090909093265006,\n",
       "  19: 0.4893280636180531,\n",
       "  20: 0.4877470359029506,\n",
       "  21: 0.4877470359500689,\n",
       "  22: 0.4869565221160768,\n",
       "  23: 0.4901185774520452,\n",
       "  24: 0.49565217428998987,\n",
       "  25: 0.46561264855117196,\n",
       "  26: 0.4885375497369427,\n",
       "  27: 0.49249011895402145,\n",
       "  28: 0.48458498061410055,\n",
       "  29: 0.4869565221160768,\n",
       "  30: 0.4861660082349664,\n",
       "  31: 0.49486166040887947,\n",
       "  32: 0.48063241109075283,\n",
       "  33: 0.49328063278801354,\n",
       "  34: 0.49249011860063424,\n",
       "  35: 0.49090909123891896}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_transformerxl_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Round 0",
         "type": "scatter",
         "uid": "c257b3ea-8e55-4456-91f6-ff60648fab8a",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.4924901189069032,
          0.49644268802974534,
          0.49011857740492687,
          0.49090909123891896,
          0.4869565221160768,
          0.4869565220689585,
          0.4869565220689585,
          0.4885375497369427,
          0.47826086989504546,
          0.49644268812398196,
          0.4893280636180531,
          0.4924901189069032,
          0.4940711465748874,
          0.49169960507291105,
          0.4932806327408953,
          0.48379446673299015,
          0.49169960512002936,
          0.49090909093265006,
          0.4885375497369427,
          0.4735177868910929,
          0.4924901189069032,
          0.49802371574484783,
          0.49723320191085574,
          0.4861660082349664,
          0.49249011895402145,
          0.48458498056698224,
          0.4774703561081717,
          0.48379446673299015,
          0.49090909123891896,
          0.48300395289899806,
          0.4940711466220057
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 1",
         "type": "scatter",
         "uid": "90ba27fd-68d4-47b8-9809-7cc72a513918",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.474308300725085,
          0.49090909123891896,
          0.49486166040887947,
          0.48063241139702173,
          0.4830039529461163,
          0.4869565221160768,
          0.49169960507291105,
          0.4885375497369427,
          0.48063241144414004,
          0.48537549444809264,
          0.4885375497369427,
          0.4901185774520452,
          0.4869565220689585,
          0.4948616604559978,
          0.4869565221160768,
          0.49723320195797405,
          0.49723320191085574,
          0.4885375497369427,
          0.4893280632646659,
          0.49723320195797405,
          0.4932806327408953,
          0.4956521739366026,
          0.48537549444809264,
          0.49090909093265006,
          0.4893280635709348,
          0.4869565220689585,
          0.47905138372903755,
          0.48063241139702173,
          0.49644268807686365,
          0.49169960507291105,
          0.4869565220689585
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 2",
         "type": "scatter",
         "uid": "ae9f1300-7906-449a-b6f0-cff19f23d369",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.4932806327408953,
          0.49328063278801354,
          0.49723320191085574,
          0.48379446668587184,
          0.499604743412832,
          0.49169960507291105,
          0.4901185774520452,
          0.4861660082349664,
          0.4837944667801084,
          0.5043478264167846,
          0.49090909123891896,
          0.4853754944009743,
          0.4845849802607133,
          0.49565217428998987,
          0.48379446673299015,
          0.3541501976402381,
          0.48537549444809264,
          0.4901185774520452,
          0.482213438758737,
          0.49565217424287156,
          0.4940711465748874,
          0.4948616604559978,
          0.4948616604559978,
          0.4924901189069032,
          0.45928853762008454,
          0.49169960512002936,
          0.4893280635709348,
          0.4893280635709348,
          0.48537549444809264,
          0.4940711466220057,
          0.4814229252310138
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 3",
         "type": "scatter",
         "uid": "2559d646-5815-4893-bec9-edfc8c75779a",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.4806324113499035,
          0.49090909123891896,
          0.4901185773578086,
          0.5043478264167846,
          0.49644268807686365,
          0.49011857740492687,
          0.4885375497369427,
          0.48537549444809264,
          0.4996047434599503,
          0.49090909123891896,
          0.4885375497369427,
          0.43715415052745654,
          0.49802371574484783,
          0.4885375497369427,
          0.49802371574484783,
          0.4853754944009743,
          0.49011857740492687,
          0.4901185774520452,
          0.49169960512002936,
          0.4956521739837209,
          0.4853754944009743,
          0.4869565220689585,
          0.4940711465748874,
          0.4861660082349664,
          0.49090909123891896,
          0.4948616604559978,
          0.49011857740492687,
          0.48142292527813213,
          0.4932806327408953,
          0.48537549444809264,
          0.48458498061410055
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 4",
         "type": "scatter",
         "uid": "9bcb899c-4f5d-4ec7-a421-89b3fc6fb34e",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.4940711465748874,
          0.4869565220689585,
          0.4940711465748874,
          0.4932806327408953,
          0.49486166036176116,
          0.49802371574484783,
          0.4924901189069032,
          0.4877470359029506,
          0.488537549784061,
          0.49644268807686365,
          0.49565217428998987,
          0.49723320191085574,
          0.4893280635709348,
          0.49090909093265006,
          0.4893280636180531,
          0.4877470359029506,
          0.4877470359500689,
          0.4869565221160768,
          0.4901185774520452,
          0.49565217428998987,
          0.46561264855117196,
          0.4885375497369427,
          0.49249011895402145,
          0.48458498061410055,
          0.4869565221160768,
          0.4861660082349664,
          0.49486166040887947,
          0.48063241109075283,
          0.49328063278801354,
          0.49249011860063424,
          0.49090909123891896
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "CNN test set accuracy of padded Transformer-XL dataset with variable maximum lengths"
        }
       }
      },
      "text/html": [
       "<div id=\"1d3ebf88-e0c0-4260-bb3c-20848c2519fa\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"1d3ebf88-e0c0-4260-bb3c-20848c2519fa\")) {\n",
       "    Plotly.newPlot(\"1d3ebf88-e0c0-4260-bb3c-20848c2519fa\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4924901189069032, 0.49644268802974534, 0.49011857740492687, 0.49090909123891896, 0.4869565221160768, 0.4869565220689585, 0.4869565220689585, 0.4885375497369427, 0.47826086989504546, 0.49644268812398196, 0.4893280636180531, 0.4924901189069032, 0.4940711465748874, 0.49169960507291105, 0.4932806327408953, 0.48379446673299015, 0.49169960512002936, 0.49090909093265006, 0.4885375497369427, 0.4735177868910929, 0.4924901189069032, 0.49802371574484783, 0.49723320191085574, 0.4861660082349664, 0.49249011895402145, 0.48458498056698224, 0.4774703561081717, 0.48379446673299015, 0.49090909123891896, 0.48300395289899806, 0.4940711466220057], \"type\": \"scatter\", \"uid\": \"c257b3ea-8e55-4456-91f6-ff60648fab8a\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.474308300725085, 0.49090909123891896, 0.49486166040887947, 0.48063241139702173, 0.4830039529461163, 0.4869565221160768, 0.49169960507291105, 0.4885375497369427, 0.48063241144414004, 0.48537549444809264, 0.4885375497369427, 0.4901185774520452, 0.4869565220689585, 0.4948616604559978, 0.4869565221160768, 0.49723320195797405, 0.49723320191085574, 0.4885375497369427, 0.4893280632646659, 0.49723320195797405, 0.4932806327408953, 0.4956521739366026, 0.48537549444809264, 0.49090909093265006, 0.4893280635709348, 0.4869565220689585, 0.47905138372903755, 0.48063241139702173, 0.49644268807686365, 0.49169960507291105, 0.4869565220689585], \"type\": \"scatter\", \"uid\": \"90ba27fd-68d4-47b8-9809-7cc72a513918\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4932806327408953, 0.49328063278801354, 0.49723320191085574, 0.48379446668587184, 0.499604743412832, 0.49169960507291105, 0.4901185774520452, 0.4861660082349664, 0.4837944667801084, 0.5043478264167846, 0.49090909123891896, 0.4853754944009743, 0.4845849802607133, 0.49565217428998987, 0.48379446673299015, 0.3541501976402381, 0.48537549444809264, 0.4901185774520452, 0.482213438758737, 0.49565217424287156, 0.4940711465748874, 0.4948616604559978, 0.4948616604559978, 0.4924901189069032, 0.45928853762008454, 0.49169960512002936, 0.4893280635709348, 0.4893280635709348, 0.48537549444809264, 0.4940711466220057, 0.4814229252310138], \"type\": \"scatter\", \"uid\": \"ae9f1300-7906-449a-b6f0-cff19f23d369\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4806324113499035, 0.49090909123891896, 0.4901185773578086, 0.5043478264167846, 0.49644268807686365, 0.49011857740492687, 0.4885375497369427, 0.48537549444809264, 0.4996047434599503, 0.49090909123891896, 0.4885375497369427, 0.43715415052745654, 0.49802371574484783, 0.4885375497369427, 0.49802371574484783, 0.4853754944009743, 0.49011857740492687, 0.4901185774520452, 0.49169960512002936, 0.4956521739837209, 0.4853754944009743, 0.4869565220689585, 0.4940711465748874, 0.4861660082349664, 0.49090909123891896, 0.4948616604559978, 0.49011857740492687, 0.48142292527813213, 0.4932806327408953, 0.48537549444809264, 0.48458498061410055], \"type\": \"scatter\", \"uid\": \"2559d646-5815-4893-bec9-edfc8c75779a\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4940711465748874, 0.4869565220689585, 0.4940711465748874, 0.4932806327408953, 0.49486166036176116, 0.49802371574484783, 0.4924901189069032, 0.4877470359029506, 0.488537549784061, 0.49644268807686365, 0.49565217428998987, 0.49723320191085574, 0.4893280635709348, 0.49090909093265006, 0.4893280636180531, 0.4877470359029506, 0.4877470359500689, 0.4869565221160768, 0.4901185774520452, 0.49565217428998987, 0.46561264855117196, 0.4885375497369427, 0.49249011895402145, 0.48458498061410055, 0.4869565221160768, 0.4861660082349664, 0.49486166040887947, 0.48063241109075283, 0.49328063278801354, 0.49249011860063424, 0.49090909123891896], \"type\": \"scatter\", \"uid\": \"9bcb899c-4f5d-4ec7-a421-89b3fc6fb34e\"}], {\"title\": {\"text\": \"CNN test set accuracy of padded Transformer-XL dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"1d3ebf88-e0c0-4260-bb3c-20848c2519fa\")) {window._Plotly.Plots.resize(document.getElementById(\"1d3ebf88-e0c0-4260-bb3c-20848c2519fa\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"1d3ebf88-e0c0-4260-bb3c-20848c2519fa\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"1d3ebf88-e0c0-4260-bb3c-20848c2519fa\")) {\n",
       "    Plotly.newPlot(\"1d3ebf88-e0c0-4260-bb3c-20848c2519fa\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4924901189069032, 0.49644268802974534, 0.49011857740492687, 0.49090909123891896, 0.4869565221160768, 0.4869565220689585, 0.4869565220689585, 0.4885375497369427, 0.47826086989504546, 0.49644268812398196, 0.4893280636180531, 0.4924901189069032, 0.4940711465748874, 0.49169960507291105, 0.4932806327408953, 0.48379446673299015, 0.49169960512002936, 0.49090909093265006, 0.4885375497369427, 0.4735177868910929, 0.4924901189069032, 0.49802371574484783, 0.49723320191085574, 0.4861660082349664, 0.49249011895402145, 0.48458498056698224, 0.4774703561081717, 0.48379446673299015, 0.49090909123891896, 0.48300395289899806, 0.4940711466220057], \"type\": \"scatter\", \"uid\": \"c257b3ea-8e55-4456-91f6-ff60648fab8a\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.474308300725085, 0.49090909123891896, 0.49486166040887947, 0.48063241139702173, 0.4830039529461163, 0.4869565221160768, 0.49169960507291105, 0.4885375497369427, 0.48063241144414004, 0.48537549444809264, 0.4885375497369427, 0.4901185774520452, 0.4869565220689585, 0.4948616604559978, 0.4869565221160768, 0.49723320195797405, 0.49723320191085574, 0.4885375497369427, 0.4893280632646659, 0.49723320195797405, 0.4932806327408953, 0.4956521739366026, 0.48537549444809264, 0.49090909093265006, 0.4893280635709348, 0.4869565220689585, 0.47905138372903755, 0.48063241139702173, 0.49644268807686365, 0.49169960507291105, 0.4869565220689585], \"type\": \"scatter\", \"uid\": \"90ba27fd-68d4-47b8-9809-7cc72a513918\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4932806327408953, 0.49328063278801354, 0.49723320191085574, 0.48379446668587184, 0.499604743412832, 0.49169960507291105, 0.4901185774520452, 0.4861660082349664, 0.4837944667801084, 0.5043478264167846, 0.49090909123891896, 0.4853754944009743, 0.4845849802607133, 0.49565217428998987, 0.48379446673299015, 0.3541501976402381, 0.48537549444809264, 0.4901185774520452, 0.482213438758737, 0.49565217424287156, 0.4940711465748874, 0.4948616604559978, 0.4948616604559978, 0.4924901189069032, 0.45928853762008454, 0.49169960512002936, 0.4893280635709348, 0.4893280635709348, 0.48537549444809264, 0.4940711466220057, 0.4814229252310138], \"type\": \"scatter\", \"uid\": \"ae9f1300-7906-449a-b6f0-cff19f23d369\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4806324113499035, 0.49090909123891896, 0.4901185773578086, 0.5043478264167846, 0.49644268807686365, 0.49011857740492687, 0.4885375497369427, 0.48537549444809264, 0.4996047434599503, 0.49090909123891896, 0.4885375497369427, 0.43715415052745654, 0.49802371574484783, 0.4885375497369427, 0.49802371574484783, 0.4853754944009743, 0.49011857740492687, 0.4901185774520452, 0.49169960512002936, 0.4956521739837209, 0.4853754944009743, 0.4869565220689585, 0.4940711465748874, 0.4861660082349664, 0.49090909123891896, 0.4948616604559978, 0.49011857740492687, 0.48142292527813213, 0.4932806327408953, 0.48537549444809264, 0.48458498061410055], \"type\": \"scatter\", \"uid\": \"2559d646-5815-4893-bec9-edfc8c75779a\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4940711465748874, 0.4869565220689585, 0.4940711465748874, 0.4932806327408953, 0.49486166036176116, 0.49802371574484783, 0.4924901189069032, 0.4877470359029506, 0.488537549784061, 0.49644268807686365, 0.49565217428998987, 0.49723320191085574, 0.4893280635709348, 0.49090909093265006, 0.4893280636180531, 0.4877470359029506, 0.4877470359500689, 0.4869565221160768, 0.4901185774520452, 0.49565217428998987, 0.46561264855117196, 0.4885375497369427, 0.49249011895402145, 0.48458498061410055, 0.4869565221160768, 0.4861660082349664, 0.49486166040887947, 0.48063241109075283, 0.49328063278801354, 0.49249011860063424, 0.49090909123891896], \"type\": \"scatter\", \"uid\": \"9bcb899c-4f5d-4ec7-a421-89b3fc6fb34e\"}], {\"title\": {\"text\": \"CNN test set accuracy of padded Transformer-XL dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"1d3ebf88-e0c0-4260-bb3c-20848c2519fa\")) {window._Plotly.Plots.resize(document.getElementById(\"1d3ebf88-e0c0-4260-bb3c-20848c2519fa\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traces = cnn_transformerxl_rounds\n",
    "\n",
    "# Create traces\n",
    "def create_scatter(counter):\n",
    "    acc_dict = traces[counter]\n",
    "    \n",
    "    return go.Scatter(\n",
    "        x = list(acc_dict.keys()),\n",
    "        y = list(acc_dict.values()),\n",
    "        mode = 'lines+markers',\n",
    "        name = 'Round ' + str(counter)\n",
    "    )\n",
    "\n",
    "trace_data = [create_scatter(trace) for trace in range(len(traces))]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'CNN test set accuracy of padded Transformer-XL dataset with variable maximum lengths',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = trace_data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair = data.get_flair()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_flair = {\n",
    "    dataset: [np.concatenate(np.array(statement)) for statement in flair[dataset].statement]\n",
    "    for dataset in flair.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "2019-06-11 19:18:17,751 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "2019-06-11 19:18:17,911 From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-06-11 19:18:18,420 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0588 - acc: 0.4339 - val_loss: 1.0350 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0556 - acc: 0.4383 - val_loss: 1.0333 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0522 - acc: 0.4456 - val_loss: 1.0287 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0479 - acc: 0.4498 - val_loss: 1.0272 - val_acc: 0.4938\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 159s 16ms/step - loss: 1.0458 - acc: 0.4579 - val_loss: 1.0301 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.46877470393425863\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0601 - acc: 0.4296 - val_loss: 1.0361 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 155s 15ms/step - loss: 1.0551 - acc: 0.4338 - val_loss: 1.0377 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 155s 15ms/step - loss: 1.0524 - acc: 0.4441 - val_loss: 1.0335 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 155s 15ms/step - loss: 1.0502 - acc: 0.4455 - val_loss: 1.0306 - val_acc: 0.4938\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 155s 15ms/step - loss: 1.0476 - acc: 0.4515 - val_loss: 1.0260 - val_acc: 0.4984\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.4592885378792352\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 153s 15ms/step - loss: 1.0598 - acc: 0.4326 - val_loss: 1.0409 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 154s 15ms/step - loss: 1.0538 - acc: 0.4426 - val_loss: 1.0339 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 154s 15ms/step - loss: 1.0495 - acc: 0.4455 - val_loss: 1.0282 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 155s 15ms/step - loss: 1.0483 - acc: 0.4526 - val_loss: 1.0256 - val_acc: 0.4984\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 155s 15ms/step - loss: 1.0446 - acc: 0.4592 - val_loss: 1.0221 - val_acc: 0.4984\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.4592885378792352\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 158s 15ms/step - loss: 1.0584 - acc: 0.4352 - val_loss: 1.0381 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 158s 15ms/step - loss: 1.0524 - acc: 0.4459 - val_loss: 1.0345 - val_acc: 0.4852\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 158s 15ms/step - loss: 1.0492 - acc: 0.4558 - val_loss: 1.0364 - val_acc: 0.4891\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 158s 15ms/step - loss: 1.0442 - acc: 0.4640 - val_loss: 1.0201 - val_acc: 0.4977\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 158s 15ms/step - loss: 1.0381 - acc: 0.4733 - val_loss: 1.0160 - val_acc: 0.4977\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.4727272730571008\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0599 - acc: 0.4313 - val_loss: 1.0478 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0528 - acc: 0.4450 - val_loss: 1.0354 - val_acc: 0.4790\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 155s 15ms/step - loss: 1.0478 - acc: 0.4501 - val_loss: 1.0264 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0418 - acc: 0.4614 - val_loss: 1.0279 - val_acc: 0.4875\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0384 - acc: 0.4700 - val_loss: 1.0183 - val_acc: 0.4914\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.47905138377615586\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0597 - acc: 0.4320 - val_loss: 1.0349 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0526 - acc: 0.4429 - val_loss: 1.0278 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0457 - acc: 0.4596 - val_loss: 1.0252 - val_acc: 0.4899\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 159s 16ms/step - loss: 1.0418 - acc: 0.4714 - val_loss: 1.0170 - val_acc: 0.4961\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0359 - acc: 0.4772 - val_loss: 1.0141 - val_acc: 0.4992\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.4885375497369427\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0589 - acc: 0.4296 - val_loss: 1.0395 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0517 - acc: 0.4437 - val_loss: 1.0268 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0457 - acc: 0.4595 - val_loss: 1.0216 - val_acc: 0.4821\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0404 - acc: 0.4698 - val_loss: 1.0201 - val_acc: 0.4969\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0358 - acc: 0.4782 - val_loss: 1.0149 - val_acc: 0.4961\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.4869565221160768\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0601 - acc: 0.4282 - val_loss: 1.0337 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0527 - acc: 0.4457 - val_loss: 1.0300 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0465 - acc: 0.4546 - val_loss: 1.0258 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0391 - acc: 0.4694 - val_loss: 1.0247 - val_acc: 0.4953\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0363 - acc: 0.4775 - val_loss: 1.0177 - val_acc: 0.4930\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.488537549784061\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 176s 17ms/step - loss: 1.0606 - acc: 0.4250 - val_loss: 1.0370 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0523 - acc: 0.4494 - val_loss: 1.0304 - val_acc: 0.4813\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0458 - acc: 0.4583 - val_loss: 1.0204 - val_acc: 0.4821\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0395 - acc: 0.4711 - val_loss: 1.0186 - val_acc: 0.4969\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0342 - acc: 0.4822 - val_loss: 1.0198 - val_acc: 0.4883\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.49090909093265006\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 180s 18ms/step - loss: 1.0595 - acc: 0.4346 - val_loss: 1.0380 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0508 - acc: 0.4483 - val_loss: 1.0256 - val_acc: 0.4860\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0441 - acc: 0.4596 - val_loss: 1.0298 - val_acc: 0.4891\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0374 - acc: 0.4783 - val_loss: 1.0154 - val_acc: 0.4953\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 173s 17ms/step - loss: 1.0303 - acc: 0.4886 - val_loss: 1.0226 - val_acc: 0.5008\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.4830039526398474\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 183s 18ms/step - loss: 1.0612 - acc: 0.4325 - val_loss: 1.0404 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0526 - acc: 0.4468 - val_loss: 1.0275 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0467 - acc: 0.4573 - val_loss: 1.0212 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0393 - acc: 0.4735 - val_loss: 1.0162 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0342 - acc: 0.4821 - val_loss: 1.0152 - val_acc: 0.4953\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.4885375497369427\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 185s 18ms/step - loss: 1.0596 - acc: 0.4368 - val_loss: 1.0427 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 176s 17ms/step - loss: 1.0513 - acc: 0.4425 - val_loss: 1.0257 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0458 - acc: 0.4582 - val_loss: 1.0220 - val_acc: 0.5109\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0360 - acc: 0.4723 - val_loss: 1.0154 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0304 - acc: 0.4885 - val_loss: 1.0052 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.4893280635709348\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 191s 19ms/step - loss: 1.0600 - acc: 0.4280 - val_loss: 1.0380 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0513 - acc: 0.4507 - val_loss: 1.0327 - val_acc: 0.5047\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0437 - acc: 0.4596 - val_loss: 1.0223 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0353 - acc: 0.4760 - val_loss: 1.0376 - val_acc: 0.4603\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0290 - acc: 0.4837 - val_loss: 1.0066 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.49644268802974534\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 192s 19ms/step - loss: 1.0596 - acc: 0.4360 - val_loss: 1.0375 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0508 - acc: 0.4489 - val_loss: 1.0279 - val_acc: 0.4860\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0456 - acc: 0.4595 - val_loss: 1.0229 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0362 - acc: 0.4722 - val_loss: 1.0213 - val_acc: 0.4883\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0306 - acc: 0.4867 - val_loss: 1.0255 - val_acc: 0.4813\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.4790513835170052\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 193s 19ms/step - loss: 1.0585 - acc: 0.4349 - val_loss: 1.0390 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0527 - acc: 0.4466 - val_loss: 1.0296 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0439 - acc: 0.4603 - val_loss: 1.0157 - val_acc: 0.4922\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0362 - acc: 0.4750 - val_loss: 1.0180 - val_acc: 0.4875\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0281 - acc: 0.4847 - val_loss: 1.0239 - val_acc: 0.4953\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.485375494188942\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 196s 19ms/step - loss: 1.0611 - acc: 0.4308 - val_loss: 1.0376 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0508 - acc: 0.4451 - val_loss: 1.0293 - val_acc: 0.4875\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0449 - acc: 0.4622 - val_loss: 1.0239 - val_acc: 0.4969\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0354 - acc: 0.4732 - val_loss: 1.0191 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0306 - acc: 0.4823 - val_loss: 1.0089 - val_acc: 0.5008\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.493280632693777\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 198s 19ms/step - loss: 1.0603 - acc: 0.4289 - val_loss: 1.0352 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 173s 17ms/step - loss: 1.0515 - acc: 0.4431 - val_loss: 1.0298 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 173s 17ms/step - loss: 1.0437 - acc: 0.4564 - val_loss: 1.0176 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 173s 17ms/step - loss: 1.0367 - acc: 0.4786 - val_loss: 1.0083 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 173s 17ms/step - loss: 1.0299 - acc: 0.4797 - val_loss: 1.0062 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "0.49644268802974534\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 201s 20ms/step - loss: 1.0616 - acc: 0.4297 - val_loss: 1.0456 - val_acc: 0.4782\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0528 - acc: 0.4438 - val_loss: 1.0397 - val_acc: 0.4751\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0457 - acc: 0.4580 - val_loss: 1.0201 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0382 - acc: 0.4708 - val_loss: 1.0264 - val_acc: 0.4883\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0306 - acc: 0.4877 - val_loss: 1.0050 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "0.48458498056698224\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 204s 20ms/step - loss: 1.0625 - acc: 0.4293 - val_loss: 1.0375 - val_acc: 0.4836\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0514 - acc: 0.4473 - val_loss: 1.0245 - val_acc: 0.4821\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0430 - acc: 0.4579 - val_loss: 1.0366 - val_acc: 0.4463\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0369 - acc: 0.4762 - val_loss: 1.0174 - val_acc: 0.5000\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0282 - acc: 0.4889 - val_loss: 1.0052 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "0.4877470359029506\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 207s 20ms/step - loss: 1.0608 - acc: 0.4303 - val_loss: 1.0333 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0510 - acc: 0.4463 - val_loss: 1.0327 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0409 - acc: 0.4671 - val_loss: 1.0239 - val_acc: 0.4969\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0338 - acc: 0.4831 - val_loss: 1.0075 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0273 - acc: 0.4826 - val_loss: 1.0130 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "0.49169960507291105\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 210s 21ms/step - loss: 1.0611 - acc: 0.4362 - val_loss: 1.0365 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 175s 17ms/step - loss: 1.0518 - acc: 0.4479 - val_loss: 1.0389 - val_acc: 0.4704\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 175s 17ms/step - loss: 1.0445 - acc: 0.4570 - val_loss: 1.0195 - val_acc: 0.5016\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0361 - acc: 0.4736 - val_loss: 1.0149 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 173s 17ms/step - loss: 1.0285 - acc: 0.4872 - val_loss: 1.0041 - val_acc: 0.5132\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "0.5035573125827925\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 220s 21ms/step - loss: 1.0616 - acc: 0.4287 - val_loss: 1.0385 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 178s 17ms/step - loss: 1.0506 - acc: 0.4465 - val_loss: 1.0300 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 178s 17ms/step - loss: 1.0417 - acc: 0.4658 - val_loss: 1.0208 - val_acc: 0.4836\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 178s 17ms/step - loss: 1.0363 - acc: 0.4730 - val_loss: 1.0096 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 178s 17ms/step - loss: 1.0293 - acc: 0.4819 - val_loss: 1.0027 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 12s 9ms/step\n",
      "0.5035573125827925\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 215s 21ms/step - loss: 1.0599 - acc: 0.4343 - val_loss: 1.0335 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 178s 17ms/step - loss: 1.0516 - acc: 0.4435 - val_loss: 1.0284 - val_acc: 0.4922\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 179s 17ms/step - loss: 1.0439 - acc: 0.4586 - val_loss: 1.0210 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0358 - acc: 0.4748 - val_loss: 1.0118 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 178s 17ms/step - loss: 1.0291 - acc: 0.4873 - val_loss: 1.0037 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "0.4853754944009743\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 219s 21ms/step - loss: 1.0597 - acc: 0.4361 - val_loss: 1.0410 - val_acc: 0.4868\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 180s 18ms/step - loss: 1.0523 - acc: 0.4413 - val_loss: 1.0262 - val_acc: 0.4790\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 180s 18ms/step - loss: 1.0439 - acc: 0.4572 - val_loss: 1.0170 - val_acc: 0.4883\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 180s 18ms/step - loss: 1.0361 - acc: 0.4690 - val_loss: 1.0139 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 180s 18ms/step - loss: 1.0288 - acc: 0.4841 - val_loss: 1.0129 - val_acc: 0.4899\n",
      "1265/1265 [==============================] - 12s 9ms/step\n",
      "0.4450592888673775\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 221s 22ms/step - loss: 1.0619 - acc: 0.4276 - val_loss: 1.0480 - val_acc: 0.4548\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0514 - acc: 0.4410 - val_loss: 1.0310 - val_acc: 0.4922\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0437 - acc: 0.4597 - val_loss: 1.0188 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 183s 18ms/step - loss: 1.0356 - acc: 0.4720 - val_loss: 1.0308 - val_acc: 0.4766\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0295 - acc: 0.4811 - val_loss: 1.0076 - val_acc: 0.5055\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.5003952572939424\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 225s 22ms/step - loss: 1.0612 - acc: 0.4320 - val_loss: 1.0363 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0533 - acc: 0.4420 - val_loss: 1.0305 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 183s 18ms/step - loss: 1.0429 - acc: 0.4603 - val_loss: 1.0244 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0338 - acc: 0.4808 - val_loss: 1.0174 - val_acc: 0.5031\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0286 - acc: 0.4830 - val_loss: 1.0111 - val_acc: 0.5070\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.505138340297895\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 235s 23ms/step - loss: 1.0600 - acc: 0.4359 - val_loss: 1.0363 - val_acc: 0.4844\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 193s 19ms/step - loss: 1.0518 - acc: 0.4417 - val_loss: 1.0241 - val_acc: 0.4836\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 191s 19ms/step - loss: 1.0412 - acc: 0.4690 - val_loss: 1.0271 - val_acc: 0.4844\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 193s 19ms/step - loss: 1.0335 - acc: 0.4794 - val_loss: 1.0099 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 193s 19ms/step - loss: 1.0271 - acc: 0.4913 - val_loss: 1.0044 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 13s 11ms/step\n",
      "0.5067193678716426\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 230s 22ms/step - loss: 1.0629 - acc: 0.4245 - val_loss: 1.0358 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 189s 18ms/step - loss: 1.0503 - acc: 0.4481 - val_loss: 1.0267 - val_acc: 0.4821\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 187s 18ms/step - loss: 1.0427 - acc: 0.4625 - val_loss: 1.0211 - val_acc: 0.5117\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 187s 18ms/step - loss: 1.0351 - acc: 0.4758 - val_loss: 1.0098 - val_acc: 0.4992\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 188s 18ms/step - loss: 1.0290 - acc: 0.4844 - val_loss: 1.0181 - val_acc: 0.5008\n",
      "1265/1265 [==============================] - 12s 10ms/step\n",
      "0.49723320195797405\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 238s 23ms/step - loss: 1.0619 - acc: 0.4286 - val_loss: 1.0414 - val_acc: 0.4836\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 193s 19ms/step - loss: 1.0513 - acc: 0.4429 - val_loss: 1.0297 - val_acc: 0.4914\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 190s 19ms/step - loss: 1.0423 - acc: 0.4573 - val_loss: 1.0164 - val_acc: 0.4852\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 190s 19ms/step - loss: 1.0362 - acc: 0.4740 - val_loss: 1.0203 - val_acc: 0.5023\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 189s 18ms/step - loss: 1.0284 - acc: 0.4882 - val_loss: 1.0033 - val_acc: 0.5195\n",
      "1265/1265 [==============================] - 13s 11ms/step\n",
      "0.5003952571997058\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 238s 23ms/step - loss: 1.0603 - acc: 0.4306 - val_loss: 1.0388 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 193s 19ms/step - loss: 1.0485 - acc: 0.4484 - val_loss: 1.0235 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 190s 19ms/step - loss: 1.0400 - acc: 0.4636 - val_loss: 1.0159 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 191s 19ms/step - loss: 1.0313 - acc: 0.4841 - val_loss: 1.0052 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 190s 19ms/step - loss: 1.0237 - acc: 0.4908 - val_loss: 1.0046 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 13s 11ms/step\n",
      "0.49723320195797405\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 238s 23ms/step - loss: 1.0606 - acc: 0.4367 - val_loss: 1.0377 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 195s 19ms/step - loss: 1.0484 - acc: 0.4531 - val_loss: 1.0253 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 191s 19ms/step - loss: 1.0410 - acc: 0.4700 - val_loss: 1.0124 - val_acc: 0.4984\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 195s 19ms/step - loss: 1.0326 - acc: 0.4822 - val_loss: 1.0170 - val_acc: 0.5148\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 192s 19ms/step - loss: 1.0265 - acc: 0.4893 - val_loss: 1.0016 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.4940711465748874\n",
      "{5: 0.46877470393425863, 6: 0.4592885378792352, 7: 0.4592885378792352, 8: 0.4727272730571008, 9: 0.47905138377615586, 10: 0.4885375497369427, 11: 0.4869565221160768, 12: 0.488537549784061, 13: 0.49090909093265006, 14: 0.4830039526398474, 15: 0.4885375497369427, 16: 0.4893280635709348, 17: 0.49644268802974534, 18: 0.4790513835170052, 19: 0.485375494188942, 20: 0.493280632693777, 21: 0.49644268802974534, 22: 0.48458498056698224, 23: 0.4877470359029506, 24: 0.49169960507291105, 25: 0.5035573125827925, 26: 0.5035573125827925, 27: 0.4853754944009743, 28: 0.4450592888673775, 29: 0.5003952572939424, 30: 0.505138340297895, 31: 0.5067193678716426, 32: 0.49723320195797405, 33: 0.5003952571997058, 34: 0.49723320195797405, 35: 0.4940711465748874}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0597 - acc: 0.4311 - val_loss: 1.0372 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0545 - acc: 0.4399 - val_loss: 1.0354 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 158s 15ms/step - loss: 1.0530 - acc: 0.4412 - val_loss: 1.0386 - val_acc: 0.4969\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 158s 15ms/step - loss: 1.0496 - acc: 0.4484 - val_loss: 1.0272 - val_acc: 0.4883\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 158s 15ms/step - loss: 1.0454 - acc: 0.4543 - val_loss: 1.0261 - val_acc: 0.4953\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.46482213471717987\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 157s 15ms/step - loss: 1.0583 - acc: 0.4318 - val_loss: 1.0363 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0544 - acc: 0.4374 - val_loss: 1.0377 - val_acc: 0.4813\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0518 - acc: 0.4452 - val_loss: 1.0357 - val_acc: 0.4821\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0487 - acc: 0.4474 - val_loss: 1.0278 - val_acc: 0.4930\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 155s 15ms/step - loss: 1.0450 - acc: 0.4609 - val_loss: 1.0226 - val_acc: 0.4914\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.4569169963772589\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0597 - acc: 0.4320 - val_loss: 1.0376 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0545 - acc: 0.4395 - val_loss: 1.0323 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0515 - acc: 0.4437 - val_loss: 1.0274 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0471 - acc: 0.4564 - val_loss: 1.0249 - val_acc: 0.4984\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0442 - acc: 0.4592 - val_loss: 1.0225 - val_acc: 0.5000\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.4687747038871403\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 157s 15ms/step - loss: 1.0603 - acc: 0.4315 - val_loss: 1.0361 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 155s 15ms/step - loss: 1.0524 - acc: 0.4413 - val_loss: 1.0305 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 155s 15ms/step - loss: 1.0491 - acc: 0.4489 - val_loss: 1.0316 - val_acc: 0.4953\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 154s 15ms/step - loss: 1.0458 - acc: 0.4603 - val_loss: 1.0224 - val_acc: 0.4930\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 155s 15ms/step - loss: 1.0397 - acc: 0.4686 - val_loss: 1.0180 - val_acc: 0.4961\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "0.44743083036935377\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0597 - acc: 0.4316 - val_loss: 1.0390 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0531 - acc: 0.4428 - val_loss: 1.0312 - val_acc: 0.4836\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0475 - acc: 0.4524 - val_loss: 1.0284 - val_acc: 0.4945\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0415 - acc: 0.4698 - val_loss: 1.0204 - val_acc: 0.4945\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0373 - acc: 0.4752 - val_loss: 1.0145 - val_acc: 0.5023\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.4640316208831878\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0600 - acc: 0.4286 - val_loss: 1.0397 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 159s 16ms/step - loss: 1.0535 - acc: 0.4429 - val_loss: 1.0298 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0486 - acc: 0.4506 - val_loss: 1.0310 - val_acc: 0.4961\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0441 - acc: 0.4596 - val_loss: 1.0216 - val_acc: 0.4891\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0397 - acc: 0.4682 - val_loss: 1.0210 - val_acc: 0.4953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.4877470359500689\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0567 - acc: 0.4334 - val_loss: 1.0398 - val_acc: 0.4844\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0528 - acc: 0.4475 - val_loss: 1.0303 - val_acc: 0.4868\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0467 - acc: 0.4610 - val_loss: 1.0265 - val_acc: 0.4977\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0398 - acc: 0.4736 - val_loss: 1.0181 - val_acc: 0.4977\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0337 - acc: 0.4769 - val_loss: 1.0219 - val_acc: 0.4984\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.4893280636180531\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 178s 17ms/step - loss: 1.0592 - acc: 0.4347 - val_loss: 1.0395 - val_acc: 0.4844\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0515 - acc: 0.4442 - val_loss: 1.0306 - val_acc: 0.4945\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0469 - acc: 0.4596 - val_loss: 1.0247 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0383 - acc: 0.4730 - val_loss: 1.0186 - val_acc: 0.4984\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0341 - acc: 0.4762 - val_loss: 1.0203 - val_acc: 0.4899\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "0.47588932808680023\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 180s 18ms/step - loss: 1.0593 - acc: 0.4353 - val_loss: 1.0360 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0507 - acc: 0.4496 - val_loss: 1.0357 - val_acc: 0.4977\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0453 - acc: 0.4589 - val_loss: 1.0296 - val_acc: 0.5016\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0395 - acc: 0.4750 - val_loss: 1.0128 - val_acc: 0.4977\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0333 - acc: 0.4816 - val_loss: 1.0149 - val_acc: 0.5023\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.49169960512002936\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 186s 18ms/step - loss: 1.0587 - acc: 0.4346 - val_loss: 1.0406 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0523 - acc: 0.4457 - val_loss: 1.0383 - val_acc: 0.4930\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0460 - acc: 0.4579 - val_loss: 1.0253 - val_acc: 0.4984\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0376 - acc: 0.4725 - val_loss: 1.0234 - val_acc: 0.4922\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0343 - acc: 0.4839 - val_loss: 1.0096 - val_acc: 0.5055\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.488537549784061\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 189s 18ms/step - loss: 1.0593 - acc: 0.4347 - val_loss: 1.0349 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0511 - acc: 0.4486 - val_loss: 1.0306 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0432 - acc: 0.4660 - val_loss: 1.0254 - val_acc: 0.4969\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0374 - acc: 0.4759 - val_loss: 1.0101 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0293 - acc: 0.4862 - val_loss: 1.0158 - val_acc: 0.4977\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.4940711462686184\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 188s 18ms/step - loss: 1.0599 - acc: 0.4269 - val_loss: 1.0361 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0526 - acc: 0.4469 - val_loss: 1.0321 - val_acc: 0.4813\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0462 - acc: 0.4559 - val_loss: 1.0192 - val_acc: 0.4813\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0395 - acc: 0.4746 - val_loss: 1.0134 - val_acc: 0.5000\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0310 - acc: 0.4845 - val_loss: 1.0112 - val_acc: 0.4984\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.5051383402036584\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 190s 19ms/step - loss: 1.0608 - acc: 0.4307 - val_loss: 1.0349 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0515 - acc: 0.4401 - val_loss: 1.0286 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0454 - acc: 0.4582 - val_loss: 1.0298 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0394 - acc: 0.4682 - val_loss: 1.0135 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0327 - acc: 0.4829 - val_loss: 1.0092 - val_acc: 0.5055\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5011857710808162\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 194s 19ms/step - loss: 1.0599 - acc: 0.4345 - val_loss: 1.0353 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0488 - acc: 0.4483 - val_loss: 1.0253 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0448 - acc: 0.4630 - val_loss: 1.0269 - val_acc: 0.4899\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0356 - acc: 0.4797 - val_loss: 1.0250 - val_acc: 0.4922\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0290 - acc: 0.4868 - val_loss: 1.0162 - val_acc: 0.5000\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.4948616604559978\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 197s 19ms/step - loss: 1.0601 - acc: 0.4295 - val_loss: 1.0393 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0523 - acc: 0.4432 - val_loss: 1.0316 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0432 - acc: 0.4629 - val_loss: 1.0285 - val_acc: 0.4805\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0356 - acc: 0.4719 - val_loss: 1.0145 - val_acc: 0.5031\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0306 - acc: 0.4834 - val_loss: 1.0059 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.48458498056698224\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 199s 19ms/step - loss: 1.0592 - acc: 0.4343 - val_loss: 1.0337 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 173s 17ms/step - loss: 1.0511 - acc: 0.4453 - val_loss: 1.0305 - val_acc: 0.4875\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0419 - acc: 0.4646 - val_loss: 1.0210 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0338 - acc: 0.4801 - val_loss: 1.0087 - val_acc: 0.4961\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0270 - acc: 0.4892 - val_loss: 1.0166 - val_acc: 0.4945\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5059288540376505\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 202s 20ms/step - loss: 1.0611 - acc: 0.4362 - val_loss: 1.0376 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0512 - acc: 0.4491 - val_loss: 1.0336 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0447 - acc: 0.4597 - val_loss: 1.0201 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0371 - acc: 0.4745 - val_loss: 1.0144 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0285 - acc: 0.4832 - val_loss: 1.0275 - val_acc: 0.4774\n",
      "1265/1265 [==============================] - 11s 8ms/step\n",
      "0.47509881434704476\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 206s 20ms/step - loss: 1.0607 - acc: 0.4344 - val_loss: 1.0339 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0519 - acc: 0.4387 - val_loss: 1.0323 - val_acc: 0.4907\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0431 - acc: 0.4620 - val_loss: 1.0181 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0353 - acc: 0.4757 - val_loss: 1.0225 - val_acc: 0.4914\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0302 - acc: 0.4821 - val_loss: 1.0052 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 11s 8ms/step\n",
      "0.5106719370416031\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 208s 20ms/step - loss: 1.0624 - acc: 0.4316 - val_loss: 1.0385 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 175s 17ms/step - loss: 1.0508 - acc: 0.4453 - val_loss: 1.0341 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 173s 17ms/step - loss: 1.0446 - acc: 0.4531 - val_loss: 1.0213 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0358 - acc: 0.4714 - val_loss: 1.0110 - val_acc: 0.5179\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 175s 17ms/step - loss: 1.0291 - acc: 0.4831 - val_loss: 1.0215 - val_acc: 0.4969\n",
      "1265/1265 [==============================] - 11s 8ms/step\n",
      "0.49249011895402145\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 209s 20ms/step - loss: 1.0623 - acc: 0.4332 - val_loss: 1.0413 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 175s 17ms/step - loss: 1.0517 - acc: 0.4452 - val_loss: 1.0304 - val_acc: 0.4899\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0453 - acc: 0.4537 - val_loss: 1.0181 - val_acc: 0.4961\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 175s 17ms/step - loss: 1.0389 - acc: 0.4719 - val_loss: 1.0111 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0290 - acc: 0.4871 - val_loss: 1.0050 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "0.49090909123891896\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 213s 21ms/step - loss: 1.0615 - acc: 0.4315 - val_loss: 1.0401 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0504 - acc: 0.4481 - val_loss: 1.0270 - val_acc: 0.4875\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 176s 17ms/step - loss: 1.0431 - acc: 0.4656 - val_loss: 1.0272 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 175s 17ms/step - loss: 1.0363 - acc: 0.4724 - val_loss: 1.0117 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 176s 17ms/step - loss: 1.0282 - acc: 0.4863 - val_loss: 1.0071 - val_acc: 0.5039\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.5043478264639029\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 217s 21ms/step - loss: 1.0619 - acc: 0.4308 - val_loss: 1.0438 - val_acc: 0.4868\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 176s 17ms/step - loss: 1.0503 - acc: 0.4497 - val_loss: 1.0260 - val_acc: 0.4813\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0442 - acc: 0.4550 - val_loss: 1.0377 - val_acc: 0.4961\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 176s 17ms/step - loss: 1.0341 - acc: 0.4796 - val_loss: 1.0151 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0303 - acc: 0.4842 - val_loss: 1.0202 - val_acc: 0.5008\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "0.4845849803078316\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 228s 22ms/step - loss: 1.0618 - acc: 0.4308 - val_loss: 1.0445 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 175s 17ms/step - loss: 1.0512 - acc: 0.4478 - val_loss: 1.0295 - val_acc: 0.4922\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0446 - acc: 0.4569 - val_loss: 1.0261 - val_acc: 0.4953\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0360 - acc: 0.4743 - val_loss: 1.0216 - val_acc: 0.4883\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 176s 17ms/step - loss: 1.0304 - acc: 0.4830 - val_loss: 1.0049 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 12s 10ms/step\n",
      "0.49090909123891896\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 225s 22ms/step - loss: 1.0598 - acc: 0.4302 - val_loss: 1.0355 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0497 - acc: 0.4480 - val_loss: 1.0280 - val_acc: 0.4945\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0439 - acc: 0.4636 - val_loss: 1.0264 - val_acc: 0.4836\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 182s 18ms/step - loss: 1.0336 - acc: 0.4809 - val_loss: 1.0069 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 183s 18ms/step - loss: 1.0281 - acc: 0.4888 - val_loss: 1.0058 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.474308300725085\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 229s 22ms/step - loss: 1.0610 - acc: 0.4317 - val_loss: 1.0479 - val_acc: 0.4611\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0512 - acc: 0.4440 - val_loss: 1.0251 - val_acc: 0.4852\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0442 - acc: 0.4618 - val_loss: 1.0149 - val_acc: 0.5016\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 182s 18ms/step - loss: 1.0345 - acc: 0.4745 - val_loss: 1.0115 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 183s 18ms/step - loss: 1.0295 - acc: 0.4872 - val_loss: 1.0054 - val_acc: 0.5055\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.5043478264639029\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 232s 23ms/step - loss: 1.0609 - acc: 0.4289 - val_loss: 1.0344 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 185s 18ms/step - loss: 1.0504 - acc: 0.4474 - val_loss: 1.0267 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 185s 18ms/step - loss: 1.0430 - acc: 0.4646 - val_loss: 1.0338 - val_acc: 0.4821\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 185s 18ms/step - loss: 1.0347 - acc: 0.4787 - val_loss: 1.0246 - val_acc: 0.4782\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 185s 18ms/step - loss: 1.0301 - acc: 0.4887 - val_loss: 1.0030 - val_acc: 0.5101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.5019762849619266\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 244s 24ms/step - loss: 1.0605 - acc: 0.4313 - val_loss: 1.0328 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 195s 19ms/step - loss: 1.0512 - acc: 0.4460 - val_loss: 1.0340 - val_acc: 0.5086\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 193s 19ms/step - loss: 1.0407 - acc: 0.4617 - val_loss: 1.0315 - val_acc: 0.4759\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 193s 19ms/step - loss: 1.0313 - acc: 0.4839 - val_loss: 1.0155 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 193s 19ms/step - loss: 1.0271 - acc: 0.4869 - val_loss: 1.0070 - val_acc: 0.5039\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.47984189756302964\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 244s 24ms/step - loss: 1.0606 - acc: 0.4350 - val_loss: 1.0351 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 189s 18ms/step - loss: 1.0515 - acc: 0.4469 - val_loss: 1.0352 - val_acc: 0.4992\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 187s 18ms/step - loss: 1.0425 - acc: 0.4626 - val_loss: 1.0241 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 188s 18ms/step - loss: 1.0332 - acc: 0.4807 - val_loss: 1.0153 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 188s 18ms/step - loss: 1.0267 - acc: 0.4872 - val_loss: 1.0091 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.5083003955867451\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 241s 24ms/step - loss: 1.0600 - acc: 0.4327 - val_loss: 1.0330 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 191s 19ms/step - loss: 1.0509 - acc: 0.4457 - val_loss: 1.0347 - val_acc: 0.4961\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 191s 19ms/step - loss: 1.0421 - acc: 0.4650 - val_loss: 1.0225 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 191s 19ms/step - loss: 1.0344 - acc: 0.4800 - val_loss: 1.0125 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 189s 18ms/step - loss: 1.0280 - acc: 0.4892 - val_loss: 1.0064 - val_acc: 0.5031\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.47747035606105337\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 243s 24ms/step - loss: 1.0616 - acc: 0.4310 - val_loss: 1.0378 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 194s 19ms/step - loss: 1.0515 - acc: 0.4469 - val_loss: 1.0263 - val_acc: 0.4829\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 193s 19ms/step - loss: 1.0410 - acc: 0.4631 - val_loss: 1.0193 - val_acc: 0.4938\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 191s 19ms/step - loss: 1.0332 - acc: 0.4753 - val_loss: 1.0086 - val_acc: 0.5202\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 199s 19ms/step - loss: 1.0272 - acc: 0.4858 - val_loss: 1.0067 - val_acc: 0.5148\n",
      "1265/1265 [==============================] - 13s 11ms/step\n",
      "0.5075098817056347\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 248s 24ms/step - loss: 1.0597 - acc: 0.4352 - val_loss: 1.0333 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 210s 20ms/step - loss: 1.0506 - acc: 0.4464 - val_loss: 1.0334 - val_acc: 0.4945\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 203s 20ms/step - loss: 1.0448 - acc: 0.4586 - val_loss: 1.0257 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 196s 19ms/step - loss: 1.0374 - acc: 0.4779 - val_loss: 1.0107 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 200s 20ms/step - loss: 1.0290 - acc: 0.4904 - val_loss: 1.0094 - val_acc: 0.5016\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.4529644272072984\n",
      "{5: 0.46482213471717987, 6: 0.4569169963772589, 7: 0.4687747038871403, 8: 0.44743083036935377, 9: 0.4640316208831878, 10: 0.4877470359500689, 11: 0.4893280636180531, 12: 0.47588932808680023, 13: 0.49169960512002936, 14: 0.488537549784061, 15: 0.4940711462686184, 16: 0.5051383402036584, 17: 0.5011857710808162, 18: 0.4948616604559978, 19: 0.48458498056698224, 20: 0.5059288540376505, 21: 0.47509881434704476, 22: 0.5106719370416031, 23: 0.49249011895402145, 24: 0.49090909123891896, 25: 0.5043478264639029, 26: 0.4845849803078316, 27: 0.49090909123891896, 28: 0.474308300725085, 29: 0.5043478264639029, 30: 0.5019762849619266, 31: 0.47984189756302964, 32: 0.5083003955867451, 33: 0.47747035606105337, 34: 0.5075098817056347, 35: 0.4529644272072984}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0605 - acc: 0.4305 - val_loss: 1.0364 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0552 - acc: 0.4388 - val_loss: 1.0384 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0517 - acc: 0.4416 - val_loss: 1.0349 - val_acc: 0.4821\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0498 - acc: 0.4493 - val_loss: 1.0319 - val_acc: 0.4883\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0467 - acc: 0.4539 - val_loss: 1.0262 - val_acc: 0.4969\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.4577075101641327\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 159s 16ms/step - loss: 1.0592 - acc: 0.4304 - val_loss: 1.0421 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 157s 15ms/step - loss: 1.0547 - acc: 0.4378 - val_loss: 1.0348 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 157s 15ms/step - loss: 1.0510 - acc: 0.4433 - val_loss: 1.0400 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 157s 15ms/step - loss: 1.0479 - acc: 0.4518 - val_loss: 1.0264 - val_acc: 0.4961\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 158s 15ms/step - loss: 1.0444 - acc: 0.4626 - val_loss: 1.0292 - val_acc: 0.4945\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.46166007938121145\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 165s 16ms/step - loss: 1.0599 - acc: 0.4356 - val_loss: 1.0414 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0535 - acc: 0.4388 - val_loss: 1.0345 - val_acc: 0.4813\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 158s 15ms/step - loss: 1.0488 - acc: 0.4498 - val_loss: 1.0309 - val_acc: 0.4907\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 155s 15ms/step - loss: 1.0471 - acc: 0.4546 - val_loss: 1.0237 - val_acc: 0.4945\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0422 - acc: 0.4688 - val_loss: 1.0241 - val_acc: 0.4953\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.49565217424287156\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0599 - acc: 0.4342 - val_loss: 1.0376 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 159s 16ms/step - loss: 1.0533 - acc: 0.4404 - val_loss: 1.0310 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0496 - acc: 0.4489 - val_loss: 1.0313 - val_acc: 0.4969\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0456 - acc: 0.4608 - val_loss: 1.0260 - val_acc: 0.4969\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0407 - acc: 0.4708 - val_loss: 1.0190 - val_acc: 0.5008\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.47667984217994297\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0602 - acc: 0.4335 - val_loss: 1.0375 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0528 - acc: 0.4446 - val_loss: 1.0406 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0497 - acc: 0.4501 - val_loss: 1.0334 - val_acc: 0.4969\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0445 - acc: 0.4561 - val_loss: 1.0223 - val_acc: 0.4930\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0411 - acc: 0.4702 - val_loss: 1.0235 - val_acc: 0.4945\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.4758893284401875\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0599 - acc: 0.4326 - val_loss: 1.0410 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0532 - acc: 0.4432 - val_loss: 1.0369 - val_acc: 0.4930\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0484 - acc: 0.4515 - val_loss: 1.0242 - val_acc: 0.4914\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0416 - acc: 0.4667 - val_loss: 1.0279 - val_acc: 0.4907\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0365 - acc: 0.4763 - val_loss: 1.0175 - val_acc: 0.4930\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.4893280636180531\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0600 - acc: 0.4276 - val_loss: 1.0350 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 165s 16ms/step - loss: 1.0531 - acc: 0.4416 - val_loss: 1.0301 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 165s 16ms/step - loss: 1.0473 - acc: 0.4518 - val_loss: 1.0337 - val_acc: 0.4907\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0421 - acc: 0.4646 - val_loss: 1.0185 - val_acc: 0.4969\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0372 - acc: 0.4746 - val_loss: 1.0148 - val_acc: 0.4938\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.47747035606105337\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 180s 18ms/step - loss: 1.0592 - acc: 0.4341 - val_loss: 1.0338 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0517 - acc: 0.4476 - val_loss: 1.0277 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0462 - acc: 0.4609 - val_loss: 1.0273 - val_acc: 0.4945\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0392 - acc: 0.4746 - val_loss: 1.0146 - val_acc: 0.4961\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0334 - acc: 0.4808 - val_loss: 1.0271 - val_acc: 0.4899\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.4418972335314091\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0590 - acc: 0.4340 - val_loss: 1.0323 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0510 - acc: 0.4522 - val_loss: 1.0283 - val_acc: 0.4829\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0456 - acc: 0.4547 - val_loss: 1.0412 - val_acc: 0.4681\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0363 - acc: 0.4776 - val_loss: 1.0139 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0331 - acc: 0.4816 - val_loss: 1.0109 - val_acc: 0.4938\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.49723320191085574\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 183s 18ms/step - loss: 1.0617 - acc: 0.4324 - val_loss: 1.0402 - val_acc: 0.4790\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0510 - acc: 0.4451 - val_loss: 1.0373 - val_acc: 0.4883\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0451 - acc: 0.4580 - val_loss: 1.0394 - val_acc: 0.4720\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0396 - acc: 0.4705 - val_loss: 1.0192 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0317 - acc: 0.4832 - val_loss: 1.0142 - val_acc: 0.5039\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.4877470359500689\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 189s 18ms/step - loss: 1.0602 - acc: 0.4312 - val_loss: 1.0344 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0507 - acc: 0.4486 - val_loss: 1.0328 - val_acc: 0.4922\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0459 - acc: 0.4611 - val_loss: 1.0205 - val_acc: 0.4914\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0383 - acc: 0.4702 - val_loss: 1.0165 - val_acc: 0.5000\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0326 - acc: 0.4847 - val_loss: 1.0217 - val_acc: 0.4907\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.48063241113787114\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 192s 19ms/step - loss: 1.0602 - acc: 0.4302 - val_loss: 1.0421 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0507 - acc: 0.4486 - val_loss: 1.0332 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0437 - acc: 0.4658 - val_loss: 1.0284 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0380 - acc: 0.4721 - val_loss: 1.0133 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0303 - acc: 0.4838 - val_loss: 1.0110 - val_acc: 0.4938\n",
      "1265/1265 [==============================] - 9s 8ms/step\n",
      "0.5027667987016821\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 194s 19ms/step - loss: 1.0607 - acc: 0.4345 - val_loss: 1.0403 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0508 - acc: 0.4463 - val_loss: 1.0256 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0449 - acc: 0.4629 - val_loss: 1.0175 - val_acc: 0.4922\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0393 - acc: 0.4733 - val_loss: 1.0118 - val_acc: 0.4961\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0340 - acc: 0.4809 - val_loss: 1.0139 - val_acc: 0.5031\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5106719370416031\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 198s 19ms/step - loss: 1.0607 - acc: 0.4324 - val_loss: 1.0432 - val_acc: 0.4891\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0504 - acc: 0.4467 - val_loss: 1.0291 - val_acc: 0.4875\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0437 - acc: 0.4622 - val_loss: 1.0200 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0357 - acc: 0.4768 - val_loss: 1.0110 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0293 - acc: 0.4827 - val_loss: 1.0045 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.49486166036176116\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 201s 20ms/step - loss: 1.0597 - acc: 0.4294 - val_loss: 1.0357 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0510 - acc: 0.4468 - val_loss: 1.0266 - val_acc: 0.4821\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0439 - acc: 0.4618 - val_loss: 1.0196 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0343 - acc: 0.4828 - val_loss: 1.0301 - val_acc: 0.4696\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0308 - acc: 0.4863 - val_loss: 1.0201 - val_acc: 0.4992\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.4901185771457763\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 202s 20ms/step - loss: 1.0619 - acc: 0.4259 - val_loss: 1.0399 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0513 - acc: 0.4452 - val_loss: 1.0315 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0449 - acc: 0.4624 - val_loss: 1.0265 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0370 - acc: 0.4697 - val_loss: 1.0176 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0301 - acc: 0.4858 - val_loss: 1.0063 - val_acc: 0.5070\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "0.4940711465748874\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 205s 20ms/step - loss: 1.0600 - acc: 0.4294 - val_loss: 1.0348 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0504 - acc: 0.4540 - val_loss: 1.0291 - val_acc: 0.4992\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0397 - acc: 0.4696 - val_loss: 1.0164 - val_acc: 0.5078\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0344 - acc: 0.4769 - val_loss: 1.0087 - val_acc: 0.5093\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0279 - acc: 0.4899 - val_loss: 1.0049 - val_acc: 0.5093\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.49723320191085574\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 209s 20ms/step - loss: 1.0608 - acc: 0.4334 - val_loss: 1.0373 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0533 - acc: 0.4448 - val_loss: 1.0336 - val_acc: 0.4945\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0460 - acc: 0.4522 - val_loss: 1.0201 - val_acc: 0.4984\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0358 - acc: 0.4719 - val_loss: 1.0093 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0296 - acc: 0.4826 - val_loss: 1.0034 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "0.48300395289899806\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 208s 20ms/step - loss: 1.0595 - acc: 0.4360 - val_loss: 1.0389 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0496 - acc: 0.4514 - val_loss: 1.0284 - val_acc: 0.4883\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0412 - acc: 0.4683 - val_loss: 1.0246 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0360 - acc: 0.4715 - val_loss: 1.0136 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 175s 17ms/step - loss: 1.0293 - acc: 0.4908 - val_loss: 1.0106 - val_acc: 0.5008\n",
      "1265/1265 [==============================] - 12s 9ms/step\n",
      "0.5051383402507766\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 217s 21ms/step - loss: 1.0614 - acc: 0.4296 - val_loss: 1.0351 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 176s 17ms/step - loss: 1.0523 - acc: 0.4458 - val_loss: 1.0265 - val_acc: 0.4821\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 175s 17ms/step - loss: 1.0426 - acc: 0.4610 - val_loss: 1.0171 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 176s 17ms/step - loss: 1.0351 - acc: 0.4757 - val_loss: 1.0100 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0269 - acc: 0.4897 - val_loss: 1.0048 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "0.5027667987959187\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 219s 21ms/step - loss: 1.0618 - acc: 0.4283 - val_loss: 1.0354 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0510 - acc: 0.4460 - val_loss: 1.0322 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 178s 17ms/step - loss: 1.0425 - acc: 0.4605 - val_loss: 1.0225 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 179s 17ms/step - loss: 1.0356 - acc: 0.4756 - val_loss: 1.0157 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 175s 17ms/step - loss: 1.0280 - acc: 0.4826 - val_loss: 1.0170 - val_acc: 0.4977\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "0.49090909093265006\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 221s 22ms/step - loss: 1.0606 - acc: 0.4358 - val_loss: 1.0397 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 178s 17ms/step - loss: 1.0515 - acc: 0.4469 - val_loss: 1.0325 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0459 - acc: 0.4571 - val_loss: 1.0184 - val_acc: 0.4868\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0355 - acc: 0.4777 - val_loss: 1.0156 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0295 - acc: 0.4814 - val_loss: 1.0068 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 12s 9ms/step\n",
      "0.4877470359029506\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 225s 22ms/step - loss: 1.0619 - acc: 0.4333 - val_loss: 1.0396 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 180s 18ms/step - loss: 1.0523 - acc: 0.4395 - val_loss: 1.0315 - val_acc: 0.4836\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 180s 18ms/step - loss: 1.0454 - acc: 0.4531 - val_loss: 1.0242 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0381 - acc: 0.4705 - val_loss: 1.0164 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 183s 18ms/step - loss: 1.0318 - acc: 0.4796 - val_loss: 1.0059 - val_acc: 0.5164\n",
      "1265/1265 [==============================] - 12s 10ms/step\n",
      "0.49169960507291105\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 227s 22ms/step - loss: 1.0599 - acc: 0.4358 - val_loss: 1.0332 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 180s 18ms/step - loss: 1.0492 - acc: 0.4492 - val_loss: 1.0280 - val_acc: 0.4953\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0409 - acc: 0.4705 - val_loss: 1.0148 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0342 - acc: 0.4806 - val_loss: 1.0399 - val_acc: 0.4533\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0283 - acc: 0.4850 - val_loss: 1.0118 - val_acc: 0.4961\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.49011857740492687\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 232s 23ms/step - loss: 1.0618 - acc: 0.4287 - val_loss: 1.0341 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 185s 18ms/step - loss: 1.0496 - acc: 0.4510 - val_loss: 1.0295 - val_acc: 0.5070\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0413 - acc: 0.4638 - val_loss: 1.0116 - val_acc: 0.4907\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0340 - acc: 0.4801 - val_loss: 1.0140 - val_acc: 0.4899\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0281 - acc: 0.4878 - val_loss: 1.0029 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.5035573126299108\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 237s 23ms/step - loss: 1.0612 - acc: 0.4303 - val_loss: 1.0342 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 185s 18ms/step - loss: 1.0518 - acc: 0.4441 - val_loss: 1.0300 - val_acc: 0.5016\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0433 - acc: 0.4561 - val_loss: 1.0236 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 185s 18ms/step - loss: 1.0364 - acc: 0.4787 - val_loss: 1.0095 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 185s 18ms/step - loss: 1.0309 - acc: 0.4811 - val_loss: 1.0042 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.4853754944009743\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 253s 25ms/step - loss: 1.0613 - acc: 0.4284 - val_loss: 1.0343 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 196s 19ms/step - loss: 1.0495 - acc: 0.4471 - val_loss: 1.0293 - val_acc: 0.4860\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 199s 19ms/step - loss: 1.0423 - acc: 0.4607 - val_loss: 1.0229 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 195s 19ms/step - loss: 1.0337 - acc: 0.4739 - val_loss: 1.0080 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 193s 19ms/step - loss: 1.0295 - acc: 0.4855 - val_loss: 1.0031 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 14s 11ms/step\n",
      "0.4814229252310138\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 242s 24ms/step - loss: 1.0612 - acc: 0.4274 - val_loss: 1.0432 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 189s 18ms/step - loss: 1.0503 - acc: 0.4496 - val_loss: 1.0318 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 188s 18ms/step - loss: 1.0417 - acc: 0.4633 - val_loss: 1.0227 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 187s 18ms/step - loss: 1.0349 - acc: 0.4787 - val_loss: 1.0113 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 188s 18ms/step - loss: 1.0268 - acc: 0.4889 - val_loss: 1.0094 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 12s 10ms/step\n",
      "0.47826086989504546\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 245s 24ms/step - loss: 1.0612 - acc: 0.4298 - val_loss: 1.0348 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 191s 19ms/step - loss: 1.0509 - acc: 0.4507 - val_loss: 1.0328 - val_acc: 0.4907\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 189s 18ms/step - loss: 1.0430 - acc: 0.4627 - val_loss: 1.0312 - val_acc: 0.4743\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 190s 19ms/step - loss: 1.0353 - acc: 0.4807 - val_loss: 1.0151 - val_acc: 0.5125\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 189s 19ms/step - loss: 1.0247 - acc: 0.4944 - val_loss: 1.0019 - val_acc: 0.5187\n",
      "1265/1265 [==============================] - 13s 11ms/step\n",
      "0.4853754944009743\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 249s 24ms/step - loss: 1.0607 - acc: 0.4328 - val_loss: 1.0343 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 202s 20ms/step - loss: 1.0489 - acc: 0.4529 - val_loss: 1.0334 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 197s 19ms/step - loss: 1.0427 - acc: 0.4617 - val_loss: 1.0159 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 192s 19ms/step - loss: 1.0318 - acc: 0.4832 - val_loss: 1.0070 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 193s 19ms/step - loss: 1.0271 - acc: 0.4883 - val_loss: 1.0030 - val_acc: 0.5171\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.49565217424287156\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 251s 25ms/step - loss: 1.0607 - acc: 0.4323 - val_loss: 1.0355 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 201s 20ms/step - loss: 1.0519 - acc: 0.4465 - val_loss: 1.0328 - val_acc: 0.4891\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 195s 19ms/step - loss: 1.0445 - acc: 0.4583 - val_loss: 1.0209 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 198s 19ms/step - loss: 1.0365 - acc: 0.4728 - val_loss: 1.0220 - val_acc: 0.4883\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 195s 19ms/step - loss: 1.0296 - acc: 0.4860 - val_loss: 1.0221 - val_acc: 0.5000\n",
      "1265/1265 [==============================] - 13s 11ms/step\n",
      "0.49328063278801354\n",
      "{5: 0.4577075101641327, 6: 0.46166007938121145, 7: 0.49565217424287156, 8: 0.47667984217994297, 9: 0.4758893284401875, 10: 0.4893280636180531, 11: 0.47747035606105337, 12: 0.4418972335314091, 13: 0.49723320191085574, 14: 0.4877470359500689, 15: 0.48063241113787114, 16: 0.5027667987016821, 17: 0.5106719370416031, 18: 0.49486166036176116, 19: 0.4901185771457763, 20: 0.4940711465748874, 21: 0.49723320191085574, 22: 0.48300395289899806, 23: 0.5051383402507766, 24: 0.5027667987959187, 25: 0.49090909093265006, 26: 0.4877470359029506, 27: 0.49169960507291105, 28: 0.49011857740492687, 29: 0.5035573126299108, 30: 0.4853754944009743, 31: 0.4814229252310138, 32: 0.47826086989504546, 33: 0.4853754944009743, 34: 0.49565217424287156, 35: 0.49328063278801354}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 165s 16ms/step - loss: 1.0595 - acc: 0.4397 - val_loss: 1.0378 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0552 - acc: 0.4353 - val_loss: 1.0332 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0523 - acc: 0.4414 - val_loss: 1.0335 - val_acc: 0.4883\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0500 - acc: 0.4468 - val_loss: 1.0281 - val_acc: 0.4899\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0471 - acc: 0.4546 - val_loss: 1.0242 - val_acc: 0.4961\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.45533596870927473\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 165s 16ms/step - loss: 1.0607 - acc: 0.4277 - val_loss: 1.0398 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 159s 16ms/step - loss: 1.0544 - acc: 0.4394 - val_loss: 1.0382 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0519 - acc: 0.4430 - val_loss: 1.0345 - val_acc: 0.4829\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0484 - acc: 0.4493 - val_loss: 1.0370 - val_acc: 0.4992\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 157s 15ms/step - loss: 1.0460 - acc: 0.4560 - val_loss: 1.0257 - val_acc: 0.4953\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.4632411070491957\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0592 - acc: 0.4360 - val_loss: 1.0354 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 158s 15ms/step - loss: 1.0539 - acc: 0.4383 - val_loss: 1.0304 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 158s 15ms/step - loss: 1.0502 - acc: 0.4460 - val_loss: 1.0355 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 159s 16ms/step - loss: 1.0467 - acc: 0.4512 - val_loss: 1.0228 - val_acc: 0.4961\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 159s 16ms/step - loss: 1.0421 - acc: 0.4648 - val_loss: 1.0196 - val_acc: 0.4984\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.47826086989504546\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 165s 16ms/step - loss: 1.0605 - acc: 0.4317 - val_loss: 1.0429 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0538 - acc: 0.4411 - val_loss: 1.0333 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0496 - acc: 0.4435 - val_loss: 1.0392 - val_acc: 0.4883\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0450 - acc: 0.4625 - val_loss: 1.0232 - val_acc: 0.4914\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0410 - acc: 0.4706 - val_loss: 1.0168 - val_acc: 0.4930\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.4695652177211324\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 176s 17ms/step - loss: 1.0580 - acc: 0.4383 - val_loss: 1.0332 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 158s 15ms/step - loss: 1.0537 - acc: 0.4417 - val_loss: 1.0369 - val_acc: 0.4790\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 157s 15ms/step - loss: 1.0490 - acc: 0.4471 - val_loss: 1.0262 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 157s 15ms/step - loss: 1.0431 - acc: 0.4654 - val_loss: 1.0214 - val_acc: 0.4907\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 158s 15ms/step - loss: 1.0381 - acc: 0.4740 - val_loss: 1.0166 - val_acc: 0.4930\n",
      "1265/1265 [==============================] - 8s 6ms/step\n",
      "0.47826086989504546\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0599 - acc: 0.4299 - val_loss: 1.0346 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0531 - acc: 0.4412 - val_loss: 1.0294 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0492 - acc: 0.4508 - val_loss: 1.0320 - val_acc: 0.4961\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0431 - acc: 0.4651 - val_loss: 1.0191 - val_acc: 0.4953\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0373 - acc: 0.4729 - val_loss: 1.0203 - val_acc: 0.5000\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.4711462454362349\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0586 - acc: 0.4345 - val_loss: 1.0382 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0514 - acc: 0.4444 - val_loss: 1.0335 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0462 - acc: 0.4564 - val_loss: 1.0208 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0392 - acc: 0.4744 - val_loss: 1.0207 - val_acc: 0.4977\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0323 - acc: 0.4841 - val_loss: 1.0192 - val_acc: 0.4953\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.4877470359500689\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 186s 18ms/step - loss: 1.0597 - acc: 0.4314 - val_loss: 1.0348 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 165s 16ms/step - loss: 1.0531 - acc: 0.4418 - val_loss: 1.0342 - val_acc: 0.4860\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0468 - acc: 0.4528 - val_loss: 1.0235 - val_acc: 0.4790\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0402 - acc: 0.4720 - val_loss: 1.0190 - val_acc: 0.4961\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0355 - acc: 0.4787 - val_loss: 1.0133 - val_acc: 0.4977\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.4830039529461163\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 242s 24ms/step - loss: 1.0587 - acc: 0.4367 - val_loss: 1.0345 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 191s 19ms/step - loss: 1.0519 - acc: 0.4434 - val_loss: 1.0320 - val_acc: 0.4899\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 165s 16ms/step - loss: 1.0458 - acc: 0.4544 - val_loss: 1.0271 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0395 - acc: 0.4703 - val_loss: 1.0199 - val_acc: 0.4969\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0326 - acc: 0.4821 - val_loss: 1.0287 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.4782608696358948\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 187s 18ms/step - loss: 1.0601 - acc: 0.4295 - val_loss: 1.0444 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0519 - acc: 0.4482 - val_loss: 1.0330 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0444 - acc: 0.4606 - val_loss: 1.0420 - val_acc: 0.4735\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0382 - acc: 0.4733 - val_loss: 1.0125 - val_acc: 0.4984\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0315 - acc: 0.4760 - val_loss: 1.0107 - val_acc: 0.5031\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.4940711466220057\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 192s 19ms/step - loss: 1.0599 - acc: 0.4353 - val_loss: 1.0318 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0521 - acc: 0.4478 - val_loss: 1.0402 - val_acc: 0.4821\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0453 - acc: 0.4615 - val_loss: 1.0294 - val_acc: 0.4907\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0369 - acc: 0.4800 - val_loss: 1.0103 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0323 - acc: 0.4825 - val_loss: 1.0084 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 9s 8ms/step\n",
      "0.5019762849148083\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 193s 19ms/step - loss: 1.0606 - acc: 0.4329 - val_loss: 1.0383 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0509 - acc: 0.4458 - val_loss: 1.0303 - val_acc: 0.4992\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0453 - acc: 0.4587 - val_loss: 1.0187 - val_acc: 0.4836\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0386 - acc: 0.4746 - val_loss: 1.0122 - val_acc: 0.5031\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0322 - acc: 0.4787 - val_loss: 1.0329 - val_acc: 0.4782\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.46956521746198177\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 197s 19ms/step - loss: 1.0602 - acc: 0.4353 - val_loss: 1.0379 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0510 - acc: 0.4470 - val_loss: 1.0346 - val_acc: 0.4961\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0440 - acc: 0.4620 - val_loss: 1.0366 - val_acc: 0.4852\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0360 - acc: 0.4770 - val_loss: 1.0125 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0310 - acc: 0.4860 - val_loss: 1.0243 - val_acc: 0.4922\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.4450592888673775\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 201s 20ms/step - loss: 1.0599 - acc: 0.4330 - val_loss: 1.0407 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0513 - acc: 0.4523 - val_loss: 1.0345 - val_acc: 0.5055\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0435 - acc: 0.4613 - val_loss: 1.0194 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0361 - acc: 0.4772 - val_loss: 1.0218 - val_acc: 0.4984\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0295 - acc: 0.4881 - val_loss: 1.0197 - val_acc: 0.5016\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.4774703558019027\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 202s 20ms/step - loss: 1.0593 - acc: 0.4339 - val_loss: 1.0354 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0502 - acc: 0.4441 - val_loss: 1.0313 - val_acc: 0.4914\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0433 - acc: 0.4617 - val_loss: 1.0164 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0344 - acc: 0.4778 - val_loss: 1.0139 - val_acc: 0.5031\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0288 - acc: 0.4885 - val_loss: 1.0132 - val_acc: 0.5000\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.49960474336571375\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 206s 20ms/step - loss: 1.0618 - acc: 0.4289 - val_loss: 1.0364 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 173s 17ms/step - loss: 1.0521 - acc: 0.4414 - val_loss: 1.0277 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0457 - acc: 0.4580 - val_loss: 1.0198 - val_acc: 0.4883\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0366 - acc: 0.4768 - val_loss: 1.0134 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0315 - acc: 0.4760 - val_loss: 1.0083 - val_acc: 0.5047\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "0.49644268802974534\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 208s 20ms/step - loss: 1.0605 - acc: 0.4294 - val_loss: 1.0354 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 175s 17ms/step - loss: 1.0485 - acc: 0.4486 - val_loss: 1.0335 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 175s 17ms/step - loss: 1.0427 - acc: 0.4638 - val_loss: 1.0196 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0320 - acc: 0.4830 - val_loss: 1.0245 - val_acc: 0.4868\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0267 - acc: 0.4885 - val_loss: 1.0024 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5043478264167846\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 211s 21ms/step - loss: 1.0590 - acc: 0.4329 - val_loss: 1.0399 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0490 - acc: 0.4530 - val_loss: 1.0298 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0419 - acc: 0.4636 - val_loss: 1.0147 - val_acc: 0.4969\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0352 - acc: 0.4754 - val_loss: 1.0189 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0286 - acc: 0.4833 - val_loss: 1.0098 - val_acc: 0.5000\n",
      "1265/1265 [==============================] - 11s 8ms/step\n",
      "0.5083003955867451\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 213s 21ms/step - loss: 1.0600 - acc: 0.4319 - val_loss: 1.0348 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0507 - acc: 0.4447 - val_loss: 1.0252 - val_acc: 0.4836\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 173s 17ms/step - loss: 1.0432 - acc: 0.4614 - val_loss: 1.0176 - val_acc: 0.5093\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0332 - acc: 0.4774 - val_loss: 1.0115 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 175s 17ms/step - loss: 1.0286 - acc: 0.4878 - val_loss: 1.0034 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.5027667987488004\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 216s 21ms/step - loss: 1.0614 - acc: 0.4311 - val_loss: 1.0392 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 175s 17ms/step - loss: 1.0523 - acc: 0.4508 - val_loss: 1.0326 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 175s 17ms/step - loss: 1.0412 - acc: 0.4588 - val_loss: 1.0167 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 175s 17ms/step - loss: 1.0365 - acc: 0.4770 - val_loss: 1.0107 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 175s 17ms/step - loss: 1.0285 - acc: 0.4889 - val_loss: 1.0042 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "0.499604743412832\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 223s 22ms/step - loss: 1.0603 - acc: 0.4327 - val_loss: 1.0395 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 176s 17ms/step - loss: 1.0502 - acc: 0.4469 - val_loss: 1.0272 - val_acc: 0.4852\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 176s 17ms/step - loss: 1.0417 - acc: 0.4661 - val_loss: 1.0209 - val_acc: 0.5179\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 176s 17ms/step - loss: 1.0373 - acc: 0.4784 - val_loss: 1.0085 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 176s 17ms/step - loss: 1.0286 - acc: 0.4855 - val_loss: 1.0291 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "0.46403162056513925\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 226s 22ms/step - loss: 1.0607 - acc: 0.4321 - val_loss: 1.0390 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 179s 17ms/step - loss: 1.0507 - acc: 0.4504 - val_loss: 1.0277 - val_acc: 0.4907\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 180s 18ms/step - loss: 1.0401 - acc: 0.4630 - val_loss: 1.0248 - val_acc: 0.5047\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 179s 17ms/step - loss: 1.0347 - acc: 0.4787 - val_loss: 1.0198 - val_acc: 0.4930\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 179s 17ms/step - loss: 1.0276 - acc: 0.4948 - val_loss: 1.0039 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 12s 9ms/step\n",
      "0.4932806327408953\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 233s 23ms/step - loss: 1.0618 - acc: 0.4311 - val_loss: 1.0341 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0512 - acc: 0.4494 - val_loss: 1.0287 - val_acc: 0.4899\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 180s 18ms/step - loss: 1.0421 - acc: 0.4639 - val_loss: 1.0233 - val_acc: 0.5016\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0369 - acc: 0.4764 - val_loss: 1.0148 - val_acc: 0.5164\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 182s 18ms/step - loss: 1.0274 - acc: 0.4870 - val_loss: 1.0041 - val_acc: 0.5055\n",
      "1265/1265 [==============================] - 12s 9ms/step\n",
      "0.48063241139702173\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 232s 23ms/step - loss: 1.0604 - acc: 0.4362 - val_loss: 1.0350 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0509 - acc: 0.4483 - val_loss: 1.0243 - val_acc: 0.4790\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0413 - acc: 0.4654 - val_loss: 1.0143 - val_acc: 0.4938\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0331 - acc: 0.4842 - val_loss: 1.0089 - val_acc: 0.5171\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 186s 18ms/step - loss: 1.0293 - acc: 0.4853 - val_loss: 1.0050 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 12s 10ms/step\n",
      "0.4861660082349664\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 236s 23ms/step - loss: 1.0606 - acc: 0.4358 - val_loss: 1.0370 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0512 - acc: 0.4502 - val_loss: 1.0256 - val_acc: 0.4852\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 183s 18ms/step - loss: 1.0421 - acc: 0.4649 - val_loss: 1.0296 - val_acc: 0.4712\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 183s 18ms/step - loss: 1.0342 - acc: 0.4737 - val_loss: 1.0119 - val_acc: 0.5109\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0305 - acc: 0.4860 - val_loss: 1.0053 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 12s 10ms/step\n",
      "0.5019762849148083\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 238s 23ms/step - loss: 1.0609 - acc: 0.4343 - val_loss: 1.0374 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 186s 18ms/step - loss: 1.0492 - acc: 0.4475 - val_loss: 1.0346 - val_acc: 0.5031\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 186s 18ms/step - loss: 1.0405 - acc: 0.4678 - val_loss: 1.0140 - val_acc: 0.5055\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 185s 18ms/step - loss: 1.0342 - acc: 0.4763 - val_loss: 1.0059 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 186s 18ms/step - loss: 1.0245 - acc: 0.4895 - val_loss: 1.0057 - val_acc: 0.5023\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.5035573126299108\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 251s 25ms/step - loss: 1.0612 - acc: 0.4279 - val_loss: 1.0374 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 195s 19ms/step - loss: 1.0522 - acc: 0.4500 - val_loss: 1.0301 - val_acc: 0.4868\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 196s 19ms/step - loss: 1.0448 - acc: 0.4597 - val_loss: 1.0281 - val_acc: 0.4992\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 194s 19ms/step - loss: 1.0373 - acc: 0.4730 - val_loss: 1.0113 - val_acc: 0.5140\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 195s 19ms/step - loss: 1.0309 - acc: 0.4820 - val_loss: 1.0287 - val_acc: 0.4813\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.47430830051305267\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 244s 24ms/step - loss: 1.0606 - acc: 0.4358 - val_loss: 1.0362 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 190s 19ms/step - loss: 1.0498 - acc: 0.4498 - val_loss: 1.0275 - val_acc: 0.4844\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 188s 18ms/step - loss: 1.0422 - acc: 0.4615 - val_loss: 1.0229 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 188s 18ms/step - loss: 1.0332 - acc: 0.4749 - val_loss: 1.0218 - val_acc: 0.4945\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 187s 18ms/step - loss: 1.0269 - acc: 0.4900 - val_loss: 1.0068 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.49090909123891896\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 255s 25ms/step - loss: 1.0611 - acc: 0.4325 - val_loss: 1.0383 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 198s 19ms/step - loss: 1.0521 - acc: 0.4433 - val_loss: 1.0352 - val_acc: 0.5008\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 190s 19ms/step - loss: 1.0442 - acc: 0.4626 - val_loss: 1.0367 - val_acc: 0.4696\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 191s 19ms/step - loss: 1.0374 - acc: 0.4719 - val_loss: 1.0275 - val_acc: 0.4821\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 191s 19ms/step - loss: 1.0292 - acc: 0.4820 - val_loss: 1.0107 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.5019762849619266\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 252s 25ms/step - loss: 1.0607 - acc: 0.4380 - val_loss: 1.0420 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 197s 19ms/step - loss: 1.0496 - acc: 0.4481 - val_loss: 1.0363 - val_acc: 0.4922\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 192s 19ms/step - loss: 1.0421 - acc: 0.4592 - val_loss: 1.0173 - val_acc: 0.4875\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 192s 19ms/step - loss: 1.0353 - acc: 0.4771 - val_loss: 1.0188 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 191s 19ms/step - loss: 1.0278 - acc: 0.4847 - val_loss: 1.0082 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.50197628486769\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 254s 25ms/step - loss: 1.0616 - acc: 0.4307 - val_loss: 1.0420 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 201s 20ms/step - loss: 1.0529 - acc: 0.4425 - val_loss: 1.0372 - val_acc: 0.4930\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 196s 19ms/step - loss: 1.0450 - acc: 0.4588 - val_loss: 1.0196 - val_acc: 0.4984\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 194s 19ms/step - loss: 1.0366 - acc: 0.4749 - val_loss: 1.0193 - val_acc: 0.5031\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 195s 19ms/step - loss: 1.0305 - acc: 0.4907 - val_loss: 1.0028 - val_acc: 0.5156\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.4940711465748874\n",
      "{5: 0.45533596870927473, 6: 0.4632411070491957, 7: 0.47826086989504546, 8: 0.4695652177211324, 9: 0.47826086989504546, 10: 0.4711462454362349, 11: 0.4877470359500689, 12: 0.4830039529461163, 13: 0.4782608696358948, 14: 0.4940711466220057, 15: 0.5019762849148083, 16: 0.46956521746198177, 17: 0.4450592888673775, 18: 0.4774703558019027, 19: 0.49960474336571375, 20: 0.49644268802974534, 21: 0.5043478264167846, 22: 0.5083003955867451, 23: 0.5027667987488004, 24: 0.499604743412832, 25: 0.46403162056513925, 26: 0.4932806327408953, 27: 0.48063241139702173, 28: 0.4861660082349664, 29: 0.5019762849148083, 30: 0.5035573126299108, 31: 0.47430830051305267, 32: 0.49090909123891896, 33: 0.5019762849619266, 34: 0.50197628486769, 35: 0.4940711465748874}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0594 - acc: 0.4315 - val_loss: 1.0347 - val_acc: 0.4798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0546 - acc: 0.4384 - val_loss: 1.0356 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0514 - acc: 0.4426 - val_loss: 1.0310 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0484 - acc: 0.4497 - val_loss: 1.0372 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0465 - acc: 0.4557 - val_loss: 1.0288 - val_acc: 0.4969\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.4561264824961485\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0612 - acc: 0.4323 - val_loss: 1.0380 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0546 - acc: 0.4398 - val_loss: 1.0362 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0518 - acc: 0.4416 - val_loss: 1.0309 - val_acc: 0.4790\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0472 - acc: 0.4522 - val_loss: 1.0330 - val_acc: 0.4938\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0451 - acc: 0.4568 - val_loss: 1.0344 - val_acc: 0.4977\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.4822134391121242\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 165s 16ms/step - loss: 1.0590 - acc: 0.4306 - val_loss: 1.0332 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 157s 15ms/step - loss: 1.0544 - acc: 0.4381 - val_loss: 1.0354 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0502 - acc: 0.4437 - val_loss: 1.0294 - val_acc: 0.4813\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0467 - acc: 0.4571 - val_loss: 1.0251 - val_acc: 0.4961\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0437 - acc: 0.4662 - val_loss: 1.0189 - val_acc: 0.4984\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.4592885378792352\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 165s 16ms/step - loss: 1.0591 - acc: 0.4321 - val_loss: 1.0376 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0532 - acc: 0.4421 - val_loss: 1.0299 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0494 - acc: 0.4513 - val_loss: 1.0341 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 159s 16ms/step - loss: 1.0432 - acc: 0.4606 - val_loss: 1.0207 - val_acc: 0.4969\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 156s 15ms/step - loss: 1.0389 - acc: 0.4715 - val_loss: 1.0197 - val_acc: 0.4961\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.4766798422741796\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0586 - acc: 0.4306 - val_loss: 1.0359 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0525 - acc: 0.4451 - val_loss: 1.0333 - val_acc: 0.4875\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0490 - acc: 0.4489 - val_loss: 1.0393 - val_acc: 0.4875\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0450 - acc: 0.4591 - val_loss: 1.0233 - val_acc: 0.4977\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0406 - acc: 0.4702 - val_loss: 1.0202 - val_acc: 0.4930\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.48379446673299015\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 165s 16ms/step - loss: 1.0593 - acc: 0.4364 - val_loss: 1.0430 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 159s 16ms/step - loss: 1.0524 - acc: 0.4474 - val_loss: 1.0338 - val_acc: 0.4883\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0469 - acc: 0.4563 - val_loss: 1.0262 - val_acc: 0.4914\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0420 - acc: 0.4689 - val_loss: 1.0192 - val_acc: 0.4930\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 162s 16ms/step - loss: 1.0367 - acc: 0.4752 - val_loss: 1.0208 - val_acc: 0.4945\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.4837944667801084\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0601 - acc: 0.4280 - val_loss: 1.0332 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0514 - acc: 0.4476 - val_loss: 1.0314 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0462 - acc: 0.4645 - val_loss: 1.0221 - val_acc: 0.4984\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0389 - acc: 0.4727 - val_loss: 1.0257 - val_acc: 0.4984\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 164s 16ms/step - loss: 1.0358 - acc: 0.4792 - val_loss: 1.0147 - val_acc: 0.4977\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "0.49169960476664215\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 178s 17ms/step - loss: 1.0600 - acc: 0.4372 - val_loss: 1.0391 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 165s 16ms/step - loss: 1.0524 - acc: 0.4418 - val_loss: 1.0306 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0486 - acc: 0.4517 - val_loss: 1.0307 - val_acc: 0.4938\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0429 - acc: 0.4679 - val_loss: 1.0301 - val_acc: 0.4914\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0358 - acc: 0.4780 - val_loss: 1.0238 - val_acc: 0.4899\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.49169960512002936\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 191s 19ms/step - loss: 1.0603 - acc: 0.4298 - val_loss: 1.0360 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0513 - acc: 0.4446 - val_loss: 1.0295 - val_acc: 0.4844\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 161s 16ms/step - loss: 1.0450 - acc: 0.4590 - val_loss: 1.0199 - val_acc: 0.4899\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 160s 16ms/step - loss: 1.0368 - acc: 0.4745 - val_loss: 1.0136 - val_acc: 0.4938\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0322 - acc: 0.4788 - val_loss: 1.0141 - val_acc: 0.5016\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.49565217428998987\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 191s 19ms/step - loss: 1.0587 - acc: 0.4292 - val_loss: 1.0346 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0502 - acc: 0.4497 - val_loss: 1.0287 - val_acc: 0.4813\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 166s 16ms/step - loss: 1.0449 - acc: 0.4607 - val_loss: 1.0190 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0348 - acc: 0.4855 - val_loss: 1.0276 - val_acc: 0.4883\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0308 - acc: 0.4879 - val_loss: 1.0097 - val_acc: 0.4977\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.474308300725085\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 193s 19ms/step - loss: 1.0609 - acc: 0.4279 - val_loss: 1.0406 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0514 - acc: 0.4503 - val_loss: 1.0354 - val_acc: 0.4969\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0443 - acc: 0.4670 - val_loss: 1.0192 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0364 - acc: 0.4755 - val_loss: 1.0149 - val_acc: 0.5016\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0322 - acc: 0.4800 - val_loss: 1.0237 - val_acc: 0.4938\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.49011857709865797\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 197s 19ms/step - loss: 1.0606 - acc: 0.4326 - val_loss: 1.0376 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0530 - acc: 0.4433 - val_loss: 1.0343 - val_acc: 0.4922\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0455 - acc: 0.4600 - val_loss: 1.0194 - val_acc: 0.4899\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0387 - acc: 0.4712 - val_loss: 1.0140 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0317 - acc: 0.4849 - val_loss: 1.0172 - val_acc: 0.5000\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "0.49328063243462633\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 200s 20ms/step - loss: 1.0589 - acc: 0.4346 - val_loss: 1.0396 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 169s 16ms/step - loss: 1.0506 - acc: 0.4525 - val_loss: 1.0267 - val_acc: 0.4836\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0431 - acc: 0.4634 - val_loss: 1.0201 - val_acc: 0.5023\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0368 - acc: 0.4739 - val_loss: 1.0141 - val_acc: 0.5023\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 163s 16ms/step - loss: 1.0308 - acc: 0.4849 - val_loss: 1.0196 - val_acc: 0.4914\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.49802371574484783\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 204s 20ms/step - loss: 1.0604 - acc: 0.4337 - val_loss: 1.0355 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0520 - acc: 0.4439 - val_loss: 1.0300 - val_acc: 0.4922\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0438 - acc: 0.4608 - val_loss: 1.0250 - val_acc: 0.5132\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0381 - acc: 0.4764 - val_loss: 1.0170 - val_acc: 0.5000\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0307 - acc: 0.4818 - val_loss: 1.0135 - val_acc: 0.4992\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.4924901189069032\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 207s 20ms/step - loss: 1.0612 - acc: 0.4310 - val_loss: 1.0335 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0523 - acc: 0.4429 - val_loss: 1.0327 - val_acc: 0.4977\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0433 - acc: 0.4610 - val_loss: 1.0200 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 171s 17ms/step - loss: 1.0350 - acc: 0.4739 - val_loss: 1.0089 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 167s 16ms/step - loss: 1.0303 - acc: 0.4836 - val_loss: 1.0047 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.49486166040887947\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 208s 20ms/step - loss: 1.0605 - acc: 0.4293 - val_loss: 1.0444 - val_acc: 0.4961\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0521 - acc: 0.4440 - val_loss: 1.0344 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0436 - acc: 0.4653 - val_loss: 1.0194 - val_acc: 0.5086\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 168s 16ms/step - loss: 1.0362 - acc: 0.4754 - val_loss: 1.0163 - val_acc: 0.5070\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 169s 17ms/step - loss: 1.0304 - acc: 0.4856 - val_loss: 1.0032 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 11s 8ms/step\n",
      "0.48221343906500597\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 210s 21ms/step - loss: 1.0596 - acc: 0.4328 - val_loss: 1.0421 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 170s 17ms/step - loss: 1.0517 - acc: 0.4455 - val_loss: 1.0296 - val_acc: 0.4805\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0457 - acc: 0.4562 - val_loss: 1.0274 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0360 - acc: 0.4688 - val_loss: 1.0157 - val_acc: 0.4883\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0304 - acc: 0.4790 - val_loss: 1.0103 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 10s 8ms/step\n",
      "0.49644268807686365\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 212s 21ms/step - loss: 1.0607 - acc: 0.4355 - val_loss: 1.0361 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 172s 17ms/step - loss: 1.0516 - acc: 0.4500 - val_loss: 1.0263 - val_acc: 0.4844\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 173s 17ms/step - loss: 1.0432 - acc: 0.4634 - val_loss: 1.0170 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0361 - acc: 0.4745 - val_loss: 1.0203 - val_acc: 0.5008\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 174s 17ms/step - loss: 1.0286 - acc: 0.4830 - val_loss: 1.0052 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "0.47747035606105337\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 219s 21ms/step - loss: 1.0607 - acc: 0.4332 - val_loss: 1.0365 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 176s 17ms/step - loss: 1.0513 - acc: 0.4469 - val_loss: 1.0326 - val_acc: 0.4875\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0432 - acc: 0.4657 - val_loss: 1.0177 - val_acc: 0.4977\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0353 - acc: 0.4736 - val_loss: 1.0105 - val_acc: 0.5117\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 227s 22ms/step - loss: 1.0282 - acc: 0.4872 - val_loss: 1.0065 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 12s 9ms/step\n",
      "0.49565217424287156\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 222s 22ms/step - loss: 1.0608 - acc: 0.4311 - val_loss: 1.0376 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0521 - acc: 0.4480 - val_loss: 1.0315 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0417 - acc: 0.4626 - val_loss: 1.0197 - val_acc: 0.5070\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0343 - acc: 0.4769 - val_loss: 1.0118 - val_acc: 0.5132\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0286 - acc: 0.4854 - val_loss: 1.0049 - val_acc: 0.5062\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "0.4853754944009743\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 229s 22ms/step - loss: 1.0591 - acc: 0.4316 - val_loss: 1.0389 - val_acc: 0.4805\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 178s 17ms/step - loss: 1.0515 - acc: 0.4490 - val_loss: 1.0241 - val_acc: 0.4844\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 178s 17ms/step - loss: 1.0419 - acc: 0.4644 - val_loss: 1.0561 - val_acc: 0.4112\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 177s 17ms/step - loss: 1.0346 - acc: 0.4739 - val_loss: 1.0182 - val_acc: 0.4961\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 179s 17ms/step - loss: 1.0270 - acc: 0.4857 - val_loss: 1.0153 - val_acc: 0.4984\n",
      "1265/1265 [==============================] - 12s 9ms/step\n",
      "0.49723320160458684\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 228s 22ms/step - loss: 1.0596 - acc: 0.4382 - val_loss: 1.0432 - val_acc: 0.4883\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 178s 17ms/step - loss: 1.0519 - acc: 0.4474 - val_loss: 1.0255 - val_acc: 0.4829\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 178s 17ms/step - loss: 1.0427 - acc: 0.4582 - val_loss: 1.0237 - val_acc: 0.5008\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 178s 17ms/step - loss: 1.0354 - acc: 0.4763 - val_loss: 1.0171 - val_acc: 0.4844\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 179s 17ms/step - loss: 1.0295 - acc: 0.4874 - val_loss: 1.0061 - val_acc: 0.5016\n",
      "1265/1265 [==============================] - 11s 9ms/step\n",
      "0.5011857710336979\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 233s 23ms/step - loss: 1.0623 - acc: 0.4242 - val_loss: 1.0348 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 183s 18ms/step - loss: 1.0520 - acc: 0.4412 - val_loss: 1.0414 - val_acc: 0.4564\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0424 - acc: 0.4650 - val_loss: 1.0192 - val_acc: 0.5062\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0379 - acc: 0.4700 - val_loss: 1.0101 - val_acc: 0.5062\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 181s 18ms/step - loss: 1.0283 - acc: 0.4885 - val_loss: 1.0066 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 12s 9ms/step\n",
      "0.47905138372903755\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 236s 23ms/step - loss: 1.0615 - acc: 0.4327 - val_loss: 1.0384 - val_acc: 0.4829\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 183s 18ms/step - loss: 1.0526 - acc: 0.4450 - val_loss: 1.0319 - val_acc: 0.4953\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 185s 18ms/step - loss: 1.0432 - acc: 0.4629 - val_loss: 1.0172 - val_acc: 0.4852\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0347 - acc: 0.4763 - val_loss: 1.0081 - val_acc: 0.5023\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 185s 18ms/step - loss: 1.0292 - acc: 0.4841 - val_loss: 1.0244 - val_acc: 0.4836\n",
      "1265/1265 [==============================] - 12s 9ms/step\n",
      "0.485375494188942\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 242s 24ms/step - loss: 1.0595 - acc: 0.4320 - val_loss: 1.0353 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 185s 18ms/step - loss: 1.0513 - acc: 0.4461 - val_loss: 1.0275 - val_acc: 0.5062\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 184s 18ms/step - loss: 1.0405 - acc: 0.4664 - val_loss: 1.0180 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 185s 18ms/step - loss: 1.0325 - acc: 0.4798 - val_loss: 1.0067 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 185s 18ms/step - loss: 1.0269 - acc: 0.4877 - val_loss: 1.0041 - val_acc: 0.5125\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.5011857711279345\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 244s 24ms/step - loss: 1.0616 - acc: 0.4308 - val_loss: 1.0379 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 188s 18ms/step - loss: 1.0534 - acc: 0.4426 - val_loss: 1.0331 - val_acc: 0.4907\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 187s 18ms/step - loss: 1.0434 - acc: 0.4568 - val_loss: 1.0185 - val_acc: 0.4883\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 186s 18ms/step - loss: 1.0372 - acc: 0.4684 - val_loss: 1.0114 - val_acc: 0.4945\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 186s 18ms/step - loss: 1.0291 - acc: 0.4816 - val_loss: 1.0173 - val_acc: 0.4907\n",
      "1265/1265 [==============================] - 13s 11ms/step\n",
      "0.49802371579196614\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 257s 25ms/step - loss: 1.0609 - acc: 0.4309 - val_loss: 1.0388 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 199s 19ms/step - loss: 1.0512 - acc: 0.4513 - val_loss: 1.0358 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 196s 19ms/step - loss: 1.0410 - acc: 0.4647 - val_loss: 1.0204 - val_acc: 0.5101\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 198s 19ms/step - loss: 1.0343 - acc: 0.4742 - val_loss: 1.0078 - val_acc: 0.5156\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 200s 19ms/step - loss: 1.0259 - acc: 0.4880 - val_loss: 1.0011 - val_acc: 0.5179\n",
      "1265/1265 [==============================] - 13s 11ms/step\n",
      "0.4988142295788399\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 249s 24ms/step - loss: 1.0614 - acc: 0.4347 - val_loss: 1.0417 - val_acc: 0.4977\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 191s 19ms/step - loss: 1.0512 - acc: 0.4435 - val_loss: 1.0278 - val_acc: 0.4836\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 189s 18ms/step - loss: 1.0452 - acc: 0.4577 - val_loss: 1.0193 - val_acc: 0.4844\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 189s 18ms/step - loss: 1.0370 - acc: 0.4696 - val_loss: 1.0139 - val_acc: 0.5023\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 188s 18ms/step - loss: 1.0285 - acc: 0.4870 - val_loss: 1.0290 - val_acc: 0.4751\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.4671936759600055\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 251s 25ms/step - loss: 1.0622 - acc: 0.4302 - val_loss: 1.0390 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 194s 19ms/step - loss: 1.0524 - acc: 0.4406 - val_loss: 1.0282 - val_acc: 0.4836\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 189s 19ms/step - loss: 1.0440 - acc: 0.4561 - val_loss: 1.0241 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 191s 19ms/step - loss: 1.0359 - acc: 0.4722 - val_loss: 1.0135 - val_acc: 0.5086\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 190s 19ms/step - loss: 1.0287 - acc: 0.4861 - val_loss: 1.0070 - val_acc: 0.5140\n",
      "1265/1265 [==============================] - 14s 11ms/step\n",
      "0.4924901189069032\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 252s 25ms/step - loss: 1.0601 - acc: 0.4408 - val_loss: 1.0365 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 199s 19ms/step - loss: 1.0499 - acc: 0.4532 - val_loss: 1.0299 - val_acc: 0.4821\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 193s 19ms/step - loss: 1.0419 - acc: 0.4649 - val_loss: 1.0143 - val_acc: 0.4984\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 191s 19ms/step - loss: 1.0335 - acc: 0.4777 - val_loss: 1.0082 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 198s 19ms/step - loss: 1.0264 - acc: 0.4885 - val_loss: 1.0076 - val_acc: 0.5101\n",
      "1265/1265 [==============================] - 14s 11ms/step\n",
      "0.5035573125356742\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 258s 25ms/step - loss: 1.0631 - acc: 0.4257 - val_loss: 1.0382 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 204s 20ms/step - loss: 1.0528 - acc: 0.4392 - val_loss: 1.0331 - val_acc: 0.4868\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 197s 19ms/step - loss: 1.0460 - acc: 0.4572 - val_loss: 1.0223 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 197s 19ms/step - loss: 1.0383 - acc: 0.4684 - val_loss: 1.0138 - val_acc: 0.4836\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 195s 19ms/step - loss: 1.0326 - acc: 0.4753 - val_loss: 1.0125 - val_acc: 0.5086\n",
      "1265/1265 [==============================] - 13s 10ms/step\n",
      "0.5043478264639029\n",
      "{5: 0.4561264824961485, 6: 0.4822134391121242, 7: 0.4592885378792352, 8: 0.4766798422741796, 9: 0.48379446673299015, 10: 0.4837944667801084, 11: 0.49169960476664215, 12: 0.49169960512002936, 13: 0.49565217428998987, 14: 0.474308300725085, 15: 0.49011857709865797, 16: 0.49328063243462633, 17: 0.49802371574484783, 18: 0.4924901189069032, 19: 0.49486166040887947, 20: 0.48221343906500597, 21: 0.49644268807686365, 22: 0.47747035606105337, 23: 0.49565217424287156, 24: 0.4853754944009743, 25: 0.49723320160458684, 26: 0.5011857710336979, 27: 0.47905138372903755, 28: 0.485375494188942, 29: 0.5011857711279345, 30: 0.49802371579196614, 31: 0.4988142295788399, 32: 0.4671936759600055, 33: 0.4924901189069032, 34: 0.5035573125356742, 35: 0.5043478264639029}\n",
      "CPU times: user 8d 18h 26min 11s, sys: 20h 4s, total: 9d 14h 26min 15s\n",
      "Wall time: 1d 17h 38min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cnn_flair_rounds = [calculate_round(concatenated_flair) for round in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{5: 0.46877470393425863,\n",
       "  6: 0.4592885378792352,\n",
       "  7: 0.4592885378792352,\n",
       "  8: 0.4727272730571008,\n",
       "  9: 0.47905138377615586,\n",
       "  10: 0.4885375497369427,\n",
       "  11: 0.4869565221160768,\n",
       "  12: 0.488537549784061,\n",
       "  13: 0.49090909093265006,\n",
       "  14: 0.4830039526398474,\n",
       "  15: 0.4885375497369427,\n",
       "  16: 0.4893280635709348,\n",
       "  17: 0.49644268802974534,\n",
       "  18: 0.4790513835170052,\n",
       "  19: 0.485375494188942,\n",
       "  20: 0.493280632693777,\n",
       "  21: 0.49644268802974534,\n",
       "  22: 0.48458498056698224,\n",
       "  23: 0.4877470359029506,\n",
       "  24: 0.49169960507291105,\n",
       "  25: 0.5035573125827925,\n",
       "  26: 0.5035573125827925,\n",
       "  27: 0.4853754944009743,\n",
       "  28: 0.4450592888673775,\n",
       "  29: 0.5003952572939424,\n",
       "  30: 0.505138340297895,\n",
       "  31: 0.5067193678716426,\n",
       "  32: 0.49723320195797405,\n",
       "  33: 0.5003952571997058,\n",
       "  34: 0.49723320195797405,\n",
       "  35: 0.4940711465748874},\n",
       " {5: 0.46482213471717987,\n",
       "  6: 0.4569169963772589,\n",
       "  7: 0.4687747038871403,\n",
       "  8: 0.44743083036935377,\n",
       "  9: 0.4640316208831878,\n",
       "  10: 0.4877470359500689,\n",
       "  11: 0.4893280636180531,\n",
       "  12: 0.47588932808680023,\n",
       "  13: 0.49169960512002936,\n",
       "  14: 0.488537549784061,\n",
       "  15: 0.4940711462686184,\n",
       "  16: 0.5051383402036584,\n",
       "  17: 0.5011857710808162,\n",
       "  18: 0.4948616604559978,\n",
       "  19: 0.48458498056698224,\n",
       "  20: 0.5059288540376505,\n",
       "  21: 0.47509881434704476,\n",
       "  22: 0.5106719370416031,\n",
       "  23: 0.49249011895402145,\n",
       "  24: 0.49090909123891896,\n",
       "  25: 0.5043478264639029,\n",
       "  26: 0.4845849803078316,\n",
       "  27: 0.49090909123891896,\n",
       "  28: 0.474308300725085,\n",
       "  29: 0.5043478264639029,\n",
       "  30: 0.5019762849619266,\n",
       "  31: 0.47984189756302964,\n",
       "  32: 0.5083003955867451,\n",
       "  33: 0.47747035606105337,\n",
       "  34: 0.5075098817056347,\n",
       "  35: 0.4529644272072984},\n",
       " {5: 0.4577075101641327,\n",
       "  6: 0.46166007938121145,\n",
       "  7: 0.49565217424287156,\n",
       "  8: 0.47667984217994297,\n",
       "  9: 0.4758893284401875,\n",
       "  10: 0.4893280636180531,\n",
       "  11: 0.47747035606105337,\n",
       "  12: 0.4418972335314091,\n",
       "  13: 0.49723320191085574,\n",
       "  14: 0.4877470359500689,\n",
       "  15: 0.48063241113787114,\n",
       "  16: 0.5027667987016821,\n",
       "  17: 0.5106719370416031,\n",
       "  18: 0.49486166036176116,\n",
       "  19: 0.4901185771457763,\n",
       "  20: 0.4940711465748874,\n",
       "  21: 0.49723320191085574,\n",
       "  22: 0.48300395289899806,\n",
       "  23: 0.5051383402507766,\n",
       "  24: 0.5027667987959187,\n",
       "  25: 0.49090909093265006,\n",
       "  26: 0.4877470359029506,\n",
       "  27: 0.49169960507291105,\n",
       "  28: 0.49011857740492687,\n",
       "  29: 0.5035573126299108,\n",
       "  30: 0.4853754944009743,\n",
       "  31: 0.4814229252310138,\n",
       "  32: 0.47826086989504546,\n",
       "  33: 0.4853754944009743,\n",
       "  34: 0.49565217424287156,\n",
       "  35: 0.49328063278801354},\n",
       " {5: 0.45533596870927473,\n",
       "  6: 0.4632411070491957,\n",
       "  7: 0.47826086989504546,\n",
       "  8: 0.4695652177211324,\n",
       "  9: 0.47826086989504546,\n",
       "  10: 0.4711462454362349,\n",
       "  11: 0.4877470359500689,\n",
       "  12: 0.4830039529461163,\n",
       "  13: 0.4782608696358948,\n",
       "  14: 0.4940711466220057,\n",
       "  15: 0.5019762849148083,\n",
       "  16: 0.46956521746198177,\n",
       "  17: 0.4450592888673775,\n",
       "  18: 0.4774703558019027,\n",
       "  19: 0.49960474336571375,\n",
       "  20: 0.49644268802974534,\n",
       "  21: 0.5043478264167846,\n",
       "  22: 0.5083003955867451,\n",
       "  23: 0.5027667987488004,\n",
       "  24: 0.499604743412832,\n",
       "  25: 0.46403162056513925,\n",
       "  26: 0.4932806327408953,\n",
       "  27: 0.48063241139702173,\n",
       "  28: 0.4861660082349664,\n",
       "  29: 0.5019762849148083,\n",
       "  30: 0.5035573126299108,\n",
       "  31: 0.47430830051305267,\n",
       "  32: 0.49090909123891896,\n",
       "  33: 0.5019762849619266,\n",
       "  34: 0.50197628486769,\n",
       "  35: 0.4940711465748874},\n",
       " {5: 0.4561264824961485,\n",
       "  6: 0.4822134391121242,\n",
       "  7: 0.4592885378792352,\n",
       "  8: 0.4766798422741796,\n",
       "  9: 0.48379446673299015,\n",
       "  10: 0.4837944667801084,\n",
       "  11: 0.49169960476664215,\n",
       "  12: 0.49169960512002936,\n",
       "  13: 0.49565217428998987,\n",
       "  14: 0.474308300725085,\n",
       "  15: 0.49011857709865797,\n",
       "  16: 0.49328063243462633,\n",
       "  17: 0.49802371574484783,\n",
       "  18: 0.4924901189069032,\n",
       "  19: 0.49486166040887947,\n",
       "  20: 0.48221343906500597,\n",
       "  21: 0.49644268807686365,\n",
       "  22: 0.47747035606105337,\n",
       "  23: 0.49565217424287156,\n",
       "  24: 0.4853754944009743,\n",
       "  25: 0.49723320160458684,\n",
       "  26: 0.5011857710336979,\n",
       "  27: 0.47905138372903755,\n",
       "  28: 0.485375494188942,\n",
       "  29: 0.5011857711279345,\n",
       "  30: 0.49802371579196614,\n",
       "  31: 0.4988142295788399,\n",
       "  32: 0.4671936759600055,\n",
       "  33: 0.4924901189069032,\n",
       "  34: 0.5035573125356742,\n",
       "  35: 0.5043478264639029}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_flair_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Round 0",
         "type": "scatter",
         "uid": "0fd0ba98-3944-4d9c-b292-efbd38e637e0",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.46877470393425863,
          0.4592885378792352,
          0.4592885378792352,
          0.4727272730571008,
          0.47905138377615586,
          0.4885375497369427,
          0.4869565221160768,
          0.488537549784061,
          0.49090909093265006,
          0.4830039526398474,
          0.4885375497369427,
          0.4893280635709348,
          0.49644268802974534,
          0.4790513835170052,
          0.485375494188942,
          0.493280632693777,
          0.49644268802974534,
          0.48458498056698224,
          0.4877470359029506,
          0.49169960507291105,
          0.5035573125827925,
          0.5035573125827925,
          0.4853754944009743,
          0.4450592888673775,
          0.5003952572939424,
          0.505138340297895,
          0.5067193678716426,
          0.49723320195797405,
          0.5003952571997058,
          0.49723320195797405,
          0.4940711465748874
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 1",
         "type": "scatter",
         "uid": "e84be057-b7d2-4f14-bb13-7bd506b31e75",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.46482213471717987,
          0.4569169963772589,
          0.4687747038871403,
          0.44743083036935377,
          0.4640316208831878,
          0.4877470359500689,
          0.4893280636180531,
          0.47588932808680023,
          0.49169960512002936,
          0.488537549784061,
          0.4940711462686184,
          0.5051383402036584,
          0.5011857710808162,
          0.4948616604559978,
          0.48458498056698224,
          0.5059288540376505,
          0.47509881434704476,
          0.5106719370416031,
          0.49249011895402145,
          0.49090909123891896,
          0.5043478264639029,
          0.4845849803078316,
          0.49090909123891896,
          0.474308300725085,
          0.5043478264639029,
          0.5019762849619266,
          0.47984189756302964,
          0.5083003955867451,
          0.47747035606105337,
          0.5075098817056347,
          0.4529644272072984
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 2",
         "type": "scatter",
         "uid": "d793057e-8144-4cd1-8b6b-4e78ee0a67a6",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.4577075101641327,
          0.46166007938121145,
          0.49565217424287156,
          0.47667984217994297,
          0.4758893284401875,
          0.4893280636180531,
          0.47747035606105337,
          0.4418972335314091,
          0.49723320191085574,
          0.4877470359500689,
          0.48063241113787114,
          0.5027667987016821,
          0.5106719370416031,
          0.49486166036176116,
          0.4901185771457763,
          0.4940711465748874,
          0.49723320191085574,
          0.48300395289899806,
          0.5051383402507766,
          0.5027667987959187,
          0.49090909093265006,
          0.4877470359029506,
          0.49169960507291105,
          0.49011857740492687,
          0.5035573126299108,
          0.4853754944009743,
          0.4814229252310138,
          0.47826086989504546,
          0.4853754944009743,
          0.49565217424287156,
          0.49328063278801354
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 3",
         "type": "scatter",
         "uid": "4c4f875e-c334-4b1f-ac5c-d156a5d0c41b",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.45533596870927473,
          0.4632411070491957,
          0.47826086989504546,
          0.4695652177211324,
          0.47826086989504546,
          0.4711462454362349,
          0.4877470359500689,
          0.4830039529461163,
          0.4782608696358948,
          0.4940711466220057,
          0.5019762849148083,
          0.46956521746198177,
          0.4450592888673775,
          0.4774703558019027,
          0.49960474336571375,
          0.49644268802974534,
          0.5043478264167846,
          0.5083003955867451,
          0.5027667987488004,
          0.499604743412832,
          0.46403162056513925,
          0.4932806327408953,
          0.48063241139702173,
          0.4861660082349664,
          0.5019762849148083,
          0.5035573126299108,
          0.47430830051305267,
          0.49090909123891896,
          0.5019762849619266,
          0.50197628486769,
          0.4940711465748874
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 4",
         "type": "scatter",
         "uid": "1fb21953-794b-448e-902f-6f01e9f62ac5",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.4561264824961485,
          0.4822134391121242,
          0.4592885378792352,
          0.4766798422741796,
          0.48379446673299015,
          0.4837944667801084,
          0.49169960476664215,
          0.49169960512002936,
          0.49565217428998987,
          0.474308300725085,
          0.49011857709865797,
          0.49328063243462633,
          0.49802371574484783,
          0.4924901189069032,
          0.49486166040887947,
          0.48221343906500597,
          0.49644268807686365,
          0.47747035606105337,
          0.49565217424287156,
          0.4853754944009743,
          0.49723320160458684,
          0.5011857710336979,
          0.47905138372903755,
          0.485375494188942,
          0.5011857711279345,
          0.49802371579196614,
          0.4988142295788399,
          0.4671936759600055,
          0.4924901189069032,
          0.5035573125356742,
          0.5043478264639029
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "CNN test set accuracy of padded Flair dataset with variable maximum lengths"
        }
       }
      },
      "text/html": [
       "<div id=\"1a834294-6f7b-4540-971f-00bd1979bec8\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"1a834294-6f7b-4540-971f-00bd1979bec8\")) {\n",
       "    Plotly.newPlot(\"1a834294-6f7b-4540-971f-00bd1979bec8\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.46877470393425863, 0.4592885378792352, 0.4592885378792352, 0.4727272730571008, 0.47905138377615586, 0.4885375497369427, 0.4869565221160768, 0.488537549784061, 0.49090909093265006, 0.4830039526398474, 0.4885375497369427, 0.4893280635709348, 0.49644268802974534, 0.4790513835170052, 0.485375494188942, 0.493280632693777, 0.49644268802974534, 0.48458498056698224, 0.4877470359029506, 0.49169960507291105, 0.5035573125827925, 0.5035573125827925, 0.4853754944009743, 0.4450592888673775, 0.5003952572939424, 0.505138340297895, 0.5067193678716426, 0.49723320195797405, 0.5003952571997058, 0.49723320195797405, 0.4940711465748874], \"type\": \"scatter\", \"uid\": \"0fd0ba98-3944-4d9c-b292-efbd38e637e0\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.46482213471717987, 0.4569169963772589, 0.4687747038871403, 0.44743083036935377, 0.4640316208831878, 0.4877470359500689, 0.4893280636180531, 0.47588932808680023, 0.49169960512002936, 0.488537549784061, 0.4940711462686184, 0.5051383402036584, 0.5011857710808162, 0.4948616604559978, 0.48458498056698224, 0.5059288540376505, 0.47509881434704476, 0.5106719370416031, 0.49249011895402145, 0.49090909123891896, 0.5043478264639029, 0.4845849803078316, 0.49090909123891896, 0.474308300725085, 0.5043478264639029, 0.5019762849619266, 0.47984189756302964, 0.5083003955867451, 0.47747035606105337, 0.5075098817056347, 0.4529644272072984], \"type\": \"scatter\", \"uid\": \"e84be057-b7d2-4f14-bb13-7bd506b31e75\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4577075101641327, 0.46166007938121145, 0.49565217424287156, 0.47667984217994297, 0.4758893284401875, 0.4893280636180531, 0.47747035606105337, 0.4418972335314091, 0.49723320191085574, 0.4877470359500689, 0.48063241113787114, 0.5027667987016821, 0.5106719370416031, 0.49486166036176116, 0.4901185771457763, 0.4940711465748874, 0.49723320191085574, 0.48300395289899806, 0.5051383402507766, 0.5027667987959187, 0.49090909093265006, 0.4877470359029506, 0.49169960507291105, 0.49011857740492687, 0.5035573126299108, 0.4853754944009743, 0.4814229252310138, 0.47826086989504546, 0.4853754944009743, 0.49565217424287156, 0.49328063278801354], \"type\": \"scatter\", \"uid\": \"d793057e-8144-4cd1-8b6b-4e78ee0a67a6\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.45533596870927473, 0.4632411070491957, 0.47826086989504546, 0.4695652177211324, 0.47826086989504546, 0.4711462454362349, 0.4877470359500689, 0.4830039529461163, 0.4782608696358948, 0.4940711466220057, 0.5019762849148083, 0.46956521746198177, 0.4450592888673775, 0.4774703558019027, 0.49960474336571375, 0.49644268802974534, 0.5043478264167846, 0.5083003955867451, 0.5027667987488004, 0.499604743412832, 0.46403162056513925, 0.4932806327408953, 0.48063241139702173, 0.4861660082349664, 0.5019762849148083, 0.5035573126299108, 0.47430830051305267, 0.49090909123891896, 0.5019762849619266, 0.50197628486769, 0.4940711465748874], \"type\": \"scatter\", \"uid\": \"4c4f875e-c334-4b1f-ac5c-d156a5d0c41b\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4561264824961485, 0.4822134391121242, 0.4592885378792352, 0.4766798422741796, 0.48379446673299015, 0.4837944667801084, 0.49169960476664215, 0.49169960512002936, 0.49565217428998987, 0.474308300725085, 0.49011857709865797, 0.49328063243462633, 0.49802371574484783, 0.4924901189069032, 0.49486166040887947, 0.48221343906500597, 0.49644268807686365, 0.47747035606105337, 0.49565217424287156, 0.4853754944009743, 0.49723320160458684, 0.5011857710336979, 0.47905138372903755, 0.485375494188942, 0.5011857711279345, 0.49802371579196614, 0.4988142295788399, 0.4671936759600055, 0.4924901189069032, 0.5035573125356742, 0.5043478264639029], \"type\": \"scatter\", \"uid\": \"1fb21953-794b-448e-902f-6f01e9f62ac5\"}], {\"title\": {\"text\": \"CNN test set accuracy of padded Flair dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"1a834294-6f7b-4540-971f-00bd1979bec8\")) {window._Plotly.Plots.resize(document.getElementById(\"1a834294-6f7b-4540-971f-00bd1979bec8\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"1a834294-6f7b-4540-971f-00bd1979bec8\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"1a834294-6f7b-4540-971f-00bd1979bec8\")) {\n",
       "    Plotly.newPlot(\"1a834294-6f7b-4540-971f-00bd1979bec8\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.46877470393425863, 0.4592885378792352, 0.4592885378792352, 0.4727272730571008, 0.47905138377615586, 0.4885375497369427, 0.4869565221160768, 0.488537549784061, 0.49090909093265006, 0.4830039526398474, 0.4885375497369427, 0.4893280635709348, 0.49644268802974534, 0.4790513835170052, 0.485375494188942, 0.493280632693777, 0.49644268802974534, 0.48458498056698224, 0.4877470359029506, 0.49169960507291105, 0.5035573125827925, 0.5035573125827925, 0.4853754944009743, 0.4450592888673775, 0.5003952572939424, 0.505138340297895, 0.5067193678716426, 0.49723320195797405, 0.5003952571997058, 0.49723320195797405, 0.4940711465748874], \"type\": \"scatter\", \"uid\": \"0fd0ba98-3944-4d9c-b292-efbd38e637e0\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.46482213471717987, 0.4569169963772589, 0.4687747038871403, 0.44743083036935377, 0.4640316208831878, 0.4877470359500689, 0.4893280636180531, 0.47588932808680023, 0.49169960512002936, 0.488537549784061, 0.4940711462686184, 0.5051383402036584, 0.5011857710808162, 0.4948616604559978, 0.48458498056698224, 0.5059288540376505, 0.47509881434704476, 0.5106719370416031, 0.49249011895402145, 0.49090909123891896, 0.5043478264639029, 0.4845849803078316, 0.49090909123891896, 0.474308300725085, 0.5043478264639029, 0.5019762849619266, 0.47984189756302964, 0.5083003955867451, 0.47747035606105337, 0.5075098817056347, 0.4529644272072984], \"type\": \"scatter\", \"uid\": \"e84be057-b7d2-4f14-bb13-7bd506b31e75\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4577075101641327, 0.46166007938121145, 0.49565217424287156, 0.47667984217994297, 0.4758893284401875, 0.4893280636180531, 0.47747035606105337, 0.4418972335314091, 0.49723320191085574, 0.4877470359500689, 0.48063241113787114, 0.5027667987016821, 0.5106719370416031, 0.49486166036176116, 0.4901185771457763, 0.4940711465748874, 0.49723320191085574, 0.48300395289899806, 0.5051383402507766, 0.5027667987959187, 0.49090909093265006, 0.4877470359029506, 0.49169960507291105, 0.49011857740492687, 0.5035573126299108, 0.4853754944009743, 0.4814229252310138, 0.47826086989504546, 0.4853754944009743, 0.49565217424287156, 0.49328063278801354], \"type\": \"scatter\", \"uid\": \"d793057e-8144-4cd1-8b6b-4e78ee0a67a6\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.45533596870927473, 0.4632411070491957, 0.47826086989504546, 0.4695652177211324, 0.47826086989504546, 0.4711462454362349, 0.4877470359500689, 0.4830039529461163, 0.4782608696358948, 0.4940711466220057, 0.5019762849148083, 0.46956521746198177, 0.4450592888673775, 0.4774703558019027, 0.49960474336571375, 0.49644268802974534, 0.5043478264167846, 0.5083003955867451, 0.5027667987488004, 0.499604743412832, 0.46403162056513925, 0.4932806327408953, 0.48063241139702173, 0.4861660082349664, 0.5019762849148083, 0.5035573126299108, 0.47430830051305267, 0.49090909123891896, 0.5019762849619266, 0.50197628486769, 0.4940711465748874], \"type\": \"scatter\", \"uid\": \"4c4f875e-c334-4b1f-ac5c-d156a5d0c41b\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4561264824961485, 0.4822134391121242, 0.4592885378792352, 0.4766798422741796, 0.48379446673299015, 0.4837944667801084, 0.49169960476664215, 0.49169960512002936, 0.49565217428998987, 0.474308300725085, 0.49011857709865797, 0.49328063243462633, 0.49802371574484783, 0.4924901189069032, 0.49486166040887947, 0.48221343906500597, 0.49644268807686365, 0.47747035606105337, 0.49565217424287156, 0.4853754944009743, 0.49723320160458684, 0.5011857710336979, 0.47905138372903755, 0.485375494188942, 0.5011857711279345, 0.49802371579196614, 0.4988142295788399, 0.4671936759600055, 0.4924901189069032, 0.5035573125356742, 0.5043478264639029], \"type\": \"scatter\", \"uid\": \"1fb21953-794b-448e-902f-6f01e9f62ac5\"}], {\"title\": {\"text\": \"CNN test set accuracy of padded Flair dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"1a834294-6f7b-4540-971f-00bd1979bec8\")) {window._Plotly.Plots.resize(document.getElementById(\"1a834294-6f7b-4540-971f-00bd1979bec8\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traces = cnn_flair_rounds\n",
    "\n",
    "# Create traces\n",
    "def create_scatter(counter):\n",
    "    acc_dict = traces[counter]\n",
    "    \n",
    "    return go.Scatter(\n",
    "        x = list(acc_dict.keys()),\n",
    "        y = list(acc_dict.values()),\n",
    "        mode = 'lines+markers',\n",
    "        name = 'Round ' + str(counter)\n",
    "    )\n",
    "\n",
    "trace_data = [create_scatter(trace) for trace in range(len(traces))]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'CNN test set accuracy of padded Flair dataset with variable maximum lengths',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = trace_data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = data.get_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_gpt = {\n",
    "    dataset: [np.concatenate(np.array(statement)) for statement in gpt[dataset].statement]\n",
    "    for dataset in gpt.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "2019-06-14 22:29:10,996 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "2019-06-14 22:29:11,087 From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-06-14 22:29:11,181 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0872 - acc: 0.4126 - val_loss: 1.3453 - val_acc: 0.1931\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 79s 8ms/step - loss: 2.2617 - acc: 0.3929 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 6.9925 - acc: 0.3783 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 56s 6ms/step - loss: 7.4062 - acc: 0.3649 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 4.0335 - acc: 0.4191 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 6.6171 - acc: 0.4258 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 56s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0726 - acc: 0.4262 - val_loss: 1.0877 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 5.2577 - acc: 0.4227 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0854 - acc: 0.4204 - val_loss: 1.0858 - val_acc: 0.3769\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.8817 - acc: 0.4074 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0252 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.1171 - acc: 0.4012 - val_loss: 1.0521 - val_acc: 0.4268\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0835 - acc: 0.4348 - val_loss: 1.0253 - val_acc: 0.4821\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 1.0636 - acc: 0.4468 - val_loss: 1.0246 - val_acc: 0.4844\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 4.8181 - acc: 0.4323 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 2.4710 - acc: 0.4020 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 3.6531 - acc: 0.4152 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0938 - acc: 0.4156 - val_loss: 1.0251 - val_acc: 0.4930\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.7212 - acc: 0.4211 - val_loss: 10.8457 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 12.6508 - acc: 0.2150 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 12.3389 - acc: 0.2338 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.1703 - acc: 0.4031 - val_loss: 1.0277 - val_acc: 0.4860\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 9.5178 - acc: 0.2706 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 11.2122 - acc: 0.3039 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.1114 - acc: 0.4050 - val_loss: 1.1427 - val_acc: 0.3341\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 11.6919 - acc: 0.2437 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.20869565223281092\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 6.5374 - acc: 0.3655 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 9.1468 - acc: 0.3516 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 10.8057 - acc: 0.2376 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.20869565223281092\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 7.2796 - acc: 0.4282 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 7.3988 - acc: 0.4251 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 11.3846 - acc: 0.2285 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.20869565223281092\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 7.3226 - acc: 0.4254 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 10.9029 - acc: 0.2404 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.20869565223281092\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.0161 - acc: 0.2558 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.20869565223281092\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 6.5011 - acc: 0.4227 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 8.4488 - acc: 0.4201 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 8.7315 - acc: 0.3571 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 7.4177 - acc: 0.4277 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 9.2382 - acc: 0.3598 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0678 - acc: 0.4301 - val_loss: 1.0296 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0482 - acc: 0.4479 - val_loss: 1.0220 - val_acc: 0.4899\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0366 - acc: 0.4719 - val_loss: 1.0290 - val_acc: 0.4743\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0311 - acc: 0.4788 - val_loss: 1.0161 - val_acc: 0.4875\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0211 - acc: 0.4879 - val_loss: 1.0192 - val_acc: 0.4945\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.48142292518389557\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0719 - acc: 0.4196 - val_loss: 1.0327 - val_acc: 0.4759\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0488 - acc: 0.4509 - val_loss: 1.0265 - val_acc: 0.4836\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0387 - acc: 0.4684 - val_loss: 1.0295 - val_acc: 0.4813\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0326 - acc: 0.4737 - val_loss: 1.0198 - val_acc: 0.4860\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0281 - acc: 0.4818 - val_loss: 1.0178 - val_acc: 0.4914\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.474308300725085\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0676 - acc: 0.4225 - val_loss: 1.0381 - val_acc: 0.4930\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0471 - acc: 0.4509 - val_loss: 1.0335 - val_acc: 0.4829\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0380 - acc: 0.4719 - val_loss: 1.0213 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0298 - acc: 0.4775 - val_loss: 1.0181 - val_acc: 0.4969\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 67s 6ms/step - loss: 1.0241 - acc: 0.4838 - val_loss: 1.0341 - val_acc: 0.4766\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.46324110709631394\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0708 - acc: 0.4210 - val_loss: 1.0307 - val_acc: 0.4852\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0464 - acc: 0.4599 - val_loss: 1.0253 - val_acc: 0.4992\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0368 - acc: 0.4684 - val_loss: 1.0163 - val_acc: 0.4977\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0292 - acc: 0.4801 - val_loss: 1.0304 - val_acc: 0.4759\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0243 - acc: 0.4857 - val_loss: 1.0143 - val_acc: 0.4860\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.45375494099417224\n",
      "{5: 0.43715415052745654, 6: 0.3541501976402381, 7: 0.3541501976402381, 8: 0.43715415052745654, 9: 0.43715415052745654, 10: 0.43715415052745654, 11: 0.43715415052745654, 12: 0.43715415052745654, 13: 0.3541501976402381, 14: 0.43715415052745654, 15: 0.3541501976402381, 16: 0.3541501976402381, 17: 0.20869565223281092, 18: 0.3541501976402381, 19: 0.3541501976402381, 20: 0.20869565223281092, 21: 0.43715415052745654, 22: 0.43715415052745654, 23: 0.20869565223281092, 24: 0.43715415052745654, 25: 0.20869565223281092, 26: 0.20869565223281092, 27: 0.43715415052745654, 28: 0.43715415052745654, 29: 0.3541501976402381, 30: 0.43715415052745654, 31: 0.3541501976402381, 32: 0.48142292518389557, 33: 0.474308300725085, 34: 0.46324110709631394, 35: 0.45375494099417224}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 4.5599 - acc: 0.4252 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 56s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 56s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 56s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 7.0425 - acc: 0.3721 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 56s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 56s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 6.9313 - acc: 0.4265 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 8.0779 - acc: 0.3725 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 7.3633 - acc: 0.4267 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 4.2296 - acc: 0.3960 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 5.9552 - acc: 0.4212 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 5.6971 - acc: 0.4200 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0915 - acc: 0.4214 - val_loss: 1.0503 - val_acc: 0.4034\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 2.3749 - acc: 0.4205 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0851 - acc: 0.4153 - val_loss: 1.0652 - val_acc: 0.3894\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0901 - acc: 0.4260 - val_loss: 1.0276 - val_acc: 0.4984\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0719 - acc: 0.4438 - val_loss: 1.0538 - val_acc: 0.4805\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0588 - acc: 0.4475 - val_loss: 1.0313 - val_acc: 0.4938\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0720 - acc: 0.4547 - val_loss: 1.0307 - val_acc: 0.4836\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.44822134420334586\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0859 - acc: 0.4194 - val_loss: 1.0814 - val_acc: 0.3933\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0776 - acc: 0.4341 - val_loss: 1.0539 - val_acc: 0.4213\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 1.0485 - acc: 0.4635 - val_loss: 1.0303 - val_acc: 0.4751\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0472 - acc: 0.4634 - val_loss: 1.0176 - val_acc: 0.5078\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0482 - acc: 0.4611 - val_loss: 1.0435 - val_acc: 0.4494\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.4553359684501241\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.1197 - acc: 0.4032 - val_loss: 1.0456 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0885 - acc: 0.4346 - val_loss: 1.0305 - val_acc: 0.4977\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.1696 - acc: 0.4257 - val_loss: 1.0274 - val_acc: 0.4821\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0809 - acc: 0.4378 - val_loss: 1.0271 - val_acc: 0.5023\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 1.0748 - acc: 0.4489 - val_loss: 1.0532 - val_acc: 0.4408\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.44347826103447924\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 3.9372 - acc: 0.4107 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.7913 - acc: 0.4083 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.4606 - acc: 0.3601 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 8.0481 - acc: 0.4304 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 64s 6ms/step - loss: 7.9239 - acc: 0.4301 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 11.4729 - acc: 0.2290 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 12.7889 - acc: 0.2065 - val_loss: 13.0048 - val_acc: 0.1931\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 10.6136 - acc: 0.3410 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 8.0769 - acc: 0.4297 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0694 - acc: 0.3584 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 8.8087 - acc: 0.4010 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 9.2129 - acc: 0.3629 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 10.2193 - acc: 0.2541 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.20869565223281092\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 9.4365 - acc: 0.3579 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 9.8441 - acc: 0.3574 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 8.1414 - acc: 0.4340 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 70s 7ms/step - loss: 7.8353 - acc: 0.4320 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 9.0550 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 9.0549 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0665 - acc: 0.4310 - val_loss: 1.0333 - val_acc: 0.4751\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0473 - acc: 0.4536 - val_loss: 1.0244 - val_acc: 0.4891\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0404 - acc: 0.4642 - val_loss: 1.0211 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0293 - acc: 0.4770 - val_loss: 1.0212 - val_acc: 0.5008\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0250 - acc: 0.4813 - val_loss: 1.0220 - val_acc: 0.4945\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.46324110674292673\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0690 - acc: 0.4230 - val_loss: 1.0333 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0468 - acc: 0.4531 - val_loss: 1.0264 - val_acc: 0.4899\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0385 - acc: 0.4648 - val_loss: 1.0222 - val_acc: 0.4891\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0306 - acc: 0.4764 - val_loss: 1.0243 - val_acc: 0.4922\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0246 - acc: 0.4892 - val_loss: 1.0192 - val_acc: 0.4945\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.4703557315551245\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0681 - acc: 0.4255 - val_loss: 1.0308 - val_acc: 0.4821\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0456 - acc: 0.4526 - val_loss: 1.0245 - val_acc: 0.4922\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0386 - acc: 0.4683 - val_loss: 1.0241 - val_acc: 0.4961\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0303 - acc: 0.4779 - val_loss: 1.0304 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0257 - acc: 0.4856 - val_loss: 1.0149 - val_acc: 0.4922\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.4632411070491957\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.0672 - acc: 0.4322 - val_loss: 1.0332 - val_acc: 0.4868\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0473 - acc: 0.4532 - val_loss: 1.0229 - val_acc: 0.4891\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0359 - acc: 0.4706 - val_loss: 1.0194 - val_acc: 0.5016\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0313 - acc: 0.4778 - val_loss: 1.0255 - val_acc: 0.4945\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0259 - acc: 0.4829 - val_loss: 1.0162 - val_acc: 0.4977\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.4735177868439746\n",
      "{5: 0.43715415052745654, 6: 0.3541501976402381, 7: 0.43715415052745654, 8: 0.3541501976402381, 9: 0.43715415052745654, 10: 0.3541501976402381, 11: 0.43715415052745654, 12: 0.43715415052745654, 13: 0.43715415052745654, 14: 0.44822134420334586, 15: 0.4553359684501241, 16: 0.44347826103447924, 17: 0.43715415052745654, 18: 0.43715415052745654, 19: 0.3541501976402381, 20: 0.43715415052745654, 21: 0.43715415052745654, 22: 0.3541501976402381, 23: 0.43715415052745654, 24: 0.3541501976402381, 25: 0.43715415052745654, 26: 0.3541501976402381, 27: 0.20869565223281092, 28: 0.3541501976402381, 29: 0.3541501976402381, 30: 0.43715415052745654, 31: 0.43715415052745654, 32: 0.46324110674292673, 33: 0.4703557315551245, 34: 0.4632411070491957, 35: 0.4735177868439746}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 3.4822 - acc: 0.4072 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 56s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 56s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 56s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 56s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 7.0371 - acc: 0.4280 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 56s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 56s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 6.4913 - acc: 0.4220 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 5.6425 - acc: 0.4188 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0549 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0514 - acc: 0.4376 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 6.2224 - acc: 0.4220 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.1048 - acc: 0.4139 - val_loss: 1.1026 - val_acc: 0.3427\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 8.3333 - acc: 0.3602 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.7730 - acc: 0.4025 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0751 - acc: 0.4256 - val_loss: 1.0334 - val_acc: 0.4727\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 7.9600 - acc: 0.4139 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 2.6414 - acc: 0.3908 - val_loss: 6.1730 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0445 - acc: 0.4380 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0919 - acc: 0.4152 - val_loss: 1.0282 - val_acc: 0.4907\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 6.5351 - acc: 0.3770 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.5884 - acc: 0.4041 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.1433 - acc: 0.4023 - val_loss: 1.0493 - val_acc: 0.4206\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 6.3853 - acc: 0.4252 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.1111 - acc: 0.4132 - val_loss: 1.0398 - val_acc: 0.4844\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0834 - acc: 0.4371 - val_loss: 1.0272 - val_acc: 0.4821\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0855 - acc: 0.4433 - val_loss: 1.0520 - val_acc: 0.4984\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0821 - acc: 0.4404 - val_loss: 1.0287 - val_acc: 0.4829\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0863 - acc: 0.4423 - val_loss: 1.0593 - val_acc: 0.4611\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.44505928891449575\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0985 - acc: 0.4136 - val_loss: 1.0374 - val_acc: 0.4556\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 8.8622 - acc: 0.3803 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 7.6201 - acc: 0.3603 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 11.4643 - acc: 0.2270 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 12.7889 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.9846 - acc: 0.3183 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 8.1103 - acc: 0.4306 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 10.8130 - acc: 0.2396 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.20869565223281092\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 8.6814 - acc: 0.3854 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 9.0196 - acc: 0.3583 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3920 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3918 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 8.7619 - acc: 0.3814 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 9.0551 - acc: 0.3570 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 7.7006 - acc: 0.4306 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0549 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0480 - acc: 0.4373 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 7.8438 - acc: 0.4265 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 8.5651 - acc: 0.3658 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 7.6107 - acc: 0.4256 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 65s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 11.0206 - acc: 0.2361 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.20869565223281092\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 11.5489 - acc: 0.2309 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 1.0668 - acc: 0.4270 - val_loss: 1.0297 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0464 - acc: 0.4560 - val_loss: 1.0259 - val_acc: 0.4938\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0360 - acc: 0.4707 - val_loss: 1.0211 - val_acc: 0.4790\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0301 - acc: 0.4792 - val_loss: 1.0201 - val_acc: 0.4922\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0250 - acc: 0.4817 - val_loss: 1.0257 - val_acc: 0.4891\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.46166007907494255\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.0643 - acc: 0.4245 - val_loss: 1.0258 - val_acc: 0.4836\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0461 - acc: 0.4586 - val_loss: 1.0254 - val_acc: 0.4953\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0354 - acc: 0.4769 - val_loss: 1.0167 - val_acc: 0.4961\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0304 - acc: 0.4781 - val_loss: 1.0216 - val_acc: 0.4914\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0227 - acc: 0.4909 - val_loss: 1.0212 - val_acc: 0.4829\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.4671936759128872\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 77s 7ms/step - loss: 1.0631 - acc: 0.4326 - val_loss: 1.0259 - val_acc: 0.4805\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0460 - acc: 0.4540 - val_loss: 1.0362 - val_acc: 0.4790\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0363 - acc: 0.4698 - val_loss: 1.0414 - val_acc: 0.4494\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0294 - acc: 0.4788 - val_loss: 1.0137 - val_acc: 0.5101\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0246 - acc: 0.4872 - val_loss: 1.0190 - val_acc: 0.5031\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "0.4727272730571008\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 82s 8ms/step - loss: 1.0654 - acc: 0.4270 - val_loss: 1.0307 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0485 - acc: 0.4532 - val_loss: 1.0296 - val_acc: 0.4891\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0380 - acc: 0.4714 - val_loss: 1.0160 - val_acc: 0.4930\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0296 - acc: 0.4768 - val_loss: 1.0239 - val_acc: 0.4961\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0226 - acc: 0.4841 - val_loss: 1.0211 - val_acc: 0.4836\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "0.47351778693821117\n",
      "{5: 0.43715415052745654, 6: 0.43715415052745654, 7: 0.43715415052745654, 8: 0.43715415052745654, 9: 0.43715415052745654, 10: 0.3541501976402381, 11: 0.43715415052745654, 12: 0.43715415052745654, 13: 0.43715415052745654, 14: 0.43715415052745654, 15: 0.43715415052745654, 16: 0.44505928891449575, 17: 0.3541501976402381, 18: 0.3541501976402381, 19: 0.43715415052745654, 20: 0.43715415052745654, 21: 0.20869565223281092, 22: 0.43715415052745654, 23: 0.3541501976402381, 24: 0.3541501976402381, 25: 0.3541501976402381, 26: 0.43715415052745654, 27: 0.43715415052745654, 28: 0.3541501976402381, 29: 0.43715415052745654, 30: 0.20869565223281092, 31: 0.43715415052745654, 32: 0.46166007907494255, 33: 0.4671936759128872, 34: 0.4727272730571008, 35: 0.47351778693821117}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 5.2880 - acc: 0.4204 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 56s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 6.7751 - acc: 0.3720 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 7.5869 - acc: 0.2984 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 57s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.20869565223281092\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 6.6896 - acc: 0.3768 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 6.8045 - acc: 0.3733 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 3.3487 - acc: 0.4226 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0908 - acc: 0.4095 - val_loss: 1.1245 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.1278 - acc: 0.4132 - val_loss: 1.0318 - val_acc: 0.4751\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 8.2573 - acc: 0.4285 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 4.5009 - acc: 0.3903 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.3581 - acc: 0.4061 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 9.0511 - acc: 0.4377 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.1087 - acc: 0.4102 - val_loss: 1.0294 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 6.0700 - acc: 0.3840 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 12.1017 - acc: 0.2471 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 10.6016 - acc: 0.3422 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0916 - acc: 0.4168 - val_loss: 1.0345 - val_acc: 0.4938\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 3.3237 - acc: 0.4369 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0956 - acc: 0.4173 - val_loss: 1.0269 - val_acc: 0.5047\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0886 - acc: 0.4340 - val_loss: 1.0163 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0776 - acc: 0.4390 - val_loss: 1.0535 - val_acc: 0.4813\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0797 - acc: 0.4478 - val_loss: 1.0901 - val_acc: 0.3941\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0667 - acc: 0.4600 - val_loss: 1.0198 - val_acc: 0.4984\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.46482213471717987\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.1008 - acc: 0.4037 - val_loss: 1.0418 - val_acc: 0.4579\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0801 - acc: 0.4319 - val_loss: 1.0295 - val_acc: 0.4922\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0819 - acc: 0.4380 - val_loss: 1.0445 - val_acc: 0.4673\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0692 - acc: 0.4473 - val_loss: 1.0348 - val_acc: 0.4868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0623 - acc: 0.4531 - val_loss: 1.0233 - val_acc: 0.5008\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.4553359686621564\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 6.5080 - acc: 0.4219 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.1240 - acc: 0.4058 - val_loss: 1.1114 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 9.2242 - acc: 0.3600 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 8.0335 - acc: 0.3664 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 7.1905 - acc: 0.3750 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 9.0174 - acc: 0.3610 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 7.8745 - acc: 0.4353 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 7.7961 - acc: 0.4315 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 7.3963 - acc: 0.4121 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0722 - acc: 0.4362 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 8.1939 - acc: 0.4285 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 99s 10ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 101s 10ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 8.0599 - acc: 0.4308 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 9.3033 - acc: 0.3595 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 11.6251 - acc: 0.2227 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 11.3942 - acc: 0.2926 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 8.1464 - acc: 0.4327 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 9.5039 - acc: 0.3584 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.0692 - acc: 0.4281 - val_loss: 1.0300 - val_acc: 0.5078\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0492 - acc: 0.4510 - val_loss: 1.0246 - val_acc: 0.5023\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0367 - acc: 0.4691 - val_loss: 1.0197 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0324 - acc: 0.4748 - val_loss: 1.0237 - val_acc: 0.4875\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0241 - acc: 0.4874 - val_loss: 1.0112 - val_acc: 0.5078\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.4735177868439746\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 79s 8ms/step - loss: 1.0685 - acc: 0.4307 - val_loss: 1.0501 - val_acc: 0.4439\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0481 - acc: 0.4548 - val_loss: 1.0351 - val_acc: 0.4618\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0358 - acc: 0.4710 - val_loss: 1.0326 - val_acc: 0.4805\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0312 - acc: 0.4808 - val_loss: 1.0282 - val_acc: 0.4813\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0243 - acc: 0.4858 - val_loss: 1.0141 - val_acc: 0.4914\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.4766798422270613\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 81s 8ms/step - loss: 1.0680 - acc: 0.4273 - val_loss: 1.0342 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0493 - acc: 0.4540 - val_loss: 1.0340 - val_acc: 0.4759\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0382 - acc: 0.4702 - val_loss: 1.0423 - val_acc: 0.4455\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0319 - acc: 0.4807 - val_loss: 1.0300 - val_acc: 0.4868\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0256 - acc: 0.4846 - val_loss: 1.0131 - val_acc: 0.4945\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "0.47905138368191924\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 83s 8ms/step - loss: 1.0665 - acc: 0.4298 - val_loss: 1.0449 - val_acc: 0.4548\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0459 - acc: 0.4591 - val_loss: 1.0258 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0383 - acc: 0.4696 - val_loss: 1.0289 - val_acc: 0.4907\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0343 - acc: 0.4720 - val_loss: 1.0190 - val_acc: 0.4969\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0257 - acc: 0.4790 - val_loss: 1.0195 - val_acc: 0.5047\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "0.46877470393425863\n",
      "{5: 0.43715415052745654, 6: 0.3541501976402381, 7: 0.20869565223281092, 8: 0.3541501976402381, 9: 0.3541501976402381, 10: 0.43715415052745654, 11: 0.43715415052745654, 12: 0.3541501976402381, 13: 0.43715415052745654, 14: 0.3541501976402381, 15: 0.43715415052745654, 16: 0.46482213471717987, 17: 0.4553359686621564, 18: 0.43715415052745654, 19: 0.3541501976402381, 20: 0.3541501976402381, 21: 0.3541501976402381, 22: 0.3541501976402381, 23: 0.43715415052745654, 24: 0.43715415052745654, 25: 0.43715415052745654, 26: 0.43715415052745654, 27: 0.43715415052745654, 28: 0.3541501976402381, 29: 0.43715415052745654, 30: 0.43715415052745654, 31: 0.3541501976402381, 32: 0.4735177868439746, 33: 0.4766798422270613, 34: 0.47905138368191924, 35: 0.46877470393425863}\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 5.0931 - acc: 0.3799 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 63s 6ms/step - loss: 4.8297 - acc: 0.3839 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 5.0213 - acc: 0.3922 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 57s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 7.5138 - acc: 0.3668 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 7.9583 - acc: 0.3666 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 58s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 4.2599 - acc: 0.3559 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.20869565223281092\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 1.0862 - acc: 0.4102 - val_loss: 1.0400 - val_acc: 0.4587\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 4.7420 - acc: 0.3891 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0886 - acc: 0.4136 - val_loss: 1.0506 - val_acc: 0.4603\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 7.6253 - acc: 0.3107 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "1265/1265 [==============================] - 2s 2ms/step\n",
      "0.20869565223281092\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 3.6417 - acc: 0.3583 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 60s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.20869565223281092\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0947 - acc: 0.4145 - val_loss: 1.0293 - val_acc: 0.4813\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0691 - acc: 0.4362 - val_loss: 1.0374 - val_acc: 0.4899\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.1070 - acc: 0.4311 - val_loss: 1.0677 - val_acc: 0.4050\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.1139 - acc: 0.4315 - val_loss: 1.0463 - val_acc: 0.4790\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 61s 6ms/step - loss: 1.0698 - acc: 0.4494 - val_loss: 1.0298 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.4584980240923614\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.1027 - acc: 0.4086 - val_loss: 1.0465 - val_acc: 0.4455\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.1403 - acc: 0.4188 - val_loss: 1.0826 - val_acc: 0.4424\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0816 - acc: 0.4342 - val_loss: 1.0315 - val_acc: 0.4766\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0656 - acc: 0.4481 - val_loss: 1.0181 - val_acc: 0.4977\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 1.0561 - acc: 0.4606 - val_loss: 1.0329 - val_acc: 0.4704\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.46245059326232185\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0868 - acc: 0.4197 - val_loss: 1.0372 - val_acc: 0.4696\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 7.2642 - acc: 0.4311 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 1.2140 - acc: 0.4084 - val_loss: 1.0588 - val_acc: 0.4237\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0942 - acc: 0.4333 - val_loss: 1.0270 - val_acc: 0.4907\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0695 - acc: 0.4502 - val_loss: 1.0278 - val_acc: 0.4891\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0591 - acc: 0.4555 - val_loss: 1.0190 - val_acc: 0.4961\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 1.0439 - acc: 0.4685 - val_loss: 1.0207 - val_acc: 0.4992\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.46403162083606947\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 7.3064 - acc: 0.4279 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 8.2169 - acc: 0.4323 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 6.9618 - acc: 0.4249 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 8.9043 - acc: 0.3625 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 10.9225 - acc: 0.2365 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.20869565223281092\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 7.9072 - acc: 0.4268 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 71s 7ms/step - loss: 7.7896 - acc: 0.4357 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 62s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 10.7539 - acc: 0.2533 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 12.7890 - acc: 0.2065 - val_loss: 13.0049 - val_acc: 0.1931\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.20869565223281092\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235/10235 [==============================] - 74s 7ms/step - loss: 9.2903 - acc: 0.3574 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 7.8169 - acc: 0.4332 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 9.4001 - acc: 0.3569 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 8.1916 - acc: 0.4286 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 64s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 9.0551 - acc: 0.4382 - val_loss: 8.3854 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 3s 2ms/step\n",
      "0.43715415052745654\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 73s 7ms/step - loss: 8.6603 - acc: 0.3665 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 76s 7ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 9.3489 - acc: 0.3606 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 65s 6ms/step - loss: 10.3921 - acc: 0.3553 - val_loss: 10.8458 - val_acc: 0.3271\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.3541501976402381\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 77s 7ms/step - loss: 1.0675 - acc: 0.4285 - val_loss: 1.0335 - val_acc: 0.4922\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0469 - acc: 0.4550 - val_loss: 1.0305 - val_acc: 0.4836\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0373 - acc: 0.4689 - val_loss: 1.0233 - val_acc: 0.4945\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0319 - acc: 0.4769 - val_loss: 1.0218 - val_acc: 0.4977\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0235 - acc: 0.4826 - val_loss: 1.0165 - val_acc: 0.4961\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.474308300725085\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 82s 8ms/step - loss: 1.0660 - acc: 0.4299 - val_loss: 1.0308 - val_acc: 0.4844\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0494 - acc: 0.4520 - val_loss: 1.0261 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0378 - acc: 0.4624 - val_loss: 1.0229 - val_acc: 0.5031\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0297 - acc: 0.4798 - val_loss: 1.0160 - val_acc: 0.5008\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0257 - acc: 0.4893 - val_loss: 1.0129 - val_acc: 0.5000\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "0.468774703840022\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 78s 8ms/step - loss: 1.0664 - acc: 0.4345 - val_loss: 1.0269 - val_acc: 0.4875\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0446 - acc: 0.4536 - val_loss: 1.0319 - val_acc: 0.4720\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0370 - acc: 0.4693 - val_loss: 1.0250 - val_acc: 0.4883\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0281 - acc: 0.4844 - val_loss: 1.0189 - val_acc: 0.4860\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0250 - acc: 0.4873 - val_loss: 1.0148 - val_acc: 0.4907\n",
      "1265/1265 [==============================] - 4s 3ms/step\n",
      "0.4719367592231087\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 82s 8ms/step - loss: 1.0694 - acc: 0.4320 - val_loss: 1.0289 - val_acc: 0.4782\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0455 - acc: 0.4547 - val_loss: 1.0260 - val_acc: 0.5093\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0380 - acc: 0.4689 - val_loss: 1.0254 - val_acc: 0.4961\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0293 - acc: 0.4752 - val_loss: 1.0288 - val_acc: 0.4727\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 70s 7ms/step - loss: 1.0209 - acc: 0.4828 - val_loss: 1.0171 - val_acc: 0.5023\n",
      "1265/1265 [==============================] - 3s 3ms/step\n",
      "0.4727272730571008\n",
      "{5: 0.3541501976402381, 6: 0.3541501976402381, 7: 0.3541501976402381, 8: 0.3541501976402381, 9: 0.3541501976402381, 10: 0.20869565223281092, 11: 0.3541501976402381, 12: 0.20869565223281092, 13: 0.20869565223281092, 14: 0.4584980240923614, 15: 0.46245059326232185, 16: 0.43715415052745654, 17: 0.46403162083606947, 18: 0.43715415052745654, 19: 0.43715415052745654, 20: 0.43715415052745654, 21: 0.3541501976402381, 22: 0.20869565223281092, 23: 0.43715415052745654, 24: 0.43715415052745654, 25: 0.20869565223281092, 26: 0.3541501976402381, 27: 0.43715415052745654, 28: 0.3541501976402381, 29: 0.43715415052745654, 30: 0.3541501976402381, 31: 0.3541501976402381, 32: 0.474308300725085, 33: 0.468774703840022, 34: 0.4719367592231087, 35: 0.4727272730571008}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3d 2h 15min 16s, sys: 5h 1min 8s, total: 3d 7h 16min 25s\n",
      "Wall time: 13h 47min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cnn_gpt_rounds = [calculate_round(concatenated_gpt) for round in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{5: 0.43715415052745654,\n",
       "  6: 0.3541501976402381,\n",
       "  7: 0.3541501976402381,\n",
       "  8: 0.43715415052745654,\n",
       "  9: 0.43715415052745654,\n",
       "  10: 0.43715415052745654,\n",
       "  11: 0.43715415052745654,\n",
       "  12: 0.43715415052745654,\n",
       "  13: 0.3541501976402381,\n",
       "  14: 0.43715415052745654,\n",
       "  15: 0.3541501976402381,\n",
       "  16: 0.3541501976402381,\n",
       "  17: 0.20869565223281092,\n",
       "  18: 0.3541501976402381,\n",
       "  19: 0.3541501976402381,\n",
       "  20: 0.20869565223281092,\n",
       "  21: 0.43715415052745654,\n",
       "  22: 0.43715415052745654,\n",
       "  23: 0.20869565223281092,\n",
       "  24: 0.43715415052745654,\n",
       "  25: 0.20869565223281092,\n",
       "  26: 0.20869565223281092,\n",
       "  27: 0.43715415052745654,\n",
       "  28: 0.43715415052745654,\n",
       "  29: 0.3541501976402381,\n",
       "  30: 0.43715415052745654,\n",
       "  31: 0.3541501976402381,\n",
       "  32: 0.48142292518389557,\n",
       "  33: 0.474308300725085,\n",
       "  34: 0.46324110709631394,\n",
       "  35: 0.45375494099417224},\n",
       " {5: 0.43715415052745654,\n",
       "  6: 0.3541501976402381,\n",
       "  7: 0.43715415052745654,\n",
       "  8: 0.3541501976402381,\n",
       "  9: 0.43715415052745654,\n",
       "  10: 0.3541501976402381,\n",
       "  11: 0.43715415052745654,\n",
       "  12: 0.43715415052745654,\n",
       "  13: 0.43715415052745654,\n",
       "  14: 0.44822134420334586,\n",
       "  15: 0.4553359684501241,\n",
       "  16: 0.44347826103447924,\n",
       "  17: 0.43715415052745654,\n",
       "  18: 0.43715415052745654,\n",
       "  19: 0.3541501976402381,\n",
       "  20: 0.43715415052745654,\n",
       "  21: 0.43715415052745654,\n",
       "  22: 0.3541501976402381,\n",
       "  23: 0.43715415052745654,\n",
       "  24: 0.3541501976402381,\n",
       "  25: 0.43715415052745654,\n",
       "  26: 0.3541501976402381,\n",
       "  27: 0.20869565223281092,\n",
       "  28: 0.3541501976402381,\n",
       "  29: 0.3541501976402381,\n",
       "  30: 0.43715415052745654,\n",
       "  31: 0.43715415052745654,\n",
       "  32: 0.46324110674292673,\n",
       "  33: 0.4703557315551245,\n",
       "  34: 0.4632411070491957,\n",
       "  35: 0.4735177868439746},\n",
       " {5: 0.43715415052745654,\n",
       "  6: 0.43715415052745654,\n",
       "  7: 0.43715415052745654,\n",
       "  8: 0.43715415052745654,\n",
       "  9: 0.43715415052745654,\n",
       "  10: 0.3541501976402381,\n",
       "  11: 0.43715415052745654,\n",
       "  12: 0.43715415052745654,\n",
       "  13: 0.43715415052745654,\n",
       "  14: 0.43715415052745654,\n",
       "  15: 0.43715415052745654,\n",
       "  16: 0.44505928891449575,\n",
       "  17: 0.3541501976402381,\n",
       "  18: 0.3541501976402381,\n",
       "  19: 0.43715415052745654,\n",
       "  20: 0.43715415052745654,\n",
       "  21: 0.20869565223281092,\n",
       "  22: 0.43715415052745654,\n",
       "  23: 0.3541501976402381,\n",
       "  24: 0.3541501976402381,\n",
       "  25: 0.3541501976402381,\n",
       "  26: 0.43715415052745654,\n",
       "  27: 0.43715415052745654,\n",
       "  28: 0.3541501976402381,\n",
       "  29: 0.43715415052745654,\n",
       "  30: 0.20869565223281092,\n",
       "  31: 0.43715415052745654,\n",
       "  32: 0.46166007907494255,\n",
       "  33: 0.4671936759128872,\n",
       "  34: 0.4727272730571008,\n",
       "  35: 0.47351778693821117},\n",
       " {5: 0.43715415052745654,\n",
       "  6: 0.3541501976402381,\n",
       "  7: 0.20869565223281092,\n",
       "  8: 0.3541501976402381,\n",
       "  9: 0.3541501976402381,\n",
       "  10: 0.43715415052745654,\n",
       "  11: 0.43715415052745654,\n",
       "  12: 0.3541501976402381,\n",
       "  13: 0.43715415052745654,\n",
       "  14: 0.3541501976402381,\n",
       "  15: 0.43715415052745654,\n",
       "  16: 0.46482213471717987,\n",
       "  17: 0.4553359686621564,\n",
       "  18: 0.43715415052745654,\n",
       "  19: 0.3541501976402381,\n",
       "  20: 0.3541501976402381,\n",
       "  21: 0.3541501976402381,\n",
       "  22: 0.3541501976402381,\n",
       "  23: 0.43715415052745654,\n",
       "  24: 0.43715415052745654,\n",
       "  25: 0.43715415052745654,\n",
       "  26: 0.43715415052745654,\n",
       "  27: 0.43715415052745654,\n",
       "  28: 0.3541501976402381,\n",
       "  29: 0.43715415052745654,\n",
       "  30: 0.43715415052745654,\n",
       "  31: 0.3541501976402381,\n",
       "  32: 0.4735177868439746,\n",
       "  33: 0.4766798422270613,\n",
       "  34: 0.47905138368191924,\n",
       "  35: 0.46877470393425863},\n",
       " {5: 0.3541501976402381,\n",
       "  6: 0.3541501976402381,\n",
       "  7: 0.3541501976402381,\n",
       "  8: 0.3541501976402381,\n",
       "  9: 0.3541501976402381,\n",
       "  10: 0.20869565223281092,\n",
       "  11: 0.3541501976402381,\n",
       "  12: 0.20869565223281092,\n",
       "  13: 0.20869565223281092,\n",
       "  14: 0.4584980240923614,\n",
       "  15: 0.46245059326232185,\n",
       "  16: 0.43715415052745654,\n",
       "  17: 0.46403162083606947,\n",
       "  18: 0.43715415052745654,\n",
       "  19: 0.43715415052745654,\n",
       "  20: 0.43715415052745654,\n",
       "  21: 0.3541501976402381,\n",
       "  22: 0.20869565223281092,\n",
       "  23: 0.43715415052745654,\n",
       "  24: 0.43715415052745654,\n",
       "  25: 0.20869565223281092,\n",
       "  26: 0.3541501976402381,\n",
       "  27: 0.43715415052745654,\n",
       "  28: 0.3541501976402381,\n",
       "  29: 0.43715415052745654,\n",
       "  30: 0.3541501976402381,\n",
       "  31: 0.3541501976402381,\n",
       "  32: 0.474308300725085,\n",
       "  33: 0.468774703840022,\n",
       "  34: 0.4719367592231087,\n",
       "  35: 0.4727272730571008}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_gpt_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Round 0",
         "type": "scatter",
         "uid": "d83c2d45-50cc-4904-be7c-0c6bc319b0eb",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.43715415052745654,
          0.3541501976402381,
          0.3541501976402381,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.3541501976402381,
          0.43715415052745654,
          0.3541501976402381,
          0.3541501976402381,
          0.20869565223281092,
          0.3541501976402381,
          0.3541501976402381,
          0.20869565223281092,
          0.43715415052745654,
          0.43715415052745654,
          0.20869565223281092,
          0.43715415052745654,
          0.20869565223281092,
          0.20869565223281092,
          0.43715415052745654,
          0.43715415052745654,
          0.3541501976402381,
          0.43715415052745654,
          0.3541501976402381,
          0.48142292518389557,
          0.474308300725085,
          0.46324110709631394,
          0.45375494099417224
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 1",
         "type": "scatter",
         "uid": "49e0b47c-9b5e-43cc-9aae-c34900ed5f99",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.43715415052745654,
          0.3541501976402381,
          0.43715415052745654,
          0.3541501976402381,
          0.43715415052745654,
          0.3541501976402381,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.44822134420334586,
          0.4553359684501241,
          0.44347826103447924,
          0.43715415052745654,
          0.43715415052745654,
          0.3541501976402381,
          0.43715415052745654,
          0.43715415052745654,
          0.3541501976402381,
          0.43715415052745654,
          0.3541501976402381,
          0.43715415052745654,
          0.3541501976402381,
          0.20869565223281092,
          0.3541501976402381,
          0.3541501976402381,
          0.43715415052745654,
          0.43715415052745654,
          0.46324110674292673,
          0.4703557315551245,
          0.4632411070491957,
          0.4735177868439746
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 2",
         "type": "scatter",
         "uid": "47fc1951-d6cf-437c-9733-87ad14c80b3c",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.3541501976402381,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.44505928891449575,
          0.3541501976402381,
          0.3541501976402381,
          0.43715415052745654,
          0.43715415052745654,
          0.20869565223281092,
          0.43715415052745654,
          0.3541501976402381,
          0.3541501976402381,
          0.3541501976402381,
          0.43715415052745654,
          0.43715415052745654,
          0.3541501976402381,
          0.43715415052745654,
          0.20869565223281092,
          0.43715415052745654,
          0.46166007907494255,
          0.4671936759128872,
          0.4727272730571008,
          0.47351778693821117
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 3",
         "type": "scatter",
         "uid": "15e87182-04f6-4cbf-b0ca-9951643fecc6",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.43715415052745654,
          0.3541501976402381,
          0.20869565223281092,
          0.3541501976402381,
          0.3541501976402381,
          0.43715415052745654,
          0.43715415052745654,
          0.3541501976402381,
          0.43715415052745654,
          0.3541501976402381,
          0.43715415052745654,
          0.46482213471717987,
          0.4553359686621564,
          0.43715415052745654,
          0.3541501976402381,
          0.3541501976402381,
          0.3541501976402381,
          0.3541501976402381,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.3541501976402381,
          0.43715415052745654,
          0.43715415052745654,
          0.3541501976402381,
          0.4735177868439746,
          0.4766798422270613,
          0.47905138368191924,
          0.46877470393425863
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Round 4",
         "type": "scatter",
         "uid": "5cdc02a0-8273-4c8f-99da-ea722613d031",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.3541501976402381,
          0.3541501976402381,
          0.3541501976402381,
          0.3541501976402381,
          0.3541501976402381,
          0.20869565223281092,
          0.3541501976402381,
          0.20869565223281092,
          0.20869565223281092,
          0.4584980240923614,
          0.46245059326232185,
          0.43715415052745654,
          0.46403162083606947,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.3541501976402381,
          0.20869565223281092,
          0.43715415052745654,
          0.43715415052745654,
          0.20869565223281092,
          0.3541501976402381,
          0.43715415052745654,
          0.3541501976402381,
          0.43715415052745654,
          0.3541501976402381,
          0.3541501976402381,
          0.474308300725085,
          0.468774703840022,
          0.4719367592231087,
          0.4727272730571008
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "CNN test set accuracy of padded GPT dataset with variable maximum lengths"
        }
       }
      },
      "text/html": [
       "<div id=\"a14689d5-f4cb-474b-87ff-e725c567f2d7\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"a14689d5-f4cb-474b-87ff-e725c567f2d7\")) {\n",
       "    Plotly.newPlot(\"a14689d5-f4cb-474b-87ff-e725c567f2d7\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.43715415052745654, 0.3541501976402381, 0.3541501976402381, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.3541501976402381, 0.3541501976402381, 0.20869565223281092, 0.3541501976402381, 0.3541501976402381, 0.20869565223281092, 0.43715415052745654, 0.43715415052745654, 0.20869565223281092, 0.43715415052745654, 0.20869565223281092, 0.20869565223281092, 0.43715415052745654, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.3541501976402381, 0.48142292518389557, 0.474308300725085, 0.46324110709631394, 0.45375494099417224], \"type\": \"scatter\", \"uid\": \"d83c2d45-50cc-4904-be7c-0c6bc319b0eb\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.44822134420334586, 0.4553359684501241, 0.44347826103447924, 0.43715415052745654, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.3541501976402381, 0.20869565223281092, 0.3541501976402381, 0.3541501976402381, 0.43715415052745654, 0.43715415052745654, 0.46324110674292673, 0.4703557315551245, 0.4632411070491957, 0.4735177868439746], \"type\": \"scatter\", \"uid\": \"49e0b47c-9b5e-43cc-9aae-c34900ed5f99\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.44505928891449575, 0.3541501976402381, 0.3541501976402381, 0.43715415052745654, 0.43715415052745654, 0.20869565223281092, 0.43715415052745654, 0.3541501976402381, 0.3541501976402381, 0.3541501976402381, 0.43715415052745654, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.20869565223281092, 0.43715415052745654, 0.46166007907494255, 0.4671936759128872, 0.4727272730571008, 0.47351778693821117], \"type\": \"scatter\", \"uid\": \"47fc1951-d6cf-437c-9733-87ad14c80b3c\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.43715415052745654, 0.3541501976402381, 0.20869565223281092, 0.3541501976402381, 0.3541501976402381, 0.43715415052745654, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.46482213471717987, 0.4553359686621564, 0.43715415052745654, 0.3541501976402381, 0.3541501976402381, 0.3541501976402381, 0.3541501976402381, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.43715415052745654, 0.3541501976402381, 0.4735177868439746, 0.4766798422270613, 0.47905138368191924, 0.46877470393425863], \"type\": \"scatter\", \"uid\": \"15e87182-04f6-4cbf-b0ca-9951643fecc6\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.3541501976402381, 0.3541501976402381, 0.3541501976402381, 0.3541501976402381, 0.3541501976402381, 0.20869565223281092, 0.3541501976402381, 0.20869565223281092, 0.20869565223281092, 0.4584980240923614, 0.46245059326232185, 0.43715415052745654, 0.46403162083606947, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.3541501976402381, 0.20869565223281092, 0.43715415052745654, 0.43715415052745654, 0.20869565223281092, 0.3541501976402381, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.3541501976402381, 0.3541501976402381, 0.474308300725085, 0.468774703840022, 0.4719367592231087, 0.4727272730571008], \"type\": \"scatter\", \"uid\": \"5cdc02a0-8273-4c8f-99da-ea722613d031\"}], {\"title\": {\"text\": \"CNN test set accuracy of padded GPT dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"a14689d5-f4cb-474b-87ff-e725c567f2d7\")) {window._Plotly.Plots.resize(document.getElementById(\"a14689d5-f4cb-474b-87ff-e725c567f2d7\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"a14689d5-f4cb-474b-87ff-e725c567f2d7\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"a14689d5-f4cb-474b-87ff-e725c567f2d7\")) {\n",
       "    Plotly.newPlot(\"a14689d5-f4cb-474b-87ff-e725c567f2d7\", [{\"mode\": \"lines+markers\", \"name\": \"Round 0\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.43715415052745654, 0.3541501976402381, 0.3541501976402381, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.3541501976402381, 0.3541501976402381, 0.20869565223281092, 0.3541501976402381, 0.3541501976402381, 0.20869565223281092, 0.43715415052745654, 0.43715415052745654, 0.20869565223281092, 0.43715415052745654, 0.20869565223281092, 0.20869565223281092, 0.43715415052745654, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.3541501976402381, 0.48142292518389557, 0.474308300725085, 0.46324110709631394, 0.45375494099417224], \"type\": \"scatter\", \"uid\": \"d83c2d45-50cc-4904-be7c-0c6bc319b0eb\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 1\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.44822134420334586, 0.4553359684501241, 0.44347826103447924, 0.43715415052745654, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.3541501976402381, 0.20869565223281092, 0.3541501976402381, 0.3541501976402381, 0.43715415052745654, 0.43715415052745654, 0.46324110674292673, 0.4703557315551245, 0.4632411070491957, 0.4735177868439746], \"type\": \"scatter\", \"uid\": \"49e0b47c-9b5e-43cc-9aae-c34900ed5f99\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 2\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.44505928891449575, 0.3541501976402381, 0.3541501976402381, 0.43715415052745654, 0.43715415052745654, 0.20869565223281092, 0.43715415052745654, 0.3541501976402381, 0.3541501976402381, 0.3541501976402381, 0.43715415052745654, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.20869565223281092, 0.43715415052745654, 0.46166007907494255, 0.4671936759128872, 0.4727272730571008, 0.47351778693821117], \"type\": \"scatter\", \"uid\": \"47fc1951-d6cf-437c-9733-87ad14c80b3c\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 3\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.43715415052745654, 0.3541501976402381, 0.20869565223281092, 0.3541501976402381, 0.3541501976402381, 0.43715415052745654, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.46482213471717987, 0.4553359686621564, 0.43715415052745654, 0.3541501976402381, 0.3541501976402381, 0.3541501976402381, 0.3541501976402381, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.43715415052745654, 0.3541501976402381, 0.4735177868439746, 0.4766798422270613, 0.47905138368191924, 0.46877470393425863], \"type\": \"scatter\", \"uid\": \"15e87182-04f6-4cbf-b0ca-9951643fecc6\"}, {\"mode\": \"lines+markers\", \"name\": \"Round 4\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.3541501976402381, 0.3541501976402381, 0.3541501976402381, 0.3541501976402381, 0.3541501976402381, 0.20869565223281092, 0.3541501976402381, 0.20869565223281092, 0.20869565223281092, 0.4584980240923614, 0.46245059326232185, 0.43715415052745654, 0.46403162083606947, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.3541501976402381, 0.20869565223281092, 0.43715415052745654, 0.43715415052745654, 0.20869565223281092, 0.3541501976402381, 0.43715415052745654, 0.3541501976402381, 0.43715415052745654, 0.3541501976402381, 0.3541501976402381, 0.474308300725085, 0.468774703840022, 0.4719367592231087, 0.4727272730571008], \"type\": \"scatter\", \"uid\": \"5cdc02a0-8273-4c8f-99da-ea722613d031\"}], {\"title\": {\"text\": \"CNN test set accuracy of padded GPT dataset with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"a14689d5-f4cb-474b-87ff-e725c567f2d7\")) {window._Plotly.Plots.resize(document.getElementById(\"a14689d5-f4cb-474b-87ff-e725c567f2d7\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traces = cnn_gpt_rounds\n",
    "\n",
    "# Create traces\n",
    "def create_scatter(counter):\n",
    "    acc_dict = traces[counter]\n",
    "    \n",
    "    return go.Scatter(\n",
    "        x = list(acc_dict.keys()),\n",
    "        y = list(acc_dict.values()),\n",
    "        mode = 'lines+markers',\n",
    "        name = 'Round ' + str(counter)\n",
    "    )\n",
    "\n",
    "trace_data = [create_scatter(trace) for trace in range(len(traces))]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'CNN test set accuracy of padded GPT dataset with variable maximum lengths',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = trace_data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "fill": "tozerox",
         "fillcolor": "rgba(0,100,80,0.2)",
         "line": {
          "color": "rgba(255,255,255,0)"
         },
         "name": "BERT",
         "showlegend": false,
         "type": "scatter",
         "uid": "10c37dce-bbfc-4255-a95f-583607b9b494",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          35,
          34,
          33,
          32,
          31,
          30,
          29,
          28,
          27,
          26,
          25,
          24,
          23,
          22,
          21,
          20,
          19,
          18,
          17,
          16,
          15,
          14,
          13,
          12,
          11,
          10,
          9,
          8,
          7,
          6,
          5
         ],
         "y": [
          0.5169960477606581,
          0.5241106723137052,
          0.5193675889563655,
          0.5225296443865705,
          0.5106719371358397,
          0.5138339924246897,
          0.5201581031437448,
          0.507509881752753,
          0.507509881752753,
          0.5090909093736189,
          0.5146245063058001,
          0.5154150200926739,
          0.5114624508755952,
          0.5098814233018476,
          0.513833992471808,
          0.507509881752753,
          0.5209486169777369,
          0.518577075381524,
          0.5114624509698318,
          0.507509881446484,
          0.5122529644504367,
          0.5177865612883813,
          0.5130434785906977,
          0.5177865615475319,
          0.5114624509698318,
          0.5138339923775714,
          0.5209486169306186,
          0.513043478496461,
          0.5098814233018476,
          0.5169960478077764,
          0.5106719370416031,
          0.5035573125827925,
          0.5059288537784998,
          0.49169960481376046,
          0.5051383399445077,
          0.4988142296259582,
          0.5043478264639029,
          0.49249011895402145,
          0.4877470355966817,
          0.5019762846085394,
          0.49644268807686365,
          0.4877470359029506,
          0.4758893284401875,
          0.49723320191085574,
          0.5067193679658791,
          0.49723320191085574,
          0.4988142295788399,
          0.5011857710808162,
          0.49723320191085574,
          0.47905138342276865,
          0.4980237156977295,
          0.5035573122765236,
          0.48458498056698224,
          0.4948616601026105,
          0.4861660080229341,
          0.48695652180980786,
          0.5035573122765236,
          0.5043478264167846,
          0.509881423207611,
          0.485375494188942,
          0.5027667984896498,
          0.49249011860063424
         ]
        },
        {
         "line": {
          "color": "rgb(0,100,80)"
         },
         "mode": "lines+markers",
         "name": "BERT",
         "type": "scatter",
         "uid": "e197365d-2d2f-4cd9-8df9-028766dfd399",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5043478263225479,
          0.5124110674481146,
          0.5041897235039194,
          0.5150988144130104,
          0.5084584982404595,
          0.5081422927869638,
          0.5032411069455354,
          0.5016600793576523,
          0.5002371544187719,
          0.5000790516943799,
          0.5071936761767497,
          0.5046640318184502,
          0.4932806326702178,
          0.5026086959301719,
          0.5078260872675026,
          0.5037154152977608,
          0.5124110675423512,
          0.5122529647567055,
          0.5019762848535545,
          0.49644268792137325,
          0.5035573125215387,
          0.5071936761673259,
          0.506086956799737,
          0.5000790516425498,
          0.5037154153872855,
          0.5084584983205607,
          0.5114624508897305,
          0.5086166010402409,
          0.5045059290799228,
          0.5089328065502785,
          0.5081422928010995
         ]
        },
        {
         "fill": "tozerox",
         "fillcolor": "rgba(0,176,246,0.2)",
         "line": {
          "color": "rgba(255,255,255,0)"
         },
         "name": "ELMo",
         "showlegend": false,
         "type": "scatter",
         "uid": "05ec9f12-213b-4862-9776-2439d615c287",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          35,
          34,
          33,
          32,
          31,
          30,
          29,
          28,
          27,
          26,
          25,
          24,
          23,
          22,
          21,
          20,
          19,
          18,
          17,
          16,
          15,
          14,
          13,
          12,
          11,
          10,
          9,
          8,
          7,
          6,
          5
         ],
         "y": [
          0.5098814232547293,
          0.5114624509698318,
          0.5304347829385238,
          0.513043478637816,
          0.5177865616417685,
          0.5130434782844288,
          0.5098814233018476,
          0.5185770751223734,
          0.5146245062586818,
          0.505138340297895,
          0.5177865615946502,
          0.5185770754286423,
          0.5201581030966265,
          0.5169960477606581,
          0.5280632414365475,
          0.5359683797293501,
          0.5264822138156815,
          0.5375494074444526,
          0.5185770754757606,
          0.5193675892155161,
          0.5288537553176579,
          0.5225296445986027,
          0.5335968382744921,
          0.5241106722665869,
          0.5320158105593896,
          0.5217391307646106,
          0.5177865615475319,
          0.5264822135094126,
          0.5217391307646106,
          0.5217391307646106,
          0.5217391307174923,
          0.5019762849148083,
          0.5090909094207372,
          0.5003952569405552,
          0.5043478264167846,
          0.5051383402507766,
          0.5035573125827925,
          0.5027667987488004,
          0.4830039526398474,
          0.5083003956338633,
          0.5154150200926739,
          0.4940711462686184,
          0.4988142295788399,
          0.5114624509698318,
          0.49565217424287156,
          0.49565217424287156,
          0.5035573125827925,
          0.513833992471808,
          0.505138340297895,
          0.505138340297895,
          0.5011857707745473,
          0.49644268777059475,
          0.4988142295788399,
          0.5011857707745473,
          0.5011857710336979,
          0.5011857711279345,
          0.49644268807686365,
          0.49249011860063424,
          0.5067193679658791,
          0.5090909094678555,
          0.5019762849148083,
          0.49565217424287156
         ]
        },
        {
         "line": {
          "color": "rgb(0,176,246)"
         },
         "mode": "lines+markers",
         "name": "ELMo",
         "type": "scatter",
         "uid": "7c9a9196-7cb7-4b5f-9d62-2f75961a49d9",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5048221346040959,
          0.5089328066115323,
          0.5179446643096185,
          0.5101976286328357,
          0.5019762847404706,
          0.5040316206570199,
          0.5064031624228587,
          0.5079841899918944,
          0.5087747038258866,
          0.5019762849430792,
          0.5081422927681165,
          0.511936759161855,
          0.5127272730853718,
          0.5108300398649435,
          0.5187351781954407,
          0.5162055339078186,
          0.5147826089830737,
          0.5184189726100138,
          0.5150988145779245,
          0.5101976287176486,
          0.5150988144978234,
          0.5190513837196139,
          0.5192094864534295,
          0.5086166010590881,
          0.5122529646766044,
          0.5152569173258754,
          0.5111462453184391,
          0.51478260892182,
          0.5117786564044801,
          0.5149407118017024,
          0.5105138343124993
         ]
        },
        {
         "fill": "tozerox",
         "fillcolor": "rgba(231,107,243,0.2)",
         "line": {
          "color": "rgba(255,255,255,0)"
         },
         "name": "Transformer-XL",
         "showlegend": false,
         "type": "scatter",
         "uid": "33db4b49-24b7-442f-aed0-c772832bef77",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          35,
          34,
          33,
          32,
          31,
          30,
          29,
          28,
          27,
          26,
          25,
          24,
          23,
          22,
          21,
          20,
          19,
          18,
          17,
          16,
          15,
          14,
          13,
          12,
          11,
          10,
          9,
          8,
          7,
          6,
          5
         ],
         "y": [
          0.4940711465748874,
          0.49644268802974534,
          0.49723320191085574,
          0.5043478264167846,
          0.499604743412832,
          0.49802371574484783,
          0.4924901189069032,
          0.4885375497369427,
          0.4996047434599503,
          0.5043478264167846,
          0.49565217428998987,
          0.49723320191085574,
          0.49802371574484783,
          0.49565217428998987,
          0.49802371574484783,
          0.49723320195797405,
          0.49723320191085574,
          0.49090909093265006,
          0.49169960512002936,
          0.49723320195797405,
          0.4940711465748874,
          0.49802371574484783,
          0.49723320191085574,
          0.4924901189069032,
          0.49249011895402145,
          0.4948616604559978,
          0.49486166040887947,
          0.4893280635709348,
          0.49644268807686365,
          0.4940711466220057,
          0.4940711466220057,
          0.4814229252310138,
          0.48300395289899806,
          0.48537549444809264,
          0.48063241109075283,
          0.4774703561081717,
          0.48458498056698224,
          0.45928853762008454,
          0.48458498061410055,
          0.48537549444809264,
          0.4869565220689585,
          0.46561264855117196,
          0.4735177868910929,
          0.482213438758737,
          0.4869565221160768,
          0.48537549444809264,
          0.3541501976402381,
          0.48379446673299015,
          0.4885375497369427,
          0.4845849802607133,
          0.43715415052745654,
          0.4885375497369427,
          0.48537549444809264,
          0.47826086989504546,
          0.48537549444809264,
          0.4869565220689585,
          0.4869565220689585,
          0.4830039529461163,
          0.48063241139702173,
          0.4901185773578086,
          0.4869565220689585,
          0.474308300725085
         ]
        },
        {
         "line": {
          "color": "rgb(231,107,243)"
         },
         "mode": "lines+markers",
         "name": "Transformer-XL",
         "type": "scatter",
         "uid": "42c3db5c-6ccc-451b-9254-7c74407aa950",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.4869565220595349,
          0.4916996050729111,
          0.4932806327314716,
          0.49059288569589843,
          0.4921739133827299,
          0.49075098848154425,
          0.48996047464755216,
          0.48727272761197904,
          0.486166008272661,
          0.49470355766092833,
          0.4905928857241695,
          0.48047430863964696,
          0.49059288564406833,
          0.49233201609769833,
          0.49027668019057263,
          0.46166007932702546,
          0.4904347829667947,
          0.48932806353795194,
          0.48837944686648405,
          0.49154150227312987,
          0.4861660082349665,
          0.49280632438866984,
          0.492806324468771,
          0.4880632413847173,
          0.4837944667000073,
          0.4888537552893869,
          0.48616600824439005,
          0.4831620556139663,
          0.4918577078585568,
          0.4893280635285283,
          0.4875889331549995
         ]
        },
        {
         "fill": "tozerox",
         "fillcolor": "rgba(205,12,24,0.2)",
         "line": {
          "color": "rgba(205,12,24,0)"
         },
         "name": "Flair",
         "showlegend": false,
         "type": "scatter",
         "uid": "f149f195-88c1-47bf-a5bb-eca3f18c66e0",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          35,
          34,
          33,
          32,
          31,
          30,
          29,
          28,
          27,
          26,
          25,
          24,
          23,
          22,
          21,
          20,
          19,
          18,
          17,
          16,
          15,
          14,
          13,
          12,
          11,
          10,
          9,
          8,
          7,
          6,
          5
         ],
         "y": [
          0.46877470393425863,
          0.4822134391121242,
          0.49565217424287156,
          0.4766798422741796,
          0.48379446673299015,
          0.4893280636180531,
          0.49169960476664215,
          0.49169960512002936,
          0.49723320191085574,
          0.4940711466220057,
          0.5019762849148083,
          0.5051383402036584,
          0.5106719370416031,
          0.4948616604559978,
          0.49960474336571375,
          0.5059288540376505,
          0.5043478264167846,
          0.5106719370416031,
          0.5051383402507766,
          0.5027667987959187,
          0.5043478264639029,
          0.5035573125827925,
          0.49169960507291105,
          0.49011857740492687,
          0.5043478264639029,
          0.505138340297895,
          0.5067193678716426,
          0.5083003955867451,
          0.5019762849619266,
          0.5075098817056347,
          0.5043478264639029,
          0.4529644272072984,
          0.49565217424287156,
          0.47747035606105337,
          0.4671936759600055,
          0.47430830051305267,
          0.4853754944009743,
          0.5003952572939424,
          0.4450592888673775,
          0.47905138372903755,
          0.4845849803078316,
          0.46403162056513925,
          0.4853754944009743,
          0.4877470359029506,
          0.47747035606105337,
          0.47509881434704476,
          0.48221343906500597,
          0.48458498056698224,
          0.4774703558019027,
          0.4450592888673775,
          0.46956521746198177,
          0.48063241113787114,
          0.474308300725085,
          0.4782608696358948,
          0.4418972335314091,
          0.47747035606105337,
          0.4711462454362349,
          0.4640316208831878,
          0.44743083036935377,
          0.4592885378792352,
          0.4569169963772589,
          0.45533596870927473
         ]
        },
        {
         "line": {
          "color": "rgb(205,12,24)"
         },
         "mode": "lines+markers",
         "name": "Flair",
         "type": "scatter",
         "uid": "41ae17ac-d1f5-4a54-ac25-c1d793ede857",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.4605533600041989,
          0.4646640319598051,
          0.4722529647567056,
          0.46861660112034187,
          0.47620553394551335,
          0.4841106723042817,
          0.4866403165023788,
          0.47620553389368314,
          0.49075098837788395,
          0.48553359714421357,
          0.4910671938313797,
          0.49201581047457665,
          0.490276680152878,
          0.48774703580871404,
          0.4909090911352588,
          0.4943873520802132,
          0.4939130437562588,
          0.49280632443107636,
          0.4967588936198841,
          0.494071146584311,
          0.49201581042981435,
          0.4940711465136335,
          0.48553359716777267,
          0.4762055338842595,
          0.5022924904860997,
          0.49881422961653454,
          0.4882213441515157,
          0.48837944692773777,
          0.49154150230611265,
          0.5011857710619688,
          0.48774703592179786
         ]
        },
        {
         "fill": "tozerox",
         "fillcolor": "rgba(81, 45, 168, 0.2)",
         "line": {
          "color": "rgba(81, 45, 168, 0)"
         },
         "name": "GPT",
         "showlegend": false,
         "type": "scatter",
         "uid": "c9b5892a-ef43-4028-93ed-b3dfe7475355",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          35,
          34,
          33,
          32,
          31,
          30,
          29,
          28,
          27,
          26,
          25,
          24,
          23,
          22,
          21,
          20,
          19,
          18,
          17,
          16,
          15,
          14,
          13,
          12,
          11,
          10,
          9,
          8,
          7,
          6,
          5
         ],
         "y": [
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.4584980240923614,
          0.46245059326232185,
          0.46482213471717987,
          0.46403162083606947,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.43715415052745654,
          0.48142292518389557,
          0.4766798422270613,
          0.47905138368191924,
          0.47351778693821117,
          0.45375494099417224,
          0.4632411070491957,
          0.4671936759128872,
          0.46166007907494255,
          0.3541501976402381,
          0.20869565223281092,
          0.3541501976402381,
          0.3541501976402381,
          0.20869565223281092,
          0.20869565223281092,
          0.20869565223281092,
          0.3541501976402381,
          0.20869565223281092,
          0.20869565223281092,
          0.20869565223281092,
          0.20869565223281092,
          0.3541501976402381,
          0.3541501976402381,
          0.20869565223281092,
          0.3541501976402381,
          0.3541501976402381,
          0.3541501976402381,
          0.20869565223281092,
          0.20869565223281092,
          0.3541501976402381,
          0.20869565223281092,
          0.3541501976402381,
          0.3541501976402381,
          0.20869565223281092,
          0.3541501976402381,
          0.3541501976402381
         ]
        },
        {
         "line": {
          "color": "rgb(81, 45, 168)"
         },
         "mode": "lines+markers",
         "name": "GPT",
         "type": "scatter",
         "uid": "016e2983-bc37-4e86-993e-0d911cabb23f",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.42055335995001286,
          0.37075098821768177,
          0.3582608697136401,
          0.3873517787951255,
          0.4039525693725691,
          0.3582608697136401,
          0.42055335995001286,
          0.37486166029108375,
          0.37486166029108375,
          0.42703557339817166,
          0.42924901208151944,
          0.4289328065667699,
          0.3838735179797463,
          0.4039525693725691,
          0.38735177879512545,
          0.37486166029108375,
          0.3582608697136401,
          0.3582608697136401,
          0.37486166029108375,
          0.4039525693725691,
          0.32916996063215465,
          0.35826086971364,
          0.39146245086852743,
          0.3707509882176818,
          0.4039525693725691,
          0.37486166029108375,
          0.3873517787951255,
          0.47083003971416487,
          0.471462450852036,
          0.47003952602152765,
          0.46845849835354353
         ]
        },
        {
         "line": {
          "color": "rgb(0,0,0)"
         },
         "mode": "lines",
         "name": "Majority vote",
         "type": "scatter",
         "uid": "549f5030-f911-448c-b4ad-e8a6f3505b9f",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428,
          0.4428
         ]
        },
        {
         "mode": "lines",
         "name": "Khurana (2017)",
         "type": "scatter",
         "uid": "aaf109eb-33d0-4d5c-b038-db9167facca9",
         "x": [
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903,
          0.4903
         ]
        }
       ],
       "layout": {
        "paper_bgcolor": "rgb(255,255,255)",
        "plot_bgcolor": "rgb(229,229,229)",
        "title": {
         "text": "CNN test set accuracy of padded datasets with variable maximum lengths"
        },
        "xaxis": {
         "gridcolor": "rgb(255,255,255)",
         "range": [
          5,
          35
         ],
         "showgrid": true,
         "showline": false,
         "showticklabels": true,
         "tickcolor": "rgb(127,127,127)",
         "ticks": "outside",
         "zeroline": false
        },
        "yaxis": {
         "gridcolor": "rgb(255,255,255)",
         "showgrid": true,
         "showline": false,
         "showticklabels": true,
         "tickcolor": "rgb(127,127,127)",
         "ticks": "outside",
         "zeroline": false
        }
       }
      },
      "text/html": [
       "<div id=\"fd89acec-42fe-4f92-850b-27f478249932\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"fd89acec-42fe-4f92-850b-27f478249932\")) {\n",
       "    Plotly.newPlot(\"fd89acec-42fe-4f92-850b-27f478249932\", [{\"fill\": \"tozerox\", \"fillcolor\": \"rgba(0,100,80,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"BERT\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.5169960477606581, 0.5241106723137052, 0.5193675889563655, 0.5225296443865705, 0.5106719371358397, 0.5138339924246897, 0.5201581031437448, 0.507509881752753, 0.507509881752753, 0.5090909093736189, 0.5146245063058001, 0.5154150200926739, 0.5114624508755952, 0.5098814233018476, 0.513833992471808, 0.507509881752753, 0.5209486169777369, 0.518577075381524, 0.5114624509698318, 0.507509881446484, 0.5122529644504367, 0.5177865612883813, 0.5130434785906977, 0.5177865615475319, 0.5114624509698318, 0.5138339923775714, 0.5209486169306186, 0.513043478496461, 0.5098814233018476, 0.5169960478077764, 0.5106719370416031, 0.5035573125827925, 0.5059288537784998, 0.49169960481376046, 0.5051383399445077, 0.4988142296259582, 0.5043478264639029, 0.49249011895402145, 0.4877470355966817, 0.5019762846085394, 0.49644268807686365, 0.4877470359029506, 0.4758893284401875, 0.49723320191085574, 0.5067193679658791, 0.49723320191085574, 0.4988142295788399, 0.5011857710808162, 0.49723320191085574, 0.47905138342276865, 0.4980237156977295, 0.5035573122765236, 0.48458498056698224, 0.4948616601026105, 0.4861660080229341, 0.48695652180980786, 0.5035573122765236, 0.5043478264167846, 0.509881423207611, 0.485375494188942, 0.5027667984896498, 0.49249011860063424], \"type\": \"scatter\", \"uid\": \"10c37dce-bbfc-4255-a95f-583607b9b494\"}, {\"line\": {\"color\": \"rgb(0,100,80)\"}, \"mode\": \"lines+markers\", \"name\": \"BERT\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5043478263225479, 0.5124110674481146, 0.5041897235039194, 0.5150988144130104, 0.5084584982404595, 0.5081422927869638, 0.5032411069455354, 0.5016600793576523, 0.5002371544187719, 0.5000790516943799, 0.5071936761767497, 0.5046640318184502, 0.4932806326702178, 0.5026086959301719, 0.5078260872675026, 0.5037154152977608, 0.5124110675423512, 0.5122529647567055, 0.5019762848535545, 0.49644268792137325, 0.5035573125215387, 0.5071936761673259, 0.506086956799737, 0.5000790516425498, 0.5037154153872855, 0.5084584983205607, 0.5114624508897305, 0.5086166010402409, 0.5045059290799228, 0.5089328065502785, 0.5081422928010995], \"type\": \"scatter\", \"uid\": \"e197365d-2d2f-4cd9-8df9-028766dfd399\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(0,176,246,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"ELMo\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.5098814232547293, 0.5114624509698318, 0.5304347829385238, 0.513043478637816, 0.5177865616417685, 0.5130434782844288, 0.5098814233018476, 0.5185770751223734, 0.5146245062586818, 0.505138340297895, 0.5177865615946502, 0.5185770754286423, 0.5201581030966265, 0.5169960477606581, 0.5280632414365475, 0.5359683797293501, 0.5264822138156815, 0.5375494074444526, 0.5185770754757606, 0.5193675892155161, 0.5288537553176579, 0.5225296445986027, 0.5335968382744921, 0.5241106722665869, 0.5320158105593896, 0.5217391307646106, 0.5177865615475319, 0.5264822135094126, 0.5217391307646106, 0.5217391307646106, 0.5217391307174923, 0.5019762849148083, 0.5090909094207372, 0.5003952569405552, 0.5043478264167846, 0.5051383402507766, 0.5035573125827925, 0.5027667987488004, 0.4830039526398474, 0.5083003956338633, 0.5154150200926739, 0.4940711462686184, 0.4988142295788399, 0.5114624509698318, 0.49565217424287156, 0.49565217424287156, 0.5035573125827925, 0.513833992471808, 0.505138340297895, 0.505138340297895, 0.5011857707745473, 0.49644268777059475, 0.4988142295788399, 0.5011857707745473, 0.5011857710336979, 0.5011857711279345, 0.49644268807686365, 0.49249011860063424, 0.5067193679658791, 0.5090909094678555, 0.5019762849148083, 0.49565217424287156], \"type\": \"scatter\", \"uid\": \"05ec9f12-213b-4862-9776-2439d615c287\"}, {\"line\": {\"color\": \"rgb(0,176,246)\"}, \"mode\": \"lines+markers\", \"name\": \"ELMo\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5048221346040959, 0.5089328066115323, 0.5179446643096185, 0.5101976286328357, 0.5019762847404706, 0.5040316206570199, 0.5064031624228587, 0.5079841899918944, 0.5087747038258866, 0.5019762849430792, 0.5081422927681165, 0.511936759161855, 0.5127272730853718, 0.5108300398649435, 0.5187351781954407, 0.5162055339078186, 0.5147826089830737, 0.5184189726100138, 0.5150988145779245, 0.5101976287176486, 0.5150988144978234, 0.5190513837196139, 0.5192094864534295, 0.5086166010590881, 0.5122529646766044, 0.5152569173258754, 0.5111462453184391, 0.51478260892182, 0.5117786564044801, 0.5149407118017024, 0.5105138343124993], \"type\": \"scatter\", \"uid\": \"7c9a9196-7cb7-4b5f-9d62-2f75961a49d9\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(231,107,243,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"Transformer-XL\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.4940711465748874, 0.49644268802974534, 0.49723320191085574, 0.5043478264167846, 0.499604743412832, 0.49802371574484783, 0.4924901189069032, 0.4885375497369427, 0.4996047434599503, 0.5043478264167846, 0.49565217428998987, 0.49723320191085574, 0.49802371574484783, 0.49565217428998987, 0.49802371574484783, 0.49723320195797405, 0.49723320191085574, 0.49090909093265006, 0.49169960512002936, 0.49723320195797405, 0.4940711465748874, 0.49802371574484783, 0.49723320191085574, 0.4924901189069032, 0.49249011895402145, 0.4948616604559978, 0.49486166040887947, 0.4893280635709348, 0.49644268807686365, 0.4940711466220057, 0.4940711466220057, 0.4814229252310138, 0.48300395289899806, 0.48537549444809264, 0.48063241109075283, 0.4774703561081717, 0.48458498056698224, 0.45928853762008454, 0.48458498061410055, 0.48537549444809264, 0.4869565220689585, 0.46561264855117196, 0.4735177868910929, 0.482213438758737, 0.4869565221160768, 0.48537549444809264, 0.3541501976402381, 0.48379446673299015, 0.4885375497369427, 0.4845849802607133, 0.43715415052745654, 0.4885375497369427, 0.48537549444809264, 0.47826086989504546, 0.48537549444809264, 0.4869565220689585, 0.4869565220689585, 0.4830039529461163, 0.48063241139702173, 0.4901185773578086, 0.4869565220689585, 0.474308300725085], \"type\": \"scatter\", \"uid\": \"33db4b49-24b7-442f-aed0-c772832bef77\"}, {\"line\": {\"color\": \"rgb(231,107,243)\"}, \"mode\": \"lines+markers\", \"name\": \"Transformer-XL\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4869565220595349, 0.4916996050729111, 0.4932806327314716, 0.49059288569589843, 0.4921739133827299, 0.49075098848154425, 0.48996047464755216, 0.48727272761197904, 0.486166008272661, 0.49470355766092833, 0.4905928857241695, 0.48047430863964696, 0.49059288564406833, 0.49233201609769833, 0.49027668019057263, 0.46166007932702546, 0.4904347829667947, 0.48932806353795194, 0.48837944686648405, 0.49154150227312987, 0.4861660082349665, 0.49280632438866984, 0.492806324468771, 0.4880632413847173, 0.4837944667000073, 0.4888537552893869, 0.48616600824439005, 0.4831620556139663, 0.4918577078585568, 0.4893280635285283, 0.4875889331549995], \"type\": \"scatter\", \"uid\": \"42c3db5c-6ccc-451b-9254-7c74407aa950\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(205,12,24,0.2)\", \"line\": {\"color\": \"rgba(205,12,24,0)\"}, \"name\": \"Flair\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.46877470393425863, 0.4822134391121242, 0.49565217424287156, 0.4766798422741796, 0.48379446673299015, 0.4893280636180531, 0.49169960476664215, 0.49169960512002936, 0.49723320191085574, 0.4940711466220057, 0.5019762849148083, 0.5051383402036584, 0.5106719370416031, 0.4948616604559978, 0.49960474336571375, 0.5059288540376505, 0.5043478264167846, 0.5106719370416031, 0.5051383402507766, 0.5027667987959187, 0.5043478264639029, 0.5035573125827925, 0.49169960507291105, 0.49011857740492687, 0.5043478264639029, 0.505138340297895, 0.5067193678716426, 0.5083003955867451, 0.5019762849619266, 0.5075098817056347, 0.5043478264639029, 0.4529644272072984, 0.49565217424287156, 0.47747035606105337, 0.4671936759600055, 0.47430830051305267, 0.4853754944009743, 0.5003952572939424, 0.4450592888673775, 0.47905138372903755, 0.4845849803078316, 0.46403162056513925, 0.4853754944009743, 0.4877470359029506, 0.47747035606105337, 0.47509881434704476, 0.48221343906500597, 0.48458498056698224, 0.4774703558019027, 0.4450592888673775, 0.46956521746198177, 0.48063241113787114, 0.474308300725085, 0.4782608696358948, 0.4418972335314091, 0.47747035606105337, 0.4711462454362349, 0.4640316208831878, 0.44743083036935377, 0.4592885378792352, 0.4569169963772589, 0.45533596870927473], \"type\": \"scatter\", \"uid\": \"f149f195-88c1-47bf-a5bb-eca3f18c66e0\"}, {\"line\": {\"color\": \"rgb(205,12,24)\"}, \"mode\": \"lines+markers\", \"name\": \"Flair\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4605533600041989, 0.4646640319598051, 0.4722529647567056, 0.46861660112034187, 0.47620553394551335, 0.4841106723042817, 0.4866403165023788, 0.47620553389368314, 0.49075098837788395, 0.48553359714421357, 0.4910671938313797, 0.49201581047457665, 0.490276680152878, 0.48774703580871404, 0.4909090911352588, 0.4943873520802132, 0.4939130437562588, 0.49280632443107636, 0.4967588936198841, 0.494071146584311, 0.49201581042981435, 0.4940711465136335, 0.48553359716777267, 0.4762055338842595, 0.5022924904860997, 0.49881422961653454, 0.4882213441515157, 0.48837944692773777, 0.49154150230611265, 0.5011857710619688, 0.48774703592179786], \"type\": \"scatter\", \"uid\": \"41ae17ac-d1f5-4a54-ac25-c1d793ede857\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(81, 45, 168, 0.2)\", \"line\": {\"color\": \"rgba(81, 45, 168, 0)\"}, \"name\": \"GPT\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.4584980240923614, 0.46245059326232185, 0.46482213471717987, 0.46403162083606947, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.48142292518389557, 0.4766798422270613, 0.47905138368191924, 0.47351778693821117, 0.45375494099417224, 0.4632411070491957, 0.4671936759128872, 0.46166007907494255, 0.3541501976402381, 0.20869565223281092, 0.3541501976402381, 0.3541501976402381, 0.20869565223281092, 0.20869565223281092, 0.20869565223281092, 0.3541501976402381, 0.20869565223281092, 0.20869565223281092, 0.20869565223281092, 0.20869565223281092, 0.3541501976402381, 0.3541501976402381, 0.20869565223281092, 0.3541501976402381, 0.3541501976402381, 0.3541501976402381, 0.20869565223281092, 0.20869565223281092, 0.3541501976402381, 0.20869565223281092, 0.3541501976402381, 0.3541501976402381, 0.20869565223281092, 0.3541501976402381, 0.3541501976402381], \"type\": \"scatter\", \"uid\": \"c9b5892a-ef43-4028-93ed-b3dfe7475355\"}, {\"line\": {\"color\": \"rgb(81, 45, 168)\"}, \"mode\": \"lines+markers\", \"name\": \"GPT\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.42055335995001286, 0.37075098821768177, 0.3582608697136401, 0.3873517787951255, 0.4039525693725691, 0.3582608697136401, 0.42055335995001286, 0.37486166029108375, 0.37486166029108375, 0.42703557339817166, 0.42924901208151944, 0.4289328065667699, 0.3838735179797463, 0.4039525693725691, 0.38735177879512545, 0.37486166029108375, 0.3582608697136401, 0.3582608697136401, 0.37486166029108375, 0.4039525693725691, 0.32916996063215465, 0.35826086971364, 0.39146245086852743, 0.3707509882176818, 0.4039525693725691, 0.37486166029108375, 0.3873517787951255, 0.47083003971416487, 0.471462450852036, 0.47003952602152765, 0.46845849835354353], \"type\": \"scatter\", \"uid\": \"016e2983-bc37-4e86-993e-0d911cabb23f\"}, {\"line\": {\"color\": \"rgb(0,0,0)\"}, \"mode\": \"lines\", \"name\": \"Majority vote\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428], \"type\": \"scatter\", \"uid\": \"549f5030-f911-448c-b4ad-e8a6f3505b9f\"}, {\"mode\": \"lines\", \"name\": \"Khurana (2017)\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903], \"type\": \"scatter\", \"uid\": \"aaf109eb-33d0-4d5c-b038-db9167facca9\"}], {\"paper_bgcolor\": \"rgb(255,255,255)\", \"plot_bgcolor\": \"rgb(229,229,229)\", \"title\": {\"text\": \"CNN test set accuracy of padded datasets with variable maximum lengths\"}, \"xaxis\": {\"gridcolor\": \"rgb(255,255,255)\", \"range\": [5, 35], \"showgrid\": true, \"showline\": false, \"showticklabels\": true, \"tickcolor\": \"rgb(127,127,127)\", \"ticks\": \"outside\", \"zeroline\": false}, \"yaxis\": {\"gridcolor\": \"rgb(255,255,255)\", \"showgrid\": true, \"showline\": false, \"showticklabels\": true, \"tickcolor\": \"rgb(127,127,127)\", \"ticks\": \"outside\", \"zeroline\": false}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"fd89acec-42fe-4f92-850b-27f478249932\")) {window._Plotly.Plots.resize(document.getElementById(\"fd89acec-42fe-4f92-850b-27f478249932\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"fd89acec-42fe-4f92-850b-27f478249932\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"fd89acec-42fe-4f92-850b-27f478249932\")) {\n",
       "    Plotly.newPlot(\"fd89acec-42fe-4f92-850b-27f478249932\", [{\"fill\": \"tozerox\", \"fillcolor\": \"rgba(0,100,80,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"BERT\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.5169960477606581, 0.5241106723137052, 0.5193675889563655, 0.5225296443865705, 0.5106719371358397, 0.5138339924246897, 0.5201581031437448, 0.507509881752753, 0.507509881752753, 0.5090909093736189, 0.5146245063058001, 0.5154150200926739, 0.5114624508755952, 0.5098814233018476, 0.513833992471808, 0.507509881752753, 0.5209486169777369, 0.518577075381524, 0.5114624509698318, 0.507509881446484, 0.5122529644504367, 0.5177865612883813, 0.5130434785906977, 0.5177865615475319, 0.5114624509698318, 0.5138339923775714, 0.5209486169306186, 0.513043478496461, 0.5098814233018476, 0.5169960478077764, 0.5106719370416031, 0.5035573125827925, 0.5059288537784998, 0.49169960481376046, 0.5051383399445077, 0.4988142296259582, 0.5043478264639029, 0.49249011895402145, 0.4877470355966817, 0.5019762846085394, 0.49644268807686365, 0.4877470359029506, 0.4758893284401875, 0.49723320191085574, 0.5067193679658791, 0.49723320191085574, 0.4988142295788399, 0.5011857710808162, 0.49723320191085574, 0.47905138342276865, 0.4980237156977295, 0.5035573122765236, 0.48458498056698224, 0.4948616601026105, 0.4861660080229341, 0.48695652180980786, 0.5035573122765236, 0.5043478264167846, 0.509881423207611, 0.485375494188942, 0.5027667984896498, 0.49249011860063424], \"type\": \"scatter\", \"uid\": \"10c37dce-bbfc-4255-a95f-583607b9b494\"}, {\"line\": {\"color\": \"rgb(0,100,80)\"}, \"mode\": \"lines+markers\", \"name\": \"BERT\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5043478263225479, 0.5124110674481146, 0.5041897235039194, 0.5150988144130104, 0.5084584982404595, 0.5081422927869638, 0.5032411069455354, 0.5016600793576523, 0.5002371544187719, 0.5000790516943799, 0.5071936761767497, 0.5046640318184502, 0.4932806326702178, 0.5026086959301719, 0.5078260872675026, 0.5037154152977608, 0.5124110675423512, 0.5122529647567055, 0.5019762848535545, 0.49644268792137325, 0.5035573125215387, 0.5071936761673259, 0.506086956799737, 0.5000790516425498, 0.5037154153872855, 0.5084584983205607, 0.5114624508897305, 0.5086166010402409, 0.5045059290799228, 0.5089328065502785, 0.5081422928010995], \"type\": \"scatter\", \"uid\": \"e197365d-2d2f-4cd9-8df9-028766dfd399\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(0,176,246,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"ELMo\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.5098814232547293, 0.5114624509698318, 0.5304347829385238, 0.513043478637816, 0.5177865616417685, 0.5130434782844288, 0.5098814233018476, 0.5185770751223734, 0.5146245062586818, 0.505138340297895, 0.5177865615946502, 0.5185770754286423, 0.5201581030966265, 0.5169960477606581, 0.5280632414365475, 0.5359683797293501, 0.5264822138156815, 0.5375494074444526, 0.5185770754757606, 0.5193675892155161, 0.5288537553176579, 0.5225296445986027, 0.5335968382744921, 0.5241106722665869, 0.5320158105593896, 0.5217391307646106, 0.5177865615475319, 0.5264822135094126, 0.5217391307646106, 0.5217391307646106, 0.5217391307174923, 0.5019762849148083, 0.5090909094207372, 0.5003952569405552, 0.5043478264167846, 0.5051383402507766, 0.5035573125827925, 0.5027667987488004, 0.4830039526398474, 0.5083003956338633, 0.5154150200926739, 0.4940711462686184, 0.4988142295788399, 0.5114624509698318, 0.49565217424287156, 0.49565217424287156, 0.5035573125827925, 0.513833992471808, 0.505138340297895, 0.505138340297895, 0.5011857707745473, 0.49644268777059475, 0.4988142295788399, 0.5011857707745473, 0.5011857710336979, 0.5011857711279345, 0.49644268807686365, 0.49249011860063424, 0.5067193679658791, 0.5090909094678555, 0.5019762849148083, 0.49565217424287156], \"type\": \"scatter\", \"uid\": \"05ec9f12-213b-4862-9776-2439d615c287\"}, {\"line\": {\"color\": \"rgb(0,176,246)\"}, \"mode\": \"lines+markers\", \"name\": \"ELMo\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5048221346040959, 0.5089328066115323, 0.5179446643096185, 0.5101976286328357, 0.5019762847404706, 0.5040316206570199, 0.5064031624228587, 0.5079841899918944, 0.5087747038258866, 0.5019762849430792, 0.5081422927681165, 0.511936759161855, 0.5127272730853718, 0.5108300398649435, 0.5187351781954407, 0.5162055339078186, 0.5147826089830737, 0.5184189726100138, 0.5150988145779245, 0.5101976287176486, 0.5150988144978234, 0.5190513837196139, 0.5192094864534295, 0.5086166010590881, 0.5122529646766044, 0.5152569173258754, 0.5111462453184391, 0.51478260892182, 0.5117786564044801, 0.5149407118017024, 0.5105138343124993], \"type\": \"scatter\", \"uid\": \"7c9a9196-7cb7-4b5f-9d62-2f75961a49d9\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(231,107,243,0.2)\", \"line\": {\"color\": \"rgba(255,255,255,0)\"}, \"name\": \"Transformer-XL\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.4940711465748874, 0.49644268802974534, 0.49723320191085574, 0.5043478264167846, 0.499604743412832, 0.49802371574484783, 0.4924901189069032, 0.4885375497369427, 0.4996047434599503, 0.5043478264167846, 0.49565217428998987, 0.49723320191085574, 0.49802371574484783, 0.49565217428998987, 0.49802371574484783, 0.49723320195797405, 0.49723320191085574, 0.49090909093265006, 0.49169960512002936, 0.49723320195797405, 0.4940711465748874, 0.49802371574484783, 0.49723320191085574, 0.4924901189069032, 0.49249011895402145, 0.4948616604559978, 0.49486166040887947, 0.4893280635709348, 0.49644268807686365, 0.4940711466220057, 0.4940711466220057, 0.4814229252310138, 0.48300395289899806, 0.48537549444809264, 0.48063241109075283, 0.4774703561081717, 0.48458498056698224, 0.45928853762008454, 0.48458498061410055, 0.48537549444809264, 0.4869565220689585, 0.46561264855117196, 0.4735177868910929, 0.482213438758737, 0.4869565221160768, 0.48537549444809264, 0.3541501976402381, 0.48379446673299015, 0.4885375497369427, 0.4845849802607133, 0.43715415052745654, 0.4885375497369427, 0.48537549444809264, 0.47826086989504546, 0.48537549444809264, 0.4869565220689585, 0.4869565220689585, 0.4830039529461163, 0.48063241139702173, 0.4901185773578086, 0.4869565220689585, 0.474308300725085], \"type\": \"scatter\", \"uid\": \"33db4b49-24b7-442f-aed0-c772832bef77\"}, {\"line\": {\"color\": \"rgb(231,107,243)\"}, \"mode\": \"lines+markers\", \"name\": \"Transformer-XL\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4869565220595349, 0.4916996050729111, 0.4932806327314716, 0.49059288569589843, 0.4921739133827299, 0.49075098848154425, 0.48996047464755216, 0.48727272761197904, 0.486166008272661, 0.49470355766092833, 0.4905928857241695, 0.48047430863964696, 0.49059288564406833, 0.49233201609769833, 0.49027668019057263, 0.46166007932702546, 0.4904347829667947, 0.48932806353795194, 0.48837944686648405, 0.49154150227312987, 0.4861660082349665, 0.49280632438866984, 0.492806324468771, 0.4880632413847173, 0.4837944667000073, 0.4888537552893869, 0.48616600824439005, 0.4831620556139663, 0.4918577078585568, 0.4893280635285283, 0.4875889331549995], \"type\": \"scatter\", \"uid\": \"42c3db5c-6ccc-451b-9254-7c74407aa950\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(205,12,24,0.2)\", \"line\": {\"color\": \"rgba(205,12,24,0)\"}, \"name\": \"Flair\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.46877470393425863, 0.4822134391121242, 0.49565217424287156, 0.4766798422741796, 0.48379446673299015, 0.4893280636180531, 0.49169960476664215, 0.49169960512002936, 0.49723320191085574, 0.4940711466220057, 0.5019762849148083, 0.5051383402036584, 0.5106719370416031, 0.4948616604559978, 0.49960474336571375, 0.5059288540376505, 0.5043478264167846, 0.5106719370416031, 0.5051383402507766, 0.5027667987959187, 0.5043478264639029, 0.5035573125827925, 0.49169960507291105, 0.49011857740492687, 0.5043478264639029, 0.505138340297895, 0.5067193678716426, 0.5083003955867451, 0.5019762849619266, 0.5075098817056347, 0.5043478264639029, 0.4529644272072984, 0.49565217424287156, 0.47747035606105337, 0.4671936759600055, 0.47430830051305267, 0.4853754944009743, 0.5003952572939424, 0.4450592888673775, 0.47905138372903755, 0.4845849803078316, 0.46403162056513925, 0.4853754944009743, 0.4877470359029506, 0.47747035606105337, 0.47509881434704476, 0.48221343906500597, 0.48458498056698224, 0.4774703558019027, 0.4450592888673775, 0.46956521746198177, 0.48063241113787114, 0.474308300725085, 0.4782608696358948, 0.4418972335314091, 0.47747035606105337, 0.4711462454362349, 0.4640316208831878, 0.44743083036935377, 0.4592885378792352, 0.4569169963772589, 0.45533596870927473], \"type\": \"scatter\", \"uid\": \"f149f195-88c1-47bf-a5bb-eca3f18c66e0\"}, {\"line\": {\"color\": \"rgb(205,12,24)\"}, \"mode\": \"lines+markers\", \"name\": \"Flair\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4605533600041989, 0.4646640319598051, 0.4722529647567056, 0.46861660112034187, 0.47620553394551335, 0.4841106723042817, 0.4866403165023788, 0.47620553389368314, 0.49075098837788395, 0.48553359714421357, 0.4910671938313797, 0.49201581047457665, 0.490276680152878, 0.48774703580871404, 0.4909090911352588, 0.4943873520802132, 0.4939130437562588, 0.49280632443107636, 0.4967588936198841, 0.494071146584311, 0.49201581042981435, 0.4940711465136335, 0.48553359716777267, 0.4762055338842595, 0.5022924904860997, 0.49881422961653454, 0.4882213441515157, 0.48837944692773777, 0.49154150230611265, 0.5011857710619688, 0.48774703592179786], \"type\": \"scatter\", \"uid\": \"41ae17ac-d1f5-4a54-ac25-c1d793ede857\"}, {\"fill\": \"tozerox\", \"fillcolor\": \"rgba(81, 45, 168, 0.2)\", \"line\": {\"color\": \"rgba(81, 45, 168, 0)\"}, \"name\": \"GPT\", \"showlegend\": false, \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5], \"y\": [0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.4584980240923614, 0.46245059326232185, 0.46482213471717987, 0.46403162083606947, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.43715415052745654, 0.48142292518389557, 0.4766798422270613, 0.47905138368191924, 0.47351778693821117, 0.45375494099417224, 0.4632411070491957, 0.4671936759128872, 0.46166007907494255, 0.3541501976402381, 0.20869565223281092, 0.3541501976402381, 0.3541501976402381, 0.20869565223281092, 0.20869565223281092, 0.20869565223281092, 0.3541501976402381, 0.20869565223281092, 0.20869565223281092, 0.20869565223281092, 0.20869565223281092, 0.3541501976402381, 0.3541501976402381, 0.20869565223281092, 0.3541501976402381, 0.3541501976402381, 0.3541501976402381, 0.20869565223281092, 0.20869565223281092, 0.3541501976402381, 0.20869565223281092, 0.3541501976402381, 0.3541501976402381, 0.20869565223281092, 0.3541501976402381, 0.3541501976402381], \"type\": \"scatter\", \"uid\": \"c9b5892a-ef43-4028-93ed-b3dfe7475355\"}, {\"line\": {\"color\": \"rgb(81, 45, 168)\"}, \"mode\": \"lines+markers\", \"name\": \"GPT\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.42055335995001286, 0.37075098821768177, 0.3582608697136401, 0.3873517787951255, 0.4039525693725691, 0.3582608697136401, 0.42055335995001286, 0.37486166029108375, 0.37486166029108375, 0.42703557339817166, 0.42924901208151944, 0.4289328065667699, 0.3838735179797463, 0.4039525693725691, 0.38735177879512545, 0.37486166029108375, 0.3582608697136401, 0.3582608697136401, 0.37486166029108375, 0.4039525693725691, 0.32916996063215465, 0.35826086971364, 0.39146245086852743, 0.3707509882176818, 0.4039525693725691, 0.37486166029108375, 0.3873517787951255, 0.47083003971416487, 0.471462450852036, 0.47003952602152765, 0.46845849835354353], \"type\": \"scatter\", \"uid\": \"016e2983-bc37-4e86-993e-0d911cabb23f\"}, {\"line\": {\"color\": \"rgb(0,0,0)\"}, \"mode\": \"lines\", \"name\": \"Majority vote\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428, 0.4428], \"type\": \"scatter\", \"uid\": \"549f5030-f911-448c-b4ad-e8a6f3505b9f\"}, {\"mode\": \"lines\", \"name\": \"Khurana (2017)\", \"x\": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903, 0.4903], \"type\": \"scatter\", \"uid\": \"aaf109eb-33d0-4d5c-b038-db9167facca9\"}], {\"paper_bgcolor\": \"rgb(255,255,255)\", \"plot_bgcolor\": \"rgb(229,229,229)\", \"title\": {\"text\": \"CNN test set accuracy of padded datasets with variable maximum lengths\"}, \"xaxis\": {\"gridcolor\": \"rgb(255,255,255)\", \"range\": [5, 35], \"showgrid\": true, \"showline\": false, \"showticklabels\": true, \"tickcolor\": \"rgb(127,127,127)\", \"ticks\": \"outside\", \"zeroline\": false}, \"yaxis\": {\"gridcolor\": \"rgb(255,255,255)\", \"showgrid\": true, \"showline\": false, \"showticklabels\": true, \"tickcolor\": \"rgb(127,127,127)\", \"ticks\": \"outside\", \"zeroline\": false}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"fd89acec-42fe-4f92-850b-27f478249932\")) {window._Plotly.Plots.resize(document.getElementById(\"fd89acec-42fe-4f92-850b-27f478249932\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(5, 36))\n",
    "x_rev = x[::-1]\n",
    "\n",
    "# BERT\n",
    "bert_matrix = np.transpose(np.array([np.array(list(acc_round.values())) for acc_round in cnn_bert_rounds]))\n",
    "bert_y = [np.average(row) for row in bert_matrix]\n",
    "bert_y_upper = [row.max() for row in bert_matrix]\n",
    "bert_y_lower = [row.min() for row in bert_matrix]\n",
    "bert_y_lower = bert_y_lower[::-1]\n",
    "\n",
    "bert1 = go.Scatter(\n",
    "    x = x + x_rev,\n",
    "    y = bert_y_upper + bert_y_lower,\n",
    "    fill = 'tozerox',\n",
    "    fillcolor = 'rgba(0,100,80,0.2)',\n",
    "    line = dict(color = 'rgba(255,255,255,0)'),\n",
    "    showlegend = False,\n",
    "    name = 'BERT',\n",
    ")\n",
    "bert2 = go.Scatter(\n",
    "    x = x,\n",
    "    y = bert_y,\n",
    "    line = dict(color='rgb(0,100,80)'),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'BERT',\n",
    ")\n",
    "\n",
    "# ELMo\n",
    "elmo_matrix = np.transpose(np.array([np.array(list(acc_round.values())) for acc_round in cnn_elmo_rounds]))\n",
    "elmo_y = [np.average(row) for row in elmo_matrix]\n",
    "elmo_y_upper = [row.max() for row in elmo_matrix]\n",
    "elmo_y_lower = [row.min() for row in elmo_matrix]\n",
    "elmo_y_lower = elmo_y_lower[::-1]\n",
    "\n",
    "elmo1 = go.Scatter(\n",
    "    x = x + x_rev,\n",
    "    y = elmo_y_upper + elmo_y_lower,\n",
    "    fill = 'tozerox',\n",
    "    fillcolor = 'rgba(0,176,246,0.2)',\n",
    "    line = dict(color = 'rgba(255,255,255,0)'),\n",
    "    showlegend = False,\n",
    "    name = 'ELMo',\n",
    ")\n",
    "elmo2 = go.Scatter(\n",
    "    x = x,\n",
    "    y = elmo_y,\n",
    "    line = dict(color='rgb(0,176,246)'),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'ELMo',\n",
    ")\n",
    "\n",
    "# Transformer-XL\n",
    "transformerxl_matrix = np.transpose(np.array([np.array(list(acc_round.values())) for acc_round in cnn_transformerxl_rounds]))\n",
    "transformerxl_y = [np.average(row) for row in transformerxl_matrix]\n",
    "transformerxl_y_upper = [row.max() for row in transformerxl_matrix]\n",
    "transformerxl_y_lower = [row.min() for row in transformerxl_matrix]\n",
    "transformerxl_y_lower = transformerxl_y_lower[::-1]\n",
    "\n",
    "transformerxl1 = go.Scatter(\n",
    "    x = x + x_rev,\n",
    "    y = transformerxl_y_upper + transformerxl_y_lower,\n",
    "    fill = 'tozerox',\n",
    "    fillcolor = 'rgba(231,107,243,0.2)',\n",
    "    line = dict(color = 'rgba(255,255,255,0)'),\n",
    "    showlegend = False,\n",
    "    name = 'Transformer-XL',\n",
    ")\n",
    "transformerxl2 = go.Scatter(\n",
    "    x = x,\n",
    "    y = transformerxl_y,\n",
    "    line = dict(color='rgb(231,107,243)'),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Transformer-XL',\n",
    ")\n",
    "\n",
    "# GPT\n",
    "gpt_matrix = np.transpose(np.array([np.array(list(acc_round.values())) for acc_round in cnn_gpt_rounds]))\n",
    "gpt_y = [np.average(row) for row in gpt_matrix]\n",
    "gpt_y_upper = [row.max() for row in gpt_matrix]\n",
    "gpt_y_lower = [row.min() for row in gpt_matrix]\n",
    "gpt_y_lower = gpt_y_lower[::-1]\n",
    "\n",
    "gpt1 = go.Scatter(\n",
    "    x = x + x_rev,\n",
    "    y = gpt_y_upper + gpt_y_lower,\n",
    "    fill = 'tozerox',\n",
    "    fillcolor = 'rgba(81, 45, 168, 0.2)',\n",
    "    line = dict(color = 'rgba(81, 45, 168, 0)'),\n",
    "    showlegend = False,\n",
    "    name = 'GPT',\n",
    ")\n",
    "gpt2 = go.Scatter(\n",
    "    x = x,\n",
    "    y = gpt_y,\n",
    "    line = dict(color='rgb(81, 45, 168)'),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'GPT',\n",
    ")\n",
    "\n",
    "# Flair\n",
    "flair_matrix = np.transpose(np.array([np.array(list(acc_round.values())) for acc_round in cnn_flair_rounds]))\n",
    "flair_y = [np.average(row) for row in flair_matrix]\n",
    "flair_y_upper = [row.max() for row in flair_matrix]\n",
    "flair_y_lower = [row.min() for row in flair_matrix]\n",
    "flair_y_lower = flair_y_lower[::-1]\n",
    "\n",
    "flair1 = go.Scatter(\n",
    "    x = x + x_rev,\n",
    "    y = flair_y_upper + flair_y_lower,\n",
    "    fill = 'tozerox',\n",
    "    fillcolor = 'rgba(205,12,24,0.2)',\n",
    "    line = dict(color = 'rgba(205,12,24,0)'),\n",
    "    showlegend = False,\n",
    "    name = 'Flair',\n",
    ")\n",
    "flair2 = go.Scatter(\n",
    "    x = x,\n",
    "    y = flair_y,\n",
    "    line = dict(color='rgb(205,12,24)'),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Flair',\n",
    ")\n",
    "\n",
    "# Baseline and highest performance\n",
    "baseline = go.Scatter(\n",
    "    x = x,\n",
    "    y = [0.4428 for i in x],\n",
    "    line = dict(color='rgb(0,0,0)'),\n",
    "    mode = 'lines',\n",
    "    name = 'Majority vote',\n",
    ")\n",
    "best_past_performance = go.Scatter(\n",
    "    x = x,\n",
    "    y = [0.4903 for i in x],\n",
    "    mode = 'lines',\n",
    "    name = 'Khurana (2017)',\n",
    ")\n",
    "\n",
    "data = [bert1, bert2, elmo1, elmo2, transformerxl1, transformerxl2, flair1, flair2, gpt1, gpt2, baseline, best_past_performance]\n",
    "layout = go.Layout(\n",
    "    title = 'CNN test set accuracy of padded datasets with variable maximum lengths',\n",
    "    paper_bgcolor = 'rgb(255,255,255)',\n",
    "    plot_bgcolor = 'rgb(229,229,229)',\n",
    "    xaxis = dict(\n",
    "        gridcolor = 'rgb(255,255,255)',\n",
    "        range = [5,35],\n",
    "        showgrid = True,\n",
    "        showline = False,\n",
    "        showticklabels = True,\n",
    "        tickcolor = 'rgb(127,127,127)',\n",
    "        ticks = 'outside',\n",
    "        zeroline = False\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        gridcolor='rgb(255,255,255)',\n",
    "        showgrid = True,\n",
    "        showline = False,\n",
    "        showticklabels = True,\n",
    "        tickcolor = 'rgb(127,127,127)',\n",
    "        ticks = 'outside',\n",
    "        zeroline = False\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### References\n",
    "\n",
    "```\n",
    "@article{DBLP:journals/corr/Wang17j,\n",
    "  author    = {William Yang Wang},\n",
    "  title     = {\"Liar, Liar Pants on Fire\": {A} New Benchmark Dataset for Fake News\n",
    "               Detection},\n",
    "  journal   = {CoRR},\n",
    "  volume    = {abs/1705.00648},\n",
    "  year      = {2017},\n",
    "  url       = {http://arxiv.org/abs/1705.00648},\n",
    "  archivePrefix = {arXiv},\n",
    "  eprint    = {1705.00648},\n",
    "  timestamp = {Mon, 13 Aug 2018 16:48:58 +0200},\n",
    "  biburl    = {https://dblp.org/rec/bib/journals/corr/Wang17j},\n",
    "  bibsource = {dblp computer science bibliography, https://dblp.org}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
