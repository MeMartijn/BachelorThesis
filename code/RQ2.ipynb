{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using neural classification\n",
    "As has been proven by [Wang (2017)](https://arxiv.org/abs/1705.00648), neural classifiers carry better results than non-neural classifiers when detecting fake news. However, it is unknown how well neural networks classify fake news when using previously mentioned text embeddings. \n",
    "In this notebook, the second research question will be answered: *how well do neural network architecture classify fake news compared to non-neural classification algorithms?*\n",
    "\n",
    "<hr>\n",
    "\n",
    "## On the usage of neural networks\n",
    "Literature on CNNs and Bi-LSTMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Reshape, Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "# Set offline mode for plotly\n",
    "init_notebook_mode(connected = True)\n",
    "\n",
    "# The DataLoader class gives access to pretrained vectors from the Liar dataset\n",
    "from data_loader import DataLoader\n",
    "data = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "general = data.get_dfs()\n",
    "\n",
    "# Recode labels from 6 to 3\n",
    "def recode(label):\n",
    "    if label == 'false' or label == 'pants-fire' or label == 'barely-true':\n",
    "        return 0\n",
    "    elif label == 'true' or label == 'mostly-true':\n",
    "        return 2\n",
    "    elif label == 'half-true':\n",
    "        return 1\n",
    "\n",
    "for dataset in general.keys():\n",
    "    general[dataset]['label'] = general[dataset]['label'].apply(lambda label: recode(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Bidirectional LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = data.get_bert()\n",
    "\n",
    "# Get max-pooled BERT embeddings from RQ1\n",
    "def max_pool(statement):\n",
    "    if len(statement) > 1:\n",
    "        return [row.max() for row in np.transpose([[token_row.max() for token_row in np.transpose(np.array(sentence))] for sentence in statement])]\n",
    "    else:\n",
    "        return [token_row.max() for token_row in np.transpose(statement[0])]\n",
    "\n",
    "max_pooled_bert = {\n",
    "    dataset: pd.DataFrame(list(bert[dataset].statement.apply(lambda statement: max_pool(statement)).values))\n",
    "    for dataset in bert.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = data.get_bert()\n",
    "\n",
    "padded_bert = {\n",
    "    dataset: sequence.pad_sequences([np.concatenate(np.array(statement)) for statement in bert[dataset].statement], maxlen = 25, dtype = float)\n",
    "    for dataset in bert.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_score(X_train, X_test, X_validation, y_train = general['train']['label'], y_test = general['test']['label'], y_validation = general['validation']['label'], reshape = True):\n",
    "    # Rearrange data types    \n",
    "    params = locals().copy()    \n",
    "    inputs = {\n",
    "        dataset: np.array(params[dataset])\n",
    "        for dataset in params.keys()\n",
    "    }\n",
    "    \n",
    "    for dataset in inputs.keys():\n",
    "        if dataset[0:1] == 'X' and reshape:\n",
    "            # Reshape datasets from 2D to 3D\n",
    "            inputs[dataset] = np.reshape(inputs[dataset], (inputs[dataset].shape[0], inputs[dataset].shape[1], 1))\n",
    "        elif dataset[0:1] == 'y':\n",
    "            inputs[dataset] = np_utils.to_categorical(np.array(inputs[dataset]), 3)\n",
    "    \n",
    "    # Set model parameters\n",
    "    epochs = 5\n",
    "    batch_size = 32\n",
    "    input_shape = X_train.shape\n",
    "\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, input_shape = input_shape)))\n",
    "    model.add(Dropout(0.8))\n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "    model.compile('sgd', 'categorical_crossentropy', metrics = ['accuracy']) \n",
    "    \n",
    "    # Fit the training set over the model and correct on the validation set\n",
    "    model.fit(inputs['X_train'], inputs['y_train'],\n",
    "            batch_size = batch_size,\n",
    "            epochs = epochs,\n",
    "            validation_data = (inputs['X_validation'], inputs['y_validation']))\n",
    "    \n",
    "    # Get score over the test set\n",
    "    score, acc = model.evaluate(inputs['X_test'], inputs['y_test'])\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "2019-05-21 12:40:37,047 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "2019-05-21 12:40:37,448 From /Users/martijn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-05-21 12:40:37,537 From /Users/martijn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 829s 81ms/step - loss: 1.0743 - acc: 0.4087 - val_loss: 1.0423 - val_acc: 0.4798\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 844s 82ms/step - loss: 1.0632 - acc: 0.4300 - val_loss: 1.0392 - val_acc: 0.4798\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 838s 82ms/step - loss: 1.0590 - acc: 0.4354 - val_loss: 1.0408 - val_acc: 0.4798\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 840s 82ms/step - loss: 1.0592 - acc: 0.4354 - val_loss: 1.0432 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 809s 79ms/step - loss: 1.0591 - acc: 0.4337 - val_loss: 1.0395 - val_acc: 0.4798\n",
      "1265/1265 [==============================] - 16s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.43715415052745654"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bilstm_score(max_pooled_bert['train'], max_pooled_bert['test'], max_pooled_bert['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, the condensed datasets from RQ1 do not perform well when using a neural classifier. The next step is trying out a padding approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 79s 8ms/step - loss: 1.0980 - acc: 0.4256 - val_loss: 1.0154 - val_acc: 0.5109\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0562 - acc: 0.4586 - val_loss: 1.0149 - val_acc: 0.5187\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0459 - acc: 0.4670 - val_loss: 1.0089 - val_acc: 0.5226\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0332 - acc: 0.4796 - val_loss: 1.0057 - val_acc: 0.5280\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 51s 5ms/step - loss: 1.0267 - acc: 0.4864 - val_loss: 1.0080 - val_acc: 0.5241\n",
      "1265/1265 [==============================] - 6s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 82s 8ms/step - loss: 1.0963 - acc: 0.4217 - val_loss: 1.0139 - val_acc: 0.5187\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 66s 6ms/step - loss: 1.0558 - acc: 0.4580 - val_loss: 1.0120 - val_acc: 0.5265\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0428 - acc: 0.4729 - val_loss: 1.0064 - val_acc: 0.5273\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 53s 5ms/step - loss: 1.0379 - acc: 0.4768 - val_loss: 1.0106 - val_acc: 0.5210\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 52s 5ms/step - loss: 1.0271 - acc: 0.4934 - val_loss: 1.0005 - val_acc: 0.5304\n",
      "1265/1265 [==============================] - 7s 5ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 85s 8ms/step - loss: 1.0945 - acc: 0.4284 - val_loss: 1.0146 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0521 - acc: 0.4651 - val_loss: 1.0135 - val_acc: 0.5257\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 55s 5ms/step - loss: 1.0409 - acc: 0.4744 - val_loss: 1.0071 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0320 - acc: 0.4875 - val_loss: 1.0005 - val_acc: 0.5195\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 54s 5ms/step - loss: 1.0260 - acc: 0.4939 - val_loss: 1.0046 - val_acc: 0.5109\n",
      "1265/1265 [==============================] - 7s 6ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 90s 9ms/step - loss: 1.0989 - acc: 0.4238 - val_loss: 1.0188 - val_acc: 0.5117\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 68s 7ms/step - loss: 1.0521 - acc: 0.4617 - val_loss: 1.0138 - val_acc: 0.5125\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 56s 6ms/step - loss: 1.0425 - acc: 0.4717 - val_loss: 1.0089 - val_acc: 0.5187\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 56s 5ms/step - loss: 1.0339 - acc: 0.4807 - val_loss: 1.0050 - val_acc: 0.5187\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 56s 6ms/step - loss: 1.0279 - acc: 0.4887 - val_loss: 1.0061 - val_acc: 0.5117\n",
      "1265/1265 [==============================] - 9s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 90s 9ms/step - loss: 1.0980 - acc: 0.4227 - val_loss: 1.0275 - val_acc: 0.5125\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 74s 7ms/step - loss: 1.0563 - acc: 0.4626 - val_loss: 1.0176 - val_acc: 0.5132\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 67s 7ms/step - loss: 1.0442 - acc: 0.4683 - val_loss: 1.0144 - val_acc: 0.5202\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0370 - acc: 0.4788 - val_loss: 1.0037 - val_acc: 0.5226\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 59s 6ms/step - loss: 1.0299 - acc: 0.4943 - val_loss: 1.0029 - val_acc: 0.5234\n",
      "1265/1265 [==============================] - 8s 7ms/step\n",
      "Train on 10235 samples, validate on 1284 samples\n",
      "Epoch 1/5\n",
      "10235/10235 [==============================] - 94s 9ms/step - loss: 1.0909 - acc: 0.4265 - val_loss: 1.0232 - val_acc: 0.5023\n",
      "Epoch 2/5\n",
      "10235/10235 [==============================] - 75s 7ms/step - loss: 1.0552 - acc: 0.4615 - val_loss: 1.0166 - val_acc: 0.5171\n",
      "Epoch 3/5\n",
      "10235/10235 [==============================] - 69s 7ms/step - loss: 1.0404 - acc: 0.4811 - val_loss: 1.0097 - val_acc: 0.5171\n",
      "Epoch 4/5\n",
      "10235/10235 [==============================] - 63s 6ms/step - loss: 1.0375 - acc: 0.4807 - val_loss: 1.0115 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "10235/10235 [==============================] - 72s 7ms/step - loss: 1.0310 - acc: 0.4870 - val_loss: 1.0086 - val_acc: 0.5055\n",
      "1265/1265 [==============================] - 9s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# Store accuracies\n",
    "accuracies = {\n",
    "    padding_len: 0.0 for padding_len in list(range(20,36))\n",
    "}\n",
    "\n",
    "concatenated_bert = {\n",
    "    dataset: [np.concatenate(np.array(statement)) for statement in bert[dataset].statement]\n",
    "    for dataset in bert.keys()\n",
    "}\n",
    "\n",
    "for max_len in accuracies.keys():\n",
    "    padded_bert = {\n",
    "        dataset: sequence.pad_sequences(concatenated_bert[dataset], maxlen = max_len, dtype = float)\n",
    "        for dataset in concatenated_bert.keys()\n",
    "    }\n",
    "    \n",
    "    accuracies[max_len] = get_bilstm_score(padded_bert['train'], padded_bert['test'], padded_bert['validation'], reshape = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "BERT",
         "type": "scatter",
         "uid": "4bafcb1f-6d4d-4127-8af4-d03da6bbf90e",
         "x": [
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "y": [
          0.5098814233018476,
          0.5146245059524129,
          0.5114624509698318,
          0.5098814233018476,
          0.5146245059524129,
          0.5162055339737843,
          0.515415019786405,
          0.5098814229484603,
          0.5106719371358397,
          0.5067193676124919,
          0.5106719367824524,
          0.5154150201397922,
          0.5090909091615865,
          0.5083003952804761,
          0.5114624509698318,
          0.513043478637816
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "Test set accuracy of padded datasets with variable maximum lengths"
        }
       }
      },
      "text/html": [
       "<div id=\"bf2e8719-617e-4235-b831-8a149eda2370\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"bf2e8719-617e-4235-b831-8a149eda2370\")) {\n",
       "    Plotly.newPlot(\"bf2e8719-617e-4235-b831-8a149eda2370\", [{\"mode\": \"lines+markers\", \"name\": \"BERT\", \"x\": [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5098814233018476, 0.5146245059524129, 0.5114624509698318, 0.5098814233018476, 0.5146245059524129, 0.5162055339737843, 0.515415019786405, 0.5098814229484603, 0.5106719371358397, 0.5067193676124919, 0.5106719367824524, 0.5154150201397922, 0.5090909091615865, 0.5083003952804761, 0.5114624509698318, 0.513043478637816], \"type\": \"scatter\", \"uid\": \"4bafcb1f-6d4d-4127-8af4-d03da6bbf90e\"}], {\"title\": {\"text\": \"Test set accuracy of padded datasets with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"bf2e8719-617e-4235-b831-8a149eda2370\")) {window._Plotly.Plots.resize(document.getElementById(\"bf2e8719-617e-4235-b831-8a149eda2370\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"bf2e8719-617e-4235-b831-8a149eda2370\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"bf2e8719-617e-4235-b831-8a149eda2370\")) {\n",
       "    Plotly.newPlot(\"bf2e8719-617e-4235-b831-8a149eda2370\", [{\"mode\": \"lines+markers\", \"name\": \"BERT\", \"x\": [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], \"y\": [0.5098814233018476, 0.5146245059524129, 0.5114624509698318, 0.5098814233018476, 0.5146245059524129, 0.5162055339737843, 0.515415019786405, 0.5098814229484603, 0.5106719371358397, 0.5067193676124919, 0.5106719367824524, 0.5154150201397922, 0.5090909091615865, 0.5083003952804761, 0.5114624509698318, 0.513043478637816], \"type\": \"scatter\", \"uid\": \"4bafcb1f-6d4d-4127-8af4-d03da6bbf90e\"}], {\"title\": {\"text\": \"Test set accuracy of padded datasets with variable maximum lengths\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"bf2e8719-617e-4235-b831-8a149eda2370\")) {window._Plotly.Plots.resize(document.getElementById(\"bf2e8719-617e-4235-b831-8a149eda2370\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create traces\n",
    "trace = go.Scatter(\n",
    "    x = list(accuracies.keys()),\n",
    "    y = list(accuracies.values()),\n",
    "    mode = 'lines+markers',\n",
    "    name = 'BERT'\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'Test set accuracy of padded datasets with variable maximum lengths',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Convolutional neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_score(X_train, X_test, X_validation, y_train = general['train']['label'], y_test = general['test']['label'], y_validation = general['validation']['label']):\n",
    "    # Rearrange data types    \n",
    "    params = locals().copy()    \n",
    "    inputs = {\n",
    "        dataset: np.array(params[dataset])\n",
    "        for dataset in params.keys()\n",
    "    }\n",
    "    \n",
    "    # Reshape datasets\n",
    "    for dataset in inputs.keys():\n",
    "        if dataset[0:1] == 'X':\n",
    "            inputs[dataset] = np.reshape(inputs[dataset], (inputs[dataset].shape[0], inputs[dataset].shape[1], 1))\n",
    "        elif dataset[0:1] == 'y':\n",
    "            inputs[dataset] = np_utils.to_categorical(np.array(inputs[dataset]), 3)\n",
    "    \n",
    "    # Set model parameters\n",
    "    epochs = 5\n",
    "    batch_size = 32\n",
    "    input_shape =  inputs['X_train'].shape\n",
    "    print(inputs['X_train'].shape)\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape = input_shape))\n",
    "    #model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.25))\n",
    "    #model.add(Flatten())\n",
    "    #model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "    model.compile('sgd', 'categorical_crossentropy', metrics = ['accuracy']) \n",
    "    \n",
    "    # Fit the training set over the model and correct on the validation set\n",
    "    model.fit(inputs['X_train'], inputs['y_train'],\n",
    "            batch_size = batch_size,\n",
    "            epochs = epochs,\n",
    "            validation_data = (inputs['X_validation'], inputs['y_validation']))\n",
    "    \n",
    "    # Get score over the test set\n",
    "    score, acc = model.evaluate(inputs['X_test'], inputs['y_test'])\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cnn_score(max_pooled_bert['train'], max_pooled_bert['test'], max_pooled_bert['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### References\n",
    "\n",
    "```\n",
    "@article{DBLP:journals/corr/Wang17j,\n",
    "  author    = {William Yang Wang},\n",
    "  title     = {\"Liar, Liar Pants on Fire\": {A} New Benchmark Dataset for Fake News\n",
    "               Detection},\n",
    "  journal   = {CoRR},\n",
    "  volume    = {abs/1705.00648},\n",
    "  year      = {2017},\n",
    "  url       = {http://arxiv.org/abs/1705.00648},\n",
    "  archivePrefix = {arXiv},\n",
    "  eprint    = {1705.00648},\n",
    "  timestamp = {Mon, 13 Aug 2018 16:48:58 +0200},\n",
    "  biburl    = {https://dblp.org/rec/bib/journals/corr/Wang17j},\n",
    "  bibsource = {dblp computer science bibliography, https://dblp.org}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
