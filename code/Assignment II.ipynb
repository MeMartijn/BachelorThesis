{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Assignment II\n",
    "### By Wesley Teunissen (11040246) and Martijn Schouten (11295562)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "As we were combining our files, we weren't able to get all of our inputs in the same file due to time constraints. We do, however, have conclusions written that take into account the outcome of the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy import stats\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os\n",
    "import operator\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IIa\n",
    "## 1. Implementation Details\n",
    "In this assignment, we will be looking at data preprocessing as well as classification tasks. Since they have a fixed sequence of execution, you will be required to use the sklearn Pipeline functionality to encapsulate your preprocessing transforms as well as classification models into a single estimator. In the following assignments, you should perform fitting and prediction with a Pipeline estimator.  \n",
    "\n",
    "Any grid search should also be performed on the Pipeline, not on independent transforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('election_data/votes.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do we want to do with 0.5 scores?\n",
    "data['targetVariable'] = np.where(data['Clinton'] > 0.5, 'Clinton', 'Trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_votes_2016</th>\n",
       "      <th>votes_dem_2016</th>\n",
       "      <th>Clinton</th>\n",
       "      <th>Clinton_Prediction</th>\n",
       "      <th>votes_gop_2016</th>\n",
       "      <th>Trump</th>\n",
       "      <th>Trump_Prediction</th>\n",
       "      <th>targetVariable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24661</td>\n",
       "      <td>5908</td>\n",
       "      <td>0.239569</td>\n",
       "      <td>0.340493</td>\n",
       "      <td>18110</td>\n",
       "      <td>0.734358</td>\n",
       "      <td>0.620859</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94090</td>\n",
       "      <td>18409</td>\n",
       "      <td>0.195653</td>\n",
       "      <td>0.359502</td>\n",
       "      <td>72780</td>\n",
       "      <td>0.773515</td>\n",
       "      <td>0.586749</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10390</td>\n",
       "      <td>4848</td>\n",
       "      <td>0.466603</td>\n",
       "      <td>0.474693</td>\n",
       "      <td>5431</td>\n",
       "      <td>0.522714</td>\n",
       "      <td>0.517832</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8748</td>\n",
       "      <td>1874</td>\n",
       "      <td>0.214220</td>\n",
       "      <td>0.286031</td>\n",
       "      <td>6733</td>\n",
       "      <td>0.769662</td>\n",
       "      <td>0.692227</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25384</td>\n",
       "      <td>2150</td>\n",
       "      <td>0.084699</td>\n",
       "      <td>0.177490</td>\n",
       "      <td>22808</td>\n",
       "      <td>0.898519</td>\n",
       "      <td>0.789649</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4701</td>\n",
       "      <td>3530</td>\n",
       "      <td>0.750904</td>\n",
       "      <td>0.572216</td>\n",
       "      <td>1139</td>\n",
       "      <td>0.242289</td>\n",
       "      <td>0.445269</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8685</td>\n",
       "      <td>3716</td>\n",
       "      <td>0.427864</td>\n",
       "      <td>0.463313</td>\n",
       "      <td>4891</td>\n",
       "      <td>0.563155</td>\n",
       "      <td>0.522492</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>47376</td>\n",
       "      <td>13197</td>\n",
       "      <td>0.278559</td>\n",
       "      <td>0.337479</td>\n",
       "      <td>32803</td>\n",
       "      <td>0.692397</td>\n",
       "      <td>0.633974</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13778</td>\n",
       "      <td>5763</td>\n",
       "      <td>0.418276</td>\n",
       "      <td>0.403332</td>\n",
       "      <td>7803</td>\n",
       "      <td>0.566338</td>\n",
       "      <td>0.585176</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10503</td>\n",
       "      <td>1524</td>\n",
       "      <td>0.145101</td>\n",
       "      <td>0.204391</td>\n",
       "      <td>8809</td>\n",
       "      <td>0.838713</td>\n",
       "      <td>0.761090</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18255</td>\n",
       "      <td>2909</td>\n",
       "      <td>0.159354</td>\n",
       "      <td>0.243327</td>\n",
       "      <td>15068</td>\n",
       "      <td>0.825418</td>\n",
       "      <td>0.728334</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7268</td>\n",
       "      <td>3109</td>\n",
       "      <td>0.427766</td>\n",
       "      <td>0.402700</td>\n",
       "      <td>4102</td>\n",
       "      <td>0.564392</td>\n",
       "      <td>0.589446</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12936</td>\n",
       "      <td>5712</td>\n",
       "      <td>0.441558</td>\n",
       "      <td>0.453499</td>\n",
       "      <td>7109</td>\n",
       "      <td>0.549552</td>\n",
       "      <td>0.528903</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6572</td>\n",
       "      <td>1234</td>\n",
       "      <td>0.187766</td>\n",
       "      <td>0.228325</td>\n",
       "      <td>5230</td>\n",
       "      <td>0.795800</td>\n",
       "      <td>0.747267</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6532</td>\n",
       "      <td>684</td>\n",
       "      <td>0.104715</td>\n",
       "      <td>0.153821</td>\n",
       "      <td>5738</td>\n",
       "      <td>0.878445</td>\n",
       "      <td>0.817945</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20513</td>\n",
       "      <td>4194</td>\n",
       "      <td>0.204456</td>\n",
       "      <td>0.380051</td>\n",
       "      <td>15825</td>\n",
       "      <td>0.771462</td>\n",
       "      <td>0.580472</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24626</td>\n",
       "      <td>7296</td>\n",
       "      <td>0.296272</td>\n",
       "      <td>0.317199</td>\n",
       "      <td>16718</td>\n",
       "      <td>0.678876</td>\n",
       "      <td>0.644587</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6543</td>\n",
       "      <td>3069</td>\n",
       "      <td>0.469051</td>\n",
       "      <td>0.458974</td>\n",
       "      <td>3413</td>\n",
       "      <td>0.521626</td>\n",
       "      <td>0.524916</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5223</td>\n",
       "      <td>1780</td>\n",
       "      <td>0.340800</td>\n",
       "      <td>0.331308</td>\n",
       "      <td>3376</td>\n",
       "      <td>0.646372</td>\n",
       "      <td>0.655618</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15818</td>\n",
       "      <td>2379</td>\n",
       "      <td>0.150398</td>\n",
       "      <td>0.260562</td>\n",
       "      <td>13222</td>\n",
       "      <td>0.835883</td>\n",
       "      <td>0.709346</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6252</td>\n",
       "      <td>1663</td>\n",
       "      <td>0.265995</td>\n",
       "      <td>0.309933</td>\n",
       "      <td>4511</td>\n",
       "      <td>0.721529</td>\n",
       "      <td>0.669541</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>37278</td>\n",
       "      <td>3730</td>\n",
       "      <td>0.100059</td>\n",
       "      <td>0.204065</td>\n",
       "      <td>32734</td>\n",
       "      <td>0.878105</td>\n",
       "      <td>0.756639</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>18617</td>\n",
       "      <td>4408</td>\n",
       "      <td>0.236773</td>\n",
       "      <td>0.349515</td>\n",
       "      <td>13798</td>\n",
       "      <td>0.741151</td>\n",
       "      <td>0.611419</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18730</td>\n",
       "      <td>12826</td>\n",
       "      <td>0.684784</td>\n",
       "      <td>0.643058</td>\n",
       "      <td>5784</td>\n",
       "      <td>0.308809</td>\n",
       "      <td>0.350036</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>26086</td>\n",
       "      <td>3682</td>\n",
       "      <td>0.141149</td>\n",
       "      <td>0.191398</td>\n",
       "      <td>21779</td>\n",
       "      <td>0.834892</td>\n",
       "      <td>0.783014</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>36905</td>\n",
       "      <td>8436</td>\n",
       "      <td>0.228587</td>\n",
       "      <td>0.362290</td>\n",
       "      <td>27619</td>\n",
       "      <td>0.748381</td>\n",
       "      <td>0.597900</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15213</td>\n",
       "      <td>4698</td>\n",
       "      <td>0.308815</td>\n",
       "      <td>0.380954</td>\n",
       "      <td>10282</td>\n",
       "      <td>0.675869</td>\n",
       "      <td>0.599978</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>43474</td>\n",
       "      <td>10350</td>\n",
       "      <td>0.238073</td>\n",
       "      <td>0.295626</td>\n",
       "      <td>32132</td>\n",
       "      <td>0.739108</td>\n",
       "      <td>0.668679</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8196</td>\n",
       "      <td>1358</td>\n",
       "      <td>0.165691</td>\n",
       "      <td>0.231844</td>\n",
       "      <td>6705</td>\n",
       "      <td>0.818082</td>\n",
       "      <td>0.744792</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>11955</td>\n",
       "      <td>2197</td>\n",
       "      <td>0.183772</td>\n",
       "      <td>0.220175</td>\n",
       "      <td>9466</td>\n",
       "      <td>0.791803</td>\n",
       "      <td>0.750525</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>9180</td>\n",
       "      <td>1763</td>\n",
       "      <td>0.192048</td>\n",
       "      <td>0.201902</td>\n",
       "      <td>6971</td>\n",
       "      <td>0.759368</td>\n",
       "      <td>0.754293</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083</th>\n",
       "      <td>15080</td>\n",
       "      <td>3334</td>\n",
       "      <td>0.221088</td>\n",
       "      <td>0.159302</td>\n",
       "      <td>11077</td>\n",
       "      <td>0.734549</td>\n",
       "      <td>0.806812</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3084</th>\n",
       "      <td>2955</td>\n",
       "      <td>554</td>\n",
       "      <td>0.187479</td>\n",
       "      <td>0.134812</td>\n",
       "      <td>2284</td>\n",
       "      <td>0.772927</td>\n",
       "      <td>0.842515</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3085</th>\n",
       "      <td>6191</td>\n",
       "      <td>1348</td>\n",
       "      <td>0.217735</td>\n",
       "      <td>0.160443</td>\n",
       "      <td>4461</td>\n",
       "      <td>0.720562</td>\n",
       "      <td>0.798970</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3086</th>\n",
       "      <td>2410</td>\n",
       "      <td>383</td>\n",
       "      <td>0.158921</td>\n",
       "      <td>0.176873</td>\n",
       "      <td>1903</td>\n",
       "      <td>0.789627</td>\n",
       "      <td>0.774558</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3087</th>\n",
       "      <td>35256</td>\n",
       "      <td>8327</td>\n",
       "      <td>0.236187</td>\n",
       "      <td>0.239036</td>\n",
       "      <td>25168</td>\n",
       "      <td>0.713864</td>\n",
       "      <td>0.706330</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3088</th>\n",
       "      <td>7809</td>\n",
       "      <td>1061</td>\n",
       "      <td>0.135869</td>\n",
       "      <td>0.131479</td>\n",
       "      <td>6527</td>\n",
       "      <td>0.835830</td>\n",
       "      <td>0.842727</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3089</th>\n",
       "      <td>16420</td>\n",
       "      <td>6888</td>\n",
       "      <td>0.419488</td>\n",
       "      <td>0.551068</td>\n",
       "      <td>7601</td>\n",
       "      <td>0.462911</td>\n",
       "      <td>0.363671</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3090</th>\n",
       "      <td>5079</td>\n",
       "      <td>594</td>\n",
       "      <td>0.116952</td>\n",
       "      <td>0.258919</td>\n",
       "      <td>4067</td>\n",
       "      <td>0.800748</td>\n",
       "      <td>0.684907</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3091</th>\n",
       "      <td>17935</td>\n",
       "      <td>1324</td>\n",
       "      <td>0.073822</td>\n",
       "      <td>0.240335</td>\n",
       "      <td>15778</td>\n",
       "      <td>0.879732</td>\n",
       "      <td>0.701124</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092</th>\n",
       "      <td>6200</td>\n",
       "      <td>1279</td>\n",
       "      <td>0.206290</td>\n",
       "      <td>0.278837</td>\n",
       "      <td>4409</td>\n",
       "      <td>0.711129</td>\n",
       "      <td>0.659909</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3093</th>\n",
       "      <td>6552</td>\n",
       "      <td>668</td>\n",
       "      <td>0.101954</td>\n",
       "      <td>0.242477</td>\n",
       "      <td>5520</td>\n",
       "      <td>0.842491</td>\n",
       "      <td>0.697763</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3094</th>\n",
       "      <td>3771</td>\n",
       "      <td>271</td>\n",
       "      <td>0.071864</td>\n",
       "      <td>0.236930</td>\n",
       "      <td>3347</td>\n",
       "      <td>0.887563</td>\n",
       "      <td>0.704171</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>16543</td>\n",
       "      <td>4200</td>\n",
       "      <td>0.253884</td>\n",
       "      <td>0.351380</td>\n",
       "      <td>11167</td>\n",
       "      <td>0.675029</td>\n",
       "      <td>0.579960</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>5708</td>\n",
       "      <td>924</td>\n",
       "      <td>0.161878</td>\n",
       "      <td>0.290410</td>\n",
       "      <td>4418</td>\n",
       "      <td>0.774001</td>\n",
       "      <td>0.648272</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>2535</td>\n",
       "      <td>400</td>\n",
       "      <td>0.157791</td>\n",
       "      <td>0.218125</td>\n",
       "      <td>1939</td>\n",
       "      <td>0.764892</td>\n",
       "      <td>0.733399</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>4349</td>\n",
       "      <td>638</td>\n",
       "      <td>0.146700</td>\n",
       "      <td>0.295732</td>\n",
       "      <td>3477</td>\n",
       "      <td>0.799494</td>\n",
       "      <td>0.638343</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>39945</td>\n",
       "      <td>11572</td>\n",
       "      <td>0.289698</td>\n",
       "      <td>0.339556</td>\n",
       "      <td>24844</td>\n",
       "      <td>0.621955</td>\n",
       "      <td>0.593568</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>8398</td>\n",
       "      <td>1105</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>0.241677</td>\n",
       "      <td>6779</td>\n",
       "      <td>0.807216</td>\n",
       "      <td>0.698584</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3101</th>\n",
       "      <td>32493</td>\n",
       "      <td>6573</td>\n",
       "      <td>0.202290</td>\n",
       "      <td>0.280856</td>\n",
       "      <td>23523</td>\n",
       "      <td>0.723941</td>\n",
       "      <td>0.658126</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3102</th>\n",
       "      <td>1297</td>\n",
       "      <td>115</td>\n",
       "      <td>0.088666</td>\n",
       "      <td>0.254448</td>\n",
       "      <td>1116</td>\n",
       "      <td>0.860447</td>\n",
       "      <td>0.685545</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3103</th>\n",
       "      <td>14634</td>\n",
       "      <td>2535</td>\n",
       "      <td>0.173227</td>\n",
       "      <td>0.304426</td>\n",
       "      <td>11115</td>\n",
       "      <td>0.759533</td>\n",
       "      <td>0.632070</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3104</th>\n",
       "      <td>4460</td>\n",
       "      <td>719</td>\n",
       "      <td>0.161211</td>\n",
       "      <td>0.302879</td>\n",
       "      <td>3437</td>\n",
       "      <td>0.770628</td>\n",
       "      <td>0.630632</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3105</th>\n",
       "      <td>14187</td>\n",
       "      <td>2926</td>\n",
       "      <td>0.206245</td>\n",
       "      <td>0.303194</td>\n",
       "      <td>10266</td>\n",
       "      <td>0.723620</td>\n",
       "      <td>0.631960</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3106</th>\n",
       "      <td>4297</td>\n",
       "      <td>644</td>\n",
       "      <td>0.149872</td>\n",
       "      <td>0.281005</td>\n",
       "      <td>3409</td>\n",
       "      <td>0.793344</td>\n",
       "      <td>0.652628</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3107</th>\n",
       "      <td>16661</td>\n",
       "      <td>3233</td>\n",
       "      <td>0.194046</td>\n",
       "      <td>0.272789</td>\n",
       "      <td>12153</td>\n",
       "      <td>0.729428</td>\n",
       "      <td>0.665093</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3108</th>\n",
       "      <td>12176</td>\n",
       "      <td>7313</td>\n",
       "      <td>0.600608</td>\n",
       "      <td>0.494972</td>\n",
       "      <td>3920</td>\n",
       "      <td>0.321945</td>\n",
       "      <td>0.425316</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3109</th>\n",
       "      <td>8053</td>\n",
       "      <td>1202</td>\n",
       "      <td>0.149261</td>\n",
       "      <td>0.256831</td>\n",
       "      <td>6154</td>\n",
       "      <td>0.764187</td>\n",
       "      <td>0.685461</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3110</th>\n",
       "      <td>3715</td>\n",
       "      <td>532</td>\n",
       "      <td>0.143203</td>\n",
       "      <td>0.305111</td>\n",
       "      <td>2911</td>\n",
       "      <td>0.783580</td>\n",
       "      <td>0.632028</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3111</th>\n",
       "      <td>3334</td>\n",
       "      <td>294</td>\n",
       "      <td>0.088182</td>\n",
       "      <td>0.226544</td>\n",
       "      <td>2898</td>\n",
       "      <td>0.869226</td>\n",
       "      <td>0.717167</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3112 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      total_votes_2016  votes_dem_2016   Clinton  Clinton_Prediction  \\\n",
       "0                24661            5908  0.239569            0.340493   \n",
       "1                94090           18409  0.195653            0.359502   \n",
       "2                10390            4848  0.466603            0.474693   \n",
       "3                 8748            1874  0.214220            0.286031   \n",
       "4                25384            2150  0.084699            0.177490   \n",
       "5                 4701            3530  0.750904            0.572216   \n",
       "6                 8685            3716  0.427864            0.463313   \n",
       "7                47376           13197  0.278559            0.337479   \n",
       "8                13778            5763  0.418276            0.403332   \n",
       "9                10503            1524  0.145101            0.204391   \n",
       "10               18255            2909  0.159354            0.243327   \n",
       "11                7268            3109  0.427766            0.402700   \n",
       "12               12936            5712  0.441558            0.453499   \n",
       "13                6572            1234  0.187766            0.228325   \n",
       "14                6532             684  0.104715            0.153821   \n",
       "15               20513            4194  0.204456            0.380051   \n",
       "16               24626            7296  0.296272            0.317199   \n",
       "17                6543            3069  0.469051            0.458974   \n",
       "18                5223            1780  0.340800            0.331308   \n",
       "19               15818            2379  0.150398            0.260562   \n",
       "20                6252            1663  0.265995            0.309933   \n",
       "21               37278            3730  0.100059            0.204065   \n",
       "22               18617            4408  0.236773            0.349515   \n",
       "23               18730           12826  0.684784            0.643058   \n",
       "24               26086            3682  0.141149            0.191398   \n",
       "25               36905            8436  0.228587            0.362290   \n",
       "26               15213            4698  0.308815            0.380954   \n",
       "27               43474           10350  0.238073            0.295626   \n",
       "28                8196            1358  0.165691            0.231844   \n",
       "29               11955            2197  0.183772            0.220175   \n",
       "...                ...             ...       ...                 ...   \n",
       "3082              9180            1763  0.192048            0.201902   \n",
       "3083             15080            3334  0.221088            0.159302   \n",
       "3084              2955             554  0.187479            0.134812   \n",
       "3085              6191            1348  0.217735            0.160443   \n",
       "3086              2410             383  0.158921            0.176873   \n",
       "3087             35256            8327  0.236187            0.239036   \n",
       "3088              7809            1061  0.135869            0.131479   \n",
       "3089             16420            6888  0.419488            0.551068   \n",
       "3090              5079             594  0.116952            0.258919   \n",
       "3091             17935            1324  0.073822            0.240335   \n",
       "3092              6200            1279  0.206290            0.278837   \n",
       "3093              6552             668  0.101954            0.242477   \n",
       "3094              3771             271  0.071864            0.236930   \n",
       "3095             16543            4200  0.253884            0.351380   \n",
       "3096              5708             924  0.161878            0.290410   \n",
       "3097              2535             400  0.157791            0.218125   \n",
       "3098              4349             638  0.146700            0.295732   \n",
       "3099             39945           11572  0.289698            0.339556   \n",
       "3100              8398            1105  0.131579            0.241677   \n",
       "3101             32493            6573  0.202290            0.280856   \n",
       "3102              1297             115  0.088666            0.254448   \n",
       "3103             14634            2535  0.173227            0.304426   \n",
       "3104              4460             719  0.161211            0.302879   \n",
       "3105             14187            2926  0.206245            0.303194   \n",
       "3106              4297             644  0.149872            0.281005   \n",
       "3107             16661            3233  0.194046            0.272789   \n",
       "3108             12176            7313  0.600608            0.494972   \n",
       "3109              8053            1202  0.149261            0.256831   \n",
       "3110              3715             532  0.143203            0.305111   \n",
       "3111              3334             294  0.088182            0.226544   \n",
       "\n",
       "      votes_gop_2016     Trump  Trump_Prediction targetVariable  \n",
       "0              18110  0.734358          0.620859          Trump  \n",
       "1              72780  0.773515          0.586749          Trump  \n",
       "2               5431  0.522714          0.517832          Trump  \n",
       "3               6733  0.769662          0.692227          Trump  \n",
       "4              22808  0.898519          0.789649          Trump  \n",
       "5               1139  0.242289          0.445269        Clinton  \n",
       "6               4891  0.563155          0.522492          Trump  \n",
       "7              32803  0.692397          0.633974          Trump  \n",
       "8               7803  0.566338          0.585176          Trump  \n",
       "9               8809  0.838713          0.761090          Trump  \n",
       "10             15068  0.825418          0.728334          Trump  \n",
       "11              4102  0.564392          0.589446          Trump  \n",
       "12              7109  0.549552          0.528903          Trump  \n",
       "13              5230  0.795800          0.747267          Trump  \n",
       "14              5738  0.878445          0.817945          Trump  \n",
       "15             15825  0.771462          0.580472          Trump  \n",
       "16             16718  0.678876          0.644587          Trump  \n",
       "17              3413  0.521626          0.524916          Trump  \n",
       "18              3376  0.646372          0.655618          Trump  \n",
       "19             13222  0.835883          0.709346          Trump  \n",
       "20              4511  0.721529          0.669541          Trump  \n",
       "21             32734  0.878105          0.756639          Trump  \n",
       "22             13798  0.741151          0.611419          Trump  \n",
       "23              5784  0.308809          0.350036        Clinton  \n",
       "24             21779  0.834892          0.783014          Trump  \n",
       "25             27619  0.748381          0.597900          Trump  \n",
       "26             10282  0.675869          0.599978          Trump  \n",
       "27             32132  0.739108          0.668679          Trump  \n",
       "28              6705  0.818082          0.744792          Trump  \n",
       "29              9466  0.791803          0.750525          Trump  \n",
       "...              ...       ...               ...            ...  \n",
       "3082            6971  0.759368          0.754293          Trump  \n",
       "3083           11077  0.734549          0.806812          Trump  \n",
       "3084            2284  0.772927          0.842515          Trump  \n",
       "3085            4461  0.720562          0.798970          Trump  \n",
       "3086            1903  0.789627          0.774558          Trump  \n",
       "3087           25168  0.713864          0.706330          Trump  \n",
       "3088            6527  0.835830          0.842727          Trump  \n",
       "3089            7601  0.462911          0.363671          Trump  \n",
       "3090            4067  0.800748          0.684907          Trump  \n",
       "3091           15778  0.879732          0.701124          Trump  \n",
       "3092            4409  0.711129          0.659909          Trump  \n",
       "3093            5520  0.842491          0.697763          Trump  \n",
       "3094            3347  0.887563          0.704171          Trump  \n",
       "3095           11167  0.675029          0.579960          Trump  \n",
       "3096            4418  0.774001          0.648272          Trump  \n",
       "3097            1939  0.764892          0.733399          Trump  \n",
       "3098            3477  0.799494          0.638343          Trump  \n",
       "3099           24844  0.621955          0.593568          Trump  \n",
       "3100            6779  0.807216          0.698584          Trump  \n",
       "3101           23523  0.723941          0.658126          Trump  \n",
       "3102            1116  0.860447          0.685545          Trump  \n",
       "3103           11115  0.759533          0.632070          Trump  \n",
       "3104            3437  0.770628          0.630632          Trump  \n",
       "3105           10266  0.723620          0.631960          Trump  \n",
       "3106            3409  0.793344          0.652628          Trump  \n",
       "3107           12153  0.729428          0.665093          Trump  \n",
       "3108            3920  0.321945          0.425316        Clinton  \n",
       "3109            6154  0.764187          0.685461          Trump  \n",
       "3110            2911  0.783580          0.632028          Trump  \n",
       "3111            2898  0.869226          0.717167          Trump  \n",
       "\n",
       "[3112 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if target variable is working as intended\n",
    "data[['total_votes_2016', 'votes_dem_2016', 'Clinton', 'Clinton_Prediction', 'votes_gop_2016', 'Trump', 'Trump_Prediction', 'targetVariable']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataInput = data[['population2014', 'population2010', 'population_change', 'POP010210', 'AGE135214', 'AGE295214', \n",
    "                  'age65plus', 'SEX255214', 'White', 'Black', 'RHI325214', 'RHI425214', 'RHI525214', 'RHI625214', \n",
    "                  'Hispanic', 'RHI825214', 'POP715213', 'POP645213', 'NonEnglish', 'Edu_highschool', 'Edu_batchelors',\n",
    "                  'VET605213', 'LFE305213', 'HSG010214', 'HSG445213', 'HSG096213', 'HSG495213', 'HSD410213', \n",
    "                  'HSD310213', 'Income', 'INC110213', 'Poverty', 'BZA010213', 'BZA110213', 'BZA115213', 'NES010213',\n",
    "                  'SBO001207', 'SBO315207', 'SBO115207', 'SBO215207', 'SBO515207', 'SBO415207', 'SBO015207', \n",
    "                  'MAN450207', 'WTN220207', 'RTN130207', 'RTN131207', 'AFN120207', 'BPS030214', 'LND110210']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before checking for outliers: 3112\n",
      "Number of rows after checking for outliers: 2339\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "dataInput2 = dataInput[(np.abs(stats.zscore(dataInput)) < 3).all(axis=1)]\n",
    "\n",
    "print('Number of rows before checking for outliers:', len(dataInput))\n",
    "print('Number of rows after checking for outliers:', len(dataInput2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2334, 50) (2334,)\n",
      "(778, 50) (778,)\n"
     ]
    }
   ],
   "source": [
    "# split using default split ratio and print of how big each split is.\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataInput, data.targetVariable)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Classification\n",
    "#### 1. Create a LinearSVC classifier with default parameters. \n",
    "Fit your classifier on scaled as well as unscaled versions of the data and report the estimator scores. Additionally, analyze the effects of data preprocessing on the classification performance.  \n",
    "In no more than 50 words, explain your observations and your assessment of the underlying situa- tion. You can use a text box (i.e. Markdown Cell) in Jupyter to write down your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Picking a scaler\n",
    "To pick a scaler, we first need to find out if our data contains outliers. This will need to be taken into account when scaling the data. For example, if we were to use a MinMaxScaler, we'd need to have no outliers, as these will have a huge impact when reducing the values to values between 0 and 1. Furthermore, we must be sure our data follows the Gaussian distribution for us to be able to work with the StandardScaler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before checking for outliers: 3112\n",
      "Number of rows after checking for outliers: 2339\n"
     ]
    }
   ],
   "source": [
    "# Checking for outliers\n",
    "dataInput2 = dataInput[(np.abs(stats.zscore(dataInput)) < 3).all(axis=1)]\n",
    "\n",
    "print('Number of rows before checking for outliers:', len(dataInput))\n",
    "print('Number of rows after checking for outliers:', len(dataInput2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are quite a lot of outliers in the data. For dealing with outliers, RobustScaler is the best fit, as it ignores outliers when fitting the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Picking a cross validation strategy\n",
    "For cross validation, there are a couple of strategies we can use. First off, there are k-fold cross validation and stratified k-fold cross validation. The difference between both is that the stratified one makes sure each fold will contain the same proportions as the whole dataset. We can tackle picking one of those by looking at the proportions of the target variable of the dataset. If the proportions are almost equal, accuracy of a 'normal' k-fold cross validation would probably be just as high as the stratified one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>targetVariable</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Clinton</th>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trump</th>\n",
       "      <td>2703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   X\n",
       "targetVariable      \n",
       "Clinton          409\n",
       "Trump           2703"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('targetVariable')[['X']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the target variable does not have an equal proportion. This means it would make more sense to pick a stratified k-fold cross validation, as this reduces the chance of ruining the accuracy of our model if one fold, for example, only contains datapoints with the target variable 'Trump'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV Strategy is: 10, having a train score without scaler of: 0.8415\n"
     ]
    }
   ],
   "source": [
    "# Pipeline LinearSVC without Scaler!\n",
    "pipeline = Pipeline([('clf', LinearSVC(random_state=2))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "arrayCV = list(range(2, 11))\n",
    "\n",
    "cvDict = {}\n",
    "#CrossValidation\n",
    "for cv in arrayCV:\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=cv).mean()\n",
    "    cvDict[cv] = scores\n",
    "    \n",
    "bestCV = max(cvDict.items(), key=operator.itemgetter(1))[0]\n",
    "print('Best CV Strategy is: ' + str(bestCV) + ', having a train score without scaler of: ' + str(round(cvDict[bestCV], 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8688946015424165\n"
     ]
    }
   ],
   "source": [
    "# applying the best linearSVC model without scaler on the test set:\n",
    "print(pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV Strategy is: 3, having a train score with scaler of: 0.9323\n"
     ]
    }
   ],
   "source": [
    "# Pipeline LinearSVC with Scaler!\n",
    "pipeline = Pipeline([('scale', RobustScaler()),\n",
    "                     ('clf', LinearSVC(random_state=2))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "arrayCV = list(range(2, 11))\n",
    "\n",
    "cvDict = {}\n",
    "#CrossValidation\n",
    "for cv in arrayCV:\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=cv).mean()\n",
    "    cvDict[cv] = scores\n",
    "\n",
    "bestCV = max(cvDict.items(), key=operator.itemgetter(1))[0]\n",
    "print('Best CV Strategy is: ' + str(bestCV) + ', having a train score with scaler of: ' + str(round(cvDict[bestCV], 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9215938303341902\n"
     ]
    }
   ],
   "source": [
    "# applying the best linearSVC model with scaler on the test set:\n",
    "print(pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the two pipelines above, applying a scaler to the dataset will have a very positive effect on the performance of the Linear SVC. It went from 0.8467 to 0.9327 which is a huge difference. If we then apply the best model, which is the model using a CV strategy of 2 and the robust scaler, we get a score of 0.91645, which is very good performance. So in this case it is useful to first scale the data, which is probably helpful since there are quite alot of outliers as we showed in the code above. The lower CV strategy of 2 in combination with the scaler will result in the best possible performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create a LogisticRegression classifier. Additionally use GridSearchCV to find an optimal value for the parameter C.  \n",
    "As before, you will be required to run your estimator on scaled as well as unscaled versions of the data and report your observations for the estimator score. Additionally also analyze the effects of regularization strength on the performance of the estimator 1.  \n",
    "In not more than 50 words, explain your observations and your assessment of the effects of C and data scaling on the classification performance.\n",
    "\n",
    "##### The LogisticRegression hyperparameter C represents the inverse of regularization strength. So a smaller value of C is, in fact, stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: l2\n",
      "Best C: 0.01\n",
      "Test score accuracy: 0.9074550128534704\n"
     ]
    }
   ],
   "source": [
    "# code\n",
    "# Pipeline LogisticRegression without Scaler!\n",
    "clf = make_pipeline(GridSearchCV(LogisticRegression(random_state=2),\n",
    "                     param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "                     cv=10,\n",
    "                     refit=True))\n",
    "\n",
    "# # Fit grid search\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "\n",
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model.get_params()['gridsearchcv'].best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.get_params()['gridsearchcv'].best_estimator_.get_params()['C'])\n",
    "print('Test score accuracy:', best_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: l2\n",
      "Best C: 0.1\n",
      "Test score accuracy: 0.9421593830334191\n"
     ]
    }
   ],
   "source": [
    "# code\n",
    "# Pipeline LogisticRegression with Scaler!\n",
    "clf = make_pipeline(RobustScaler(), \n",
    "                    GridSearchCV(LogisticRegression(random_state=2),\n",
    "                     param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "                     cv=10,\n",
    "                     refit=True))\n",
    "\n",
    "# # Fit grid search\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "\n",
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model.get_params()['steps'][1][1].best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.get_params()['steps'][1][1].best_estimator_.get_params()['C'])\n",
    "print('Test score accuracy:', best_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the array of C options going from 0.001 to 100, the best possible performance actually came from the C = 100, both with and without the scaler applied. Applying the scaler does increase the performance though. This is probably again due to a large amount of outliers. It is interesting though that the best performance came from the highest C, since the lower the C, the stronger the regularization. Both with and without the best penalty performance came from the L2 (ridge). Trying out different CV's did not have any positive effect on the performance of the model. We feel that the high C value of 100 could cause this model to overfit a bit, which would make the high performance score a bit unreal, but the dataset itself was also very one sided (towards Trump), which could make it easier to differenciate the two target classes and cause a high performance score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IIb\n",
    "### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(score):\n",
    "    if score == 0:\n",
    "        return 'Not-Popular'\n",
    "    elif score == 1:\n",
    "        return 'Somewhat-Popular'\n",
    "    elif score > 1:\n",
    "        return 'Very-Popular'\n",
    "\n",
    "trainingDf = pd.read_csv('../Assignment I/ml_data/webStats_train.csv', sep=',', header=None)\n",
    "X_train = trainingDf.drop(labels=280, axis=1)\n",
    "trainingDf[[280]] = [label(popularity) for popularity in trainingDf[280]]\n",
    "y_train = np.asarray(trainingDf[280])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Classification\n",
    "#### 2.1 Data Preprocessing\n",
    "To decide which scaler to use, we've decided to form a model and compare accuracy scores to check which one has the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For StandardScaler:  0.5266345846840665\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('scale', StandardScaler()), ('clf', KNeighborsClassifier())])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print('For StandardScaler: ', cross_val_score(pipeline, X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For MinMaxScaler:  0.5319585106358975\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('scale', MinMaxScaler()), ('clf', KNeighborsClassifier())])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print('For MinMaxScaler: ', cross_val_score(pipeline, X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For MaxAbsScaler:  0.5322829603351718\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('scale', MaxAbsScaler()), ('clf', KNeighborsClassifier())])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print('For MaxAbsScaler: ', cross_val_score(pipeline, X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, the MaxAbsScaler is the best option, as it has the highest accuracy. However, differences are really small, so we've decided to also test accuracy for the LinearSVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For StandardScaler:  0.6232399707964565\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('scale', StandardScaler()), ('clf', LinearSVC())])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print('For StandardScaler: ', cross_val_score(pipeline, X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For MinMaxScaler:  0.650647131173857\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('scale', MinMaxScaler()), ('clf', LinearSVC())])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print('For MinMaxScaler: ', cross_val_score(pipeline, X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For MaxAbsScaler:  0.650322696772062\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('scale', MaxAbsScaler()), ('clf', LinearSVC())])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print('For MaxAbsScaler: ', cross_val_score(pipeline, X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, we can exclude StandardScaler, as that scaler had the lowest accuracy on both classifiers. It's a tie between MinMaxScaler and MaxAbsScaler: we need to conduct a final test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For MinMaxScaler:  0.40323040582906505\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('scale', MinMaxScaler()), ('clf', RandomForestClassifier())])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print('For MinMaxScaler: ', cross_val_score(pipeline, X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For MaxAbsScaler:  0.4112272450202103\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('scale', MaxAbsScaler()), ('clf', RandomForestClassifier())])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print('For MaxAbsScaler: ', cross_val_score(pipeline, X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in the final test there was a noticable difference between MinMaxScaler and MaxAbsScaler: MaxAbsScaler got a higher accuracy. From now on, we'll use MaxAbsScaler, because even though MinMaxScaler had a higher accuracy in one of the tests, the difference was so small, we decided to go with MaxAbsScaler for the sake of uniformity from here on.\n",
    "\n",
    "It is actually to be expected that MinMaxScaler and MaxAbsScaler are really close to each other in terms of performance, considering they both transform the data to a set range of values. Also, they should be performing well, because we removed outliers from our data when we transformed it into three labels. One of those labels, 'Very-Popular' combines all outliers, taking away weaknesses in both scalers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather test sets and process them\n",
    "testSets = list(filter(lambda x: x.split('.')[-1] == 'csv' and 'test' in x, \n",
    "                  np.concatenate([files[2] for files in os.walk('/Users/martijn/Documents/Machine Learning/Assignment I/ml_data')], axis = 0)))\n",
    "\n",
    "def processData(filename):\n",
    "    testingDf = pd.read_csv('../Assignment I/ml_data/' + filename, sep = ',', header=None)\n",
    "    return {\n",
    "        'X': testingDf.drop(labels=280, axis=1),\n",
    "        'y': np.asarray([label(popularity) for popularity in testingDf[280]])\n",
    "    }\n",
    "\n",
    "testData = [processData(test) for test in testSets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scale', MaxAbsScaler(copy=True)), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up the pipelines\n",
    "# KNN\n",
    "KNNpipe = Pipeline([('scale', MaxAbsScaler()), ('clf', KNeighborsClassifier())])\n",
    "KNNpipe.fit(X_train, y_train)\n",
    "\n",
    "# SVM, we'll apply GridSearch in the next cell\n",
    "SVMpipe = Pipeline([('scale', MaxAbsScaler()), ('clf', LinearSVC())])\n",
    "\n",
    "# Forest\n",
    "Fpipe = Pipeline([('scale', MaxAbsScaler()), ('clf', RandomForestClassifier())])\n",
    "Fpipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 48s, sys: 10.3 s, total: 17min 59s\n",
      "Wall time: 16min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SVMgrid = GridSearchCV(SVMpipe, {'clf__C': [0.001, 0.01, 0.1, 1, 10]}, cv = 10)\n",
    "SVMgrid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores for test data blocks:\n",
      "KNN:  0.7283674472881886\n",
      "SVM:  0.750464070804859\n",
      "Random Forest:  0.7869213829312518\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy_score(clf):\n",
    "    accuracyScoreList = [clf.score(testSet['X'], testSet['y']) for testSet in testData]\n",
    "    return sum(accuracyScoreList) / float(len(accuracyScoreList))\n",
    "\n",
    "print('Accuracy scores for test data blocks:')\n",
    "print('KNN: ', get_accuracy_score(KNNpipe))\n",
    "print('SVM: ', get_accuracy_score(SVMgrid))\n",
    "print('Random Forest: ', get_accuracy_score(Fpipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_val(clf):\n",
    "    return cross_val_score(clf, X_train, y_train, cv = 10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores for pipelines:\n",
      "KNN:  0.5236679220176736\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy scores for pipelines:')\n",
    "print('KNN: ', get_cross_val(KNNpipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVM: ', get_cross_val(SVMgrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:  0.30810494193299653\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest: ', get_cross_val(Fpipe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Missing features and Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nan_to_data(train_inputs, miss_prob=0.2):\n",
    "    \"\"\"\n",
    "    Randomly flips a numpy ndarray entry to NaN with a supplied \n",
    "    probability.\n",
    "    \n",
    "    WARNING: Do not try to add missing values to labels. This is \n",
    "    not unsupervised learning.\n",
    "        :param train_inputs: Numpy ndarray of dims (num_examples, feature_dims)\n",
    "        :param miss_prob=0.2: Probability that a bit is flipped to NaN. \n",
    "    \"\"\"     \n",
    "    mask = np.random.choice(2, size=train_inputs.shape, p=[miss_prob,1-miss_prob]).astype(bool)\n",
    "    input_shape = train_inputs.shape\n",
    "\n",
    "    #Flatten Inputs and Mask\n",
    "    train_inputs = train_inputs.ravel()\n",
    "    mask = mask.ravel()\n",
    "    train_inputs[~mask] = np.nan\n",
    "    \n",
    "    # reshape inputs back to the original shape.\n",
    "    missing_train_inputs = np.reshape(train_inputs, newshape=input_shape)\n",
    "    \n",
    "    return missing_train_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, again, we'll start with just a single classifier and a basic k-fold to check whether that will give us a major difference between performance of selecting either the mean, mode or median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For imputation with the mean:  0.5410246982554475\n"
     ]
    }
   ],
   "source": [
    "missing_X_train = add_nan_to_data(X_train.values)\n",
    "\n",
    "pipeline = Pipeline([('impute', Imputer(strategy='mean')),('scale', MaxAbsScaler()), ('clf', RandomForestClassifier())])\n",
    "pipeline.fit(add_nan_to_data(X_train.values), y_train)\n",
    "\n",
    "print('For imputation with the mean: ', cross_val_score(pipeline, missing_X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For imputation with the median:  0.49989731512467833\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('impute', Imputer(strategy='median')),('scale', MaxAbsScaler()), ('clf', RandomForestClassifier())])\n",
    "pipeline.fit(add_nan_to_data(X_train.values), y_train)\n",
    "\n",
    "print('For imputation with the median: ', cross_val_score(pipeline, missing_X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For imputation with the mode:  0.5100887142885607\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('impute', Imputer(strategy='most_frequent')),('scale', MaxAbsScaler()), ('clf', RandomForestClassifier())])\n",
    "pipeline.fit(add_nan_to_data(X_train.values), y_train)\n",
    "\n",
    "print('For imputation with the mode: ', cross_val_score(pipeline, missing_X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our functions show us that imputation using the mean value is the best option in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will combine all insights we've gathered to form three final pipelines for our three classifiers and report the accuracy score of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('impute', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', MaxAbsScaler(copy=True)), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            m...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up the pipelines\n",
    "# KNN\n",
    "KNNpipe = Pipeline([('impute', Imputer(strategy='mean')), ('scale', MaxAbsScaler()), ('clf', KNeighborsClassifier())])\n",
    "KNNpipe.fit(missing_X_train, y_train)\n",
    "\n",
    "# SVM\n",
    "SVMpipe = Pipeline([('impute', Imputer(strategy='mean')), ('scale', MaxAbsScaler()), ('clf', LinearSVC())])\n",
    "SVMpipe.fit(missing_X_train, y_train)\n",
    "\n",
    "# Forest\n",
    "Fpipe = Pipeline([('impute', Imputer(strategy='mean')), ('scale', MaxAbsScaler()), ('clf', RandomForestClassifier())])\n",
    "Fpipe.fit(missing_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('For KNN: ', cross_val_score(KNNpipe, X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('For SVM: ', cross_val_score(SVMpipe, X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('For Random Forest: ', cross_val_score(Fpipe, X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, we expected accuracy of the classifiers to go down, due to imputation generalizing the training data by filling in the blank spots with measures of central tendency, which means the missing values generalize more to a central form. However, differences are really small, suggesting that this was not the case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
