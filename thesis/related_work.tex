\section{Related Work}
\subsection{Automatic fake news detection}
There have been several attempts in the past to create classifiers for automatic detection of lies and fake news. 
Wang used both linear and neural classifiers to classify statements from the Liar dataset into 6 possible gradations of truthfulness. 
Furthermore, he added speaker metadata to improve the result of his classifications \cite{wang2018}.

From the same dataset, Khurana extracted linguistic features such as n-grams, sentiment, number of capital letters and POS tags to classify the data into 3 labels instead of the original 6 labels. 
For classification, she used a set of non-neural classifiers \cite{khurana2017}.

The British factchecking organization Full Fact has developed an architecture that is able to monitor and factcheck statements from the British Parliament and major media outlets in the United Kingdom. 
It can automatically factcheck the accuracy of statistical claims, for example \cite{babakar2016}.
For detecting factual claims from texts, the organization uses InferSent, which is a way of transfer learning that has been proved to perform well for the use case of Full Fact \cite{pydata2018}.

Various tools with regards to fake news detection are also available. 
Faker Fact is a tool which can classify texts into a set of categories ranging from satire to agenda-driven, the former identifying humorous intent, the later identifying manipulation \cite{fakerfact}. 
Hoaxy, on the other hand, allows for the visualization of unverified claims through Twitter networks \cite{shao2016}. 

\subsection{Pooling}
Linear classifiers need data in a two-dimensional shape to be able to perform calculations. 
In the case of raw text data, sentences in the dataset have variable word lengths, resulting in a different vector length when turning the text into a vector representation.
To turn the vector representations into a uniform length, we can either cut off the vectors at a fixed length (\textit{padding}), or we can perform calculations to reduce the length of the vectors (\textit{pooling}).

In computer vision, feature pooling is used to reduce noise in data. 
The goal of this step is to transform joint feature representations into a new, more usable one that preserves important information while discarding irrelevant details. 
Pooling techniques such as max pooling and average pooling perform mathematical operations to reduce several numbers into one \cite{boureau2010}. 
In the case of transforming the shape of the data, we can reduce vectors to the smallest vector in the dataset to create a uniform shape.

Scherer et al. compared performance of two pooling operations on a convolutional neural network architecture. 
The first pooling method extracted maximums and the second one was primarily based on working with averages.
They have shown that a max pooling operation is vastly superior for capturing invariances in image-like data \cite{scherer2010}.

Shen et al. noted that in text classification, only a small number of key words contribute to the final prediction.
As a result, simple pooling operations are surprisingly effective for representing documents \cite{shen2018}.  
Lai et al., Hu et al. and Zhang et al. use a max pooling layer in a (recurrent) convolutional neural network for identifying key features in text classification \cite{lai2015}\cite{hu2014}\cite{zhang2015}. 
In the case of text classification, max pooling strategies seem to be the most popular. 

\subsection{Padding}
When padding a sequence, a list of sequences is transformed to a specific length. 
Sequences longer than the desired length will be truncated to fit the requirement, while sequences shorter than the desired length will be padded by a specified value \cite{keraspad}. 
To fill the sequences, a value of zero is often used. Hu et al. also use zero values for padding their sequences \cite{hu2014}. 

Apart from controlling the size of the feature dimension, padding has other uses as well. 
Simard et al. make use of sequence padding for convolutional neural networks to center feature units, and concluded it did not impact the performance of the classifier significantly \cite{simard2003}. 
Wen et al. apply padding to convolutional network models to prevent dimension loss \cite{wen2018}. 

\subsection{Neural text classifiers versus linear text classifiers}
